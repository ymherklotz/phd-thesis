\chapter{Trusted Computing Base}%
\label{sec:trusted-computing-base}

\begin{quote}\itshape
  This chapter describes the trusted computing base of Vericert, which includes
  the final correctness theorem and the formalised Verilog semantics.
\end{quote}

\noindent The trusted computing base of verified software such as CompCert is
the code that is not verified, and therefore needs to be trusted by the user
when using the tool or the correctness theorem.  This is in contrast to the
untrusted code that has been verified, and therefore cannot introduce bugs.  It
is important to understand the trusted computing base of verified software such
as CompCert and Vericert to understand where bugs could still be introduced.
One example in CompCert is when \textcite{yang11_findin_under_bugs_c_compil}
fuzz tested C compilers, finding a variety of bugs in all the compilers that
were tested.  However, far fewer bugs were found in CompCert compared to
unverified C compilers, and all the bugs that were found in CompCert were in the
unverified, trusted parts of the compiler.  For example, there were bugs in the
unverified front end of CompCert, which was then addressed by reducing the
trusted computing base by verifying the correctness of the C
parser~\cite{jourdan12_valid_lr_parser}.  Another bug was found because of a
missing constraint in the semantics of the PowerPC.

The trusted computing base of CompCert comprises the Coq theorem prover itself,
the OCaml extraction from Coq which is unverified, front end semantics of C
which may not model the C standard faithfully, and finally the semantics of
CompCert C as well as the semantics for each assembly
language~\cite{monniaux22_tcbcvc}.  In general, the trusted computing base of
Vericert is the same as that of CompCert, the main two differences are described
in this section, and they are the correctness theorem and the target Verilog
semantics.

\section{Formulating the Correctness Theorem}

The main correctness theorem is analogous to that stated in \compcert{} and also
described in \cref{sec:bg:correctness-theorem}: for all Clight source programs
$C$, if the translation to the target Verilog code succeeds, $\mathit{Safe}(C)$
holds and the target Verilog has behaviour $B$ when simulated, then $C$ will
have the same behaviour $B$. $\mathit{Safe}(C)$ means all observable behaviours
of $C$ are safe, which can be defined as
$\forall B,\ C \Downarrow B \implies B \in \texttt{Safe}$.  A behaviour is in
\texttt{Safe} if it is either a final state (in the case of convergence) or
divergent, but it cannot \enquote{go wrong}, meaning it cannot contain undefined
behaviour.  In \compcert{}, a behaviour is also associated with a trace of I/O
events, but since external function calls are not supported in \vericert{}, this
trace will always be empty.

\begin{theorem}
  Whenever the translation from $C$ succeeds and produces Verilog $V\!$, and all
  observable behaviours of $C$ are safe, then $V\!$ has behaviour $B$ only if
  $C$ has behaviour $B$.

  {\normalfont\begin{equation*} \forall C\ V\ B\ldotp\quad \yhfunction{HLS} (C)
      = \yhconstant{OK} (V) \land \mathit{Safe}(C) \implies (V \Downarrow B
      \implies C \Downarrow B).
  \end{equation*}}
\end{theorem}

Why is this correctness theorem also the right one for HLS?  It could be argued
that hardware inherently runs forever and therefore does not produce a
definitive final result.  This would mean that the \compcert{} correctness
theorem would probably be unhelpful with proving hardware correctness, as the
behaviour would always be divergent.  However, in practice, HLS does not
normally produce the top-level of the design that connects all the components of
the design together, therefore needing to run forever.  Rather, \gls{HLS} often
produces smaller components that take an input, execute, and then terminate with
an answer.  To start the execution of the hardware and to signal to the HLS
component that the inputs are ready, the $\mathit{rst}$ signal is set and unset.
Then, once the result is ready, the $\mathit{fin}$ signal is set and the result
value is placed in $\mathit{ret}$.  These signals are also present in the
semantics of execution shown in \cref{fig:inference_module}.  The correctness
theorem therefore also uses these signals, and the proof shows that once the
$\mathit{fin}$ flag is set, the value in $\mathit{ret}$ is correct according to
the semantics of Verilog and CompCert C.  Note that the compiler is allowed to
fail and not produce any output; the correctness theorem only applies when the
translation succeeds.

How can we prove this theorem?  Note that like in CompCert, this theorem can be
proven using a backwards simulation.  We use the same proof technique as in
CompCert, extending the forward simulation from \rtl{} to Verilog.  Then, by
proving that the Verilog semantics are \emph{deterministic}, the composition of
forward simulation in CompCert, as well ase the simulations in Vericert imply
the backwards simulation needed to prove the theorem.  For each transformation
this dissertation will therefore describe how the forward simulations are
proven.

Because the correctness theorem is unchanged except for the target language
semantics, it means that all the corollaries proven using the top-level CompCert
correctness theorem still hold, and do not have to be proven again or modified.
Note however that Vericert does not support separate compilation of translation
units, and needs a main function.  Any corollaries and top-level theorems about
the linking of translation units therefore are not needed and hold vacuously, as
compilation would fail on these translation units.

\section{A Formal Semantics for Verilog}\label{sec:verilog}

This section describes the Verilog semantics that was chosen for the target
language, including the changes that were made to the semantics to make it a
suitable HLS target.  The Verilog standard is quite
large~\cite{06_ieee_stand_veril_hardw_descr_languag,05_ieee_stand_veril_regis_trans_level_synth},
and is split into two standards, sone for synthesis and one for simulation.  The
simulation semantics are similar to semantics one would expect, assigning
detailed behaviour to each part of the Verilog language.  However, in contrast
to programming languages, \textquote[{\textcite[p.~699]{west10_cvd}}]{HDLs are
  better understood as a shorthand for describing digital hardware}, as
synthesis tools try to understand what kind of hardware the user wanted to
describe, which may sometimes behave differently to simulating the original
Verilog design.  It is therefore important to take that into account in the
Verilog semantics and ensure that after synthesis the design still behaves
according to the semantics.  Luckily, the syntax and semantics can be reduced to
a small subset that \vericert{} can target.  This section also describes how
\vericert{}'s representation of memory differs from \compcert{}'s memory model.

The Verilog semantics we use is ported to Coq from a semantics written in HOL4
by \textcite{lööw19_proof_trans_veril_devel_hol} which was used to prove the
translation from a HOL4 description of the hardware to
Verilog~\cite{lööw19_verif_compil_verif_proces}.  This semantics is quite
practical as it is restricted to a small subset of Verilog, which can
nonetheless be used to model the hardware constructs required for HLS.  The main
features that are excluded are continuous assignment and combinational
always-blocks; these are modelled in other semantics such as that
by~\textcite{meredith10_veril}.

The semantics of Verilog differs from regular programming languages, as it is
used to describe hardware directly, which is inherently parallel, rather than an
algorithm, which is usually sequential.  The main construct in Verilog is the
always-block.  A module can contain multiple always-blocks, all of which run in
parallel.  These always-blocks further contain statements such as if-statements
or assignments to variables.  We support only \emph{synchronous} logic, which
means that the always-block is triggered on (and only on) the positive or
negative edge of a clock signal.  However, combinational expressions can still
be expressed within synchronous assignments, so in practice these features are
not needed if the Verilog is being generated.

The semantics combines the big-step and small-step styles. The overall execution
of the hardware is described using a small-step semantics, with one small step
per clock cycle; this is appropriate because hardware is routinely designed to
run for an unlimited number of clock cycles and the big-step style is ill-suited
to describing infinite executions. Then, within each clock cycle, a big-step
semantics is used to execute all the statements.  An example of a rule for
executing an always-block that is triggered at the positive edge of the clock is
shown below, where $\Sigma$ is the state of the registers in the module and $s$
is the statement inside the always-block:
%
\begin{equation*}
  \inferrule[Always]{(\Sigma, s)\downarrow_{\text{stmnt}} \Sigma'}{(\Sigma,
    \yhkeyword{always} \texttt{ @(}\yhkeyword{posedge}\texttt{ clk) } s) \downarrow_{\text{always}^{+}} \Sigma'}
\end{equation*}
%
This rule says that assuming the statement $s$ in the always-block runs with
state $\Sigma$ and produces the new state $\Sigma'$, the always-block will
result in the same final state.  The rule is triggered once for every clock
cycle.

Two types of assignments are supported in always-blocks: nonblocking and
blocking assignment.  Nonblocking assignments all take effect simultaneously at
the end of the clock cycle, while blocking assignments happen instantly so that
later assignments in the clock cycle can pick them up.  This means that
sequential logic is usually expressed using nonblocking assignments, which will
often write the assignment to a register that can be read in the next cycle,
whereas blocking assignment is used for combinational expressions within the
cycle and could therefore be thought of as a wire instead.  When writing
Verilog, this is only a guideline, and in practice the tools may create
registers for blocking assignments if the synthesis tool sees that they are used
to store state.  To mitigate this, the formal semantics specify that
communication between always blocks can only happen through nonblocking
assigment, and blocking assignment can only be used for local variables, so that
these likely will result in wires in the final hardware.  These two restrictions
do not only help produce more predictable hardware that behaves like the
simulation, but it also simplifies the overall semantics of Verilog.  As
only clocked always-blocks are supported, communication between these
always-blocks can only be performed through nonblocking assignment, which means
that the order in which the always blocks are executed does not matter and will
always result in the same final state.

To model both of these assignments, the state $\Sigma$ has to be split into two
maps: $\Gamma$, which contains the current values of all variables and arrays,
and $\Delta$, which contains the values that will be assigned at the end of the
clock cycle. $\Sigma$ can therefore be defined as follows:
$\Sigma = (\Gamma, \Delta)$.  Nonblocking assignment can therefore be expressed
as follows:
%
\begin{equation*}
  \inferrule[Nonblocking Reg]{\yhkeyword{name}\ d = \yhkeyword{OK}\ n \\ (\Gamma, e) \downarrow_{\text{expr}} v}{((\Gamma, \Delta), d\ \yhkeyword{ <= } e) \downarrow_{\text{stmnt}} (\Gamma, \Delta [n \mapsto v])}\\
\end{equation*}
%
where assuming that $\downarrow_{\text{expr}}$ evaluates an expression $e$ to a
value $v$, the nonblocking assignment $d\ \yhkeyword{ <= } e$ updates the future
state of the variable $d$ with value $v$.

Finally, the following rule dictates how the whole module runs in one clock
cycle:
%
\phantomsection{}
\begin{equation*}
  \inferrule[Program]{(\Gamma, \epsilon, \vec{m})\ \downarrow_{\mathrm{module}^{+}}
    (\Gamma', \Delta')}{(\Gamma, \yhkeyword{module } \yhconstant{main}
    \yhkeyword{(...);}\ \vec{m}\ \yhkeyword{endmodule})
    \downarrow_{\text{program}} (\Gamma' \verilogmerge \Delta')}\label{infer:module_old}
\end{equation*}
%
where $\Gamma$ is the initial state of all the variables, $\epsilon$ is the
empty map because the $\Delta$ map is assumed to be empty at the start of the
clock cycle, and $\vec{m}$ is a list of variable declarations and always-blocks
that $\downarrow_{\mathrm{module}^{+}}$ evaluates sequentially to obtain
$(\Gamma', \Delta')$.  The final state is obtained by merging these maps using
the $\verilogmerge$ operator, which gives priority to the right-hand operand in
a conflict. This rule ensures that the nonblocking assignments overwrite at the
end of the clock cycle any blocking assignments made during the cycle.

\subsection{Changes to the semantics}

Five changes were made to the semantics proposed by
\textcite{lööw19_proof_trans_veril_devel_hol} to make it suitable as an HLS
target.

\paragraph{Adding array support}
The main change is the addition of support for arrays, which are needed to model
\gls{BRAM} in Verilog.  \gls{BRAM} is needed to model the stack in C
efficiently, without having to declare a variable for each possible stack
location.  In the original semantics, RAMs (as well as inputs and outputs to the
module) could be modelled using a function from variable names to values, which
could be modified accordingly to model inputs to the module.  However, this
representation does not support merging of individual array elements.  This is
quite an abstract description of memory and can also be expressed as an array
instead, which is the path we took.  This requires the addition of array
operators to the semantics and correct reasoning of loads and stores to the
array in different always-blocks simultaneously.  Consider the following Verilog
code:
%
  \begin{center}
\begin{minted}[xleftmargin=40pt,linenos]{systemverilog}
logic [31:0] x[1:0];
always @(posedge clk) begin
  x[0] = 1;
  x[1] <= 1;
end
\end{minted}
  \end{center}
%
This always-block modifies one array element using blocking assignment and then
a second using nonblocking assignment.  If the existing semantics were used to
update the array, then during the merge, the entire array \texttt{x} from the
nonblocking association map would replace the entire array from the blocking
association map.  This would replace \texttt{x[0]} with its original value and
therefore behave incorrectly. Accordingly, we modified the maps so they record
updates on a per\?element basis. Our state $\Gamma$ is therefore further split
up into $\Gamma_{r}$ for instantaneous updates to variables, and $\Gamma_{a}$
for instantaneous updates to arrays ($\Gamma = (\Gamma_{r}, \Gamma_{a})$);
$\Delta$ is split similarly ($\Delta = (\Delta_{r}, \Delta_{a})$). The merge
function then ensures that only the modified indices get updated when
$\Gamma_{a}$ is merged with the nonblocking map equivalent $\Delta_{a}$.

The implementation of arrays is done using dependently typed arrays that keep
track of their size, such that out-of-bounds accesses can be detected and
handled appropriately.  \Cref{sec:verilog:memory} describes the implementation
of this memory model in more detail, comparing it to the CompCert memory model.

\paragraph{Adding negative edge support}

To reason about circuits that execute on the negative edge of the clock (such as
our \gls{BRAM} interface described briefly in \cref{sec:itv:rtlpar-to-htl}),
support for negative-edge-triggered always-blocks was added to the semantics.
This is shown in the modifications of the \nameref{infer:module} rule shown
below:
%
\phantomsection{}
\begin{equation*}
  \inferrule[Program$^\pm$]{(\Gamma, \epsilon, \vec{m})\ \downarrow_{\text{module}^{+}} (\Gamma', \Delta') \\ (\Gamma'\ //\ \Delta', \epsilon, \vec{m}) \downarrow_{\text{module}^{-}} (\Gamma'', \Delta'')}{(\Gamma, \yhkeyword{module}\ \yhconstant{main} \yhkeyword{(...);}\ \vec{m}\ \yhkeyword{endmodule}) \downarrow_{\text{program}} (\Gamma''\ //\ \Delta'')}\label{infer:module}
\end{equation*}
%
The main execution of the module $\downarrow_{\text{module}}$ is split into
$\downarrow_{\text{module}^{+}}$ and $\downarrow_{\text{module}^{-}}$, which are
rules that only execute always-blocks triggered at the positive and at the
negative edge respectively. The positive-edge-triggered always-blocks are
processed in the same way as in the original \nameref{infer:module_old}
rule. The output maps $\Gamma'$ and $\Delta'$ are then merged and passed as the
blocking assignments map into the negative edge execution, so that all the
blocking and nonblocking assignments are resolved.  Finally, all the
negative-edge-triggered always-blocks are processed and merged to give the final
state.

\paragraph{Adding declarations} Explicit support for declaring inputs, outputs
and internal variables was added to the semantics to make sure that the
generated Verilog also contains the correct declarations.  This adds some
guarantees to the generated Verilog and ensures that it synthesises and
simulates correctly.

\paragraph{Removing support for external inputs to modules} Support for
receiving external inputs was removed from the semantics for simplicity, as
these are not needed for an \gls{HLS} target. The main module in Verilog models
the main function in C, and since the inputs to a C function should not change
during its execution, there is no need for external inputs for Verilog modules.

\paragraph{Simplifying representation of bitvectors} Finally, we use 32-bit
integers to represent bitvectors rather than arrays of booleans. This is because
\vericert{} (currently) only supports types represented by 32 bits.  However,
extending Vericert would likely not require the full flexibility of bitvectors
as arrays of booleans either, as long as fine-grained bit-size optimisations for
registers are not introduced, because the Verilog subset could be restriced to
bitvectors of sizes 4, 8, 16, 32 and 64, which are already supported by the
CompCert integer type.

\subsection{Integrating the Verilog semantics into \compcert{}'s model}
\label{sec:verilog:integrating}

\begin{figure*}
  \centering
  \begin{minipage}{1.0\linewidth}
    \begin{mathpar}
      \inferrule[Step]{\Gamma_r[\mathit{rst}] = 0 \\ \Gamma_r[\mathit{fin}] = 0 \\ (m, (\Gamma_r, \Gamma_a))\ \downarrow_{\text{program}} (\Gamma_r', \Gamma_a')}{\yhconstant{State}\ \mathit{sf}\ m\ \ \Gamma_r[\sigma]\ \ \Gamma_r\ \Gamma_a \longrightarrow \yhconstant{State}\ \mathit{sf}\ m\ \ \Gamma_r'[\sigma]\ \ \Gamma_r'\ \Gamma_a'}\label{infer:stepstate}\and
      %
      \inferrule[Finish]{\Gamma_r[\mathit{fin}] = 1}{\yhconstant{State}\ \mathit{sf}\ m\ \sigma\ \Gamma_r\ \Gamma_a \longrightarrow \yhconstant{Returnstate}\ \mathit{sf}\ \Gamma_r[ \mathit{ret}]}\label{infer:finishstate}\and
      %
      \inferrule[Call]{ }{\yhconstant{Callstate}\ \mathit{sf}\ m\ \vec{r} \longrightarrow \yhconstant{State}\ \mathit{sf}\ m\ n\ ((\yhfunction{init\_params}\ \vec{r}\ a)[\sigma \mapsto n, \mathit{fin} \mapsto 0, \mathit{rst} \mapsto 0])\ \epsilon}\label{infer:callstate}\and
      %
      \inferrule[Return]{ }{\yhconstant{Returnstate}\ (\yhconstant{Stackframe}\ r\ m\ \mathit{pc}\ \Gamma_r\ \Gamma_a :: \mathit{sf})\ v \longrightarrow \yhconstant{State}\ \mathit{sf}\ m\ \mathit{pc}\ (\Gamma_{r} [ \sigma \mapsto \mathit{pc}, r \mapsto v ])\ \Gamma_{a}}\label{infer:returnstate}
    \end{mathpar}
  \end{minipage}
  \caption{Top-level small-step semantics for Verilog modules in \compcert{}'s computational framework.}%
  \label{fig:inference_module}
\end{figure*}

The \compcert{} computation model defines a set of states through which
execution passes. In this subsection, we explain how we extend our Verilog
semantics with four special-purpose registers in order to integrate it into
\compcert{}.

\compcert{} executions pass through three main states:
\begin{description}
\item[\texttt{State} $\mathit{sf}$ $m$ $v$ $\Gamma_{r}$ $\Gamma_{a}$] The main
  state when executing a function, with stack frame $\mathit{sf}$, current
  module $m$, current state $v$ and variable states $\Gamma_{r}$ and
  $\Gamma_{a}$.
  \item[\texttt{Callstate} $\mathit{sf}$ $m$ $\vec{r}$] The state that is
    reached when a function is called, with the current stack frame
    $\mathit{sf}$, current module $m$ and arguments $\vec{r}$.
  \item[\texttt{Returnstate} $\mathit{sf}$ $v$] The state that is reached when a
    function returns back to the caller, with stack frame $\mathit{sf}$ and
    return value $v$.
\end{description}

To support this computational model, we extend the Verilog module we generate
with the following four registers and a \gls{BRAM} block:

\begin{description}
\item[program counter] The program counter can be modelled using a register that
  keeps track of the state, denoted as $\sigma$.
  \item[function entry point] When a function is called, the entry point denotes
    the first instruction that will be executed. This can be modelled using a
    reset signal that sets the state accordingly, denoted as $\mathit{rst}$.
  \item[return value] The return value can be modelled by setting a finished
    flag to 1 when the result is ready, and putting the result into a 32-bit
    output register. These are denoted as $\mathit{fin}$ and $\mathit{ret}$
    respectively.
%\JW{Is there a mismatch between `ret' in the figure and `rtrn' in the text?}
  \item[stack] The function stack can be modelled as a \gls{BRAM} block, which
    is implemented using an array in the module, and denoted as $\mathit{stk}$.
%\JW{Is there a mismatch between `st' in the figure and `stk' in the text?}\YH{It was actually between $\Gamma_{a}$ and \mathit{stk}.  The \mathit{st} should have been $\sigma$.}
\end{description}

\Cref{fig:inference_module} shows the inference rules for moving between the
computational states.  The first, \nameref{infer:stepstate}, is the normal rule
of execution.  It defines one step in the \texttt{State} state, assuming that
the module is not being reset, that the finish state has not been reached yet,
that the current and next state are $v$ and $v'$, and that the module runs from
state $\Gamma$ to $\Gamma'$ using the \nameref{infer:stepstate} rule.  The
\nameref{infer:finishstate} rule returns the final value of running the module
and is applied when the $\mathit{fin}$ register is set; the return value is then
taken from the $\mathit{ret}$ register.

Note that there is no step from \texttt{State} to \texttt{Callstate}; this is
because function calls are not supported, and it is therefore impossible in our
semantics ever to reach a \texttt{Callstate} except for the initial call to
main. So the \nameref{infer:callstate} rule is only used at the very beginning
of execution; likewise, the \nameref{infer:returnstate} rule is only matched for
the final return value from the main function.  Therefore, in addition to the
rules shown in \cref{fig:inference_module}, an initial state and final state
need to be defined:

\begin{mathpar}
  \inferrule[Initial]{\yhfunction{is\_internal}\
    P.\texttt{main}}{\yhfunction{initial\_state}\ (\yhconstant{Callstate}\ []\
    P.\texttt{main}\ [])}%
  \label{infer:initialstate}
  \and
  \inferrule[Final]{ }{\yhfunction{final\_state}\ (\yhconstant{Returnstate}\ []\
    n)\ n}%
  \label{infer:finalstate}
\end{mathpar}

\noindent where the initial state is the \texttt{Callstate} with an empty stack
frame and no arguments for the \texttt{main} function of program $P$, where this
\texttt{main} function needs to be in the current translation unit.  The final
state results in the program output of value $n$ when reaching a
\texttt{Returnstate} with an empty stack frame.

\subsection{Memory model}\label{sec:verilog:memory}

The Verilog semantics do not define a memory model for Verilog, as this is not
needed for a hardware description language.  There is no preexisting
architecture that Verilog will produce; it can describe any memory layout that
is needed.  Instead of having specific semantics for memory, the semantics only
needs to support the language features that can produce these different memory
layouts, these being Verilog arrays.  We therefore define semantics for updating
Verilog arrays using blocking and nonblocking assignment.  We then have to prove
that the C memory model that \compcert{} uses matches with the interpretation of
arrays used in Verilog.  The \compcert{} memory model is infinite, whereas our
representation of arrays in Verilog is inherently finite.  There have already
been efforts to define a general finite memory model for all intermediate
languages in \compcert{}, such as CompCertS~\cite{besson18_compc} or
CompCert-TSO~\cite{sevcik13_compc}, or keeping the intermediate languages intact
and translate to a more concrete finite memory model in the back end, such as in
Comp\-Cert\-ELF~\cite{wang20_compc}.  We also define such a translation from
\compcert{}'s standard infinite memory model to finite arrays that can be
represented in Verilog.  There is therefore no more notion of an abstract memory
model and all the interactions to memory are encoded in the hardware itself.

%\JW{I'm not quite sure I understand. Let me check: Are you saying that previous work has shown how all the existing CompCert passes can be adapted from an infinite to a finite memory model, but what we're doing is leaving the default (infinite) memory model for the CompCert front end, and just converting from an infinite memory model to a finite memory model when we go from 3AC to HTL?}\YH{Yes exactly, most papers changed the whole memory model to thread through properties that were then needed in the back end, but we currently don't need to do that.  I need to double check though for CompCertELF, it doesn't actually seem to be the case.  Will edit this section later.}

\definecolor{compcertmemmodel}{HTML}{e2ccea}
\definecolor{vericertmemmodel}{HTML}{cbe1db}
\definecolor{compcertmemmodeldark}{HTML}{5b2c6d}
\definecolor{vericertmemmodeldark}{HTML}{386156}
\begin{figure}
  \centering
  \begin{tikzpicture}[>=Latex,font=\sffamily]
    \fill[compcertmemmodel,rounded corners=3pt] (0,1) rectangle (5,-5);
    \fill[vericertmemmodel,rounded corners=3pt] (7,1) rectangle (12.5,-5);
    \node[right] at (0,0.7) {\footnotesize \textbf{\compcert{}'s Memory Model}};
    \node[right] at (7,0.7) {\footnotesize \textbf{Verilog Memory
        Representation}};
    \node[right] (baselabel) at (0.2,-1.3) {\small \textbf{\texttt{base}}};
    \node[right] (x0) at (0.2,-1.9) {\small \texttt{0}};
    \node[right] (x1) at (0.2,-2.5) {\small \texttt{1}};
    \node[rotate=90] (x2) at (0.43,-3.1) {$\cdots$};
    \foreach \x in {0,...,6}{%
      \node[right] (s\x) at (2.5,-1-\x*0.3) {\small \texttt{\x}};
      \node[right] (t\x) at (4,-1-\x*0.3) {};
      \draw[->] (s\x) -- (t\x);
    }
    \path (s0) ++(-0.3,0.6) node[right] {\small\textbf{\texttt{addr}}};
    \path (t0) ++(0.65,0.6) node[left] {\small\textbf{\texttt{data}}};
    \node[right] at (t0) {\small \texttt{DE}};
    \node[right] at (t1) {\small \texttt{AD}};
    \node[right] at (t2) {\small \texttt{BE}};
    \node[right] at (t3) {\small \texttt{EF}};
    \node[right] at (t4) {\small \texttt{12}};
    \node[right] at (t5) {\small \texttt{34}};
    \node[right] at (t6) {\small \texttt{56}};
    \node[right] at (3.1,-3.1) {$\cdots$};

    \node[right] at (3.1,-4) {$\cdots$};
    \draw[line width=1.7mm,->] (5,-2.5) -- (6.9,-2.5);

    \draw[->] (x0) -- (s3);
    \draw[->] (x1) -- (2.5,-4);
    \node at (2.5,-4.7) {\small \texttt{x[0] = 0xDEADBEEF;}};

    \begin{scope}[xshift=2.5mm,yshift=3mm]
      \draw (7.2,-1.2) rectangle (9.4,-3.9); \draw (9.6,-1.2) rectangle
      (11.8,-3.9);

      \foreach \x in {0,...,8}{%
        \draw (7.2,-1.2-\x*0.3) -- (9.4,-1.2-\x*0.3); \draw (9.6,-1.2-\x*0.3) --
        (11.8,-1.2-\x*0.3); \node (b\x) at (8.3,-1.35-\x*0.3) {}; \node (nb\x)
        at (10.7,-1.35-\x*0.3) {}; }

      \node[scale=1.2] at (b0) {\tiny\texttt{0: Some 00000000}};
      \node[scale=1.2] at (b1) {\tiny\texttt{1: Some 12345600}};
      \node[scale=1.2] at (b2) {\tiny\texttt{2: Some 00000000}};
      \node[scale=1.2] at (b3) {\tiny\texttt{3: Some 00000000}};
      \node[scale=1.2] at (b4) {\tiny\texttt{4: Some 00000000}};
      \node[scale=1.2] at (b5) {\tiny\texttt{5: Some 00000000}};
      \node[scale=1.2] at (b6) {\tiny\texttt{6: Some 00000000}};
      \node[scale=1.2] at ($(b7) - (0,0.05)$) {$\cdots$}; \node[scale=1.2] at
      (b8) {\tiny\texttt{N: Some 00000000}};

      \node[scale=1.2] at (nb0) {\tiny\texttt{0: Some DEADBEEF}};
      \node[left,scale=1.2] at (nb1) {\tiny\texttt{1: None}};
      \node[left,scale=1.2] at (nb2) {\tiny\texttt{2: None}};
      \node[left,scale=1.2] at (nb3) {\tiny\texttt{3: None}};
      \node[left,scale=1.2] at (nb4) {\tiny\texttt{4: None}};
      \node[left,scale=1.2] at (nb5) {\tiny\texttt{5: None}};
      \node[left,scale=1.2] at (nb6) {\tiny\texttt{6: None}}; \node[scale=1.2]
      at ($(nb7) - (0,0.05)$) {$\cdots$}; \node[left,scale=1.2] at (nb8)
      {\tiny\texttt{N: None}};

      \node at (8.3,-0.9) {$\Gamma_{a}$}; \node at (10.7,-0.9) {$\Delta_{a}$};
    \end{scope}

    \node at (9.5,-4.7) {\small \texttt{stack[0] <= 0xDEADBEEF;}};

    \draw[very thick,draw=vericertmemmodeldark] (7,-4.3) -- (12.5,-4.3);
    \draw[very thick,draw=compcertmemmodeldark] (0,-4.3) -- (5,-4.3);
    \draw[very thick,draw=vericertmemmodeldark] (7,0.3) -- (12.5,0.3);
    \draw[very thick,draw=compcertmemmodeldark] (0,0.3) -- (5,0.3);
  \end{tikzpicture}
  %\Description{\compcert{}'s memory model is translated into a more concrete memory model based on Verilog arrays.  Two association maps are therefore needed to keep track of the blocking and nonblocking assignments.}
  \caption{Change in the memory model during the translation of \rtl{} into
    \htl{}.  The state of the memories in each case is right after the execution
    of the store to memory.}\label{fig:memory_model_transl}
\end{figure}

%\JW{It's not completely clear what the relationship is between your work and those works. The use of `only' suggests that you've re-done a subset of work that has already been done -- is that the right impression?}\YH{Hopefully that's more clear.}

This translation is represented in \cref{fig:memory_model_transl}.  \compcert{} defines a map from blocks to maps from memory addresses to memory contents.  Each block represents an area in memory; for example, a block can represent a global variable or a stack for a function. As there are no global variables, the main stack can be assumed to be block 0, and this is the only block we translate.
%\JW{So the stack frame for a function called by main would be in a different block, is that the idea? Seems unusual not to have a single stack.}
%\YH{Yeah exactly, it makes it much easier to reason about though, because everything is nicely isolated.  This is exactly what CompCertELF and CompCertS try and solve though.}
%\JW{Would global variables normally be put in blocks 1, 2, etc.?}
%\YH{Yes, although it may also be possible that they could be numbered 0, 1, 2, 3, 4, pushing the block of the stack higher.}
Meanwhile, our Verilog semantics defines two finite arrays of optional values, one for the blocking assignments map $\Gamma_{\mathrm{a}}$ and one for the nonblocking assignments map $\Delta_{\mathrm{a}}$.
%\JW{It's a slight shame that `block' is used in two different senses in the preceding two sentences. I guess that can't be helped.}
%\YH{Ah that's true, I hadn't even noticed.  Yeah I think it would be good to keep the name ``block'' for CompCert's blocks.}
The optional values are present to ensure correct merging of the two association maps at the end of the clock cycle. %During our translation we only convert block 0 to a Verilog memory, and ensure that it is the only block that is present.
%This means that the block necessarily represents the stack of the main function.
The invariant used in the proofs is that block 0 should be equivalent to the merged representation of the $\Gamma_{\mathrm{a}}$ and $\Delta_{\mathrm{a}}$ maps.

%However, in practice, assigning and reading from an array directly in the state machine will not produce a memory in the final hardware design, as the synthesis tool cannot identify the array as having the necessary properties that a \gls{BRAM} needs, even though this is the most natural formulation of memory.  Even though theoretically the memory will only be read from once per clock cycle, the synthesis tool cannot ensure that this is true, and will instead create a register for each memory location.  This increases the size of the circuit dramatically, as the \gls{BRAM} on the FPGA chip will not be reused.  Instead, the synthesis tool expects a specific interface that ensures these properties, and will then transform the interface into a proper \gls{BRAM} during synthesis.  Therefore, a translation has to be performed from the naive use of memory in the state machine, to a proper use of a memory interface.

%\begin{figure}
%  \centering
%  \begin{subfigure}[t]{0.48\linewidth}
%    \includegraphics[width=\linewidth]{diagrams/store_waveform.pdf}
%    \caption{Store waveform.}
%  \end{subfigure}\hfill%
%  \begin{subfigure}[t]{0.48\linewidth}
%    \includegraphics[width=\linewidth]{diagrams/load_waveform.pdf}
%    \caption{Load waveform.}
%  \end{subfigure}
%\end{figure}

\subsection{Deterministic Verilog semantics}%
\label{sec:proof:deterministic}

% Finally, to obtain the backward simulation that we want, it has to be shown
% that if we generate hardware with a specific behaviour, that it is the only
% possible program with that behaviour.  This only has to be performed for the
% final intermediate language, which is Verilog, so that the backward simulation
% holds for the whole chain from Clight to Verilog.
The final lemma we need is that the Verilog semantics is deterministic. This
result allows us to replace the forwards simulation we have proved with the
backwards simulation we desire.

\begin{lemma}\label{lemma:deterministic}
  If a Verilog program $V\!$ admits behaviours $B_1$ and $B_2$, then $B_1$ and
  $B_2$ must be the same.

  \begin{equation*}
    \forall V\ B_{1}\ B_{2}\ldotp\quad V \Downarrow B_{1} \land V \Downarrow B_{2} \implies B_{1} = B_{2}.
  \end{equation*}
\end{lemma}

\begin{proof}[Proof sketch]
  The Verilog semantics is deterministic because the order of operation of all
  the constructs is defined, so there is only one way to evaluate the module,
  and hence only one possible behaviour. This was proven for the small-step
  semantics shown in \cref{fig:inference_module}.
\end{proof}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% TeX-engine: luatex
%%% End:
