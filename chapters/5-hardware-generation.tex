\chapter{Hardware Generation}

Until now the representation of the program has still been in the form of a
software program, with virtual, infinite registers, a program counter, as well
as a rich but abstract memory model.  This representation needs to be
transformed into a more suitable representation on which hardware specific
transformations can applied, and which is closer to th structure of the final
Verilog design.  The main transformation that takes place is converting a
control-flow graph into a state machine with data-path representation, which is
also the structure of the final design.  There are various steps involved in
this refinement because of the large gap between control-flow graph semantics
and state machine semantics.  In addition to that, additional components, such
as an implementation of a memory that can be efficiently implemented in
hardware, need to be added to the hardware to produce a useful design.  Finally,
until now programs have only been executing sequentially, whereas to produce the
final hardware one will have to transform the sequential execution of operations
within each state into parallel assignments.

This chapter describes the hardware generation process, starting from \rtlpar{}
and producing a final Verilog design.  \Cref{sec:hg:state-partitioning}
describes the first step in the transformation which separates each sequential
block within a state in \rtlpar{} into separate states that can be addressed
using the program counter.  This matches addressing that the state register
would have to do in the hardware design. Next, \cref{sec:hg:htl-generation}
describes the generation of HTL, an intermediate language representing the
execution of a state machine.  This performs the main transformation from a
software representation of the program into hardware, making the execution of
the program more explicit in the design itself instead of as part of the
semantics.  \Cref{sec:hg:bram-generation} then describes the first
hardware-specific optimisation on the state-machine representation of the
hardware by adding a specification of a \gls{BRAM} to the HTL semantics and
replacing any explicit reads and writes to the array representing memory by
properly formed reads and writes to the \gls{BRAM}.  Until now, updates to
registers have been specified sequentially, so a forward substitution
transformation is describe in \cref{sec:hg:register-forward-substitution} to
parallelise the updates to registers.  Finally, \cref{sec:hg:verilog-generation}
describes the generation of the final Verilog design, which implements the
state-machine that was specified by HTL, in particular implementing the
\gls{BRAM} that was specified.

\section{State Partitioning}%
\label{sec:hg:state-partitioning}

\rtlpar{} is a control-flow graph with nodes mapping to hyperblocks.  This is
useful for the scheduling proof, as each of these hyperblocks can be compared
individually.  However, in the hardware itself, the individual sequential blocks
have to be separated into different states, as within each state the
assignments will be performed in parallel.  This first state partitioning
transformation separates operations that should execute in different clock
cycles into their own locations in the control-flow graph.

\begin{figure}
  \centering
  \begin{tikzpicture}[>=Latex,shorten >=1pt,
    label/.style={circle,draw,fill=white,inner sep=0.4mm,font=\footnotesize},
    bb/.style={align=left, draw=white, fill=black!5}]
    \node[bb] (initial block) {\rtlinline`[ [ [ r1 := r2 ] ]; `\\
                               \rtlinline`  [ [ r3 := r4 ];   `\\
                               \rtlinline`    [ goto 2   ] ] ]`};
    \node[bb, right=4cm of initial block, yshift=1cm] (first transf block)
      {\rtlinline`[ [ r1 := r2 ]; `\\
       \rtlinline`  [ goto 3   ] ]`};
    \node[bb, below=of first transf block] (second transf block)
                               {\rtlinline`  [ [ r3 := r4 ];   `\\
                                \rtlinline`    [ goto 2   ] ]`};
    \node[label] at (initial block.north west) {\texttt{1}};
    \node[label] at (first transf block.north west) {\texttt{1}};
    \node[label] at (second transf block.north west) {\texttt{3}};
    \draw[->,very thick] ($(initial block.east) + (0.5,0)$)
      -- node [below, font=\footnotesize] {state partitioning}
      ($(initial block.east) + (3,0)$);
  \end{tikzpicture}
  \caption{State partitioning transformation splitting up the hyperblock into
    multiple locations.}%
  \label{fig:hg:state-partitioning}
\end{figure}

\Cref{fig:hg:state-partitioning} shows an example state partitioning
transformation, where a new block is added at location \diaglabel{$3$} with the
contents of the second sequential block, and a \rtlinline`goto` instruction is
added to the original block leading to this next block.  Fresh locations for new
blocks are chosen by keeping track of the greatest location in the current
function.

\subsection{Proof of State Partitioning}

The proof of state partitioning is relatively simple using a
\gls{plus-forward-simulation}.  Then, for each input states, there are one or
more output states returned by the state partitioning algorithm that should be
equivalent to the execution of the input state when executed sequentially.

\section{HTL Generation}%
\label{sec:hg:htl-generation}

The most important transformation of an HLS tool is then the generation of a
hardware description from the list of instructions.  Eventually the hardware
design will be described using Verilog, however, to make the transformation more
incremental, we first turn the program represented by a control-flow graph into
a program represented by an \gls{FSMD}.  This section will describe the
\gls{FSMD} language, called HTL, by showing the syntax and semantics of the
language.  Then, the transformation from \rtlsubpar{} to HTL is shown, together
with an overview of its correctness proof.

\subsection{HTL Structure and Semantics}%
\label{sec:hg:htl-structure-and-semantics}

At a high level, HTL is structured like many other \compcert{} languages,
mapping from locations to Verilog statements.  However, contrary to many other
intermediate languages in \compcert{}, HTL does not contain any instructions,
and its semantics use a smaller state to perform the execution.  For example,
the state does not have to contain the program counter because there is an
explicit state register that keeps track of it.

HTL comprises a a lot of metadata pointing to important registers, as well as
containing a map from program locations to Verilog statements.  The syntax of
HTL is shown in \cref{fig:hg:htl-syntax}.  First, HTL contains a list of
parameters, which are additional input registers to the current module.  Next,
the $\mathit{datapath}$ contains the code for the state machine, as well as the
main data path associated with the data path.  This is done by mapping program
locations, or states, to Verilog statements $\verilogstmnttype$, which updates
registers as part of the data path, but also updates to the state.  The
computation of the next state often relies on the state of registers, which is
why it needs to be performed as part of the data path.

\begin{figure}
\centering
\begin{tabular}{rr@{~}r@{~}l@{\hspace*{2mm}}l}
  \llabel{registers} & $\reg, \mbox{\rtlinline{r1}}, \mbox{\rtlinline{r2}}, \ldots \in \regtype$ & & & \\
  \llabel{CFG node labels} & $\location \in \locationtype$ & $::=$ & $\mathbb{N}$ & \\
  \llabel{Verilog statements} & $\verilogstmnt \in \verilogstmnttype$ & & & \\
  \llabel{Code} & $\htlcode \in \locationtype \rightarrow \verilogstmnttype$ & & & \\
  \llabel{HTL} & $\htlmodule$ & $::=$ & $\{\ \mathit{params} : \regtype\
                                        \texttt{list}; $ \\
  & & & $\ \ \mathit{datapath} : \locationtype \rightarrow \verilogstmnttype; $ \\
  & & & $\ \ \mathit{entrypoint} : \mathbb{N};$ & \\
  & & & $\ \ \mathit{state} : \regtype;$ & \\
  & & & $\ \ \mathit{stack} : \regtype;$ & \\
  & & & $\ \ \mathit{stack\_size} : \mathbb{N};$ & \\
  & & & $\ \ \mathit{finish},\mathit{return},\mathit{start},\mathit{reset},\mathit{clk} :
        \regtype;$ & \\
  & & & $\ \ \mathit{ram} : \optiontype{\mathit{RAM}};$ & \\
  & & & $\ \ \mathit{order\_wf} : \mathit{state} < \mathit{finish} < \mathit{return}
        < \mathit{stack} < \mathit{reset} < \mathit{clk};$ & \\
  & & & $\ \ \mathit{ram\_wf} : \forall r\ldotp \mathit{ram} = \some{r} \implies
        \mathit{clk} < r.\mathit{raddr}; $ & \\
  & & & $\ \ \mathit{params\_wf} : \forall r \in \mathit{params}\ldotp
        r < \mathit{state} \ \}$
\end{tabular}
\caption{Syntax of HTL.}
\label{fig:hg:htl-syntax}
\end{figure}

Next, the \htl{} module contains an entry point, which is the initial starting
state of the $\mathit{state}$ register.  After the reset input wire is asserted,
the $\mathit{state}$ register will be reset to that value.  The $\mathit{state}$
register is read at every clock tick and determines the next statement that
should be executed from the $\mathit{datapath}$.  As mentioned before, the
$\mathit{state}$ register is a physical representation of the virtual program
counter from \rtlsubpar{} and other intermediate instruction languages.  We then
also have $\mathit{stack}$ register and an associated $\mathit{stack\_size}$,
which is a Verilog array storing the contents of the stack.  Initially, this
array will be accessed directly by operations in the data path, however,
\cref{sec:hg:bram-generation} describes how these direct accesses to the array
are instead turned into accesses to a \gls{BRAM}.

We then have a list of input and output control signals, which are used to
return a result by setting the $\mathit{finish}$ flag and assigning the
$\mathit{return}$ register to the result.  Next, there are $\mathit{start}$ and
$\mathit{reset}$ signals that provide a way to reset the state of the internal
state machine, as well as start the execution of the module when ready.
Finally, there is a $\mathit{clk}$ input to provide the clock to the design.
This input is not yet used by the \htl{} design, as execution is still performed
using state transitions in the semantics, however, a register is already
allocated for the clock which will be needed by the final Verilog design.  The
module then also contains a $\mathit{ram}$ which will be further described in
\cref{sec:hg:bram-generation}, because in the first translation pass it is
initialised to \cnone.

Finally, there are three \emph{well-formedness} criteria which are used to
enforce an ordering between the registers, mainly to be able to show that
registers are independent from each other.

\subsection{HTL Generation Algorithm}%
\label{sec:hg:htl-generation}

The generation of \htl{} is relatively straight-forward, as most instructions
have a direct translation to a Verilog implementation.  The Verilog
implementation can therefore follow the semantics of each operation and
implement their arithmetic behaviour directly.  The arithmetic operation is then
assigned to the destination register using \emph{blocking} assignment, so as to
preserve the sequential nature of the execution of the code and simplify the
correctness proof.  One subtle aspect of the proof is the translation of
registers and predicates into Verilog, because in \rtlsubpar{} these were
contained in separate maps, whereas in Verilog and \htl{} they need to be
combined into one register association map.  This is done by referring to
register $r$ in \rtlsubpar{} as register $r' = 2r$ in Verilog.  Predicate $p$,
on the other hand, is referred to as predicate $p' = 2p + 1$ in Verilog, thereby
combining both name spaces into one.  A short example of a translation from
\rtlsubpar{} to \htl{} is shown in \cref{fig:hg:htl-generation}.

\begin{figure}
  \centering
  \begin{tikzpicture}[>=Latex,shorten >=1pt,
    label/.style={circle,draw,fill=white,inner sep=0.4mm,font=\footnotesize},
    bb/.style={align=left, draw=white, fill=black!5}]
    \node[bb] (initial block) {\rtlinline`[ [ r1 := r2 * r3     ] ]; `\\
                               \rtlinline`[ [ p1 => r4 := r5      ]; `\\
                               \rtlinline`[ [ p2 => Stack[r6] = 3 ]; `\\
                               \rtlinline`  [ goto 2              ] ]`};
    \node[bb, right=4cm of initial block] (transf block)
      {\veriloginline`r2 = r4 * r6;`\\
       \veriloginline`r8 = r3 ? r10 : r8;`\\
       \veriloginline`if (r5) stack[r12] = 32'd3;`\\
       \veriloginline`state = 32'd2;`};
    \node[label] at (initial block.north west) {\texttt{1}};
    \node[label] at (transf block.north west) {\texttt{1}};
    \draw[->,very thick] ($(initial block.east) + (0.5,0)$)
      -- node [below, font=\footnotesize] {\htl{} generation}
      ($(initial block.east) + (3,0)$);
  \end{tikzpicture}
  \caption{Simple translation from an \rtlsubpar{} block into an \htl{} block.}%
  \label{fig:hg:htl-generation}
\end{figure}

As shown in \cref{fig:hg:htl-generation}, predicated instructions are translated
into blocking assignments of a ternary expression in Verilog.  This is the case
for all instructions except for the store instruction, which is translated to a
conditional statement.  This ensures that the memory is not modified

\subsubsection{Implementing the \texttt{Oshrximm} Instruction}%
\label{sec:algorithm:optimisation:oshrximm}

% Mention that this optimisation is not performed sometimes (clang -03).

Many of the \compcert{} instructions map well to hardware, but \texttt{Oshrximm}
(efficient signed division by a power of two using a logical shift) is expensive
if implemented na\"ively. The problem is that in \compcert{} it is specified as
a signed division:

\begin{equation*}
  \texttt{Oshrximm } x\ y = \text{round\_towards\_zero}\left(\frac{x}{2^{y}}\right)
\end{equation*}

(where $x, y \in \mathbb{Z}$, $0 \leq y < 31$, and $-2^{31} \leq x < 2^{31}$)
and instantiating divider circuits in hardware is well known to cripple
performance. Moreover, since \vericert{} requires the result of a divide
operation to be ready within a single clock cycle, the divide circuit needs to
be entirely combinational. This is inefficient in terms of area, but also in
terms of latency, because it means that the maximum frequency of the hardware
must be reduced dramatically so that the divide circuit has enough time to
finish.  It should therefore be implemented using a sequence of shifts.

\compcert{} eventually performs a translation from this representation into
assembly code which uses shifts to implement the division, however, the
specification of the instruction in 3AC itself still uses division instead of
shifts, meaning this proof of the translation cannot be reused.  In \vericert{},
the equivalence of the representation in terms of divisions and shifts is proven
over the integers and the specification, thereby making it simpler to prove the
correctness of the Verilog implementation in terms of shifts.

\subsection{HTL Generation Correctness Proof}%
\label{sec:hg:htl-generation-correctness-proof}

%\section{Correctness Proof}\label{sec:proof}

%\JW{That's quite a hard-to-parse section heading; I'd go for something simpler like `Correctness Proof' or `Proving Correctness'}

The proof of correctness of the Verilog back end is quite different from the
usual proofs performed in CompCert, mainly because of the difference in the
memory model and semantic differences between Verilog and CompCert's existing
intermediate languages.

\begin{itemize}
\item As already mentioned in Section~\ref{sec:verilog:memory}, because the
  memory model in our Verilog semantics is finite and concrete, but the CompCert
  memory model is more abstract and infinite with additional bounds, the
  equivalence of these models needs to be proven.  Moreover, our memory is
  word-addressed for efficiency reasons, whereas CompCert's memory is
  byte-addressed.

\item Second, the Verilog semantics operates quite differently to the usual
  intermediate languages in CompCert.  All the CompCert intermediate languages
  use a map from control-flow nodes to instructions.  An instruction can
  therefore be selected using an abstract program pointer. Meanwhile, in the
  Verilog semantics the whole design is executed at every clock cycle, because
  hardware is inherently parallel. The program pointer is part of the design as
  well, not just part of an abstract state. This makes the semantics of Verilog
  simpler, but comparing it to the semantics of 3AC becomes more challenging, as
  one has to map the abstract notion of the state to concrete values in
  registers.
\end{itemize}

Together, these differences mean that translating 3AC directly to Verilog is
infeasible, as the differences in the semantics are too large.  Instead, HTL,
which was introduced in Section~\ref{sec:design}, bridges the gap in the
semantics between the two languages.  HTL still consists of maps, like many of
the other CompCert languages, but each state corresponds to a Verilog statement.

\subsection{Formulating the Correctness Theorem}

The main correctness theorem is analogous to that stated in
\compcert{}~\cite{leroy09_formal_verif_realis_compil}: for all Clight source
programs $C$, if the translation to the target Verilog code succeeds,
$\mathit{Safe}(C)$ holds and the target Verilog has behaviour $B$ when
simulated, then $C$ will have the same behaviour $B$. $\mathit{Safe}(C)$ means
all observable behaviours of $C$ are safe, which can be defined as
$\forall B,\ C \Downarrow B \implies B \in \texttt{Safe}$.  A behaviour is in
\texttt{Safe} if it is either a final state (in the case of convergence) or
divergent, but it cannot `go wrong'. (This means that the source program must
not contain undefined behaviour.) In \compcert{}, a behaviour is also associated
with a trace of I/O events, but since external function calls are not supported
in \vericert{}, this trace will always be empty.

\begin{theorem}
  Whenever the translation from $C$ succeeds and produces Verilog $V$, and all
  observable behaviours of $C$ are safe, then $V$ has behaviour $B$ only if $C$
  has behaviour $B$.
  \begin{equation*}
    \forall C, V, B,\quad \yhfunction{HLS} (C) = \yhconstant{OK} (V) \land \mathit{Safe}(C) \implies (V \Downarrow B \implies C \Downarrow B).
  \end{equation*}
\end{theorem}

Why is this correctness theorem also the right one for HLS? It could be argued
that hardware inherently runs forever and therefore does not produce a
definitive final result.  This would mean that the \compcert{} correctness
theorem would probably be unhelpful with proving hardware correctness, as the
behaviour would always be divergent.  However, in practice, HLS does not
normally produce the top-level of the design that needs to connect to other
components, therefore needing to run forever.  Rather, HLS often produces
smaller components that take an input, execute, and then terminate with an
answer.  To start the execution of the hardware and to signal to the HLS
component that the inputs are ready, the $\mathit{rst}$ signal is set and unset.
Then, once the result is ready, the $\mathit{fin}$ signal is set and the result
value is placed in $\mathit{ret}$.  These signals are also present in the
semantics of execution shown in Fig.~\ref{fig:inference_module}.  The
correctness theorem therefore also uses these signals, and the proof shows that
once the $\mathit{fin}$ flag is set, the value in $\mathit{ret}$ is correct
according to the semantics of Verilog and Clight.  Note that the compiler is
allowed to fail and not produce any output; the correctness theorem only applies
when the translation succeeds.

How can we prove this theorem? First, note that the theorem is a `backwards
simulation' result (every target behaviour must also be a source behaviour),
following the terminology used in the \compcert{}
literature~\cite{leroy09_formal_verif_realis_compil}. The reverse direction
(every source behaviour must also be a target behaviour) is not demanded because
compilers are permitted to resolve any non-determinism present in their source
programs. However, since Clight programs are all deterministic, as are the
Verilog programs in the fragment we consider, we can actually reformulate the
correctness theorem above as a forwards simulation result (following standard
\compcert{} practice), which makes it easier to prove.  To prove this forward
simulation, it suffices to prove forward simulations between each pair of
consecutive intermediate languages, as these results can be composed to prove
the correctness of the whole HLS tool.  The forward simulation from 3AC to HTL
is stated in Lemma~\ref{lemma:htl} (Section~\ref{sec:proof:3ac_htl}), the
forward simulation for the RAM insertion is shown in Lemma~\ref{lemma:htl_ram}
(Section~\ref{sec:proof:ram_insertion}), then the forward simulation between HTL
and Verilog is shown in Lemma~\ref{lemma:verilog}
(Section~\ref{sec:proof:htl_verilog}), and finally, the proof that Verilog is
deterministic is given in Lemma~\ref{lemma:deterministic}
(Section~\ref{sec:proof:deterministic}).

\subsection{Forward Simulation from 3AC to HTL}\label{sec:proof:3ac_htl}

As HTL is quite far removed from 3AC, this first translation is the most
involved and therefore requires a larger proof, because the translation from 3AC
instructions to Verilog statements needs to be proven correct in this step.  In
addition to that, the semantics of HTL are also quite different to the 3AC
semantics. Instead of defining small-step semantics for each construct in
Verilog, the semantics are defined over one clock cycle and mirror the semantics
defined for Verilog.  Lemma~\ref{lemma:htl} shows the result that needs to be
proven in this subsection.

\begin{lemma}[Forward simulation from 3AC to HTL]\label{lemma:htl}
  Writing \texttt{tr\_htl} for the translation from 3AC to HTL, we have:
  \begin{equation*}
    \forall c, h, B \in \texttt{Safe},\quad \yhfunction{tr\_htl} (c) = \yhconstant{OK} (h) \land c \Downarrow B \implies h \Downarrow B.
  \end{equation*}
\end{lemma}

\begin{proof}[Proof sketch]
  We prove this lemma by first establishing a specification of the translation
  function $\yhfunction{tr\_htl}$ that captures its important properties, and
  then splitting the proof into two parts: one to show that the translation
  function does indeed meet its specification, and one to show that the
  specification implies the desired simulation result. This strategy is in
  keeping with standard \compcert{} practice.

  % The forward simulation is then proven by showing that the initial states and
  % final states between the 3AC semantics and HTL semantics match, and then
  % showing that the simulation diagram in Lemma~\ref{lemma:simulation_diagram}
  % holds.
\end{proof}

\subsubsection{From Implementation to Specification}\label{sec:proof:3ac_htl:specification}

%To simplify the proof, instead of using the translation algorithm as an
%assumption, as was done in Lemma~\ref{lemma:htl}, a specification of the
%translation can be constructed instead which contains all the properties that
%are needed to prove the correctness.  For example, for the translation from 3AC
%to HTL,

The specification for the translation of 3AC instructions into HTL data-path and
control logic can be defined by the following predicate:

\begin{equation*}
  \yhfunction{spec\_instr}\ \mathit{fin}\ \mathit{ret}\ \sigma\ \mathit{stk}\ i\ \mathit{data}\ \mathit{control}
\end{equation*}

\noindent Here, the $\mathit{control}$ and $\mathit{data}$ parameters are the
statements that the current 3AC instruction $i$ should translate to. The other
parameters are the special registers defined in
Section~\ref{sec:verilog:integrating}. An example of a rule describing the
translation of an arithmetic/logical operation from 3AC is the following:

\begin{equation*}
  \inferrule[Iop]{\yhfunction{tr\_op}\ \mathit{op}\ \vec{a} = \yhconstant{OK}\ e}{\yhfunction{spec\_instr}\ \mathit{fin}\ \mathit{ret}\ \sigma\ \mathit{stk}\ (\yhconstant{Iop}\ \mathit{op}\ \vec{a}\ d\ n)\ (d\ \yhkeyword{<=}\ e)\ (\sigma\ \yhkeyword{<=}\ n)}
\end{equation*}

\noindent Assuming that the translation of the operator $\mathit{op}$ with
operands $\vec{a}$ is successful and results in expression $e$, the rule
describes how the destination register $d$ is updated to $e$ via a non-blocking
assignment in the data path, and how the program counter $\sigma$ is updated to
point to the next CFG node $n$ via another non-blocking assignment in the
control logic.

In the following lemma, $\yhfunction{spec\_htl}$ is the top-level specification
predicate, which is built using $\yhfunction{spec\_instr}$ at the level of
instructions.

\begin{lemma}\label{lemma:specification}
  If a 3AC program $c$ is translated correctly to an HTL program $h$, then the
  specification of the translation holds.
  \begin{equation*}
    \forall c, h,\quad \yhfunction{tr\_htl} (c) = \yhconstant{OK}(h) \implies \yhfunction{spec\_htl}\ c\ h.
  \end{equation*}
\end{lemma}

%\begin{proof}
%  Follows from the definition of the specification and therefore should match
%  the implementation of the algorithm.
%\end{proof}

\subsubsection{From Specification to Simulation}

To prove that the specification predicate implies the desired forward
simulation, we must first define a relation that matches each 3AC state to an
equivalent HTL state.  This relation also captures the assumptions made about
the 3AC code that we receive from
\compcert{}. % so that these assumptions can be used to prove the translations correct.
These assumptions then have to be proven to always hold assuming the HTL code
was created by the translation algorithm.  Some of the assumptions that need to
be made about the 3AC and HTL code for a pair of states to match are:

\begin{itemize}
\item The 3AC register file $R$ needs to be `less defined' than the HTL register
  map $\Gamma_{r}$ (written $R \le \Gamma_{r}$). This means that all entries
  should be equal to each other, unless a value in $R$ is undefined, in which
  case any value can match it.
\item The RAM values represented by each Verilog array in $\Gamma_{a}$ need to
  match the 3AC function's stack contents, which are part of the memory $M$;
  that is: $M \le \Gamma_{a}$.
\item The state is well formed, which means that the value of the state register
  matches the current value of the program counter; that is:
  $\mathit{pc} = \Gamma_{r}[\sigma]$.
\end{itemize}

We also define the following set $\mathcal{I}$ of invariants that must hold for
the current state to be valid:

\begin{itemize}
\item that all pointers in the program use the stack as a base pointer,
\item that any loads or stores to locations outside of the bounds of the stack
  result in undefined behaviour (and hence we do not need to handle them),
\item that $\mathit{rst}$ and $\mathit{fin}$ are not modified and therefore stay
  at a constant 0 throughout execution, and
\item that the stack frames match.
\end{itemize}

We can now define the simulation diagram for the translation. The 3AC state can
be represented by the tuple $(R,M,\mathit{pc})$, which captures the register
file, memory, and program counter. The HTL state can be represented by the pair
$(\Gamma_{r}, \Gamma_{a})$, which captures the states of all the registers and
arrays in the module.  Finally, $\mathcal{I}$ stands for the other invariants
that need to hold for the states to match.

\begin{lemma}\label{lemma:simulation_diagram}
  Given the 3AC state $(R,M,\mathit{pc})$ and the matching HTL state
  $(\Gamma_{r}, \Gamma_{a})$, assuming one step in the 3AC semantics produces
  state $(R',M',\mathit{pc}')$, there exist one or more steps in the HTL
  semantics that result in matching states $(\Gamma_{r}', \Gamma_{a}')$.  This
  is all under the assumption that the specification $\yhfunction{spec\_{htl}}$
  holds for the translation.

  \begin{center}
    \begin{tikzpicture}
      \begin{scope}
        \node[circle] (s1) at (0,2) {$R, M, \mathit{pc}$};
        \node[circle] (r1) at (10,2) {$\Gamma_{r}, \Gamma_{a}$};
        \node[circle] (s2) at (0,0) {$R', M', \mathit{pc}'$};
        \node[circle] (r2) at (10,0) {$\Gamma_{r}', \Gamma_{a}'$};
        %\node at (6.8,0.75) {+};
        \draw (s1) -- node[above] {$\mathcal{I} \land (R \le \Gamma_{r}) \land (M \le \Gamma_{a}) \land (\mathit{pc} = \Gamma_{r}[\sigma])$} ++ (r1);
        \draw[-{Latex}] ($(s1.south) + (0,0.4)$) -- ($(s2.north) - (0,0.4)$);
        \draw[-{Latex},dashed] ($(r1.south) + (0,0.2)$) to[auto, pos=0.7] node {+} ($(r2.north) - (0,0.2)$);
        \draw[dashed] (s2) -- node[above] {$\mathcal{I} \land (R' \le \Gamma_{r}') \land (M' \le \Gamma_{a}') \land (\mathit{pc}' = \Gamma_{r}'[\sigma])$} ++ (r2);
      \end{scope}
    \end{tikzpicture}
  \end{center}
\end{lemma}

\begin{proof}[Proof sketch]
  This simulation diagram is proven by induction over the operational semantics
  of 3AC, which allows us to find one or more steps in the HTL semantics that
  will produce the same final matching state.
\end{proof}

\section{BRAM Generation}%
\label{sec:hg:bram-generation}

\subsubsection{Byte- and Word-Addressable Memories}

One big difference between C and Verilog is how memory is represented.  Although
Verilog arrays use similar syntax to C arrays, they must be treated quite
differently. To make loads and stores as efficient as possible, the RAM needs to
be word-addressable, which means that an entire integer can be loaded or stored
in one clock cycle.  However, the memory model that \compcert{} uses for its
intermediate languages is
byte-addre\-ssa\-ble~\cite{blazy05_formal_verif_memor_model_c}.  If a
byte-addressable memory was used in the target hardware, which is closer to
\compcert{}'s memory model, then a load and store would instead take four clock
cycles, because a RAM can only perform one read and write per clock cycle.  It
therefore has to be proven that the byte-addressable memory behaves in the same
way as the word-addressable memory in hardware.  Any modifications of the bytes
in the \compcert{} memory model also have to be shown to modify the
word-addressable memory in the same way.  Since only integer loads and stores
are currently supported in \vericert{}, it follows that the addresses given to
the loads and stores will be multiples of four.  Translating from byte-addressed
memory to word-addressed memory can then be done by dividing the address by
four.

\subsubsection{Implementation of RAM Interface}\label{sec:algorithm:optimisation:ram}
The simplest way to implement loads and stores in \vericert{} would be to access
the Verilog array directly from within the data-path (i.e., inside the
always-block on lines 16--32 of Fig.~\ref{fig:accumulator_v}). This would be
correct, but when a Verilog array is accessed at several program points, the
synthesis tool is unlikely to detect that it can be implemented as a RAM block,
and will resort to using lots of registers instead, ruining the circuit's area
and performance.  To avert this, we arrange that the data-path does not access
memory directly, but simply sets the address it wishes to access and then
toggles the \texttt{u\_en} flag. This activates the RAM interface (lines 9--15
of Fig.~\ref{fig:accumulator_v}) on the next falling clock edge, which performs
the requested load or store. By factoring all the memory accesses out into a
separate interface, we ensure that the underlying array is only accessed from a
single program point in the Verilog code, and thus ensure that the synthesis
tool will correctly infer a RAM block.\footnote{Interestingly, the Verilog code
  shown for the RAM interface must not be modified, because the synthesis tool
  will only generate a RAM when the code matches a small set of specific
  patterns.}
%\JW{I tweaked this slightly in an attempt to clarify; please check.} %\NR{Bring forward this sentence to help with flow.}

%\JW{I think the following sentence could be cut as we've said this kind of thing a couple of times already.} Without the interface, the array would be implemented using registers, which would increase the size of the hardware considerably.

Therefore, an extra compiler pass is added from HTL to HTL to extract all the
direct accesses to the Verilog array and replace them by signals that access the
RAM interface in a separate always-block. The translation is performed by going
through all the instructions and replacing each load and store expression in
turn.  Stores can simply be replaced by the necessary wires directly. Loads are
a little more subtle: loads that use the RAM interface take two clock cycles
where a direct load from an array takes only one, so this pass inserts an extra
state after each load.

%\JW{I've called that negedge always-block the `RAM driver' in my proposed text above -- that feels like quite a nice a word for it to my mind -- what do you think?}\YH{Yes I quite like it!}
%Verilog arrays can be used in a variety of ways, however, these do not all produce optimal hardware designs.  If, for example, arrays in Verilog are accessed immediately in the data-path, then the synthesis tool is not be able to identify it as having the right properties for a RAM, and would instead implement the array using registers.  This is extremely expensive, and for large memories this can easily blow up the area usage of the FPGA, and because of the longer wires that are needed, it would also affect the performance of the circuit.  The synthesis tools therefore provide code snippets that they know how to transform into various constructs, including snippets that will generate proper RAMs in the final hardware.  This process is called memory inference.  The initial translation from 3AC to HTL converts loads and stores to direct accesses to the memory, as this preserves the same behaviour without having to insert more registers and logic.  We therefore have another pass from HTL to itself which performs the translation from this na\"ive use of arrays to a representation which always allows for memory inference.  This pass creates a separate always-block to perform the loads and stores to the memory, and adds the necessary data, address and enable signals to communicate with that always-block from other always-blocks.  This always-block is shown between lines 10-15 in Fig.~\ref{fig:accumulator_v}.

There are two interesting parts to the inserted RAM interface.  Firstly, the
memory updates are triggered on the negative (falling) edge of the clock, out of
phase with the rest of the design which is triggered on the positive (rising)
edge of the clock.  The advantage of this is that instead of loads and stores
taking three clock cycles and two clock cycles respectively, they only take two
clock cycles and one clock cycle instead, greatly improving their
performance. %\JW{Is this a standard `trick' in hardware design? If so it might be nice to cite it.}\YH{Hmm, not really, because it has the downside of kind of halving your available clock period. However, RAMs normally come in both forms on the FPGA (Page 12, Fig. 2, \url{https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/ug/ug_ram_rom.pdf})}
% JW: thanks!
Using the negative edge of the clock is widely supported by synthesis tools, and
does not affect the maximum frequency of the final design.

Secondly, the logic in the enable signal of the RAM (\texttt{en != u\_en}) is
also atypical in hardware designs.  Enable signals are normally manually
controlled and inserted into the appropriate states, by using a check like the
following in the RAM:\@ \texttt{en == 1}.  This means that the RAM only turns on
when the enable signal is set.  However, to make the proof simpler and avoid
reasoning about possible side effects introduced by the RAM being enabled but
not used, a RAM which disables itself after every use would be ideal.  One
method for implementing this would be to insert an extra state after each load
or store that disables the RAM, but this extra state would eliminate the speed
advantage of the negative-edge-triggered RAM. Another method would be to
determine the next state after each load or store and disable the RAM in that
state, but this could quickly become complicated, especially in the case where
the next state also contains a memory operation, and hence the disable signal
should not be added. The method we ultimately chose was to have the RAM become
enabled not when the enable signal is high, but when it \emph{toggles} its
value.  This can be arranged by keeping track of the old value of the enable
signal in \texttt{en} and comparing it to the current value \texttt{u\_en} set
by the data-path.  When the values are different, the RAM gets enabled, and then
\texttt{en} is set to the value of \texttt{u\_en}. This ensures that the RAM
will always be disabled straight after it was used, without having to insert or
modify any other states.

%We can instead generate a second enable signal that is set by the user, and the original enable signal is then updated by the RAM to be equal to the value that the user set.  This means that the RAM should be enabled whenever the two signals are different, and disabled otherwise.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.48\linewidth}
    \begin{tikztimingtable}[timing/d/background/.style={fill=white}]
      \small clk & 2L 3{6C} \\
      \small u\_en & 2D{u\_en} 18D{$\overline{\text{u\_en}}$}\\
      \small addr & 2U 18D{3} \\
      \small wr\_en & 2U 18L \\
      \small en & 8D{u\_en} 12D{$\overline{\text{u\_en}}$}\\
      \small d\_out & 8U 12D{0xDEADBEEF} \\
      \small r & 14U 6D{0xDEADBEEF} \\
      \extracode
      \node[help lines] at (2,2.25) {\tiny 1};
      \node[help lines] at (8,2.25) {\tiny 2};
      \node[help lines] at (14,2.25) {\tiny 3};
      \begin{pgfonlayer}{background}
        \vertlines[help lines]{2,8,14}
      \end{pgfonlayer}
    \end{tikztimingtable}
    \caption{Timing diagram for loads. At time 1, the \texttt{u\_en} signal is toggled to enable the RAM. At time 2, \texttt{d\_out} is set to the value stored at the address in the RAM, which is finally assigned to the register \texttt{r} at time 3.}\label{fig:ram_load}
  \end{subfigure}\hfill%
  \begin{subfigure}[b]{0.48\linewidth}
    \begin{tikztimingtable}[timing/d/background/.style={fill=white}]
      \small clk & 2L 2{7C} \\
      \small u\_en & 2D{u\_en} 14D{$\overline{\text{u\_en}}$}\\
      \small addr & 2U 14D{3} \\
      \small wr\_en & 2U 14H \\
      \small d\_in & 2U 14D{0xDEADBEEF} \\
      \small en & 9D{u\_en} 7D{$\overline{\text{u\_en}}$}\\
      \small stack[addr] & 9U 7D{0xDEADBEEF} \\
      \extracode
      \node[help lines] at (2,2.25) {\tiny 1};
      \node[help lines] at (9,2.25) {\tiny 2};
      \begin{pgfonlayer}{background}
        \vertlines[help lines]{2,9}
      \end{pgfonlayer}
    \end{tikztimingtable}
    \caption{Timing diagram for stores. At time 1, the \texttt{u\_en} signal is toggled to enable the RAM, and the address \texttt{addr} and the data to store \texttt{d\_in} are set. On the negative edge at time 2, the data is stored into the RAM.}\label{fig:ram_store}
  \end{subfigure}
%  \alt{Timing diagrams of loads and stores, showing which signals are modified at which time step.}
  \caption{Timing diagrams showing the execution of loads and stores over multiple clock cycles.}\label{fig:ram_load_store}
\end{figure}

Fig.~\ref{fig:ram_load_store} gives an example of how the RAM interface behaves when values are loaded and stored.

\subsection{BRAM Model Semantics}%
\label{sec:hg:bram-model-semantics}

\subsection{BRAM Generation and Correctness Proof}%
\label{sec:hg:bram-generation-and-correctness-proof}

\subsection{Forward Simulation of RAM Insertion}\label{sec:proof:ram_insertion}

\begin{figure}
  \centering
  \begin{minipage}{1.0\linewidth}
    \begin{gather*}
      \inferrule[Idle]{\Gamma_{\mathrm{r}}[\mathit{r.en}] = \Gamma_{\mathrm{r}}[\mathit{r.u_{en}}]}{((\Gamma_{\mathrm{r}}, \Gamma_{\mathrm{a}}), \Delta, r) \downarrow_{\text{ram}} \Delta}\\
%
      \inferrule[Load]{\Gamma_{\mathrm{r}}[\mathit{r.en}] \ne \Gamma_{\mathrm{r}}[\mathit{r.u_{en}}] \\ \Gamma_{\mathrm{r}}[\mathit{r.wr_{en}}] = 0}{((\Gamma_{\mathrm{r}}, \Gamma_{\mathrm{a}}), (\Delta_{\mathrm{r}}, \Delta_{\mathrm{a}}), r) \downarrow_{\text{ram}} (\Delta_{\mathrm{r}}[\mathit{r.en} \mapsto \mathit{r.u_{en}}, \mathit{r.d_{out}} \mapsto (\Gamma_{\mathrm{a}}[\mathit{r.mem}])[ \mathit{r.addr}]], \Delta_{\mathrm{a}}) }\\
%
      \inferrule[Store]{\Gamma_{\mathrm{r}}[\mathit{r.en}] \ne \Gamma_{\mathrm{r}}[\mathit{r.u_{en}}] \\ \Gamma_{\mathrm{r}}[\mathit{r.wr_{en}}] = 1}{((\Gamma_{\mathrm{r}}, \Gamma_{\mathrm{a}}), (\Delta_{\mathrm{r}}, \Delta_{\mathrm{a}}), r) \downarrow_{\text{ram}} (\Delta_{\mathrm{r}}[\mathit{r.en} \mapsto \mathit{r.u\_en}], \Delta_{\mathrm{a}}[\mathit{r.mem} \mapsto (\Gamma_{\mathrm{a}}[ \mathit{r.mem}])[\mathit{r.addr} \mapsto \mathit{r.d_{in}}]]) }
    \end{gather*}
  \end{minipage}
  \caption{Specification for the memory implementation in HTL, where $r$ is the RAM, which is then implemented by equivalent Verilog code.}\label{fig:htl_ram_spec}
\end{figure}

HTL can only represent a single state machine, so we must model the RAM
abstractly to reason about the correctness of replacing the direct read and
writes to the array by loads and stores to a RAM.  The specification for the RAM
is shown in Fig.~\ref{fig:htl_ram_spec}, which defines how the RAM $r$ will
behave for all the possible combinations of the input signals.

\subsubsection{From Implementation to Specification}

The first step in proving the simulation correct is to build a specification of
the translation algorithm.  There are three possibilities for the transformation
of an instruction. For each Verilog statement in the map at location $i$, the
statement is either a load, a store, or neither. The load or store is translated
to the equivalent representation using the RAM specification and all other
instructions are left intact.  An example of the specification for the
translation of the store instruction is shown below, where $\sigma$ is state
register, $r$ is the RAM, $d$ and $c$ are the input data-path and control logic
maps, and $i$ is the current state. ($n$ is the newly inserted state, which only
applies to the translation of loads.)

\begin{gather*}
  \inferrule[Store Spec]{ d[i] = (r.mem\texttt{[}e_{1}\texttt{]} \texttt{ <= } e_{2}) \\ t = (r.u\_en \texttt{ <= } \neg r.u\_en; r.wr\_en \texttt{ <= } 1; r.d\_in \texttt{ <= } e_{2}; r.addr \texttt{ <= } e_{1})}%
  {\yhfunction{spec\_ram\_tr}\ \sigma\ r\ d\ c\ d[i \mapsto t]\ c\ i\ n}
\end{gather*}

A similar specification is created for the load.  We then also prove that the
implementation of the translation proves that the specification holds.

\subsubsection{From Specification to Simulation}

Another simulation proof is performed to prove that the insertion of the RAM is
semantics preserving.  As in Lemma~\ref{lemma:simulation_diagram}, we require
some invariants that always hold at the start and end of the simulation.  The
invariants needed for the simulation of the RAM insertion are quite different to
the previous ones, so we can define these invariants $\mathcal{I}_{r}$ to be the
following:

\begin{itemize}
\item The association map for arrays $\Gamma_{a}$ always needs to have the same
  arrays present, and these arrays should never change in size.
\item The RAM should always be disabled at the start and the end of each
  simulation step. (This is why self-disabling RAM is needed.)
\end{itemize}

The other invariants and assumptions for defining two matching states in HTL are
quite similar to the simulation performed in
Lemma~\ref{lemma:simulation_diagram}, such as ensuring that the states have the
same value, and that the values in the registers are less defined.  In
particular, the less defined relation matches up all the registers, except for
the new registers introduced by the RAM.

\begin{lemma}[Forward simulation from HTL to HTL after inserting the RAM]\label{lemma:htl_ram}
  Given an HTL program, the forward-simulation relation should hold after
  inserting the RAM and wiring the load, store, and control signals.

  \begin{align*}
    \forall h, h', B \in \texttt{Safe},\quad \yhfunction{tr\_ram\_ins}(h) = h' \land h \Downarrow B \implies h' \Downarrow B.
  \end{align*}
\end{lemma}

\section{Register Forward Substitution}%
\label{sec:hg:register-forward-substitution}

\subsection{Forward Substitution Correctness Proof}%
\label{sec:hg:forward-substitution-correctness-proof}

\section{Verilog Generation}%
\label{sec:hg:verilog-generation}

\subsection{Forward Simulation from HTL to Verilog}%
\label{sec:proof:htl_verilog}

The HTL-to-Verilog simulation is conceptually simple, as the only transformation
is from the map representation of the code to the case-statement representation.
The proof is more involved, as the semantics of a map structure is quite
different to that of the case-statement to which it is converted.

%\YH{Maybe want to split this up into two lemmas?  One which states the proof about the map property of uniqueness of keys, and another proving the final theorem?}
\begin{lemma}[Forward simulation from HTL to Verilog]\label{lemma:verilog}
  In the following, we write $\yhfunction{tr\_verilog}$ for the translation from
  HTL to Verilog. (Note that this translation cannot fail, so we do not need the
  \yhconstant{OK} constructor here.)
  \begin{align*}
    \forall h, V, B \in \texttt{Safe},\quad \yhfunction{tr\_verilog} (h) = V \land h \Downarrow B \implies V \Downarrow B.
  \end{align*}
\end{lemma}

\begin{proof}[Proof sketch]
  The translation from maps to case-statements is done by turning each node of
  the tree into a case-expression containing the same statements.  The main
  difficulty is that a random-access structure is being transformed into an
  inductive structure where a certain number of constructors need to be called
  to get to the correct case.
  % \JW{I would chop from here.}\YH{Looks good to me.}  The proof of the
  % translation from maps to case-statements follows by induction over the list
  % of elements in the map and the fact that each key will be unique.  In
  % addition to that, the statement that is currently being evaluated is
  % guaranteed by the correctness of the list of elements to be in that list.
  % The latter fact therefore eliminates the base case, as an empty list does
  % not contain the element we know is in the list.  The other two cases follow
  % by the fact that either the key is equal to the evaluated value of the case
  % expression, or it isn't.  In the first case we can then evaluate the
  % statement and get the state after the case expression, as the uniqueness of
  % the key tells us that the key cannot show up in the list anymore.  In the
  % other case we can just apply the inductive hypothesis and remove the current
  % case from the case statement, as it did not match.
\end{proof}

%\subsection{Coq Mechanisation}

%\JW{Would be nice to include a few high-level metrics here. How many person-years of effort was the proof (very roughly)? How many lines of Coq? How many files, how many lemmas? How long does it take for the Coq proof to execute?}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% TeX-engine: luatex
%%% End:
