\chapter{Background}%
\label{sec:background}

\begin{chapsummary}
  This chapter briefly describes \glspl{FPGA} followed by introducing
  \glsfirst{HLS} and the current state-of-the-art optimisations used by
  \gls{HLS} tools, focusing in particular on static scheduling.  Next, common
  testing and verification workflows for \gls{HLS} are also described.  Finally,
  an overview of \compcert{} is given, on which \vericert{} is built.
\end{chapsummary}

\section{Field Programmable Gate Arrays}%
\label{sec:bg:fpga}

This section introduces \glsfirstplural{FPGA}, which is assumed to be the final
target for the hardware produced by Vericert, as well as the \gls{HLS} tools
that Vericert is directly compared against.

\Glspl{FPGA} are programmable hardware chips that can be used to implement and
run custom hardware without having to tape-out an \gls{ASIC}, which may take
years of development time.  \Glspl{FPGA} instead provide a platform to test
custom hardware quickly without these turnaround times, and can be reprogrammed
at will in case the hardware ever needs to change.  Because they still allow for
reprogrammability, they can never be as efficient as an equivalent \gls{ASIC}
design, however, for many applications having the chance to reprogram the
hardware is an advantage.  In addition to that, an \gls{FPGA} will still
generally be more efficient and more performant than running the same workload
on a general-purpose processor.  \Glspl{FPGA} comprise the following four main
components~\cite{boutros21_fpga_archit}, which are also shown in
\cref{fig:bg:fpga-layout}.

\begin{description}
\item[\Gls{LUT}] A \gls{LUT} can implement any kind of logic with a set number
  of inputs and an output.  On an \gls{FPGA}, \glspl{LUT} are often grouped into
  larger programmable logic units called \emph{slices} that can handle multiple
  inputs and outputs.  \Glspl{LUT} are also normally paired with an optional
  register at the output so that they can also be used as a memory.
\item[Programmable interconnect] The \glspl{LUT} are connected using
  programmable interconnects, so that these arbitrary logical units can also be
  connected in arbitrary ways, making it possible to implement any kind of
  hardware design.
\item[\Gls{BRAM}] Instead of relying on implementing memories to store a large
  amount of data using \glspl{LUT}, there is often \gls{BRAM} on the \gls{FPGA},
  which provides efficient storage for data.
\item[\Gls{DSP}] Finally, \glspl{FPGA} also often contain \glspl{DSP}, which can
  be used to implement common arithmetic functions efficiently, that may
  otherwise take up a lot of space if implemented using \glspl{LUT}.  Some
  common arithmetic functions that are often implemented using \gls{DSP} include
  integer multipliers and multiply-accumulate operations.
\end{description}

\definecolor{connblockcolour}{HTML}{E6CEE9}
\definecolor{lutcolour}{HTML}{E9D8CE}
\definecolor{dspcolour}{HTML}{D2E9CE}
\definecolor{switchcolour}{HTML}{CEDFE9}
\definecolor{bramcolour}{HTML}{D2D1E9}

\begin{figure}
  \centering
  \begin{tikzpicture}[yscale=-1,connblock/.style={draw=black,fill=connblockcolour},
    lut/.style={draw=black,fill=lutcolour},switch/.style={draw=black,fill=switchcolour},
    dsp/.style={draw=black,fill=dspcolour},bram/.style={draw=black,fill=bramcolour},
    emphasize/.style={very thick,draw=Tomato},
    blocklabel/.style={font=\ttfamily\small}]
    \foreach \w in {0,...,3}
    {\pgfmathtruncatemacro{\sw}{2 * \w}
     \foreach \z in {0,...,2}
     {\pgfmathtruncatemacro{\sz}{\z}
       \draw ($(0.5,\sw)+(0,0.\sz)+(0,0.4)$) -- ($(6.5,\sw)+(0,0.\sz)+(0,0.4)$);
       \draw ($(\sw,0.5)+(0.\sz,0)+(0.4,0)$) -- ($(\sw,6.5)+(0.\sz,0)+(0.4,0)$);
       \ifnum\w=3
       \else
         \draw ($(\sw,0.5)+(0.\sz,0)+(1.4,0)$) -- ($(\sw,6.5)+(0.\sz,0)+(1.4,0)$);
         \draw ($(0.5,\sw)+(0,0.\sz)+(0,1.4)$) -- ($(6.5,\sw)+(0,0.\sz)+(0,1.4)$);
       \fi
       }}
    \foreach \z in {0,...,2}
     {\pgfmathtruncatemacro{\sz}{\z}
       \draw[emphasize] ($(2.5,0.5)+(0,0.\sz)-(0,0.1)$) --
       ($(6.5,0.5)+(0,0.\sz)-(0,0.1)$);
       \draw[emphasize] ($(2.5,2.5)+(0,0.\sz)-(0,0.1)$) --
       ($(4.5,2.5)+(0,0.\sz)-(0,0.1)$);
       \draw[emphasize] ($(3.5,1.5)+(0,0.\sz)-(0,0.1)$) --
       ($(5.5,1.5)+(0,0.\sz)-(0,0.1)$);
       \draw[emphasize] ($(3.5,0.5)+(0.\sz,0)-(0.1,0)$) --
       ($(3.5,2.5)+(0.\sz,0)-(0.1,0)$);
       \draw[emphasize] ($(5.5,0.5)+(0.\sz,0)-(0.1,0)$) --
       ($(5.5,1.5)+(0.\sz,0)-(0.1,0)$);
%       \draw ($(\sw,0.5)+(0.\sz,0)+(0.4,0)$) -- ($(\sw,6.5)+(0.\sz,0)+(0.4,0)$);
%       \draw ($(\sw,0.5)+(0.\sz,0)+(1.4,0)$) -- ($(\sw,6.5)+(0.\sz,0)+(1.4,0)$);
%       \draw ($(0.5,\sw)+(0,0.\sz)+(0,1.4)$) -- ($(6.5,\sw)+(0,0.\sz)+(0,1.4)$);
       }
    \foreach \x in {0,...,3}
    \foreach \y in {0,...,3}
    {\pgfmathtruncatemacro{\sx}{2 * \x}
      \pgfmathtruncatemacro{\sy}{2 * \y}
      \filldraw[lut] (\sx,\sy) rectangle ($(\sx,\sy)+(1,1)$);
      \node[blocklabel] at ($(\sx,\sy)+(0.5,0.5)$) {LUT};
      \ifnum\x=3
        \ifnum\y=3
          %
        \else
          \filldraw[connblock] ($(\sx,\sy)+(0.3,1.3)$) rectangle ($(\sx,\sy)+(0.7,1.7)$);
        \fi
        \else
        \ifnum\y=3
          \filldraw[connblock] ($(\sx,\sy)+(1.3,0.3)$) rectangle ($(\sx,\sy)+(1.7,0.7)$);
        \else
          \filldraw[connblock] ($(\sx,\sy)+(1.3,0.3)$) rectangle ($(\sx,\sy)+(1.7,0.7)$);
          \filldraw[connblock] ($(\sx,\sy)+(0.3,1.3)$) rectangle ($(\sx,\sy)+(0.7,1.7)$);
          \filldraw[switch] ($(\sx,\sy)+(1.2,1.2)$) rectangle
          ($(\sx,\sy)+(1.8,1.8)$);
          \node[blocklabel] at ($(\sx,\sy)+(1.5,1.5)$) {SW};
        \fi
      \fi
    }
    \filldraw[dsp] (2,0) rectangle ($(2,0)+(1,1)$);
    \node[blocklabel] at ($(2,0)+(0.5,0.5)$) {DSP};
    \filldraw[bram] (2,2) rectangle ($(2,2)+(1,1)$);
    \node[blocklabel] at ($(2,2)+(0.5,0.5)$) {BRAM};
    \filldraw[dsp] (4,6) rectangle ($(4,6)+(1,1)$);
    \node[blocklabel] at ($(4,6)+(0.5,0.5)$) {DSP};
    \filldraw[bram] (4,4) rectangle ($(4,4)+(1,1)$);
    \node[blocklabel] at ($(4,4)+(0.5,0.5)$) {BRAM};
    \foreach \v in {{2,2},{2,0},{4,0},{4,2},{6,0}}
    {\draw[emphasize] (\v) rectangle ($(\v)+(1,1)$);}
    \foreach \v in {{2,2},{2,0},{4,0}}
    {\draw[emphasize] ($(\v)+(1.3,0.3)$) rectangle ($(\v)+(1.7,0.7)$);}
    \foreach \v in {{2,0},{4,0}}
    {\draw[emphasize] ($(\v)+(1.2,1.2)$) rectangle ($(\v)+(1.8,1.8)$);}
    \foreach \v in {{4,0}}
    {\draw[emphasize] ($(\v)+(0.3,1.3)$) rectangle ($(\v)+(0.7,1.7)$);}
  \end{tikzpicture}
  \caption[FPGA layout showing a place and routed design.]{\Gls{FPGA} layout,
    showing an example of a design that is placed and routed on the \gls{FPGA}
    highlighted in red.  The beige blocks correspond to \glspl{LUT}, followed by
    green blocks being \glspl{DSP} and purple blocks being \glspl{BRAM}.  The
    programmable interconnects are made up of connection blocks and switches
    shown in pink and blue respectively.}%
  \label{fig:bg:fpga-layout}
\end{figure}

The standard process to translate a hardware design from an \gls{HDL}, such as
Verilog or VHDL, to then be placed onto an \gls{FPGA} is to first
\emph{synthesise} the hardware design, which generates a lower level description
of the hardware in terms of the resources that are available on the \gls{FPGA}.
Next, the netlist is place-and-routed on the \gls{FPGA}, which assigns a
physical location to each resource and programs the interconnects so that all
the components are connected properly.  This low-level description of the
hardware is then turned into a bit stream that will program all the individual
resources on the \gls{FPGA}.  The result can be seen in
\cref{fig:bg:fpga-layout} by looking at the highlighted paths in red, as the
place-and-route process placed logical functions into \glspl{LUT} and connected
them together correctly, also making use of a \gls{BRAM} and a \gls{DSP}.

\section{An Introduction to Verilog}%
\label{sec:bg:intro-to-verilog}

This section will introduce Verilog for readers who may not be familiar with the
language, concentrating on the features that are used in the output of
\vericert{}.  Verilog is an \gls{HDL} and is used to design hardware ranging
from complete CPUs that are eventually produced as integrated circuits, to small
application-specific accelerators that are placed on FPGAs.  Verilog is a
popular language because it allows for fine-grained control over the hardware,
and also provides high-level constructs to simplify development.

Verilog behaves quite differently to standard software programming languages due
to it having to express the parallel nature of hardware.  The basic construct to
achieve this is the always-block, which is a collection of assignments that are
executed every time some event occurs.  In the case of \vericert{}, this event
is either a positive (rising) or a negative (falling) clock edge.  All
always-blocks triggering on the same event are executed in
parallel. Always-blocks can also express control flow using if-statements and
case-statements.

% \NR{Might be useful to talk about registers must be updated only within an
% always-block.} \JW{That's important for Verilog programming in general, but is
% it necessary for understanding this paper?}\YH{Yeah, I don't think it is too
% important for this section.}

\begin{figure}
  \centering
  \begin{subfigure}{0.55\linewidth}
\begin{minted}[linenos,xleftmargin=20pt,fontsize=\footnotesize]{verilog}
module main(input rst, input y, input clk,
            output reg z);
  reg tmp, state;
  always @(posedge clk)
    case (state)
      1'b0: tmp <= y;
      1'b1: begin tmp <= 1'b0; z <= tmp; end
    endcase
  always @(posedge clk)
    if (rst) state <= 1'b0;
    else case (state)
      1'b0: if (y) state <= 1'b1;
            else state <= 1'b0;
      1'b1: state <= 1'b0;
    endcase
endmodule
\end{minted}
  \end{subfigure}\hfill%
  \begin{subfigure}{0.45\linewidth}
    \centering
    \begin{tikzpicture}
      \node[draw,circle,inner sep=6pt] (s0) at (0,0) {$S_{\mathit{start}} / \texttt{x}$};
      \node[draw,circle,inner sep=8pt] (s1) at (1.5,-3) {$S_{1} / \texttt{1}$};
      \node[draw,circle,inner sep=8pt] (s2) at (3,0) {$S_{0} / \texttt{1}$};
      \node (s2s) at ($(s2.west) + (-0.3,1)$) {\texttt{00}};
      \node (s2ss) at ($(s2.east) + (0.3,1)$) {\texttt{1x}};
      \draw[-{Latex[length=2mm,width=1.4mm]}] ($(s0.west) + (-0.3,1)$) to [out=0,in=120] (s0);
      \draw[-{Latex[length=2mm,width=1.4mm]}] (s0)
           to [out=-90,in=150] node[midway,left] {\texttt{01}} (s1);
      \draw[-{Latex[length=2mm,width=1.4mm]}] (s1)
      to [out=80,in=220] node[midway,left] {\texttt{xx}} (s2);
      \draw[-{Latex[length=2mm,width=1.4mm]}] (s2)
      to [out=260,in=50] node[midway,right] {\texttt{01}} (s1);
      \draw[-{Latex[length=2mm,width=1.4mm]}] (s2)
      to [out=120,in=40] ($(s2.west) + (-0.3,0.7)$) to [out=220,in=170] (s2);
      \draw[-{Latex[length=2mm,width=1.4mm]}] (s2)
      to [out=60,in=130] ($(s2.east) + (0.3,0.7)$) to [out=310,in=10] (s2);
    \end{tikzpicture}
  \end{subfigure}
%  \alt{Verilog code of a state machine, and its equivalent state machine diagram.}
  \caption[A simple Verilog implementation of a finite-state machine.]{A simple
    state machine implemented in Verilog, with its diagrammatic representation
    on the right. The \texttt{x} stands for ``don't care'' and each transition
    is labelled with the values of the inputs \texttt{rst} and \texttt{y} that
    trigger the transition.  The output that will be produced is shown in each
    state.}%
  \label{fig:tutorial:state_machine}
\end{figure}


A simple state machine can be implemented as shown in
\cref{fig:tutorial:state_machine}.  At every positive edge of the clock
(\texttt{clk}), both of the always-blocks will trigger simultaneously.  The
first always-block controls the values in the register \texttt{tmp} and the
output \texttt{z}, while the second always-block controls the next state the
state machine should go to.  When the \texttt{state} is 0, \texttt{tmp} will be
assigned to the input \texttt{y} using nonblocking assignment, denoted by
\texttt{<=}.  Nonblocking assignment assigns registers in parallel at the end of
the clock cycle, rather than sequentially throughout the always-block. In the
second always-block, the input \texttt{y} will be checked, and if it's high it
will move on to the next state, otherwise it will stay in the current state.
When \texttt{state} is 1, the first always-block will reset the value of
\texttt{tmp} and then set \texttt{z} to the original value of \texttt{tmp},
since nonblocking assignment does not change its value until the end of the
clock cycle.  Finally, the last always-block will set the state to 0 again.

\section{High-Level Synthesis}%
\label{sec:bg:hls}

\Glsfmtlong{HLS} is the transformation of software directly into hardware.
There are many different types of \gls{HLS}, which can vary in terms of the
languages they accept or the devices that are targeted, however, they often
share similar steps in how the translation is performed, as they all go from a
higher level, behavioural description of the algorithm to a timed hardware
description.  In this dissertation, I will assume that I am targeting
\glspl{FPGA} instead of \glspl{ASIC}, which lead to different resources that are
available to the \gls{HLS} tool to target.

The main steps performed in the translation of an \gls{HLS} tool is the
following~\cite{coussy09_introd_to_high_level_synth,canis13_l}:

\begin{description}
\item[Compilation of input language] First, the program or specification written
  in the input language to the \gls{HLS} tool is compiled into an intermediate
  language that is more suitable to be transformed by optimisations.  The input
  language for most traditional \gls{HLS} tools is a restricted version of C or
  C++, however, \gls{HLS} tools such as Google XLS~\cite{google23_xls} can use a
  \gls{DSL} based on communicating sequential
  processes~\cite{hoare78_commun_sequen_proces} as an input specification as
  well.  This specification is then turned into some intermediate language such
  as the LLVM \gls{IR}~\cite{lattner04_llvm}, MLIR~\cite{lattner21_mlir} or a
  custom representation of the code.  The structure of these intermediate
  languages is further discussed in \cref{sec:bg:intermediate-language}.

\item[Hardware resource allocation] Depending on if the hardware target is a
  specific \gls{FPGA} or an \gls{ASIC} built on a specific technology library,
  the \gls{HLS} tool will have to allocate resources differently.  For example,
  on \glspl{FPGA} there are only a limited number of \glspl{LUT} and \glspl{DSP}
  available.  The \gls{HLS} tool therefore often decides ahead of time which
  resources the program will need based on the operations that are present in
  the program.  A few resources are often assumed to be infinite to simplify the
  resource allocation, examples being registers, which are normally cheap
  especially on \glspl{FPGA}, or simple logic circuits that are cheap enough to
  be duplicated or may have dedicated hardware on the FPGA such as integer
  adders or multiplexers.  Other circuits may require more resources in which
  case it would make sense to only have a few instantiations of the circuit and
  share it as much as possible.  Examples of these circuits could be integer
  division modules or floating point arithmetic units that will most likely not
  have dedicated hardware on the \gls{FPGA} and would therefore have to be
  implemented on the \gls{FPGA}.  These circuits also often have trade-offs
  between area, latency and throughput which should be considered, and will be
  important in the operation scheduling step.  Finally, memory and
  multiplication units lie in the middle, where there are usually enough
  \glspl{BRAM} or \glspl{DSP} resources on the \gls{FPGA}, but they might
  introduce different challenges such leading to designs that are more difficult
  to place-and-route.  In particular, if one uses too many of these resources,
  \glspl{BRAM} and operations in the \glspl{DSP} can be implemented using
  \glspl{LUT}.

\item[Operation scheduling] Once the available resources have been chosen, the
  operations in the intermediate representation need to be scheduled into a
  clock cycle based on these resource constraints, creating a timed
  representation of the program.  The goal of the operation scheduling step is
  to maximise the instruction-level parallelism of the program while also
  honouring the various resource constraints that are imposed by the available
  resources.  Scheduling is further described in \cref{sec:bg:scheduling}.  As
  part of the scheduling step, operations are also often bound to specific
  resources.

\item[Resource binding] After scheduling, each operation is assigned a concrete
  instantiation of its resource.  For example, an integer divide operation will
  be assigned to the integer divider resource, and if this resource is used by
  another divider in the design, the inputs and outputs will have to be
  multiplexed.  The resource constrained scheduling step should have ensured
  that two divisions are not taking place in the same cycle, and that the result
  of the division will only be used when the divider resource has finished
  computing the result.

\item[Hardware description generation] Finally, the hardware description is
  generated from the code that was described in the intermediate language and
  from the states and resources that each operation and register was assigned
  to.
\end{description}

There are many examples of existing high-level synthesis tools, the most popular
ones being Bambu HLS~\cite{pilato13_bambu}, LegUp~\cite{canis13_l}, Vitis
HLS~\cite{amd23_vitis_high_synth}, Catapult
C~\cite{mentor20_catap_high_level_synth}, Google XLS~\cite{google23_xls} and
Intel's OpenCL SDK~\cite{intel20_sdk_openc_applic}.  These HLS tools all accept
general programming languages such as C/C++ or OpenCL.

The concept of \gls{HLS} has also evolved over time, from describing
synthesising the behavioural level of Verilog and VHDL to the register transfer
level in the 90s, to synthesising C code into hardware automatically like tools
do today, as synthesis tools have accepted increasingly larger subsets of
Verilog and VHDL that include the behavioural level.  In addition to that,
languages like Bluespec~\cite{nikhil04_bsv} are also \gls{HLS} tools despite
having different goals to more traditional \gls{HLS} tools accepting C code.
Bluespec provides a high-level hardware specification language that can be used
to define concurrently running rules.  These are then automatically scheduled by
the Bluespec synthesis tool and converted to a traditional synthesisable
\gls{HDL}.  Handel-C~\cite{aubury96_handel_c_languag_refer_guide,bowen98_hclrm}
is in a similar position, as is a C-like language for hardware development.  It
supports many C features such as assignments, if-statements, loops, pointers and
functions.  In addition to these constructs, Handel-C also supports explicit
timing constructs, such as explicit concurrency as well as sequential or
parallel assignment, similar to blocking and nonblocking assignments in Verilog.
It therefore was popular as a hardware/software co-design language as the
language could naturally be used to define sequential code running in software
and parallel code which could be synthesised to hardware.  However, it is
inherently timed, and the Handel-C synthesis tool does not usually perform
automatic parallelisation of the code as is the case with \gls{HLS} tools that
accept C as input.

In this dissertation I will be focusing on the more traditional \gls{HLS}
conversion from software languages into hardware designs.

\subsection{Data structures for intermediate languages}%
\label{sec:bg:data-structures-for-intermediate-languages}

This \namecref{sec:bg:data-structures-for-intermediate-languages} describes how
instructions inside functions can be represented and the differences in these
approaches, especially when integrated into an \gls{HLS} tool.  Next, in
\cref{sec:bg:intermediate-language} I will describe techniques used to group
instructions into a contiguous blocks to simplify analyses and transformations
of instructions within the blocks.

There are many ways to represent the code that makes up a function or a program.
High-level languages that are written by the programmer are normally represented
as an \gls{AST} in the front end of the compiler, which is a tree representing
the parsed source file.  An \gls{AST} is a good representation for high-level
optimisations that may need information about the exact intent of the
programmer, for example that a loop is indeed a structured for-loop instead of
unstructured goto statements.  On the opposite end, the assembly that will run
on the processor can simply be represented by a list of instructions.  This
simple representation of the program is useful because individual instructions
can directly be stored contiguously in memory and are then loaded by the
processor to be executed.  However, storing instructions as a list means that
much of the original structure of the program is lost, which makes analysing
programs and transforming them more difficult.  In between these two
representations, there is often one or more intermediate languages that can be
represented in a variety of ways, and are either used purely for analysis or
also as an intermediate transformation step.

\definecolor{nodea}{HTML}{81B8BF}
\definecolor{nodeb}{HTML}{A681BF}
\definecolor{nodec}{HTML}{BF8781}
\definecolor{noded}{HTML}{99BF81}
\definecolor{nodee}{HTML}{9FA5CE}
\definecolor{nodef}{HTML}{CE9FBC}
\definecolor{nodeg}{HTML}{CEC89F}
\definecolor{nodeh}{HTML}{9FCEB1}

\tikzset{instrnode/.style={draw=black,fill=white,circle,inner sep=3pt},
         bblock/.style={draw=black,fill=black!15,rounded corners}}

\begin{figure}
  \begin{subfigure}[b]{0.15\linewidth}
    \centering
    \begin{tikzpicture}[shorten >=1pt,>=Latex,node distance=0.75cm and 0.5cm]
      \node[instrnode,fill=nodea] (a) {};
      \node[instrnode,fill=nodeb,below=of a] (b) {};
      \node[instrnode,fill=noded,below=of b] (d) {};
      \node[instrnode,fill=nodec,below=of d] (c) {};
      \node[instrnode,fill=nodee,below=of c] (e) {};
      \node[instrnode,fill=nodef,below=of e] (f) {};
      \node[instrnode,fill=nodeg,below=of f] (g) {};
      \node[instrnode,fill=nodeh,below=of g] (h) {};
      \draw[->] (a) -- (b);
      \draw[->] (b) -- (d);
      \draw[->] (d) -- (c);
      \draw[->] (c) -- (e);
      \draw[->] (e) -- (f);
      \draw[->] (f) -- (g);
      \draw[->] (g) -- (h);
      \draw[->,dashed] (g) to [loop,looseness=1,out=110,in=250] (b);
      \draw[->,dashed] (b) to [loop,looseness=1,in=70,out=290] (c);
      \draw[->,dashed] (d) to [loop,looseness=1,in=70,out=290] (f);
    \end{tikzpicture}
    \caption{List}\label{fig:data-structure-comparison:list}
  \end{subfigure}\hfill%
  \begin{subfigure}[b]{0.2\linewidth}
    \centering
    \begin{tikzpicture}[shorten >=1pt,>=Latex,node distance=0.75cm and 0.5cm]
      \node[instrnode,fill=nodea] (a) {};
      \node[instrnode,fill=nodeb,below=of a] (b) {};
      \node[instrnode,fill=nodec,below left=of b] (c) {};
      \node[instrnode,fill=noded,below right=of b] (d) {};
      \node[instrnode,fill=nodee,below=of c] (e) {};
      \node[instrnode,fill=nodef,below right=of e] (f) {};
      \node[instrnode,fill=nodeg,below=of f] (g) {};
      \node[instrnode,fill=nodeh,below right=of g] (h) {};
      \draw[->] (a) -- (b);
      \draw[->] (b) -- (c);
      \draw[->] (b) -- (d);
      \draw[->] (d) -- (f);
      \draw[->] (c) -- (e);
      \draw[->] (e) -- (f);
      \draw[->] (f) -- (g);
      \draw[->] (g) -- (h);
      \draw[->] (g) to [loop,looseness=1.4,out=150,in=150] (b);
    \end{tikzpicture}
    \caption{\Glsfirst{CFG}}\label{fig:data-structure-comparison:cfg}
  \end{subfigure}\hfill%
  \begin{subfigure}[b]{0.3\linewidth}
    \centering
    \begin{tikzpicture}[shorten >=1pt,>=Latex,node distance=0.75cm and 0.5cm,
      munode/.style={draw=black,fill=black!15,yscale=-1,shape=trapezium}]
      \node[instrnode,fill=nodea] (a) {};
      \node[instrnode,fill=nodeb,below=15mm of a] (b) {};
      \node[instrnode,fill=nodec,below left=of b] (c) {};
      \node[instrnode,fill=noded,below right=of b] (d) {};
      \node[instrnode,fill=nodee,below=of c] (e) {};
      \node[instrnode,fill=nodef,below right=of e] (f) {};
      \node[instrnode,fill=nodeg,below=of f] (g) {};
      \node[instrnode,fill=nodeh,below right=of g] (h) {};
      \node[munode,above=9mm of b] (mb) {};
      \node[munode,above=9mm of f] (mf) {};
      \draw[->] (a) -- (mb);
      \draw[->] (mb) -- (b);
      \draw[->] (g) -- (h);
      \draw[->] (g) to [loop,looseness=1.4,out=150,in=150] (mb);
      \draw[->] (a) to [loop,looseness=1.2,out=300,in=90] (mf);
      \draw[->] (b) -- (e);
      \draw[->] (b) -- (c);
      \draw[->] (b) -- (d);
      \draw[->] (c) to [loop,looseness=1.2,out=230,in=120] (g);
      \draw[->] (d) to [loop,looseness=1.2,out=260,in=60] (g);
      \draw[->] (e) -- (g);
      \draw[->] (f) -- (g);
      \draw[->] (g) to [loop,looseness=1.5,out=20,in=60] (mf);
      \draw[->] (mf) -- (f);
    \end{tikzpicture}
    \caption{\Glsfirst{DFG}}\label{fig:data-structure-comparison:dfg}
  \end{subfigure}\hfill%
  \begin{subfigure}[b]{0.25\linewidth}
    \centering
    \begin{tikzpicture}[shorten >=1pt,>=Latex,node distance=0.75cm and 0.5cm]
      \node[instrnode,fill=nodea] (a) {};
      \node[instrnode,fill=nodeb,below=of a] (b) {};
      \node[instrnode,fill=nodec,below left=of b] (c) {};
      \node[instrnode,fill=noded,below right=of b] (d) {};
      \node[instrnode,fill=nodee,below=of c] (e) {};
      \node[instrnode,fill=nodef,below right=of e] (f) {};
      \node[instrnode,fill=nodeg,below=of f] (g) {};
      \node[instrnode,fill=nodeh,below right=of g] (h) {};
      \node[left=0.1cm of c] (padding) {};
      \begin{pgfonlayer}{background}
        \node[fit={(b)(c)(d)(e)(f)(g)(padding)},bblock] (fitall) {};
        \node[fit={(h)},bblock] (fith) {};
        \node[fit={(a)},bblock] (fita) {};
      \end{pgfonlayer}
      \draw[->,dashed] (a) -- (fitall);
      \draw[->] (b) -- (e);
      \draw[->] (b) -- (c);
      \draw[->] (b) -- (d);
      \draw[->] (c) to [loop,looseness=1.2,out=230,in=120] (g);
      \draw[->] (d) to [loop,looseness=1.2,out=260,in=60] (g);
      \draw[->] (e) -- (g);
      \draw[->] (f) -- (g);
      \draw[->,dashed] (g) -- (fith);
      \draw[->,dashed] (g) to [loop,looseness=1.4,out=150,in=150] ($(fitall.north) - (0.3,0)$);
    \end{tikzpicture}
    \caption{\Glsfirst{CDFG}}\label{fig:data-structure-comparison:cdfg}
  \end{subfigure}
  \caption{Comparison of lists, \glsfmtlongpl{CFG}, \glsfmtlongpl{DFG} and
    \glsfmtlongpl{CDFG}.}%
  \label{fig:data-structure-comparison}
\end{figure}

\subsubsection{Lists}

An example of a list representing code is shown abstractly in
\cref{fig:data-structure-comparison:list}.  In the figure, the solid arrows
between nodes show how the instructions are stored as a linked list, one
instruction feeding to the next.  However, programs may have loops in them,
which cannot directly be expressed in the list representation, as each
instruction can only have one successor.  Instead, instructions that are
represented as a list will have labels attached to them, and goto instructions
can then jump to those labels.  These jumps are represented using the dashed
arrows in the figure.  This is the simplest representation of the code as it is
completely linear, however, analysing such a program will be more difficult
because a lot of information, such as a lot of edges, is implicit in the
representation.  Every analysis pass would have to reconstruct the control-flow
edges between the nodes, which would have to be stored as a graph, so lists are
likely only to be the final representation of the code.

\subsubsection{\Glsfmtlongpl{CFG}}

Instead, a \gls{CFG} representation of the code is a graph instead of a list,
and allows any instruction to be connected to any other instruction explicitly
by a control-flow edge.  This signifies that after executing an instruction, the
execution will then move to one of the successors of the current node.  This
turns control-flow analysis into a graph problem~\cite[]{allen70_cfa}.  Because
of this, it is natural to represent code as a \gls{CFG}.
\Cref{fig:data-structure-comparison:cfg} shows how the list representation of
the code would be represented as a \gls{CFG}.

\subsubsection{\Glsfmtlongpl{DFG}}

Alternatively, instead of reasoning about the control flow of the program, one
might be interested in the \gls{data flow} of the program.  Many compiler
optimisations, such as dead-code elimination or constant propagation, need to
perform data-flow analyses on the
\gls{CFG}~\cite[]{kildall73_unified_approac_global_progr_optim,kam76_gdfaia}.
In addition to that, hardware circuits are naturally expressed using pure data
flow as netlists are essentially data flow graphs, and even \glspl{HDL} can be
modelled by synchronous data flow programming
languages~\cite{halbwachs91_sdfpll}.  One could therefore represent the code as
a pure \gls{DFG} to help with data-flow optimisations as well as the translation
to the final hardware.  An example of the \gls{CFG} shown in
\cref{fig:data-structure-comparison:cfg} being represented as a \gls{DFG} is
shown in \cref{fig:data-structure-comparison:dfg}, where arrows now represent
data dependencies between nodes instead of control-flow dependencies.

As can be observed in the diagram, the edges between the nodes are very
different to the edges in the \gls{CFG}.  Nodes in the \gls{CFG} may not have
\emph{needed} to be in that particular order, because when two instructions are
independent, one still needs to define an order between them.  Additionally, we
have added two additional nodes to handle the back edges of the loop, which are
also data dependencies.  These nodes are represented by a trapeze in the
\gls{DFG} and act as loop headers to control the loop iterations when
interpreting the graph as pure data flow.  These additional nodes are needed
because otherwise the data dependencies present because of the back edges of the
loop would conflict with the data dependency that actually enters the loop.  We
therefore need an additional node to turn the data dependencies into the correct
control-dependencies, and therefore allow data in to the loop when the loop is
not executing, but while it is executing the node should only allow data through
from the back edge of the loop.  The example shown in
\cref{fig:data-structure-comparison:dfg} is similar to the concepts of
$\mu$-nodes in gated static single assignment form and the program dependence
web~\cite{ottenstein90_progr_depen_web,campbell93_refin,havlak94_const,tu95_effic_build_placin_gatin_funct},
which are intermediate languages that can be interpreted in a data flow oriented
context.

\subsubsection{\Glsfmtlongpl{CDFG}}

As mentioned in the previous section, loops are especially problematic for pure
\glspl{DFG}.  Instead, one can try and get the best of both \glspl{CFG} and
\glspl{DFG} by creating a \gls{CDFG}.  In a \gls{CDFG}, one instead has a
\gls{CFG} where each node is a \gls{DFG} instead of just an instruction.  The
example is shown in \cref{fig:data-structure-comparison:cdfg}, where the dashed
arrows are control dependencies between \glspl{DFG}, and the solid lines are the
data dependencies within the \gls{DFG}.  In this way, loops can be supported
without having to introduce special nodes and leaving the loop back edge as a
control dependency instead, and \glspl{DFG} can represent the sections of code
without any incoming edges using data dependencies, thereby being more flexible
in how instructions are rearranged without having the downsides of having to
introduce additional nodes.

\subsection{Grouping instructions into blocks}%
\label{sec:bg:intermediate-language}

This \namecref{sec:bg:intermediate-language} describes the representation of
intermediate languages that are often used within an \gls{HLS} tool.  In
particular, we will base our intermediate language on a \gls{CFG}, but will
describe various ways to group blocks of instructions together and describe the
advantages and drawbacks that they provide.

\begin{figure}
  \begin{subfigure}[c]{0.3\linewidth}
    \centering
    \begin{tikzpicture}[shorten >=1pt,>=Latex,node distance=0.75cm and 0.5cm]
      \node[instrnode,fill=nodea] (a) {};
      \node[instrnode,fill=nodeb,below=of a] (b) {};
      \node[instrnode,fill=nodec,below left=of b] (c) {};
      \node[instrnode,fill=noded,below right=of b] (d) {};
      \node[instrnode,fill=nodee,below=of c] (e) {};
      \node[instrnode,fill=nodef,below right=of e] (f) {};
      \node[instrnode,fill=nodeg,below=of f] (g) {};
      \node[instrnode,fill=nodeh,below right=of g] (h) {};
      \begin{pgfonlayer}{background}
        \node[fit={(c)(e)},bblock] (fitce) {};
        \node[fit={(d)},bblock] (fitd) {};
        \node[fit={(b)},bblock] (fitb) {};
        \node[fit={(a)},bblock] (fita) {};
        \node[fit={(f)(g)},bblock] (fitfg) {};
        \node[fit={(h)},bblock] (fith) {};
      \end{pgfonlayer}
      \draw[->] (a) -- (fitb);
      \draw[->] (b) -- (fitce.north);
      \draw[->] (b) -- (fitd.north);
      \draw[->] (d) -- (fitfg.north east);
      \draw[->] (c) -- (e);
      \draw[->] (e) -- (fitfg.north west);
      \draw[->] (f) -- (g);
      \draw[->] (g) -- (fith);
      \draw[->] (g) to [loop,looseness=1.4,out=150,in=150] (fitb.north west);
    \end{tikzpicture}
    \caption{Basic blocks}\label{fig:block-comparison:basicblocks}
  \end{subfigure}\hfill%
  \begin{subfigure}[c]{0.3\linewidth}
    \centering
    \begin{tikzpicture}[shorten >=1pt,>=Latex,node distance=0.75cm and 0.5cm]
      \node[instrnode,fill=nodea] (a) {};
      \node[instrnode,fill=nodeb,below=of a] (b) {};
      \node[instrnode,fill=nodec,below left=of b] (c) {};
      \node[instrnode,fill=noded,below right=of b] (d) {};
      \node[instrnode,fill=nodee,below=of c] (e) {};
      \node[instrnode,fill=nodef,below=of e] (f) {};
      \path (f) -| node[instrnode,fill=nodef,dashed] (fp) {} (d);
      \node[instrnode,fill=nodeg,below=of f] (g) {};
      \node[instrnode,fill=nodeg,below=of fp,dashed] (gp) {};
      \node[instrnode,fill=nodeh,below right=of g] (h) {};
      \begin{pgfonlayer}{background}
        \node[fit={(d)}] (fitd) {};
        \node[fit={(b)}] (fitb) {};
        \node[fit={(d)(fp)(gp)},bblock] (fitd) {};
        \node[fit={(c)(e)(f)(g)}] (fitce) {};
        \filldraw[bblock] (fitb.north west)
                       -- (fitb.north east)
                       -- (fitb.south east)
                       -- ($(fitce.north east) + (0.07,-0.3)$)
                       -- ($(fitce.south east) + (0.07,0)$)
                       -- ($(fitce.south west) - (0.07,0)$)
                       -- ($(fitce.north west) - (0.07,0)$)
                       -- cycle;
        \node[fit={(a)},bblock] (fita) {};
        \node[fit={(h)},bblock] (fith) {};
      \end{pgfonlayer}
      \draw[->] (a) -- (fitb);
      \draw[->] (b) -- (c);
      \draw[->] (b) -- (fitd.north);
      \draw[->] (d) -- (fp);
      \draw[->] (c) -- (e);
      \draw[->] (e) -- (f);
      \draw[->] (f) -- (g);
      \draw[->] (g) -- (fith);
      \draw[->] (fp) -- (gp);
      \draw[->] (gp) -- (fith);
      \draw[dashed] (f) -- (fp);
      \draw[dashed] (g) -- (gp);
      \draw[->] (g) to [loop,looseness=1.2,out=150,in=150] (fitb.north west);
      \draw[->] (gp) to [loop,looseness=1.2,out=30,in=30] (fitb.north east);
    \end{tikzpicture}
    \caption{Superblocks}\label{fig:block-comparison:superblocks}
  \end{subfigure}\hfill%
  \begin{subfigure}[c]{0.3\linewidth}
    \centering
    \begin{tikzpicture}[shorten >=1pt,>=Latex,node distance=0.75cm and 0.5cm]
      \node[instrnode,fill=nodea] (a) {};
      \node[instrnode,fill=nodeb,below=of a] (b) {};
      \node[instrnode,fill=nodec,below left=of b] (c) {};
      \node[instrnode,fill=noded,below right=of b] (d) {};
      \node[instrnode,fill=nodee,below=of c] (e) {};
      \node[instrnode,fill=nodef,below right=of e] (f) {};
      \node[instrnode,fill=nodeg,below=of f] (g) {};
      \node[instrnode,fill=nodeh,below right=of g] (h) {};
      \begin{pgfonlayer}{background}
        \node[fit={(b)(c)(d)(e)(f)(g)},bblock] (fitall) {};
        \node[fit={(h)},bblock] (fith) {};
        \node[fit={(a)},bblock] (fita) {};
      \end{pgfonlayer}
      \draw[->] (a) -- (fitall);
      \draw[->] (b) -- (d);
      \draw[->] (b) -- (c);
      \draw[->] (d) -- (f);
      \draw[->] (c) -- (e);
      \draw[->] (e) -- (f);
      \draw[->] (f) -- (g);
      \draw[->] (g) -- (fith);
      \draw[->] (g) to [loop,looseness=1.4,out=150,in=150] ($(fitall.north) - (0.3,0)$);
    \end{tikzpicture}
    \caption{Hyperblocks}\label{fig:block-comparison:hyperblocks}
  \end{subfigure}
  \caption{Comparison of basic blocks, superblocks and hyperblocks.}%
  \label{fig:block-comparison}
\end{figure}

\subsubsection{Basic blocks}

To build a bit more structure in the \gls{CFG}, it is useful to group
non-branching instructions without any incoming edges together forming
\glspl{basic block}.  An example of the \gls{CFG} segmented into
\glspl{basic block} is shown in \cref{fig:block-comparison:basicblocks}.
Instructions within a \gls{basic block} can then be represented as a list of
instructions, as there cannot be any branches, and these lists of instructions
can then be safely manipulated because there is a guarantee that there is no
incoming control flow into the middle of a \gls{basic block}.  For example, as
long as two instructions in a \gls{basic block} are independent, they can be
safely reordered.  This would have otherwise not been a case, because incoming
control flow may mean that reordering the instructions now leads to the
unintended execution of an instruction.

\subsubsection{Superblocks}

As \cref{fig:block-comparison:basicblocks} shows, one issue with
\glspl{basic block} is that they are often quite small, therefore limiting the
benefits that they can provide. One extension to \glspl{basic block} is
\glspl{superblock}~\cite[]{hwu93_super}, shown in
\cref{fig:block-comparison:superblocks}.  \Glspl{superblock} extend the notion
of \glspl{basic block} to contiguous regions without any incoming control flow,
however, they allow for multiple exits out of the \gls{superblock}.  The main
benefit of this is that due to the extra flexibility of allowing multiple exits,
the \glspl{basic block} can now contain arbitrary linear traces through regions
without any incoming edges.  This means that one can, for example, group the
\enquote{hot path} of the loop body into a single \gls{superblock}, which can
then be heavily optimised.  Various other important paths through the program
can then also be grouped and optimised together.  If different paths through the
program reuse nodes from another path, as is shown with the pink and yellow
nodes in \cref{fig:block-comparison:superblocks}, then these nodes will have to
be duplicated so that they can be included in a different path.  As long as any
optimisation takes into account the arbitrary exits that are possible in
\glspl{superblock}, the nodes within a \glspl{superblock} can then be optimised
independently from the rest of the code.

\subsubsection{Hyperblocks}

One downside of \glspl{superblock} is that they cannot represent a whole section
of code without back edges as a single block.  For example, the block present in
the \gls{CDFG} shown in \cref{fig:data-structure-comparison:cdfg} could not be
represented by a \gls{superblock}.  \Glspl{hyperblock} are defined as
\glspl{basic block} of \glspl{predicated instruction} with arbitrary outgoing
edges~\cite[]{mahlke92_effec_compil_suppor_predic_execut_using_hyper}.  They can
represent arbitrary branching code without incoming edges, and are therefore an
extension to \glspl{superblock}.  This means that they could represent the
\gls{CDFG} shown in \cref{fig:data-structure-comparison:cdfg}, and an example of
a hyperblock is shown in \cref{fig:block-comparison:hyperblocks}, where code
within the \gls{hyperblock} has been linearised instead of containing a
\gls{DFG}.  This leads to possibly more complex control flow than in both of the
previous cases, however, it can be reasoned with using a \gls{SAT} or \gls{SMT}
solver.

\section{Scheduling}%
\label{sec:bg:scheduling}

Instruction scheduling is the main transformation and optimisation performed by
traditional \gls{HLS} tools.  The scheduling transformation introduces time into
the untimed input representation by placing each instruction into a clock cycle
in which it should execute.  The scheduler must take into account the resources
that were selected during the resource allocation step and schedule the
instructions so that they meet any constraints.  In this
\namecref{sec:bg:scheduling} I will discuss scheduling techniques used by
\gls{HLS} tools, first discussing static scheduling in
\cref{sec:bg:static-scheduling}, which is the scheduling algorithm used by
Vericert in this dissertation, followed by describing \gls{dynamic scheduling}
in \cref{sec:bg:dynamic-scheduling}, which is an alternative scheduling
technique with different trade-offs to static scheduling.

\subsection{Static scheduling}%
\label{sec:bg:static-scheduling}

Static scheduling is used by the majority of synthesis tools~\cite{canis13_l,
  amd23_vitis_high_synth,
  mentor20_catap_high_level_synth, intel20_sdk_openc_applic,
  roane23_autom_hw_sw_co_desig} and means that the time at which each operation
will execute is known at compile time.  The first step is to generate a
\gls{CDFG}, which is either already the structure of the code, or can be
generated from a \gls{hyperblock} \gls{CFG} representation, for example, by
using static analysis on each \gls{hyperblock} to gather all data dependencies
and convert it into a \gls{DFG}.

Scheduling can have different optimisation goals for the design in terms of
latency and resource usage.  In the simplest case, each operation in the
\gls{DFG} can be scheduled as soon as its predecessor in the graph has been
scheduled, resulting in an \gls{ASAP} schedule, where the start time of each
node corresponds to the longest path from the start of the \gls{DFG} to that
node.  If instead the start time of a node is taken to be the longest path from
the node to the end of the \gls{DFG}, then this would result in an \gls{ALAP}
schedule.  These two types of schedules show the \emph{slack} of a particular
operation, which can be exploited by schedulers to minimise resources but still
minimise latency.

A more advanced scheduler can either optimise for resource usage based on a
maximum latency or for latency based on a maximum amount of resources.  In
general this is an NP-hard problem, however, \emph{\gls{list scheduling}}
provides a heuristic for this problem by ordering nodes that can be scheduled
according to a priority function and picking a subset of these nodes to actually
be scheduled as long as the resources used by the subset is not greater than the
available resources.

\subsubsection{System of difference constraints scheduling}

The main static scheduling algorithm used by \gls{HLS} tools is the
\glsfirst{SDC}\glsunset{SDC} scheduling algorithm~\cite{cong06_sdc}.  It
generates an \gls{SDC} that is a subset of an \gls{LP} problem that can be
incrementally modified and checked for feasibility as new constraints are added.
Then, to solve the \gls{SDC} it can be converted into an efficient \gls{LP}
problem guaranteeing integer solutions.

The scheduling algorithm is built on the notion of constraining scheduling
variables for operation $v$ ($\gls{scheduling variable}_i(v)$).  Each operation
is associated with a set of scheduling variables, but at a minimum it must have
an initial and a final scheduling variable
($\gls{scheduling variable}_{\mathrm{init}}(v)$ and
$\gls{scheduling variable}_{\mathrm{fin}}(v)$).  The main advantage of this
scheduling algorithm is that one can define many different concepts as
constraints in terms of scheduling variables.  For example, if one has a data
dependency from operation $a$ to operation $b$, the following constraint is
added to the \gls{SDC}.

\begin{equation*}
  \gls{scheduling variable}_{\mathrm{fin}}(a) - \gls{scheduling
    variable}_{\mathrm{init}}(b) \leq 0
\end{equation*}

Other types of constraints that are normally added to get a valid schedule are:

\begin{description}
\item[Control dependency constraint] Constraint between blocks of instructions
  that are connected by a control-flow dependency.  This ensures that a block is
  not scheduled before another block it depends on.  In particular, loop back
  edges therefore need to be removed during the \gls{SDC} construction, as
  otherwise the system would become unsatisfiable.

\item[Relative timing constraint] This constraint ensures that two operations
  are separated by at least or at most a fixed number of cycles.  This can be
  used to satisfy I/O timings.

\item[Latency constraint] This constraint specifies a maximum latency for a set
  of blocks.

\item[Cycle time constraint] This constraint is used to target a specific final
  clock period, and split up long combinational paths through the \gls{DFG} into
  separate cycles.  This allows for \emph{\gls{operation chaining}}, whereby
  multiple operations that are estimated to have a latency less than a clock
  cycle can be chained within the same clock cycle.

\item[Resource constraint] To handle limited resources, constraints can be used
  to make it infeasible to have operations use the same resource within the same
  clock cycle.  Constructing these constraints relies on a linear ordering of
  the \gls{DFG}, so some combination of \gls{ALAP} or \gls{ASAP} scheduling can
  be done to form the linear ordering of the operations.
\end{description}

Solving the \gls{SDC} with different optimisation functions produces different
kinds of schedules.  For example, optimising the following function will produce
an \gls{ALAP} schedule, as the start time of every operation is maximised.

\begin{equation*}
  \mathrm{max} \sum_{v \in V_{\mathrm{op}}} \gls{scheduling variable}_{\mathrm{init}}(v)
\end{equation*}

Modern \gls{HLS} tools often use a variation of \gls{SDC} scheduling.  For
example, Vitis HLS and LegUp use an extension of \gls{SDC}
scheduling~\cite[]{zhang13_sdc,canis14_modul_sdc} with support for modulo
scheduling (loop pipelining)~\cite[]{rau96_iterat_modul_sched} and Bambu HLS
uses an extension of \gls{SDC} scheduling with support for speculative execution
and loop code motion~\cite[]{lattuada15_ctbsss}.  In this dissertation,
\gls{SDC} scheduling refers to the initial implementation of the scheduling
without any extensions.

\subsection{Dynamic scheduling}%
\label{sec:bg:dynamic-scheduling}

On the other hand, \gls{dynamic
  scheduling}~\cite{josipović18_dynam_sched_high_synth} does not require
the schedule to be known at compile time and instead it generates circuits using
tokens to schedule the operations in parallel at run time.  Whenever the data
for an operation is available, it sends a token to the next operation,
signalling that the data is ready to be read.  The next operation does not start
until all the required inputs to the operation are available, and once that is
the case, it computes the result and then sends another token declaring that the
result of that operation is also ready.  The benefit of this approach is that
only basic data-flow analysis is needed to connect the tokens correctly,
however, the scheduling is then done dynamically at run time, depending on how
long each primitive takes to finish and when the tokens activate the next
operations.

The benefit of this approach over static scheduling is that the latency of these
circuits is normally significantly lower than the latency of static scheduled
circuits, because they can take advantage of runtime information of the circuit.
However, because of the signalling required to perform the runtime scheduling,
the area of these circuits is usually much larger than the area of static
scheduled circuits.  In addition to that, much more analysis is needed to
properly parallelise loads and stores to prevent bugs, which requires the
addition of buffers in certain locations.

An example of a dynamically scheduled synthesis tool is
Dynamatic~\cite{josipović18_dynam_sched_high_synth}, which uses a
\gls{LSQ}~\cite{josipović17_out_order_load_store_queue_spatial_comput} to order
memory operations correctly even when loops are pipelined and there are
dependencies between iterations.  In addition to that, performance of the
dynamically scheduled code is improved by careful buffer
placement~\cite{josipović21_buffer_placem_sizin_high_perfor_dataf_circuit},
which allows for better parallelisation and pipelining of loops.

\section{Verification}%
\label{sec:bg:verification}

Theorem provers can be categorised into two main types: automatic theorem
provers described in \cref{sec:bg:automatic-theorem-provers} and interactive
theorem provers described in \cref{sec:bg:interactive-theorem-provers}.  This
section will give a brief overview of the characteristics of these different
verification tools.

\subsection{Automatic theorem provers}%
\label{sec:bg:automatic-theorem-provers}

Automatic theorem provers are tools that can reason about logic automatically,
and answer whether a theorem about some variables is true or whether there is a
counter example.  Most automatic theorem provers are implemented around
\gls{SAT} or \gls{SMT} solvers, which can be characterised as deciding if a
formula is satisfiable or not.  In \gls{SAT}, the formula is purely boolean, but
in \gls{SMT} the boolean formula may include arbitrary theories that extend the
logic, for example by including linear integer arithmetic or a theory of arrays.
This is powerful, and means that \gls{SMT} solvers such as
Z3~\cite[]{moura08_z}, cvc5~\cite[]{barbosa22_cvc5},
Boolector~\cite[]{brummayer09_b} or veriT~\cite{bouton09} are used by
higher level automatic verification tools such as bounded model checkers like
CBMC~\cite[]{kroening14_c} or verification aware programming languages like
Dafny~\cite[]{leino10_d}.

Automatic theorem provers are also the basis for formal hardware verification in
general, providing ways to prove the equivalence between hardware designs as
well as between hardware designs and their specifications using commercial
verification tools like Cadence Conformal~\cite[]{cadence23_c} and Synopsys VC
Formal~\cite[]{synopsys23_v}.  More details about current hardware verification
methodologies are given in \cref{sec:bg:unmechanised-verification-of-hls},
especially as they apply to \gls{HLS}.

How can one trust the answer of such automatic theorem provers?  From a
high-level, they are running an opaque, highly optimised algorithm and return an
answer to the problem, which may be incorrect.  If the formula is satisfiable,
the solution is simple: the solver can provide a model that satisfies the
formula.  Checking if the solver gave the right answer is just a matter of
checking that the model satisfies the formula.  If the formula is unsatisfiable,
the solver will return \enquote{unsat} as the answer as there is no model.
Checking that a formula is actually unsatisfiable without having to trust the
\gls{SMT} solver can be done if the \gls{SMT} solver can generate a proof
witness, which can be checked by an independent, trusted checker.  Proof
witnesses are normally a series of rewrites of axioms or basic lemmas that
translate the original formula into \textit{false}.  If one trusts the checker,
one does not have to trust the \gls{SMT} solver, because if it can generate a
valid proof witness, then one knows that the formula has to be equivalent to
\textit{false}.  Some examples of \gls{SMT} solvers that can generate proof
witnesses are veriT~\cite{bouton09} or cvc5~\cite[]{barbosa22_cvc5}, and the
proof witnesses generated by these tools can actually be checked by a formally
verified proof checker called SMTCoq~\cite{armand11_modul_integ_sat_smt_solver},
so that these proofs of \gls{SMT} formulas can be used in a verified context.

Unfortunately, equivalence checkers used to verify designs or formal
verification tools to prove assertions about hardware normally do not produce a
proof witness that can be independently checked.  This means that especially
with the commercial, closed source formal verification tools, one has to trust
that the result they produce is correct.

%The main advantage of using an automatic theorem prover is that if one is
%working in its constrained decidable theory, then it will be efficient at
%proving or disproving if a formula is a theorem.  However, if the theorem
%requires inductive arguments to prove, then the theorem prover might need some
%manual help from the user by adding the right lemmas to its collection of facts
%which the automatic procedure can use.  The proof itself though will still be
%automatic, which means that many of the tedious cases in the proofs can be
%ignored.
%
%However, this is also the main disadvantage of automatic theorem provers,
%because they do not provide details about the proof itself and often cannot
%communicate why they cannot prove a theorem.  This means that as a user one has
%to guess what theorems the prover is missing and try and add these to the fact
%database.

\subsection{Interactive theorem provers}%
\label{sec:bg:interactive-theorem-provers}

Interactive theorem provers like
Coq~\cite[]{bertot04_inter_theor_provin_progr_devel},
Isabelle~\cite[]{paulson94_i} or Lean~\cite[]{moura15_l}, on the other hand,
focus on checking proofs that are provided to them, instead of automatically
trying to check theorems.  These proofs can either be written manually by the
user in a tactic language provided by the theorem prover, or automatically
generated by external decision procedures, such as an automatic theorem prover
that produces a witness.  One benefit of using an interactive theorem prover is
that the proof is checked by a small, trusted kernel.  This kernel together with
the statements of the theorems in the theorem prover are the only parts that
need to be trusted, as the proofs of the theorems are untrusted.

Interactive theorem provers can therefore help with verifying complex theorems,
such as proving the correctness of a C Compiler, as is the case with CompCert,
or prove the correctness of an operating system kernel like sel4 or the four
colour theorem proof~\cite[]{gonthier08_fp}.

More details on how an interactive theorem prover is used to develop verified
programs is described in
\cref{sec:bg:mechanised-compiler-proofs,sec:bg:compcert}.

\YH{TODO: Maybe expand on section.}

\section{Verification of High-Level Synthesis}

This section describes current ways in which designs generated by \gls{HLS}
tools are validated.  First, I will describe unmechanised verification of
\gls{HLS} tools, meaning testing and verification methodologies that have not
been formalised in an interactive theorem prover.  Next, I will describe
mechanised verification around \gls{HLS}.

A summary of the related works can be found in \cref{fig:bg:related_euler},
which is represented as an Euler diagram.  The categories chosen for the Euler
diagram are: whether the tool is usable, whether it takes a high-level software
language as input, whether it has a correctness proof, and finally whether that
proof is mechanised.  The goal of \vericert{} is to cover all of these
categories.

Most practical \gls{HLS}
tools~\cite{canis13_l,amd23_vitis_high_synth,intel20_sdk_openc_applic,nigam20_predic_accel_desig_time_sensit_affin_types}
fit into the category of usable tools that take high-level inputs.  On the other
end of the spectrum, there are tools such as
BEDROC~\cite{chapman92_verif_bedroc} for which there is no practical tool, and
even though it is described as high-level synthesis, it more closely resembles
today's logic synthesis tools.

Ongoing work in translation validation~\cite{pnueli98_trans} seeks to prove
equivalence between the hardware generated by an HLS tool and the original
behavioural description in C.  An example of a tool that implements this is
Mentor's Catapult~\cite{mentor20_catap_high_level_synth}, which tries to match
the states in the hardware description to states in the original C code after an
unverified translation.  Using translation validation is quite effective for
verifying complex optimisations such as
scheduling~\cite{kim04_autom_fsmd,karfa06_formal_verif_method_sched_high_synth,chouksey20_verif_sched_condit_behav_high_level_synth}
or code
motion~\cite{banerjee14_verif_code_motion_techn_using_value_propag,chouksey19_trans_valid_code_motion_trans_invol_loops},
but the validation has to be run every time the HLS is performed.  In addition
to that, the proofs are often not mechanised or directly related to the actual
implementation, meaning the verifying algorithm might be wrong and hence could
give false positives or false negatives.

Finally, there are a few relevant mechanically verified tools.  First, K\^{o}ika
is a formally verified translator from a core fragment of Bluespec into a
circuit representation which can then be printed as a Verilog design.  This is a
translation from a high-level hardware description language into an equivalent
circuit representation, so is a different approach to HLS.
\textcite{lööw19_proof_trans_veril_devel_hol} used a proof-producing translator
from HOL4 code describing state transitions into Verilog to design a verified
processor, which is described further by
\textcite{lööw19_verif_compil_verif_proces}. \textcite{lööw21_lutsig} has also
worked on formally verifying a logic synthesis tool that can transform hardware
descriptions into low-level netlists.  This synthesis back end can seamlessly
integrate with the proof-producing HOL4 to Verilog translator as it is based on
the same Verilog semantics, and therefore creates verified translation from HOL4
circuit descriptions to synthesised Verilog netlists.
\citeauthor{perna12_mechan_wire_wise_verif_handel_c_synth} designed a formally
verified translator from a deep embedding of Handel-C into a deep embedding of a
circuit~\cite{perna12_mechan_wire_wise_verif_handel_c_synth,perna11_correc_hardw_synth}.
Finally, \textcite{ellis08_csicgfu} used Isabelle to implement and reason about
intermediate languages for software/hardware compilation, where parts could be
implemented in hardware and the correctness could still be shown.

\begin{figure}
  \centering
  \newcommand\citeshort[1]{\citeauthor*{#1}~\cite*{#1}}
  \begin{tikzpicture}[xscale=1.7]
    \def\opacity{0.2}
    \definecolor{colorusabletool}{HTML}{1b9e77}
    \definecolor{colorhighlevel}{HTML}{d95f02}
    \definecolor{colorproof}{HTML}{7570b3}
    \definecolor{colormechanised}{HTML}{e7298a}

    \tikzset{myellipse/.style={draw=none, fill opacity=0.2}}
    \tikzset{myoutline/.style={draw=white, thick}}

    \draw[myellipse, fill=colorusabletool] (-1.1,1.6) ellipse (3.1 and 4.1);
    \draw[myellipse, fill=colorhighlevel] (1.1,1.8) ellipse (2.9 and 4.1);
    \draw[myellipse, fill=colorproof] (0,-0.5) ellipse (3.7 and 3.5);
    \draw[myellipse, fill=colormechanised] (0,-0.7) ellipse (3.7 and 2);

    \draw[myoutline] (-1.1,1.6) ellipse (3.1 and 4.1);
    \draw[myoutline] (1.1,1.8) ellipse (2.9 and 4.1);
    \draw[myoutline] (0,-0.5) ellipse (3.7 and 3.5);
    \draw[myoutline] (0,-0.7) ellipse (3.7 and 2);

    \node[align=center] at (0,4) {Standard HLS tools \\
      \footnotesize\cite{canis13_l} \\[-0.2em]
      \footnotesize\cite{amd23_vitis_high_synth,intel20_sdk_openc_applic}\\[-0.2em]\footnotesize
    \cite{nigam20_predic_accel_desig_time_sensit_affin_types}};
    \node[align=center,font=\small] at (0.15,2) {Translation validation
      approaches \\
      \citeauthor*{mentor20_catap_high_level_synth}~\cite*{mentor20_catap_high_level_synth},
      \citeshort{clarke03_behav_c_veril}\\ and \citeshort{kundu08_valid_high_level_synth}};
    \node at (0,-0.5) {\textbf{\vericert{}}};
    \node[align=left] at (-2.2,-1.0) {\footnotesize K\^oika \cite{bourgeat20_essen_blues}};
    \node[align=left] at (-2.2,-0.5) { \footnotesize\citet{lööw21_lutsig}};

    \node at (2.2,-1.2) {\footnotesize\citet{ellis08_csicgfu}};
    \node at (-2,-1.5) { {\footnotesize\citet{perna12_mechan_wire_wise_verif_handel_c_synth}}};
    \node at (0,-3.3) {\footnotesize BEDROC \cite{chapman92_verif_bedroc}};

    \node[align=left] at (-2.9,-3.7) {\color{colorproof}Correctness \\ \color{colorproof}proof};
    \node[align=right] at (2.6,-3.7) (mechanisedlabel) {\color{colormechanised}Mechanised \\
      \color{colormechanised}correctness proof};
    \draw[myoutline] (0,-2.7) to ($(mechanisedlabel.north)+(0.1,-0.1)$);
    \node at (-3,6.1) {\color{colorusabletool}\strut Usable tool};
    \node at (2.1,6.1) {\color{colorhighlevel}\strut High-level software input};
  \end{tikzpicture}
  \caption{Summary of related work}\label{fig:bg:related_euler}
\end{figure}

\subsection{Unmechanised verification of HLS}%
\label{sec:bg:unmechanised-verification-of-hls}

This section describes how \gls{HLS} designs are typically verified and also
describes the current state-of-the-art in verification of \gls{HLS}
transformations.  The standard way to test designs in \gls{HLS} is either using
hardware test benches, testing the final hardware design like any other hardware
design, or by using C/Verilog co-simulation, which is a feature some \gls{HLS}
tools support, whereby the same test code that drove the C code can also be used
to drive the hardware design.

Proving the equivalence between the generated hardware and the original
behavioural description in C is an ongoing research problem.  There are
commercial tools that support such an equivalence checks, such as Siemens'
SLEC~\cite[]{chauhan20_formal_ensur_equiv_c_rtl} or Cadence's
C2RTL~\cite[]{cadence23_j}, which try to match the states in the register
transfer level description to states in the original C code after an unverified
translation.

This technique is called translation validation~\cite{pnueli98_trans}, whereby
the translation that the HLS tool performed is proven to have been correct for
that input, by showing that they behave in the same way for all possible inputs.
Using translation validation is quite effective for proving complex
optimisations such as scheduling~\cite{kim04_autom_fsmd,
  karfa06_formal_verif_method_sched_high_synth,
  chouksey20_verif_sched_condit_behav_high_level_synth} or code
motion~\cite{banerjee14_verif_code_motion_techn_using_value_propag,
  chouksey19_trans_valid_code_motion_trans_invol_loops}, however, the validation
has to be run every time the high-level synthesis is performed.  In addition to
that, the proofs are often not mechanised or directly related to the actual
implementation, meaning the verifying algorithm might be wrong and could give
false positives or false negatives.

More examples of translation validation for proofs about HLS
algorithms~\cite{karfa06_formal_verif_method_sched_high_synth,
  karfa07_hand_verif_high_synth, kundu07_autom,
  karfa08_equiv_check_method_sched_verif, kundu08_valid_high_level_synth,
  karfa10_verif_datap_contr_gener_phase, karfa12_formal_verif_code_motion_techn,
  chouksey19_trans_valid_code_motion_trans_invol_loops,
  chouksey20_verif_sched_condit_behav_high_level_synth} are performed using a
HLS tool called SPARK~\cite{gupta03_spark}.  These translation validation
algorithms can check the correctness of complicated optimisations such as code
motion or loop inversions.  However, even though the correctness of the verifier
is proven in the papers, the proof does translate directly to the algorithm that
was implemented for the verifier.  It is therefore possible that output is
accepted even though it is not equivalent to the input.  In addition to that,
these papers reason about the correctness of the algorithms in terms of the
intermediate language of SPARK, and does not extend to the high-level input
language that SPARK takes in, or the hardware description language that SPARK
outputs.

Finally there has also been work proving HLS correct without using translation
validation, but by directly showing that the translation is correct.  The first
instance of this is proving the BEDROC~\cite{chapman92_verif_bedroc} HLS tool is
correct.  This HLS tool converts a high-level description of an algorithm,
supporting loops and conditional statements, to a netlist and proves that the
output is correct.  It works in two stages, first generating a DFG from the
input language, HardwarePal.  It then optimises the DFG to improve the routing
of the design and also improving the scheduling of operations.  Finally, the
netlist is generated from the DFG by placing all the operations that do not
depend on each other into the same clock cycle.  Data path and register
allocation is performed by an unverified clique partitioning algorithm.  The
equivalence proof between the DFG and HardwarePal is done by proof by
simulation, where it is proven that, given a valid input configuration, that
applying a translation or optimisation rule will result in a valid DFG with the
same behaviour as the input.

There has also been work on proving the translation from occam to
gates~\cite{page91_compil_occam} correct using algebraic
proofs~\cite{jifeng93_towar}.  This translation resembles dynamic scheduling as
tokens are used to start the next operations.  To take advantage of the parallel
nature of hardware, occam has explicit concurrency using the PAR constructs with
channels to share state.  Handel-C, a version of occam with many more features
such as memory and pointers, and is described further in
\cref{sec:bg:mechanised-compiler-proofs} as there is mechanised translation from
Handel-C into a netlist.  One thing to note about these translations from occam
and Handel-C into netlists is that they are essentially timed languages in SEQ
blocks.  This is because every assignment takes one cycle to complete and
expressions are assumed to finish evaluation within that cycle.  No automatic
scheduling is performed by the translation, and the designer has full control
over the design and can manually pipeline and parallelise the design.

\subsection{Mechanised compiler proofs in high-level hardware design}%
\label{sec:bg:mechanised-compiler-proofs}

Even though a proof for the correctness of an algorithm might exist, this does
not guarantee that the algorithm itself behaves in the same way as the assumed
algorithm in the proof.  The implementation of the algorithm is separate from
the actual implementation, meaning there could be various implementation bugs in
the algorithm that cause it to behave incorrectly.  C compilers are a good
example of this, where many optimisations performed by the compilers have been
proven correct, however these proofs are not linked directly to the actual
implementations of these algorithms in GCC or Clang.
\textcite{yang11_findin_under_bugs_c_compil} found more than 300 bugs in GCC and
Clang, many of them appearing in the optimisation phases of the compiler.  One
way to link the proofs to the actual implementations in these compilers is to
write the compiler in a language which allows for a theorem prover to check
properties about the algorithms.  This allows for the proofs to be directly
linked to the algorithms, ensuring that the actual implementations are proven
correct.  \citeauthor{yang11_findin_under_bugs_c_compil} found that CompCert, a
formally verified C compiler, only had five bugs in all the unverified parts of
the compiler, meaning this method of proving algorithms correct ensures a
correct compiler.

This section explores formalisation of hardware design in general, focusing
specifically on higher level hardware design, by describing formalisations of
higher level hardware description languages that are synthesised to a lower
level netlist representation of the hardware design.

\textcite{perna12_mechan_wire_wise_verif_handel_c_synth} developed a
mechanically verified Handel-C to netlist translation written in HOL.  The
translation is based on previous work describing translation from occam to gates
by \textcite{page91_compil_occam}, which was proven correct by
\textcite{jifeng93_towar} using algebraic proofs.  As Handel-C is an extension
of occam with C-like operations, the translation from Handel-C to gates can
proceed in a similar way.

\citeauthor{perna12_mechan_wire_wise_verif_handel_c_synth} mechanise the
compilation of a subset of Handel-C to gates, which does not include memory,
arrays or function calls.  In addition to the constructs presented by
\citeauthor{page91_compil_occam}, the prioritised choice construct is also added
to the Handel-C subset that is supported.  The verification proceeds by first
defining the algorithm to perform the compilation, chaining operations together
with start and end signals that determine the next construct which will be
executed.  The circuits themselves are treated as black boxes by the compilation
algorithm and are chosen based on the current statement in Handel-C which is
being translated.  One interesting property about these circuits is that
control-signal is propagated correctly through the circuit, and that it ensures
that only one hardware construct is active at a time.  This is the property that
is mechanised in the work for the translation of Handel-C to netlists.  However,
the final semantic equivalence of the translation was not mechanised.

Next, Kôika~\cite[]{bourgeat20_essen_blues} is a formalisation of a derivative
of Bluespec that focuses on providing control over the scheduling of the rules.
Rule based hardware design in Kôika is therefore similar to Bluespec and
provides a useful abstraction to design hardware over using Verilog directly.
Kôika also provides a mechanised compilation from the collection of rules with a
schedule into an equivalent circuit.  This circuit can then be pretty printed as
a Verilog design.  This is a fundamentally different approach to increasing the
abstraction level of hardware design compared to translating software into
hardware.  This mainly comes down to Kôika giving fine-grained control over the
intra-cycle scheduling of rules and both Kôika and Bluespec give control of the
inter-block scheduling which is up to the hardware designer, whereas in
traditional \gls{HLS} the hardware design is automatically scheduled from the
sequential software definition.

Finally, another related formalisation is Lutsig, a formally verified Verilog
synthesis tool~\cite[]{lööw21_lutsig}.  This is concerned with translating
register transfer level Verilog into netlist level Verilog, instead of
translating from a higher level specification into hardware.

\subsection{HLS formalised in Isabelle}

Martin Ellis' work on the specification of hardware/software
co-design~\cite{ellis08_csicgfu} is the first attempt towards mechanically
verified \gls{HLS} using Isabelle.  The main goal of the thesis is to provide a
framework to prove hardware/software co-design compilers correct, where part of
the design is specified in software and other parts of the design are translated
to equivalent hardware to be accelerated.  The dissertation describes the
semantics of an \gls{SSA} based software \gls{IR} which supports partitioning of
code into hardware and software parts, as well as a custom netlist format which
is used to describe the hardware parts and is the final target of the
hardware/software compiler.  The dissertation then describes what the
correctness would look like between the software and hardware semantics. The
framework used to prove the correctness of the compilation from the IR to the
netlist format is written in Isabelle, which is a theorem prover comparable to
Coq.

As both the input IR and output netlist format have been designed from scratch,
there is no description of a translation from a higher level languages into the
\gls{SSA} form, as well as no description on how to map the netlist language
onto an \gls{FPGA}.  These translations would likely remain unverified.  A
custom IR and netlist was used to describe how the correctness between the two
languages could be stated and explores how the software language can interact
with the hardware.

Finally, it is unclear whether or not a translation algorithm from the IR to the
netlist format was implemented, as the only example in the thesis seems to be
compiled by hand to explain the correctness theorem with respect to that
example.  There are also no benchmarks on real input programs showing the
efficiency of the translation algorithm, and it is therefore unclear whether the
framework would be able to prove more complicated optimisations that a compiler
might perform on the source code.  The dissertation is likely assuming that
high-level programs and their netlist implementations are verified according to
the correctness property manually for each design, as there is no implementation
or proof of an automatic translation from the software code into the mixed
software/hardware design.

\section{CompCert}%
\label{sec:bg:compcert}

\gls{CompCert}~\cite{leroy06_formal_certif_compil_back_end,leroy09_formal_verif_realis_compil,leroy16_cfvoc}
is a formally verified C compiler written in
Coq~\cite{bertot04_inter_theor_provin_progr_devel}.  The verified compiler in
Coq is extracted to OCaml code and can then be used.  CompCert comprises eleven
intermediate languages, which are used to gradually translate C code into
assembly that has the same behaviour.  Proving the translation directly without
going through the intermediate languages would be infeasible, especially with
the many optimisations that are performed during the translation, as there is a
large semantic gap between the semantics of Assembly and the semantics of
Clight.  The first three intermediate languages (C\#minor, C\#minorgen, Cminor)
are used to transform Clight, a deterministic subset of C, into a more assembly
like language called \gls{RTL}.  This language consist of a \gls{CFG} of
instructions, and is therefore well suited for various compiler optimisations
such as constant propagation, dead-code elimination or function inlining.  After
\gls{RTL}, each intermediate language is used to get closer to the assembly
language of the architecture, performing operations such as register allocation.
\Cref{fig:bg:compcert-languages} gives a summary of each transformation that
takes place in CompCert, from the input C language to the final native code that
is run on the \gls{CPU}.

\definecolor{bgbox1}{HTML}{b3e2cd}
\definecolor{bgbox2}{HTML}{cbd5e8}
\definecolor{bgbox3}{HTML}{fdcdac}
\definecolor{ircolor}{HTML}{e78ac3}

\tikzset{
  numlabel/.style={draw,circle,inner sep=0.5mm,fill=white},
  ir/.style={draw,very thick,black, fill=ircolor!70, align=center},
  pass/.style={draw, very thick, rounded corners, fill=white, align=center},
  extpass/.style={draw, dotted, very thick, rounded corners, fill=white, align=center},
  bgbox/.style={draw=none},
  ed/.style={->, very thick, >=stealth},
}

\begin{figure}
  \centering
  \begin{tikzpicture}[node distance=1cm and 0.75cm,>=Latex,shorten >=1pt,font=\sffamily]
    \node[ir] (compcertc) {CompCert C};
    \node[pass,below=of compcertc] (evalorder) {specify evaluation\\order};
    \node[left=of evalorder] (paddingfirst) {};
    \node[ir,right=of evalorder] (cstrategy) {Cstrategy};
    \node[pass,right=of cstrategy] (sideeffect) {remove\\side-effects};
    \node[ir,right=of sideeffect] (clight) {Clight};
    \node[pass,right=of clight] (simplify) {simplify};
    \node[ir,below=of simplify] (csharpminor) {C\#minor};
    \node[pass,left=of csharpminor] (stackallocation) {stack\\allocation};
    \node[ir,left=of stackallocation] (cminor) {Cminor};
    \node[pass,left=of cminor] (instrselection) {instruction\\selection};
    \node[ir,left=of instrselection] (cminorsel) {CminorSel};
    \node[pass,below=of cminorsel] (cfgconstr) {CFG\\construction};
    \node[ir,right=of cfgconstr] (rtl) {\rtl{}};
    \node[pass,right=of rtl] (regalloc) {register\\allocation};
    \node[pass,below=of rtl] (opts) {optimisations};
    \node[ir,right=of regalloc] (ltl) {\textsc{Ltl}};
    \node[pass,right=of ltl] (linearisation) {linearisation};
    \node[ir,below=of linearisation] (linear) {Linear};
    \node[pass,below=of linear] (stackframe) {stack frame\\layout};
    \node[ir,left=of stackframe] (mach) {Mach};
    \node[pass,left=of mach] (assemblygen) {assembly\\generation};
    \node[ir,below left=of assemblygen] (assembly) {Assembly};
    \node[pass,right=of assembly] (nativegen) {native code\\generation};
    \node[ir,right=of nativegen] (nativecode) {Native Code};
    \path (paddingfirst) |- node (paddingsecond) {} (cfgconstr);
    \begin{pgfonlayer}{background}
      \node[fill=bgbox1,bgbox,fit={(paddingfirst)(instrselection)(evalorder)(cstrategy)(csharpminor)(cminor)}] (bgfirst)
      {};
      \node[fill=bgbox2,bgbox,fit={($(csharpminor.south east)-(0,2)$)(cfgconstr)(paddingsecond)(linearisation)(stackframe)}] (bgsecond)
{};
    \end{pgfonlayer}
    \path (bgfirst.north west) -- node[rotate=90,yshift=-5mm]
    (frontend) {Front End} (bgfirst.south west);
    \path (bgsecond.north west) -- node[rotate=90,yshift=-5mm]
    (backend) {Back End} (bgsecond.south west);
    \draw[ed] (compcertc) -- (evalorder) -- (cstrategy);
    \draw[ed] (cstrategy) -- (sideeffect) -- (clight);
    \draw[ed] (clight) -- (simplify) -- (csharpminor);
    \draw[ed] (csharpminor) -- (stackallocation) -- (cminor);
    \draw[ed] (cminor) -- (instrselection) -- (cminorsel);
    \draw[ed] (cminorsel) -- (cfgconstr) -- (rtl);
    \draw[ed] (rtl) -- (regalloc) -- (ltl);
    \draw[ed] (rtl) to [out=300,in=60] (opts) to [out=120,in=240] (rtl);
    \draw[ed] (ltl) -- (linearisation) -- (linear);
    \draw[ed] (linear) -- (stackframe) -- (mach);
    \draw[ed] (mach) -- (assemblygen) -| (assembly);
    \draw[ed] (assembly) -- (nativegen) -- (nativecode);
  \end{tikzpicture}
  \caption[CompCert diagram describing the intermediate languages.]{CompCert
    intermediate languages in the front end and back end of the compiler.  The
    parts outside of the coloured boxes are trusted, whereas all the languages
    transformations within the coloured boxes are verified and untrusted.}%
  \label{fig:bg:compcert-languages}
\end{figure}

\gls{CompCert} proofs follow two main patterns.  The first type of correctness
proof is a direct proof that the algorithm implemented in Coq is correct.  These
types of proofs do not carry any run time cost, because they reason purely about
the algorithm that was implemented and they can be erased when the C compiler is
extracted to OCaml code.  The translation algorithm itself is proven correct, so
no additional code that is associated with the proof is needed when one is using
the compiler.  On the other hand, some transformations are easier to build a
validator for that checks the result of the transformation instead of proving
that the transformation algorithm always produces correct results.  This is
called \emph{\gls{translation validation}}, and CompCert uses this technique to
prove the correctness of the register allocation transformation, for example.
Register allocation reduces to the graph colouring problem, where every register
with overlapping liveness properties are connected by edges.  The algorithm is
then tasked to colour the graph with the fewest colours so that connected nodes
have different colours.  It is therefore clear that checking the colouring of
such a graph is simple, as one can check that nodes connected by an edge have
different colours.  However, proving that such an algorithm will always produce
such a graph is more difficult because many heuristics can be used during the
graph colouring to try and achieve the fewest colours efficiently.  The
algorithm that checks the colours of the graph is called the validator.
CompCert implements register allocation in this way, performing the register
allocation in unverified OCaml code, and implementing a verified validator for
register allocation in Coq.  This still produces a verified translation, as
CompCert is allowed to fail on inputs, in which case the correctness theorem
holds trivially.  For translation validation, that means that as long as
compilation only proceeds when the validator succeeds, it is equivalent to
having verified the transformation correct.  However, unlike verifying
transformations correct directly, using a validator means that it needs to check
the correctness criterion every time the compiler is run, meaning the efficiency
of the validator is also important to take into account.

\subsection{CompCert correctness theorem}%
\label{sec:bg:compcert-correctness-theorem}

The correctness theorem of CompCert should ensure that the compiler does not
introduce bugs into the program as it is translated from C into Assembly.  To do
so, one must first define the execution of programs written in C, as well as
programs written in Assembly.

\paragraph{Small-step semantics}

Small-step semantics in CompCert are defined as a labelled transition system.  A
transition between states $s$ and $s'$, emitting events $t$, is denoted as a
single step $s \oset{t}{\longrightarrow} s'$.  In addition to that, one also
needs to designate an initial state and a final state, the latter returning the
final result after executing the program.  Finally, each program is also
associated with a global environment.  CompCert defines a small-step semantics
framework that can be reused by most languages, providing additional useful
constructs such as the reflexive, transitive closure of $\longrightarrow$
denoted as $\longrightarrow^{*}$ or the transitive \enquote{plus} closure
denoted as $\longrightarrow^{+}$.  A semantics for a CompCert language is
created by defining the type for the state, for a step in the semantics, and the
initial and final states and the global environment of the program.

\paragraph{Program behaviour}

Once the semantics have been defined, the global behaviour of the program can be
defined, which will be used to express the correctness theorem.  A C program can
behave in three main ways, it can either \emph{terminate successfully},
\emph{diverge}, or finally \emph{go wrong}.  Successful termination happens when
the final state is reached from the initial state, in which case the final state
will contain the return-value of the main function in addition to a finite
stream of events.  If the program diverges, then the program is executing
indefinitely, in which case it will emit a possibly infinite stream of events
but no return value.  Finally, if the program goes wrong, for example when
undefined behaviour is encountered, then that means that there is no more valid
step defined in the semantics from the current state.

\subsubsection{Correctness theorem}%
\label{sec:bg:correctness-theorem}

The correctness theorem can be stated as a simulation between the semantics of
the source program and the semantics of the compiled program.  The final
correctness theorem in CompCert is a backward simulation between the source
program semantics and compiled program semantics, stated as follows.

\begin{theorem}[Semantic preservation]\label{thm:semantic-preservation}
  If a successfully compiled assembly program $C$ has a behaviour $B$ which does
  not go wrong, then there must exist behaviour $B'$ of source program $S$ which
  may be less defined than behaviour $B$, which is expressed by $B' \succsim B$.

  {\normalfont\begin{equation*}
      \yhfunction{compile\_c}\ S = \yhconstant{OK}\ C
      \implies (\forall B\ldotp C \Downarrow B \implies (\exists B'\ldotp S
      \Downarrow B' \land B' \succsim B))
  \end{equation*}}
\end{theorem}

A direct corollary of this is the following semantic preservation, where one
proves that the source program is \emph{safe}, i.e. that it is free of behaviour
that goes wrong.  In that case, one cannot define any more behaviour, so $B$
must be a valid behaviour for the source program.

\begin{corollary}[Refinement of compilation]
  If a successfully compiled assembly program $C$ has a behaviour $B$ and the
  source program is \emph{safe}, meaning it is free of undefined behaviour, then
  $B$ must be a behaviour of source program $S$.

  {\normalfont\begin{equation*} \yhfunction{compile\_c}\ S = \yhconstant{OK}\ C
      \land \mathit{safe}(S) \implies (\forall B\ldotp C \Downarrow B \implies S
      \Downarrow B)
  \end{equation*}}
\end{corollary}

This theorem correctly conveys the property that the compilation of the source
program should not introduce any bugs, which is especially clear in the
corollary.  As long as one proves that the source program does not have any
undefined behaviour, then every behaviour of the compiled program needs to be a
behaviour of the source program.

These theorems are proven by showing simulations between the semantics of the
source program and the semantics of the compiled program.  In particular,
\cref{thm:semantic-preservation} can be proven to hold if one can show a
backward simulation between the source program and the compiled program, which
is how the \namecref{thm:semantic-preservation} is proven in CompCert.  A
backward simulation is a number of relations between states and transitions of
the semantics of both programs.  The heart of the backward simulation can be
represented as a \emph{simulation diagram} and shown in
\cref{fig:bg:backwards-simulation}.  \Cref{fig:bg:backwards-simulation-diagram}
shows the main two types of simulation diagrams that may hold for a specific
transition, where the solid lines represent what can be assumed, and the dashed
lines have to be shown to hold or exists.  First, for a step in the compiled
program from state $c_1$ to state $c_2$ that emits trace $t$ and a state $s_1$
that matches with the state $c_1$ using the $\approx$ relation, there must exist
one or more steps in the source program that also emit the same trace $t$ and
produces a state $s_2$ that matches with state $c_2$.  If the state transition
in the compiled program does not emit a trace, but some measure on the state has
a well-founded order that decreases, then the source semantics may stall as long
as $s_1$ matches with both $c_1$ and $c_2$.  In addition to that, to be able to
show semantic preservation from the backward simulation, one also needs to prove
that the compiled program makes progress if there exist transitions in the
source program from two matching states $s_1$ and $c_1$.  This is shown in
\cref{fig:bg:backwards-progress-property}.

\tikzset{imparrow/.style={shorten >=1pt,>=Latex,->}}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.6\linewidth}
    \centering
    \begin{tikzpicture}
      \node (s1) {$s_1$};
      \node[below=of s1] (s2) {$s_2$};
      \node[right=of s1] (t1) {$c_1$};
      \path (s2) -| node (t2) {$c_2$} (t1);
      \draw[imparrow,dashed] (s1) -- node[left] {$t$} node[below right,font=\scriptsize] {$+$} (s2);
      \draw[imparrow] (t1) -- node[right] {$t$} (t2);
      \draw (s1) -- node[above] {$\approx$} (t1);
      \draw[dashed] (s2) -- node[below] {$\approx$} (t2);
      \begin{scope}[xshift=5cm,yshift=1cm]
      \node (s1) {$s_1$};
      \node[below=of s1] (s2) {$s_2$};
      \node[right=of s1] (t1) {$c_1$};
      \path (s2) -| node (t2) {$c_2$} (t1);
      \draw[imparrow,dashed] (s1) -- node[left] {$t$} node[below right,font=\scriptsize] {$*$} (s2);
      \draw[imparrow] (t1) -- node[right] {$t$} (t2);
      \draw (s1) -- node[above] {$\approx$} (t1);
      \draw[dashed] (s2) -- node[below] (app) {$\approx$} (t2);
      \node[below=2mm of app] {with $|c_2| < |c_1|$};
      \end{scope}
    \end{tikzpicture}
  \caption{Backward simulation diagrams with a well-founded order on the
    compiled program state.}%
  \label{fig:bg:backwards-simulation-diagram}
  \end{subfigure}\hfill%
  \begin{subfigure}[t]{0.35\linewidth}
    \centering
    \begin{tikzpicture}
      \node (s1) {$s_1$};
      \node[below=of s1] (s2) {$s_2$};
      \node[right=of s1] (t1) {$c_1$};
      \path (s2) -| node (t2) {$c_2$} (t1);
      \draw[imparrow] (s1) -- node[left] {$t$} node[below right,font=\scriptsize] {$+$} (s2);
      \draw[imparrow,dashed] (t1) -- node[right] {$t$} (t2);
      \draw (s1) -- node[above] {$\approx$} (t1);
    \end{tikzpicture}
    \caption{Progress property needed for the backward simulation.}%
    \label{fig:bg:backwards-progress-property}
  \end{subfigure}
  \caption{Examples of simulation diagrams that make up the backward
    simulation.}%
  \label{fig:bg:backwards-simulation}
\end{figure}

Backward simulations are hard to prove by induction on the semantics of the
compiled program.  This is because the matching relation requires that one
define a decompilation of the compiled program into the source program to match
individual instructions with individual sections of the code.  This may not be
possible for many constructs, as statements generally compile to a group of
instructions.  Instead, a forward simulation, shown in
\cref{fig:bg:forwards-simulation-diagram}, is much more natural to prove
correct, because one can induct over the semantics of the source program and
define the matching relation in terms of the compilation.  However, without
additional properties, the forward simulation is not sufficient to imply
semantic preservation, because a forward simulation would allow for additional
behaviours in the compiled program that were not behaviours of the source
program.

However, if one can show that the semantics of the compiled program is
deterministic, then the forward simulation implies the backward simulation.
%\footnote{In practice, one can be more precise and have properties on
%  the source and compiled program, such that one can show that the source
%  program is \emph{receptive} and the target program \emph{determinate}.}
In addition to that, forward simulations as well as backward simulations can be
composed.  This means that one can prove a forward simulation for each
transformation shown in \cref{fig:bg:compcert-languages} from Clight to Assembly
which implies a forward simulation from Clight to Assembly.  After showing that
the semantics of Assembly is deterministic, one can then prove a backward
simulation from Assembly to Clight.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.48\linewidth}
    \centering
    \begin{tikzpicture}
      \node (s1) {$s_1$};
      \node[below=of s1] (s2) {$s_2$};
      \node[right=of s1] (t1) {$c_1$};
      \path (s2) -| node (t2) {$c_2$} (t1);
      \draw[imparrow] (s1) -- node[left] {$t$} (s2);
      \draw[imparrow,dashed] (t1) -- node[right] {$t$} node[below left,font=\scriptsize] {$+$} (t2);
      \draw (s1) -- node[above] {$\approx$} (t1);
      \draw[dashed] (s2) -- node[below] {$\approx$} (t2);
    \end{tikzpicture}
  \end{subfigure}\hfill%
  \begin{subfigure}[b]{0.48\linewidth}
    \centering
    \begin{tikzpicture}
      \node (s1) {$s_1$};
      \node[below=of s1] (s2) {$s_2$};
      \node[right=of s1] (t1) {$c_1$};
      \path (s2) -| node (t2) {$c_2$} (t1);
      \draw[imparrow] (s1) -- node[left] {$t$} (s2);
      \draw[imparrow,dashed] (t1) -- node[right] {$t$} node[below left,font=\scriptsize] {$*$} (t2);
      \draw (s1) -- node[above] {$\approx$} (t1);
      \draw[dashed] (s2) -- node[below] (app) {$\approx$} (t2);
      \node[below=2mm of app] {with $|s_2| < |s_1|$};
    \end{tikzpicture}
  \end{subfigure}
  \caption[Examples of forward simulation diagrams.]{Forward simulation
    diagrams, either performing one or more steps in the target program using
    the \enquote{plus} forward simulation, or performing zero or more steps in
    the target program with a well-founded order on the target program states.}%
  \label{fig:bg:forwards-simulation-diagram}
\end{figure}

\subsection{Instruction scheduling in CompCert}%
\label{sec:bg:instruction-scheduling-compcert}

There are a few implementations of scheduling algorithms in CompCert or in
derivatives of CompCert that are relevant to this dissertation, as the
scheduling algorithm is central to the \gls{HLS} transformation.  First, I will
describe implementations of list scheduling in CompCert, followed by an
implementation of superblock scheduling and finally an implementation of trace
scheduling.  These scheduling methods all implement validators based on symbolic
execution of the code.

Symbolic expressions are expressions in terms of initial values of registers at
the start of the block.  For example, \cref{fig:bg:symbolic-evaluation} shows an
example of symbolic execution, where the result is a map from \emph{resources},
which might be memory or registers, to symbolic expressions, which are
expressions in terms of initial values of registers.  The symbolic execution
proceeds sequentially through the linear block and updates the symbolic state.
If a register is read from, its current symbolic expression is looked up in the
symbolic state and replaces the register to build a new symbolic expression.

The symbolic state can then be used to validate code transformations.  If the
code transformation only reorders instructions, then the correctness of the
transformation only relies on data dependencies, which is naturally encoded in
the symbolic state.  As long as one can show that the symbolic expressions of
two registers are structurally equivalent, then this means that the reordering
of instructions did not violate any data dependencies, and this should imply
that the behaviour of the original block and the reordered block are the same as
well.

\begin{figure}
  \centering
  \hfill\begin{subfigure}[t]{2cm}
    \centering
\begin{rtllisting}
z := x + y;
t := z * y;
M[12] := x;
y := M[x];
\end{rtllisting}
  \end{subfigure}\hfill%
  \begin{subfigure}[t]{0.48\linewidth}
    \vspace{-1\baselineskip}
    \begin{equation*}
      \begin{array}{rcl}
        \mrtl{z} & \mapsto & \avar{x} \aadd \avar{y}\\
        \mrtl{t} & \mapsto & (\avar{x} \aadd \avar{y}) \amul \avar{y}\\
        \mrtl{M} & \mapsto & \mrtl{store}(\avar{M}, \avar{x}, 12)\\
        \mrtl{y} & \mapsto & \mrtl{load}(\mrtl{store}(\avar{M}, \avar{x}, 12),
                             \avar{x})\\
        r & \mapsto & r^0 \quad \text{for all other registers } r\\
      \end{array}
    \end{equation*}
  \end{subfigure}
  \caption[Example of symbolic execution adapted from Tristan and
  Leroy.]{Example of symbolic execution adapted from
    \textcite{tristan08_formal_verif_trans_valid}.}
  \label{fig:bg:symbolic-evaluation}
\end{figure}

\subsubsection{List scheduling}

\textcite{tristan08_formal_verif_trans_valid} were the first to propose adding
scheduling to a verified compiler.  \textcite{six20_certif_effic_instr_sched}
then optimise the validator and extend it so that it works with the Kalray KVX
\gls{VLIW} processor as a post-pass scheduling step in their translation and is
integrated into a fork of CompCert called CompCert KVX~\cite[]{six23_c}.  In
both these cases, the list scheduling algorithm works on the Mach intermediate
language in CompCert, but in the case of CompCert KVX the list scheduling pass
is also used to transform the Mach into Assembly.  List scheduling is performed
on \glspl{basic block}.  These are implicitly represented in
\textcite{tristan08_formal_verif_trans_valid}, but are explicitly constructed in
\textcite{six20_certif_effic_instr_sched} as an \textsc{AsmBlock} language,
making it easier to reason about transformations in individual blocks.

The schedule is validated after the fact by running symbolic execution before
and after scheduling.  The symbolic execution of both blocks starts with the
same symbolic state, sequentially symbolically executing each instruction while
updating the symbolic state.  The final states reached for both the block before
and after scheduling can then be compared.  If the scheduler only
\emph{reorders} instructions, structural equality suffices for comparing
symbolic expressions, as an instruction is only reordered if it does not break
any data-dependencies.  In that case, the symbolic expressions for the reordered
instruction would be identical as they were independent.  However, the scheduler
is unverified, and it could therefore introduce new instructions.  This means
that the comparison of the symbolic state is not enough to show equivalent
behaviour.  Consider the following example:

\begin{center}
\begin{tikzpicture}
\node[] (initial) {\begin{minipage}{2.6cm}
\begin{rtllisting}
r3 := r2 + 4;
\end{rtllisting}
\end{minipage}};
\node[right=3.5cm of initial] (scheduled) {
  \begin{minipage}{2.6cm}
\begin{rtllisting}
r3 := 5 / r1;
r3 := r2 + 4;
\end{rtllisting}
  \end{minipage}
};
\draw[-{Latex[length=3mm]}] ($(initial.east)+(0.5,0)$) -- node [below] {scheduling}
($(scheduled.west)-(0.5,0)$);
\end{tikzpicture}
\end{center}

In this case, the scheduler introduces an additional instruction that is later
overwritten.  The symbolic states will therefore be equivalent, however, the
transformed block may encounter undefined behaviour when \rtlinline`r1` is 0.
To guard against the scheduler introducing additional instructions that may have
undefined behaviour, \citeauthor*{tristan08_formal_verif_trans_valid} introduce
the concept of \emph{constraints}, which is the set of all previously
encountered operations.  In the case of the example, the constraint set of the
original block would be empty, whereas the constraints of the second block would
be $\set{\mrtl{5} \adiv \mrtl{r1}}$.  For correctness, the scheduled basic block
should therefore not introduce new constraints, like in the previous example.
Instead, the validation has to ensure that the constraints of the scheduled
block are a subset of the constraints of the original block.

Together, this forms a validation algorithm for list scheduling by comparing the
symbolic states of the basic blocks for equivalence, and ensuring that the
constraints form a subset.  The post-pass scheduler by
\citeauthor*{six20_certif_effic_instr_sched} optimises the validation algorithm
by using hash-consing to replace the expensive expression comparison by pointer
comparisons.

\subsection{Trace scheduling}%
\label{sec:bg:trace-scheduling}

\begin{figure}
  \begin{subfigure}[b]{0.48\linewidth}
    \centering
    \begin{tikzpicture}[shorten >=1pt,>=Latex,node distance=0.75cm and 0.5cm]
      \node[instrnode,fill=nodea] (a) {};
      \node[instrnode,fill=nodeb,below=of a] (b) {};
      \node[instrnode,fill=nodec,below left=of b] (c) {};
      \node[instrnode,fill=noded,below right=of b] (d) {};
      \node[instrnode,fill=nodee,below=of c] (e) {};
      \node[instrnode,fill=nodef,below=of e] (f) {};
      \path (f) -| node[instrnode,fill=nodef,dashed] (fp) {} (d);
      \node[instrnode,fill=nodeg,below=of f] (g) {};
      \node[instrnode,fill=nodeg,below=of fp,dashed] (gp) {};
      \node[instrnode,fill=nodeh,below right=of g] (h) {};
      \begin{pgfonlayer}{background}
        \node[fit={(b)(c)(d)(e)(f)(g)(gp)(fp)},bblock] (fitall) {};
        \node[fit={(h)},bblock] (fith) {};
        \node[fit={(a)},bblock] (fita) {};
      \end{pgfonlayer}
      \draw[->] (a) -- (fitb);
      \draw[->] (b) -- (c);
      \draw[->] (b) -- (d);
      \draw[->] (d) -- (fp);
      \draw[->] (c) -- (e);
      \draw[->] (e) -- (f);
      \draw[->] (f) -- (g);
      \draw[->] (g) -- (fith);
      \draw[->] (fp) -- (gp);
      \draw[->] (gp) -- (fith);
      \draw[dashed] (f) -- (fp);
      \draw[dashed] (g) -- (gp);
      \draw[->] (g) to [loop,looseness=1.2,out=150,in=150] (fitb.north west);
      \draw[->] (gp) to [loop,looseness=1.2,out=30,in=30] (fitb.north east);
    \end{tikzpicture}
    \caption{Graph of trees}\label{fig:block-comparison:graph-of-trees}
  \end{subfigure}\hfill%
  \begin{subfigure}[b]{0.48\linewidth}
    \centering
    \pgfdeclarelayer{bga}
    \pgfdeclarelayer{bgb}
    \pgfdeclarelayer{bgc}
    \pgfsetlayers{bgc,bgb,bga,main}
    \begin{tikzpicture}[shorten >=1pt,>=Latex,node distance=0.75cm and 0.5cm]
      \node[instrnode,fill=nodea] (a) {};
      \node[instrnode,fill=nodeb,below=of a] (b) {};
      \node[instrnode,fill=nodec,below left=of b] (c) {};
      \node[instrnode,fill=noded,below right=of b] (d) {};
      \node[instrnode,fill=nodee,below=of c] (e) {};
      \node[instrnode,fill=nodef,below right=of e,yshift=-2mm] (f) {};
      \node[instrnode,fill=nodeg,below=of f] (g) {};
      \node[instrnode,fill=nodeh,below right=of g] (h) {};
      \begin{pgfonlayer}{bga}
        \node[fit={(c)(e)},bblock,fill=black!10] (fitce) {};
      \end{pgfonlayer}
      \begin{pgfonlayer}{bgb}
        \node[fit={(b)(d)(fitce)},bblock] (fitcond) {};
        \node[fit={(f)(g)},bblock] (fitstraight) {};
      \end{pgfonlayer}
      \begin{pgfonlayer}{bgc}
        \node[fit={(fitcond)(fitstraight)},bblock,fill=black!20] (fitall) {};
        \node[fit={(h)},bblock,fill=black!20] (fith) {};
        \node[fit={(a)},bblock,fill=black!20] (fita) {};
      \end{pgfonlayer}
      \draw[->] (a) -- (fitall);
      \draw[->] (b) -- (d);
      \draw[->] (b) -- (fitce.north);
      \draw[->] (c) -- (e);
      \draw[->] (f) -- (g);
      \draw[->] (fitcond) -- (fitstraight);
      \draw[->] (g) -- (fith);
      \draw[->] (g) to [loop,looseness=1.4,out=150,in=150] ($(fitall.north) - (0.3,0)$);
    \end{tikzpicture}
    \caption{Block transfer language (\textsc{Btl})}\label{fig:block-comparison:btl}
  \end{subfigure}
  \caption[Comparison of the graph of trees structure and
  \textsc{Btl}.]{Comparison of the graph of trees structure and the block
    transfer language (\textsc{Btl}) structure, extending the comparisons shown
    in \cref{fig:block-comparison}.}%
  \label{fig:block-comparison-second}
\end{figure}

\textcite{tristan08_formal_verif_trans_valid} also formalises a version of trace
scheduling, where instructions can be moved along traces of the program that are
free of back edges, and can therefore be moved across basic block boundaries.
This gives more freedom to the scheduler, leading to more optimal code in most
cases.  The first part of the trace scheduling algorithm is to transform the
\gls{CFG} into a graph of trees representation, shown in
\cref{fig:block-comparison:graph-of-trees}.  This representation is similar to
the superblock representation shown in \cref{fig:block-comparison:superblocks}
in that tail duplication is needed to represent any internal joining control
flow.  However, this tail duplication then means that the whole block can be
represented as a single tree.  Instructions can then be reordered by the
scheduler within a tree.  In general, tail duplication should be avoided as it
can result in an exponential increase in the number of nodes.  The boundaries of
the trees are chosen by calculating possible cut points in the \gls{CFG}, which
correspond to labels that are the target of back edges, or function calls.

Validation can then be done by running the symbolic execution over the tree and
comparing symbolic states in the same way as was done for list scheduling when
one reaches the leaves of the tree (i.e. the exit nodes).  While traversing the
tree, one must also ensure that the same conditions are encountered and
evaluated, and these have to remain in the same order in the tree.  Instructions
that are moved after a control flow statement need to be duplicated by the
scheduler to remain correct.

In general, this validation technique for trace scheduling is effective at
verifying complex schedules, but it can quickly become infeasible to check the
equivalence as the size of the blocks grows.  This is mainly due to two sources
of inherent inefficiencies in the validation algorithm.  First, the size of the
symbolic expressions can grow exponentially as they do not share any
subexpressions, which means that comparing two expressions can take time.  This
affects both the list scheduling and the trace scheduling validator, and is
addressed by \citeauthor*{six20_certif_effic_instr_sched} through hash-consing.
Secondly, comparing two trees symbolically can also contain an exponential
number of comparisons of symbolic states.  This means that the choice of cut
points becomes important to try and minimise the size of the trees and still
achieve a good schedule.

\subsubsection{Superblock and extended block scheduling}

\textcite{six22_formal_verif_super_sched} then formalise superblock scheduling
as a pre-pass optimisation at the \rtl{} level, which is a restricted form of
trace scheduling that was specifically developed to target VLIW
processors~\cite[]{hwu93_super}.  The idea is to translate \rtl{} code into
\textsc{Rtlpath} code, which is a superblock representation of the original code
containing multiple traces through the extended non-branching block.  The
superblock construction needs to take into account which traces through the
program will be executed more frequently and must group those together.
\citeauthor{six22_formal_verif_super_sched}'s scheduler then reorders
instructions within a superblock trace, and also combines them where it is
advantageous to do so, making it necessary to allow for rewrites in the symbolic
expressions as well.

The validator works similarly to the post-pass list scheduling algorithm in
CompCert KVX, but it is extended to support symbolic expression normalisation to
verify rewrites and expansion of instructions correct.  It therefore also
includes all the optimisations from the list scheduling post-pass scheduler in
CompCert KVX such as hash-consing.  In addition to that, liveness is added to
the correctness theorem so that equivalence only needs to be checked for
registers that are live at this particular superblock exit.  This makes it
possible to introduce new registers, and allows for instructions to be expanded
into multiple instructions, without having to reason about any intermediate
registers that were introduced.

In contrast to the list scheduling transformation, the superblock transformation
requires validation of symbolic states at each exit of the superblock with the
corresponding exit in the scheduled \gls{superblock}.  This is similar to the
validation of graph of trees, except that traces are still lists instead of
trees.  \textcite[]{gourdin23_fvopbs} later replaced \textsc{Rtlpath} with
\gls{BTL}, a general intermediate block language shown in
\cref{fig:block-comparison:btl}, where blocks are either:
\begin{enumerate*}[label=(\arabic*)]
\item a sequence of blocks, shown in the diagram as being connected by an arrow
  inside of a grey box,
\item a conditional instruction containing two blocks, shown as a node with two
  split arrows pointing to a block,
\item a standard instruction, shown as a simple node in the graph, or finally
\item a control-flow instruction, shown as a node in the graph with an edge
  exiting the block and pointing to a new external block.
\end{enumerate*}
\textsc{Btl} is defined as a nested structure of blocks with conditionals, and
are therefore general enough to represent \glspl{hyperblock}.  The main
difference between the graph of trees representation and \textsc{Btl} is that
graph of trees cannot represent a sequence of two trees, which is representable
in \textsc{Btl}.  As the language is not restricted to superblocks anymore, the
scheduler was modified to work with \emph{extended blocks}, which are
essentially equivalent to the graph of trees representation, as they are trees
without any internal joining \gls{control flow}.  This makes the scheduler more
flexible, making it possible to apply light loop pipelining optimisations by
scheduling an unrolled loop.  However, the symbolic execution is still performed
on each trace through the program, and no sharing between tracing takes place,
meaning that the validation is exponential in the number of internal joins.
Only extended blocks are scheduled so this is avoided.

% It is notable that these unverified tools tend towards fewer, larger passes,
% where several optimisations are packed into \enquote{scheduling}.  This
% minimises the number of times the solver needs to be invoked, and gives it the
% best chance of finding a global optimum. Verified tools such as CompCert and
% Vericert, on the other hand, tend towards more, smaller passes that are
% individually feasible to prove correct.

% \subsection{Diffs w.r.t. superblock scheduling}

% \begin{itemize}

% \item Superblocks are well suited to VLIW processors [citation needed];
%  hyperblocks are well suited to HLS [citation needed].  Direct comparison is
%  tricky because the two tools are based on incompatible versions of CompCert.

%\item Both use translation validation, but we use a verified SAT solver as part
% of this.  (Superblock scheduling doesn't use predicates so doesn't need a SAT
% solver.)

% \item We introduce two intermediate languages (RTLBlock and RTLPar) that we
%   argue are easier to work with than the RTLPath language used in the superblock
%   scheduling work.

% \end{itemize}

% \JW{What's the link between hyperblocks and SDC-based scheduling? Are these
% orthogonal?}\YH{Well the original SDC papers only talk about basic blocks, and
% not SDC hyperblock scheduling.  But it is easy enough to adapt.  SDC just
% requires some kind of blocks without control-flow.} \JW{Ok cool, so you can
% have SDC scheduling without hyperblocks. Can you have hyperblocks without SDC
% scheduling? (I guess so, but just wanted to be completely sure.)}\YH{Yes
% exactly, you can perform any type of basic block scheduling on hyperblocks as
% well I think, as long as you mark the dependencies correctly.  But in the
% worst case you could even just see hyperblocks as basic blocks and ignore
% predicates.  It would still be correct but just less efficient.}
%
% \JW{It would be interesting to make the case for SDC-based scheduling. Is it
% superior to other scheduling mechanisms? Is it so much superior that it's
% worth the additional complexity, compared to, say, list scheduling? }\YH{SDC
% scheduling is more flexible with the type of constraint to optimise for
% (latency / throughput). I don't think it's too meaningful for compilers, where
% there isn't as much flexibility.}  \JW{Thanks. Does Vericert have any idea
% that the scheduling is being performed using SDC? Or is the SDC stuff entirely
% internal to your unverified OCaml code? That is, could you swap your scheduler
% with a different one that isn't based on SDC, and not have to change anything
% in Vericert?}\YH{The verified part doesn't know about SDC at all, so it could
% be changed to any other scheduler that acts on hyperblocks.}

\section{Summary}%
\label{sec:bg:summary}

This chapter described the process of high-level synthesis, as well as how
verification is normally performed when using high-level synthesis to produce
hardware designs.  The main conclusion is that when using high-level synthesis
one is mainly relying on testing to check the equivalence between high-level
designs and the low-level hardware design.  Further testing in then purely
performed on the low-level hardware design at the register-transfer level, where
it can be difficult to verify behaviour compared to using the high-level design.

I then gave an overview of CompCert, describing the correctness theorem and
describing existing attempts at formally verifying instruction scheduling, which
is the main optimisation and transformation that \gls{HLS} tools perform to
generate hardware.  The current formalised scheduling passes are mainly
targeting \glspl{CPU}, where it is more important to schedule instructions over
traces.  \gls{HLS} benefits from using general hyperblocks, especially combining
blocks where control flow is joined internally, meaning it benefits from a
different intermediate representation that is more tailored to hardware.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% TeX-engine: luatex
%%% End:
