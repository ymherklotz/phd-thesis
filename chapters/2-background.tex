\chapter{Background}%
\label{sec:background}

\begin{quote}\itshape
  This chapter briefly describes \glspl{FPGA} followed by introducing
  \glsfirst{HLS} and the current state-of-the-art optimisations used by
  \gls{HLS} tools, focusing in particular on static scheduling.  Next, common
  testing and verification workflows for \gls{HLS} are also described.  Finally,
  an overview of \compcert{} is given, on which \vericert{} is built.
\end{quote}

\section{Field Programmable Gate Arrays}%
\label{sec:bg:fpga}

This section introduces \glsfirstplural{FPGA}, which is assumed to be the final
target for the hardware produced by Vericert, as well as the \gls{HLS} tools
that Vericert is directly compared against.

\Glspl{FPGA} are programmable hardware chips that can be used to implement and
run custom hardware without having to tape-out an \gls{ASIC}, which may take
years of development time.  \Glspl{FPGA} instead provide a platform to test
custom hardware quickly without these turnaround times, and can be reprogrammed
at will in case the hardware ever needs to change.  Because they still allow for
reprogrammability, they can never be as efficient as an equivalent \gls{ASIC}
design, however, for many application having the chance to reprogram the
hardware is an advantage.  In addition to that, an \gls{FPGA} will still
generally be more efficient and more performant than running the same workload
on a general purpose processor.  \Glspl{FPGA} comprise the following four main
components~\cite{boutros21_fpga_archit}, which are also shown in
\cref{fig:bg:fpga-layout}.

\begin{description}
\item[\gls{LUT}] A \gls{LUT} can implement any kind of logic with a set number
  of inputs and a single output.  On an \gls{FPGA}, \glspl{LUT} are often
  grouped into larger programmable logic units called \emph{slices} that can
  handle multiple inputs and outputs.
\item[Programmable interconnect] The \glspl{LUT} are connected using
  programmable interconnects, so that these arbitrary logical units can also be
  connected in arbitrary ways, making it possible to implement any kind of
  hardware design.
\item[\gls{BRAM}] Instead of relying on implementing memories to store a large
  amount of data using \glspl{LUT}, there is often \gls{BRAM} on the \gls{FPGA},
  which provides efficient storage for data.
\item[\gls{DSP}] Finally, \glspl{FPGA} also often contain \glspl{DSP}, which can
  be used to implement common arithmetic functions efficiently, that may
  otherwise take up a lot of space if implemented using \glspl{LUT}.  Some
  common arithmetic functions that are often implemented using \gls{DSP} include
  integer multipliers and multiply-accumulate operations.
\end{description}

\definecolor{connblockcolour}{HTML}{E6CEE9}
\definecolor{lutcolour}{HTML}{E9D8CE}
\definecolor{dspcolour}{HTML}{D2E9CE}
\definecolor{switchcolour}{HTML}{CEDFE9}
\definecolor{bramcolour}{HTML}{D2D1E9}

\begin{figure}
  \centering
  \begin{tikzpicture}[yscale=-1,connblock/.style={draw=black,fill=connblockcolour},
    lut/.style={draw=black,fill=lutcolour},switch/.style={draw=black,fill=switchcolour},
    dsp/.style={draw=black,fill=dspcolour},bram/.style={draw=black,fill=bramcolour},
    emphasize/.style={very thick,draw=Tomato},
    blocklabel/.style={font=\ttfamily\small}]
    \foreach \w in {0,...,3}
    {\pgfmathtruncatemacro{\sw}{2 * \w}
     \foreach \z in {0,...,2}
     {\pgfmathtruncatemacro{\sz}{\z}
       \draw ($(0.5,\sw)+(0,0.\sz)+(0,0.4)$) -- ($(6.5,\sw)+(0,0.\sz)+(0,0.4)$);
       \draw ($(\sw,0.5)+(0.\sz,0)+(0.4,0)$) -- ($(\sw,6.5)+(0.\sz,0)+(0.4,0)$);
       \ifnum\w=3
       \else
         \draw ($(\sw,0.5)+(0.\sz,0)+(1.4,0)$) -- ($(\sw,6.5)+(0.\sz,0)+(1.4,0)$);
         \draw ($(0.5,\sw)+(0,0.\sz)+(0,1.4)$) -- ($(6.5,\sw)+(0,0.\sz)+(0,1.4)$);
       \fi
       }}
    \foreach \z in {0,...,2}
     {\pgfmathtruncatemacro{\sz}{\z}
       \draw[emphasize] ($(2.5,0.5)+(0,0.\sz)-(0,0.1)$) --
       ($(6.5,0.5)+(0,0.\sz)-(0,0.1)$);
       \draw[emphasize] ($(2.5,2.5)+(0,0.\sz)-(0,0.1)$) --
       ($(4.5,2.5)+(0,0.\sz)-(0,0.1)$);
       \draw[emphasize] ($(3.5,1.5)+(0,0.\sz)-(0,0.1)$) --
       ($(5.5,1.5)+(0,0.\sz)-(0,0.1)$);
       \draw[emphasize] ($(3.5,0.5)+(0.\sz,0)-(0.1,0)$) --
       ($(3.5,2.5)+(0.\sz,0)-(0.1,0)$);
       \draw[emphasize] ($(5.5,0.5)+(0.\sz,0)-(0.1,0)$) --
       ($(5.5,1.5)+(0.\sz,0)-(0.1,0)$);
%       \draw ($(\sw,0.5)+(0.\sz,0)+(0.4,0)$) -- ($(\sw,6.5)+(0.\sz,0)+(0.4,0)$);
%       \draw ($(\sw,0.5)+(0.\sz,0)+(1.4,0)$) -- ($(\sw,6.5)+(0.\sz,0)+(1.4,0)$);
%       \draw ($(0.5,\sw)+(0,0.\sz)+(0,1.4)$) -- ($(6.5,\sw)+(0,0.\sz)+(0,1.4)$);
       }
    \foreach \x in {0,...,3}
    \foreach \y in {0,...,3}
    {\pgfmathtruncatemacro{\sx}{2 * \x}
      \pgfmathtruncatemacro{\sy}{2 * \y}
      \filldraw[lut] (\sx,\sy) rectangle ($(\sx,\sy)+(1,1)$);
      \node[blocklabel] at ($(\sx,\sy)+(0.5,0.5)$) {LUT};
      \ifnum\x=3
        \ifnum\y=3
          %
        \else
          \filldraw[connblock] ($(\sx,\sy)+(0.3,1.3)$) rectangle ($(\sx,\sy)+(0.7,1.7)$);
        \fi
        \else
        \ifnum\y=3
          \filldraw[connblock] ($(\sx,\sy)+(1.3,0.3)$) rectangle ($(\sx,\sy)+(1.7,0.7)$);
        \else
          \filldraw[connblock] ($(\sx,\sy)+(1.3,0.3)$) rectangle ($(\sx,\sy)+(1.7,0.7)$);
          \filldraw[connblock] ($(\sx,\sy)+(0.3,1.3)$) rectangle ($(\sx,\sy)+(0.7,1.7)$);
          \filldraw[switch] ($(\sx,\sy)+(1.2,1.2)$) rectangle
          ($(\sx,\sy)+(1.8,1.8)$);
          \node[blocklabel] at ($(\sx,\sy)+(1.5,1.5)$) {SW};
        \fi
      \fi
    }
    \filldraw[dsp] (2,0) rectangle ($(2,0)+(1,1)$);
    \node[blocklabel] at ($(2,0)+(0.5,0.5)$) {DSP};
    \filldraw[bram] (2,2) rectangle ($(2,2)+(1,1)$);
    \node[blocklabel] at ($(2,2)+(0.5,0.5)$) {BRAM};
    \filldraw[dsp] (4,6) rectangle ($(4,6)+(1,1)$);
    \node[blocklabel] at ($(4,6)+(0.5,0.5)$) {DSP};
    \filldraw[bram] (4,4) rectangle ($(4,4)+(1,1)$);
    \node[blocklabel] at ($(4,4)+(0.5,0.5)$) {BRAM};
    \foreach \v in {{2,2},{2,0},{4,0},{4,2},{6,0}}
    {\draw[emphasize] (\v) rectangle ($(\v)+(1,1)$);}
    \foreach \v in {{2,2},{2,0},{4,0}}
    {\draw[emphasize] ($(\v)+(1.3,0.3)$) rectangle ($(\v)+(1.7,0.7)$);}
    \foreach \v in {{2,0},{4,0}}
    {\draw[emphasize] ($(\v)+(1.2,1.2)$) rectangle ($(\v)+(1.8,1.8)$);}
    \foreach \v in {{4,0}}
    {\draw[emphasize] ($(\v)+(0.3,1.3)$) rectangle ($(\v)+(0.7,1.7)$);}
  \end{tikzpicture}
  \caption[FPGA layout, showing an example of a design that is placed and routed
  on the FPGA highlighted in red.  The white blocks correspond to LUTs, followed
  by turquoise blocks being DSPs and green blocks being BRAMs.  The programmable
  interconnects are made up of connection blocks and switches shown in beige and
  yellow respectively.]{\Gls{FPGA} layout, showing an example of a design that
    is placed and routed on the \gls{FPGA} highlighted in red.  The white blocks
    correspond to \glspl{LUT}, followed by turquoise blocks being \glspl{DSP}
    and green blocks being \glspl{BRAM}.  The programmable interconnects are
    made up of connection blocks and switches shown in beige and yellow
    respectively.}%
  \label{fig:bg:fpga-layout}
\end{figure}

The standard process to translate a hardware design from an \gls{HDL}, such as
Verilog or VHDL, to then be placed onto an \gls{FPGA} is to first
\emph{synthesise} the hardware design, which generates a lower level description
of the hardware in terms of the resources that are available on the \gls{FPGA}.
Next, the net-list is place-and-routed on the \gls{FPGA}, which assigns a
physical location to each resources and programs the interconnects so that all
the components are connected properly.  This description low-level description
of the hardware is then turned into a bit stream that will program all the
individual resources on the \gls{FPGA}.  The result can be seen in
\cref{fig:bg:fpga-layout} by looking at the highlighted paths in red, as the
place-and-route process placed logical functions into \glspl{LUT} and connected
them together correctly, also making use of a \gls{BRAM} and a \gls{DSP}.

\section{High-Level Synthesis}%
\label{sec:bg:hls}

\Glsxtrlong{HLS} is the transformation of software directly into hardware.
There are many different types of \gls{HLS}, which can vary in terms of the
languages they accept or the devices that are targeted, however, they often
share similar steps in how the translation is performed, as they all go from a
higher-level, behavioural description of the algorithm to a timed hardware
description.  In this dissertation, we will assume that we are targeting
\glspl{FPGA} instead of \glspl{ASIC}, which lead to different resources that are
available to the \gls{HLS} tool to target.

The main steps performed in the translation of an \gls{HLS} tool is the
following~\cite{coussy09_introd_to_high_level_synth,canis13_legup}:

\begin{description}
\item[Compilation of input language] First, the program or specification written
  in the input language to the \gls{HLS} tool is compiled into an intermediate
  language that is more suitable to be transformed by optimisations.  The input
  language for most traditional \gls{HLS} tools is a restricted version of C or
  C++, however, \gls{HLS} tools such as Google XLS~\cite{google23_xls} can use a
  \gls{DSL} based on communicating sequential
  processes~\cite{hoare78_commun_sequen_proces} as an input specification as
  well.  This specification is then turned into some intermediate language such
  as the LLVM \gls{IR}~\cite{lattner04_llvm}, MLIR~\cite{lattner21_mlir} or a
  custom representation of the code.  The structure of these intermediate
  languages is further discussed in \cref{sec:bg:intermediate-language}.

\item[Hardware resource allocation] Depending on if the hardware target is a
  specific \gls{FPGA} or an \gls{ASIC} built on a specific technology library,
  the \gls{HLS} tool will have to allocated resources differently.  For example,
  on \glspl{FPGA} there are only a limited number of \glspl{LUT}, \gls{DSP}
  available.  The \gls{HLS} tool therefore often decides ahead of time which
  resources the program will need based on the operations that are present in
  the program.  A few resources are often assumed to be infinite to simplify the
  resource allocation, examples being registers, which are normally cheap
  especially on \glspl{FPGA}, or simple logic circuits that are cheap enough to
  be duplicated or may have dedicated hardware on the FPGA such as integer
  adders or multiplexers.  Other circuits may require more resources in which
  case it would make sense to only have a few instantiations of the circuit and
  share it as much as possible.  Examples of these circuits could be integer
  division modules or floating point arithmetic units that will most likely not
  have dedicated hardware on the \gls{FPGA} and would therefore have to be
  implemented on the \gls{FPGA}.  These circuits also often have trade-offs
  between area, latency and throughput which should be considered, and will be
  important in the operation scheduling step.  Finally, memory and
  multiplication units lie in the middle, where there are usually enough
  \glspl{BRAM} or \glspl{DSP} resources on the \gls{FPGA}, but they might
  introduce different challenges such leading to designs that are more difficult
  to place-and-route.  In particular, if one uses too many of these resources,
  \glspl{BRAM} and operations in the \glspl{DSP} can be implemented using
  \glspl{LUT}.

\item[Operation scheduling] Once the available resources have been chosen, the
  operations in the intermediate representation need to be scheduled into a
  clock cycle based on these resource constraints, creating a timed
  representation of the program.  The goal of the operation scheduling step is
  to maximise the instruction-level parallelism of the program while also
  honouring the various resource constraints that are imposed by the available
  resources.  Scheduling is further described in \cref{sec:bg:scheduling}.  As
  part of the scheduling step, operations are also often bound to specific
  resources.

\item[Hardware description generation] Finally, the hardware description is
  generated from the code that was described in the intermediate language and
  from the states and resources that each operation and register was assigned
  to.
\end{description}

There are many examples of existing high-level synthesis tools, the most popular
ones being Bambu HLS~\cite{bambu_hls}, LegUp~\cite{canis13_legup}, Vitis
HLS~\cite{amd23_vitis_high_synth}, Catapult
C~\cite{mentor20_catap_high_level_synth}, Google XLS~\cite{google23_xls} and
Intel's OpenCL SDK~\cite{intel20_sdk_openc_applic}.  These HLS tools all accept
general programming languages such as C/C++ or OpenCL, however, some HLS tools
take in languages that were designed for hardware such as
Handel-C~\cite{aubury96_handel_c_languag_refer_guide}, where time is encoded as
part of the semantics.\YH{TODO: Expand list a bit.}

\subsection{Data structures for Intermediate Languages}%
\label{sec:bg:data-structures-for-intermediate-languages}

This \namecref{sec:bg:data-structures-for-intermediate-languages} describes how
instructions inside functions can be represented and the differences in these
approaches, especially when integrated into an \gls{HLS} tool.  Next, in
\cref{sec:bg:intermediate-language} we will describe techniques used to group
instructions into a contiguous blocks to simplify analyses and transformations
of instructions within the blocks.

There are many ways to represent the code that makes up a function or a program.
High-level languages that are written by the programmer are normally represented
as an \gls{AST} in the front end of the compiler, which is a tree representing
the parsed source file.  An \gls{AST} is a good representation for high-level
optimisations that may need information about the exact intent of the
programmer, for example that a loop is indeed a structured for-loop instead of
unstructured goto statements.  On the opposite end, the assembly that will run
on the processor can simply be represented by a list of instructions.  This
simple representation of the program is useful because individual instructions
can directly be stored contiguously in memory and are then loaded by the
processor to be executed.  However, storing instructions as a list means that
much of the original structure of the program is lost, which makes analysing
programs and transforming them more difficult.  In between these two
representations, there is often one or more intermediate languages that can be
represented in a variety of ways, and are either used purely for analysis or
also as an intermediate transformation step.

\definecolor{nodea}{HTML}{81B8BF}
\definecolor{nodeb}{HTML}{A681BF}
\definecolor{nodec}{HTML}{BF8781}
\definecolor{noded}{HTML}{99BF81}
\definecolor{nodee}{HTML}{9FA5CE}
\definecolor{nodef}{HTML}{CE9FBC}
\definecolor{nodeg}{HTML}{CEC89F}
\definecolor{nodeh}{HTML}{9FCEB1}

\tikzset{instrnode/.style={draw=black,fill=white,circle,inner sep=3pt},
         bblock/.style={draw=black,fill=black!15,rounded corners}}

\begin{figure}
  \begin{subfigure}[b]{0.15\linewidth}
    \centering
    \begin{tikzpicture}[shorten >=1pt,>=Latex,node distance=0.75cm and 0.5cm]
      \node[instrnode,fill=nodea] (a) {};
      \node[instrnode,fill=nodeb,below=of a] (b) {};
      \node[instrnode,fill=noded,below=of b] (d) {};
      \node[instrnode,fill=nodec,below=of d] (c) {};
      \node[instrnode,fill=nodee,below=of c] (e) {};
      \node[instrnode,fill=nodef,below=of e] (f) {};
      \node[instrnode,fill=nodeg,below=of f] (g) {};
      \node[instrnode,fill=nodeh,below=of g] (h) {};
      \draw[->] (a) -- (b);
      \draw[->] (b) -- (d);
      \draw[->] (d) -- (c);
      \draw[->] (c) -- (e);
      \draw[->] (e) -- (f);
      \draw[->] (f) -- (g);
      \draw[->] (g) -- (h);
      \draw[->,dashed] (g) to [loop,looseness=1,out=110,in=250] (b);
      \draw[->,dashed] (b) to [loop,looseness=1,in=70,out=290] (c);
      \draw[->,dashed] (d) to [loop,looseness=1,in=70,out=290] (f);
    \end{tikzpicture}
    \caption{List}\label{fig:data-structure-comparison:list}
  \end{subfigure}\hfill%
  \begin{subfigure}[b]{0.2\linewidth}
    \centering
    \begin{tikzpicture}[shorten >=1pt,>=Latex,node distance=0.75cm and 0.5cm]
      \node[instrnode,fill=nodea] (a) {};
      \node[instrnode,fill=nodeb,below=of a] (b) {};
      \node[instrnode,fill=nodec,below left=of b] (c) {};
      \node[instrnode,fill=noded,below right=of b] (d) {};
      \node[instrnode,fill=nodee,below=of c] (e) {};
      \node[instrnode,fill=nodef,below right=of e] (f) {};
      \node[instrnode,fill=nodeg,below=of f] (g) {};
      \node[instrnode,fill=nodeh,below right=of g] (h) {};
      \draw[->] (a) -- (b);
      \draw[->] (b) -- (c);
      \draw[->] (b) -- (d);
      \draw[->] (d) -- (f);
      \draw[->] (c) -- (e);
      \draw[->] (e) -- (f);
      \draw[->] (f) -- (g);
      \draw[->] (g) -- (h);
      \draw[->] (g) to [loop,looseness=1.4,out=150,in=150] (b);
    \end{tikzpicture}
    \caption{\Glsfirst{CFG}}\label{fig:data-structure-comparison:cfg}
  \end{subfigure}\hfill%
  \begin{subfigure}[b]{0.3\linewidth}
    \centering
    \begin{tikzpicture}[shorten >=1pt,>=Latex,node distance=0.75cm and 0.5cm]
      \node[instrnode,fill=nodea] (a) {};
      \node[instrnode,fill=nodeb,below=of a] (b) {};
      \node[instrnode,fill=nodec,below left=of b] (c) {};
      \node[instrnode,fill=noded,below right=of b] (d) {};
      \node[instrnode,fill=nodee,below=of c] (e) {};
      \node[instrnode,fill=nodef,below right=of e] (f) {};
      \node[instrnode,fill=nodeg,below=of f] (g) {};
      \node[instrnode,fill=nodeh,below right=of g] (h) {};
      \draw[->,dashed] (a) -- (b);
      \draw[->,dashed] (g) -- (h);
      \draw[->,dashed] (g) to [loop,looseness=1.4,out=150,in=150] (b);
      \draw[->] (a) to [loop,looseness=1.4,out=310,in=50] (f);
      \draw[->] (b) -- (e);
      \draw[->] (b) -- (c);
      \draw[->] (b) -- (d);
      \draw[->,dashed] (c) to [loop,looseness=1.2,out=230,in=120] (g);
      \draw[->,dashed] (d) to [loop,looseness=1.2,out=260,in=60] (g);
      \draw[->] (d) -- (h);
      \draw[->,dashed] (e) -- (g);
      \draw[->,dashed] (f) -- (g);
    \end{tikzpicture}
    \caption{\Glsfirst{DFG}}\label{fig:data-structure-comparison:dfg}
  \end{subfigure}\hfill%
  \begin{subfigure}[b]{0.25\linewidth}
    \centering
    \begin{tikzpicture}[shorten >=1pt,>=Latex,node distance=0.75cm and 0.5cm]
      \node[instrnode,fill=nodea] (a) {};
      \node[instrnode,fill=nodeb,below=of a] (b) {};
      \node[instrnode,fill=nodec,below left=of b] (c) {};
      \node[instrnode,fill=noded,below right=of b] (d) {};
      \node[instrnode,fill=nodee,below=of c] (e) {};
      \node[instrnode,fill=nodef,below right=of e] (f) {};
      \node[instrnode,fill=nodeg,below=of f] (g) {};
      \node[instrnode,fill=nodeh,below right=of g] (h) {};
      \node[left=0.1cm of c] (padding) {};
      \begin{pgfonlayer}{background}
        \node[fit={(b)(c)(d)(e)(f)(g)(padding)},bblock] (fitall) {};
        \node[fit={(h)},bblock] (fith) {};
        \node[fit={(a)},bblock] (fita) {};
      \end{pgfonlayer}
      \draw[->,dashed] (a) -- (fitall);
      \draw[->] (b) -- (e);
      \draw[->] (b) -- (c);
      \draw[->] (b) -- (d);
      \draw[->] (c) to [loop,looseness=1.2,out=230,in=120] (g);
      \draw[->] (d) to [loop,looseness=1.2,out=260,in=60] (g);
      \draw[->] (e) -- (g);
      \draw[->] (f) -- (g);
      \draw[->,dashed] (g) -- (fith);
      \draw[->,dashed] (g) to [loop,looseness=1.4,out=150,in=150] ($(fitall.north) - (0.3,0)$);
    \end{tikzpicture}
    \caption{\Glsfirst{CDFG}}\label{fig:data-structure-comparison:cdfg}
  \end{subfigure}
  \caption{Comparison of lists, \glsfmtlongpl{CFG}, \glsfmtlongpl{DFG} and
    \glsfmtlongpl{CDFG}.}%
  \label{fig:data-structure-comparison}
\end{figure}

\subsubsection{Lists}

An example of a list representing code is shown abstractly in
\cref{fig:data-structure-comparison:list}.  In the figure, the solid arrows
between nodes show how the instructions are stored as a linked list, one
instruction feeding to the next.  However, programs may have loops in them,
which cannot directly be expressed in the list representation, as each
instruction can only have one successor.  Instead, instructions that are
represented as a list will have labels attached to them, and goto instructions
can then jump to those labels.  These jumps are represented using the dashed
arrows in the figure.  This is the simplest representation of the code as it is
completely linear, however, analysing such a program will be more difficult
because a lot of information, such as a lot of edges, are implicit in the
representation.

\subsubsection{\Glsfmtlongpl{CFG}}

Instead, a \gls{CFG} representation of the code is a graph instead of a list,
and allows any instruction to be connected to any other instruction explicitly
by a control-flow edge.  This signifies that after executing an instruction, the
execution will then move to one of the successors of the current node.

\subsubsection{\Glsfmtlongpl{DFG}}

\subsubsection{\Glsfmtlongpl{CDFG}}

\subsection{Grouping Instructions Into Blocks}%
\label{sec:bg:intermediate-language}

This \namecref{sec:bg:intermediate-language} describes the characteristics for
intermediate languages that are often used within an \gls{HLS} tool.  First, we
will describe different techniques to group blocks of instructions which can
simplify optimisations.

In particular, it is often useful to have contiguous blocks of instructions that
do not contain any control-flow in one list.  This means that these instructions
can safely be rearranged by only looking at local information of the block
itself, and in particular it allows for complete removal of control-flow as only
the data-flow is important in that block.

\begin{figure}
  \begin{subfigure}[c]{0.3\linewidth}
    \centering
    \begin{tikzpicture}[shorten >=1pt,>=Latex,node distance=0.75cm and 0.5cm]
      \node[instrnode,fill=nodea] (a) {};
      \node[instrnode,fill=nodeb,below=of a] (b) {};
      \node[instrnode,fill=nodec,below left=of b] (c) {};
      \node[instrnode,fill=noded,below right=of b] (d) {};
      \node[instrnode,fill=nodee,below=of c] (e) {};
      \node[instrnode,fill=nodef,below right=of e] (f) {};
      \node[instrnode,fill=nodeg,below=of f] (g) {};
      \node[instrnode,fill=nodeh,below right=of g] (h) {};
      \begin{pgfonlayer}{background}
        \node[fit={(c)(e)},bblock] (fitce) {};
        \node[fit={(d)},bblock] (fitd) {};
        \node[fit={(b)},bblock] (fitb) {};
        \node[fit={(a)},bblock] (fita) {};
        \node[fit={(f)(g)},bblock] (fitfg) {};
        \node[fit={(h)},bblock] (fith) {};
      \end{pgfonlayer}
      \draw[->] (a) -- (fitb);
      \draw[->] (b) -- (fitce.north);
      \draw[->] (b) -- (fitd.north);
      \draw[->] (d) -- (fitfg.north east);
      \draw[->] (c) -- (e);
      \draw[->] (e) -- (fitfg.north west);
      \draw[->] (f) -- (g);
      \draw[->] (g) -- (fith);
      \draw[->] (g) to [loop,looseness=1.4,out=150,in=150] (fitb.north west);
    \end{tikzpicture}
    \caption{Basic blocks}\label{fig:block-comparison:basicblocks}
  \end{subfigure}\hfill%
  \begin{subfigure}[c]{0.3\linewidth}
    \centering
    \begin{tikzpicture}[shorten >=1pt,>=Latex,node distance=0.75cm and 0.5cm]
      \node[instrnode,fill=nodea] (a) {};
      \node[instrnode,fill=nodeb,below=of a] (b) {};
      \node[instrnode,fill=nodec,below left=of b] (c) {};
      \node[instrnode,fill=noded,below right=of b] (d) {};
      \node[instrnode,fill=nodee,below=of c] (e) {};
      \node[instrnode,fill=nodef,below=of e] (f) {};
      \path (f) -| node[instrnode,fill=nodef,dashed] (fp) {} (d);
      \node[instrnode,fill=nodeg,below=of f] (g) {};
      \node[instrnode,fill=nodeg,below=of fp,dashed] (gp) {};
      \node[instrnode,fill=nodeh,below right=of g] (h) {};
      \begin{pgfonlayer}{background}
        \node[fit={(d)}] (fitd) {};
        \node[fit={(b)}] (fitb) {};
        \node[fit={(d)(fp)(gp)},bblock] (fitd) {};
        \node[fit={(c)(e)(f)(g)}] (fitce) {};
        \filldraw[bblock] (fitb.north west)
                       -- (fitb.north east)
                       -- (fitb.south east)
                       -- ($(fitce.north east) + (0.07,-0.3)$)
                       -- ($(fitce.south east) + (0.07,0)$)
                       -- ($(fitce.south west) - (0.07,0)$)
                       -- ($(fitce.north west) - (0.07,0)$)
                       -- cycle;
        \node[fit={(a)},bblock] (fita) {};
        \node[fit={(h)},bblock] (fith) {};
      \end{pgfonlayer}
      \draw[->] (a) -- (fitb);
      \draw[->] (b) -- (c);
      \draw[->] (b) -- (fitd.north);
      \draw[->] (d) -- (fp);
      \draw[->] (c) -- (e);
      \draw[->] (e) -- (f);
      \draw[->] (f) -- (g);
      \draw[->] (g) -- (fith);
      \draw[->] (fp) -- (gp);
      \draw[->] (gp) -- (fith);
      \draw[dashed] (f) -- (fp);
      \draw[dashed] (g) -- (gp);
      \draw[->] (g) to [loop,looseness=1.2,out=150,in=150] (fitb.north west);
      \draw[->] (gp) to [loop,looseness=1.2,out=30,in=30] (fitb.north east);
    \end{tikzpicture}
    \caption{Superblocks}\label{fig:block-comparison:superblocks}
  \end{subfigure}\hfill%
  \begin{subfigure}[c]{0.3\linewidth}
    \centering
    \begin{tikzpicture}[shorten >=1pt,>=Latex,node distance=0.75cm and 0.5cm]
      \node[instrnode,fill=nodea] (a) {};
      \node[instrnode,fill=nodeb,below=of a] (b) {};
      \node[instrnode,fill=nodec,below left=of b] (c) {};
      \node[instrnode,fill=noded,below right=of b] (d) {};
      \node[instrnode,fill=nodee,below=of c] (e) {};
      \node[instrnode,fill=nodef,below right=of e] (f) {};
      \node[instrnode,fill=nodeg,below=of f] (g) {};
      \node[instrnode,fill=nodeh,below right=of g] (h) {};
      \begin{pgfonlayer}{background}
        \node[fit={(b)(c)(d)(e)(f)(g)},bblock] (fitall) {};
        \node[fit={(h)},bblock] (fith) {};
        \node[fit={(a)},bblock] (fita) {};
      \end{pgfonlayer}
      \draw[->] (a) -- (fitall);
      \draw[->] (b) -- (d);
      \draw[->] (d) -- (c);
      \draw[->] (c) -- (e);
      \draw[->] (e) -- (f);
      \draw[->] (f) -- (g);
      \draw[->] (g) -- (fith);
      \draw[->] (g) to [loop,looseness=1.4,out=150,in=150] ($(fitall.north) - (0.3,0)$);
    \end{tikzpicture}
    \caption{Hyperblocks}\label{fig:block-comparison:hyperblocks}
  \end{subfigure}
  \caption{Comparison of basic blocks, superblocks and hyperblocks.}%
  \label{fig:block-comparison}
\end{figure}

\subsubsection{Basic blocks}

%TODO: Finish the basic blocks section

Basic blocks are the simplest form of structure, as these are only formed of
lists of instructions that do not include control-flow.

\subsubsection{Superblocks}

Superblocks extend the notion of basic blocks to contiguous regions without any
incoming control-flow, however, there can be multiple exits out of the block.
The main benefit of this definition is that due to the extra flexibility of
allowing multiple exits, the basic blocks can be extended, which often improves
most optimisations that make use of basic blocks.  However, the downside is that
the representation of the blocks can be more complex due to the introduction of
the extra control-flow.  Any analysis passes will have to take into account the
possibility of control-flow being present and many simplifications will not be
possible anymore.

\subsubsection{Hyperblocks}

Hyperblocks are also an extension of basic blocks similar to superblocks, but
instead of introducing special control-flow instructions into the block, every
instruction is predicated.  This leads to possibly more complex control-flow
than in both of the previous cases, however, it can be reasoned with using a
\gls{SAT} or \gls{SMT} solver.

\section{Scheduling}%
\label{sec:bg:scheduling}

\subsection{Static Scheduling}%
\label{sec:bg:static-scheduling}

Static scheduling is used by the majority of synthesis
tools~\cite{canis13_legup, xilinx20_vivad_high_synth,
  mentor20_catap_high_level_synth, intel20_sdk_openc_applic} and means that the
time at which each operation will execute is known at compile time.  Static
analysis is used to gather all the data dependencies between all the operations
to determine in which clock cycle the operations should be placed, or if these
operations can be parallelised.  Once the data-flow analysis has been performed,
various scheduling schemes can be taken into account to place operations in
various clock cycles.  Some common static-scheduling schemes are the following:

\begin{description}
\item[As soon as possible (ASAP)] scheduling will place operations into
  the first possible clock cycle that they can be executed.

\item[As late as possible (ALAP)] scheduling places operations into the last
  possible clock cycle, right before they are used by later operations.

\item[List scheduling] uses priorities associated with each operation and
  chooses operations from each priority level to be placed into the current
  clock cycle if all the data-dependencies are met.  This is done until all the
  resources for the current clock cycle are used up.
\end{description}

Static scheduling can normally produce extremely small circuits, however, it is
often not possible to guarantee that the circuits will have the best
throughput~\cite{cheng20_combin_dynam_static_sched_high_level_synth}, as this
requires extensive control-flow analysis and complex optimisations.  Especially
for loops, finding the optimal \gls{II} can be tricky if there are loads and
stores in the loop or if the loops are nested.

\subsection{Dynamic Scheduling}%
\label{sec:bg:dynamic-scheduling}

On the other hand, Dynamic
scheduling~\cite{josipovic18_dynam_sched_high_level_synth} does not require the
schedule to be known at compile time and instead it generates circuits using
tokens to schedule the operations in parallel at run time.  Whenever the data
for an operation is available, it sends a token to the next operation,
signalling that the data is ready to be read.  The next operation does not start
until all the required inputs to the operation are available, and once that is
the case, it computes the result and then sends another token declaring that the
result of that operation is also ready.  The benefit of this approach is that
only basic data-flow analysis is needed to connect the tokens correctly,
however, the scheduling is then done dynamically at run time, depending on how
long each primitive takes to finish and when the tokens activate the next
operations.

The benefits of this approach over static scheduling is that the latency of
these circuits is normally significantly lower than the latency of static
scheduled circuits, because they can take advantage of runtime information of
the circuit.  However, because of the signalling required to perform the runtime
scheduling, the area of these circuits is usually much larger than the area of
static scheduled circuits.  In addition to that, much more analysis is needed to
properly parallelise loads and stores to prevent bugs, which requires the
addition of buffers in certain locations.

An example of a dynamically scheduled synthesis tool is
Dynamatic~\cite{josipović18_dynam_sched_high_synth}, which uses a
\gls{LSQ}~\cite{josipovic17_out_of_order_load_store} to order memory operations
correctly even when loops are pipelined and there are dependencies between
iterations.  In addition to that, performance of the dynamically scheduled code
is improved by careful buffer
placement~\cite{josipovic20_buffer_placem_sizin_high_perfor_dataf_circuit},
which allows for better parallisation and pipelining of loops.

\section{Verification}%
\label{sec:bg:verification}

There are different kinds of verification tools, which can mostly be placed into
two categories: automatic theorem provers described in
\cref{sec:bg:automatic-theorem-provers} and interactive theorem provers
described in \cref{sec:bg:interactive-theorem-provers}.

\subsection{Automatic theorem provers}%
\label{sec:bg:automatic-theorem-provers}

Automatic theorem provers such as \gls{SAT} or \gls{SMT} solvers can be
characterised as decision procedures that will answer, for example, if a formula
is satisfiable or not.  However, by default these tools will only give the
answer to the initial query, without showing the reasoning.  The reasoning is
often quite complex as the \gls{SAT} or \gls{SMT} tool will implement many
optimisations to improve the performance of the decision procedure.

The main advantage of using an automatic theorem prover is that if one is
working in its constrained decidable theory, then it will be efficient at
proving or disproving if a formula is a theorem.  However, if the theorem
requires inductive arguments to prove, then the theorem prover might need some
manual help from the user by adding the right lemmas to its collection of facts
which the automatic procedure can use.  The proof itself though will still be
automatic, which means that many of the tedious cases in the proofs can be
ignored.

However, this is also the main disadvantage of automatic theorem provers,
because they do not provide details about the proof itself and often cannot
communicate why they cannot prove a theorem.  This means that as a user one has
to guess what theorems the prover is missing and try and add these to the fact
database.

Finally, automatic theorem provers do not provide reasoning for their final
answer by default, meaning one cannot check if the result is actually correct.
However, some SMT solvers support the generation of proof witnesses, which can
then be checked and reused in other theorem provers.  Some examples of these are
veriT~\cite{bouton09}, and these can then be integrated into Coq using
SMTCoq~\cite{armand11_modul_integ_sat_smt_solver}.

\subsection{Interactive theorem provers}%
\label{sec:bg:interactive-theorem-provers}

Interactive theorem provers, on the other hand, focus on checking proofs that
are provided to them.  These can either be written manually by the user, or
automatically generated by some decision procedure.  However, these two ways of
generating proofs can be combined, so the general proof structure can be
manually written, and simpler steps inside of the proof can be automatically
solved.

The main benefit of using an interactive theorem prover is that the proof is
there and can be checked by a small, trusted kernel.  This kernel does not need
to be heavily optimised, and can therefore be reasoned about.

The main cost of using an interactive theorem prover is the time it takes to
prove theorems, and the amount of formalisation that is needed to make the
proofs pass.  For a proof to be completed, one has to remove any axioms from the
proof, meaning even the smallest detail must be proven to continue.

\section{Unmechanised Verification of HLS}%
\label{sec:bg:unmechanised-verification-of-hls}

Work is being done to prove the equivalence between the generated hardware and
the original behavioural description in C.  An example of a tool that implements
this is Mentor's Catapult~\cite{mentor20_catap_high_level_synth}, which tries to
match the states in the register transfer level (RTL) description to states in
the original C code after an unverified translation.  This technique is called
translation validation~\cite{pnueli98_trans}, whereby the translation that the
HLS tool performed is proven to have been correct for that input, by showing
that they behave in the same way for all possible inputs.  Using translation
validation is quite effective for proving complex optimisations such as
scheduling~\cite{kim04_autom_fsmd, karfa06_formal_verif_method_sched_high_synth,
  chouksey20_verif_sched_condit_behav_high_level_synth} or code
motion~\cite{banerjee14_verif_code_motion_techn_using_value_propag,
  chouksey19_trans_valid_code_motion_trans_invol_loops}, however, the validation
has to be run every time the high-level synthesis is performed.  In addition to
that, the proofs are often not mechanised or directly related to the actual
implementation, meaning the verifying algorithm might be wrong and could give
false positives or false negatives.

More examples of translation validation for proofs about HLS
algorithms~\cite{karfa06_formal_verif_method_sched_high_synth,
  karfa07_hand_verif_high_synth, kundu07_autom,
  karfa08_equiv_check_method_sched_verif, kundu08_valid_high_level_synth,
  karfa10_verif_datap_contr_gener_phase, karfa12_formal_verif_code_motion_techn,
  chouksey19_trans_valid_code_motion_trans_invol_loops,
  chouksey20_verif_sched_condit_behav_high_level_synth} are performed using a
HLS tool called SPARK~\cite{gupta03_spark}.  These translation validation
algorithms can check the correctness of complicated optimisations such as code
motion or loop inversions.  However, even though the correctness of the verifier
is proven in the papers, the proof does translate directly to the algorithm that
was implemented for the verifier.  It is therefore possible that output is
accepted even though it is not equivalent to the input.  In addition to that,
these papers reason about the correctness of the algorithms in terms of the
intermediate language of SPARK, and does not extend to the high-level input
language that SPARK takes in, or the hardware description language that SPARK
outputs.

Finally there has also been work proving HLS correct without using translation
validation, but by directly showing that the translation is correct.  The first
instance of this is proving the BEDROC~\cite{chapman92_verif_bedroc} HLS tool is
correct.  This HLS tool converts a high-level description of an algorithm,
supporting loops and conditional statements, to a netlist and proves that the
output is correct.  It works in two stages, first generating a DFG from the
input language, HardwarePal.  It then optimises the DFG to improve the routing
of the design and also improving the scheduling of operations.  Finally, the
netlist is generated from the DFG by placing all the operations that do not
depend on each other into the same clock cycle.  Datapath and register
allocation is performed by an unverified clique partitioning algorithm.  The
equivalence proof between the DFG and HardwarePal is done by proof by
simulation, where it is proven that, given a valid input configuration, that
applying a translation or optimisation rule will result in a valid DFG with the
same behaviour as the input.

There has also been work on proving the translation from Occam to
gates~\cite{page91_compil_occam} correct using algebraic
proofs~\cite{jifeng93_towar}.  This translation resembles dynamic scheduling as
tokens are used to start the next operations.  However, Occam is timed by
design, and will execute assignments sequentially.  To take advantage of the
parallel nature of hardware, Occam uses explicit parallel constructs with
channels to share state.  Handel-C, a version of Occam with many more features
such as memory and pointers, has also been used to prove HLS correct, and is
described further in \cref{sec:bg:mechaniced-handel-c-to-netlist-translation}.

\section{Mechanised Compiler Proofs}%
\label{sec:bg:mechanised-compiler-proofs}

Even though a proof for the correctness of an algorithm might exist, this does
not guarantee that the algorithm itself behaves in the same way as the assumed
algorithm in the proof.  The implementation of the algorithm is separate from
the actual implementation, meaning there could be various implementation bugs in
the algorithm that cause it to behave incorrectly.  C compilers are a good
example of this, where many optimisations performed by the compilers have been
proven correct, however these proofs are not linked directly to the actual
implementations of these algorithms in GCC or Clang.  Yang et
al.~\cite{yang11_findin_under_bugs_c_compil} found more than 300 bugs in GCC and
Clang, many of them appearing in the optimisation phases of the compiler.  One
way to link the proofs to the actual implementations in these compilers is to
write the compiler in a language which allows for a theorem prover to check
properties about the algorithms.  This allows for the proofs to be directly
linked to the algorithms, ensuring that the actual implementations are proven
correct.  Yang et al.~\cite{yang11_findin_under_bugs_c_compil} found that
CompCert, a formally verified C Compiler, only had five bugs in all the
unverified parts of the compiler, meaning this method of proving algorithms
correct ensures a correct compiler.

\subsection{HLS Formalised in Isabelle}

Martin Ellis' work on correct synthesis~\cite{ellis08_correc} is the first
example of a mechanically verified high-level synthesis tool, also called
hardware/software compilers, as they produce software as well as hardware for
special functional units that should be hardware accelerated.  The main goal of
the thesis is to provide a framework to prove hardware/software compilers
correct, by defining semantics for an intermediate language (IR) which supports
partitioning of code into hardware and software parts, as well as a custom
netlist format which is used to describe the hardware parts and is the final
target of the hardware/software compiler.  The proof of correctness then says
that the hardware/software design should have the same behaviour as if the
design had only been implemented in software.  The framework used to prove the
correctness of the compilation from the IR to the netlist format is written in
Isabelle, which is a theorem prover comparable to Coq.  Any proofs in the
framework are therefore automatically checked by Isabelle for correctness.

This work first defines a static single assignment (SSA) IR, with the
capabilities of defining sections that should be hardware accelerated, and
sections that should be executed on the CPU.  This language supports
\emph{hyperblocks}, which are a list of basic blocks with one entry node and
multiple exit nodes, which is well suited for various HLS optimisations and
better scheduling.  However, as this is quite a low-level intermediate language,
the first thing that would be required is to create a tool that can actually
translate a high-level language to this intermediate language.  In addition to
that, they also describe a netlist format in which the hardware accelerated code
will be represented in.

As both the input IR and output netlist format have been designed from scratch,
they are not very useful for real world applications, as they require a
different back end to be implemented from existing compilers.  In addition to
that, it would most likely mean that the translation from higher-level language
to the IR is unverified and could therefore contain bugs.  As the resulting
netlist also uses a custom format, it cannot be fed directly to tools that can
then translate it to a bitstream to place onto an FPGA. The reason for designing
a custom IR and netlist was so that these were compatible with each other,
making proofs of equivalence between the two simpler.

Finally, it is unclear whether or not a translation algorithm from the IR to the
netlist format was actually implemented, as the only example in the thesis seems
to be compiled by hand to explain the proof.  There are also no benchmarks on
real input programs showing the efficiency of the translation algorithm, and it
is therefore unclear whether the framework would be able to prove more
complicated optimisations that a compiler might perform on the source code.  The
thesis seems to describe the correctness proofs by assuming a compiler exists
which outputs various properties that are needed by the equivalence proof, such
a mapping between variables and netlist wires and registers.

Even though this Isabelle framework does provide some equivalence relation
between an IR and a netlist and describes how the translation would be proven
correct by matching states to one another, it is actually not useable in
practice.  First of all, the IR is only represented in Isabelle and does not
have a parser or a compiler which can target this IR.  In addition to that, the
netlist format cannot be passed to any existing tool, to then be placed onto an
FPGA, meaning an additional, unverified translation would have to take place.
This further reduces the effectiveness of the correctness proof, as there are
various stages that are not addressed by the proofs and therefore have to be
assumed to be correct.

\subsection{Mechanised Handel-C to Netlist Translation}%
\label{sec:bg:mechaniced-handel-c-to-netlist-translation}

Handel-C~\cite{bowen98_handel_c_languag_refer_manual} is a C-like language for
hardware development.  It supports many C features such as assignments,
if-statements, loops, pointers and functions.  In addition to these constructs,
Handel-C also supports explicit timing constructs, such as explicit parallelism
as well as sequential or parallel assignment, similar to blocking and
nonblocking assignments in Verilog.  Perna et
al.~\cite{perna12_mechan_wire_wise_verif_handel_c_synth} developed a
mechanically verified Handel-C to netlist translation written in HOL. The
translation is based on previous work describing translation from Occam to gates
by Page et al.~\cite{page91_compil_occam}, which was proven correct by Jifeng et
al.~\cite{jifeng93_towar} using algebraic proofs.  As Handel-C is mostly an
extension of Occam with C-like operations, the translation from Handel-C to
gates can proceed in a similar way.

Perna et al. verify the compilation of a subset of Handel-C to gates, which does
not include memory, arrays or function calls.  In addition to the constructs
presented in Page et al., the prioritised choice construct is also added to the
Handel-C subset that is supported.  The verification proceeds by first defining
the algorithm to perform the compilation, chaining operations together with
start and end signals that determine the next construct which will be executed.
The circuits themselves are treated as black boxes by the compilation algorithm
and are chosen based on the current statement in Handel-C which is being
translated.  The compilation algorithm correctly has to link each black box to
one another using the start and end signals that the circuits expose to
correctly compile the Handel-C code.

The verification of the compilation is done by proving that the control signal
is propagated through the circuits correctly, and that each circuit is activated
at the right time.  It is also proven that the internal circuits of the
constructs also propagate the control signal in the correct way, and will be
active at the right clock cycle.  However, it is not proven that the circuits
have the same functionality as the Handel-C constructs, only that the control
signals are propagated in the correct manner.  In addition to that, the
assumption is made that the semantics for time are the same in Handel-C as well
as in the netlist format that is generated, which could be proven if the
constructs are shown to behave in the exact same way as the handel-C constructs.

\section{CompCert}%
\label{sec:bg:compcert}

\gls{CompCert}~\cite{leroy09_formal_verif_compil_back_end} is a formally
verified C compiler written in
Coq~\cite{bertot04_inter_theor_provin_progr_devel}.  Coq is a theorem prover,
meaning algorithms written in Coq can be reasoned about in Coq itself by proving
various properties about the algorithms.  To then run these algorithms that have
been proven correct, they can be extracted directly to OCaml code and then
executed, as there is a straightforward correspondence between Coq and OCaml
code.  During this translation, all the proofs are erased, as they are not
needed during the execution of the compiler, as they are only needed when the
correctness of the compiler needs to be checked.  With this process, one can
have a Compiler that satisfies various correctness properties and can therefore
be proven to preserve the behaviour of the code.

CompCert contains eleven intermediate languages, which are used to gradually
translate C code into assembly that has the same behaviour.  Proving the
translation directly without going through the intermediate languages would be
infeasible, especially with the many optimisations that are performed during the
translation, as assembly is very different to the abstract C code it receives.
The first three intermediate languages (C\#minor, C\#minorgen, Cminor) are used
to transform Clight into a more assembly like language called register transfer
language (RTL).  This language consist of a control-flow graph of instructions,
and is therefore well suited for various compiler optimisations such as constant
propagation, dead-code elimination or function
inlining~\cite{tristan08_formal_verif_trans_valid}.  After RTL, each
intermediate language is used to get closer to the assembly language of the
architecture, performing operations such as register allocation and making sure
that the stack variables are correctly aligned.

\subsection{CompCertSSA}%
\label{sec:bg:compcertssa}

\index{CompCertSSA}CompCertSSA is an extension of CompCert with an additional
\gls{SSA} intermediate language.  This language enforces \gls{SSA} properties
and therefore allows for many convenient proofs about optimisations performed on
this intermediate language, as many assumptions about variables can be made when
these are encountered.  The main interesting porperty that arises from using
\gls{SSA} is the \emph{equational lemma}, stating that given a variable, if it
was assigned by an operation that does not depend on memory, then loading the
destination value of that variable is the same as recomputing the value of that
variable with the current context.

Given a well formed SSA program $p$, a reachable state
$\Sigma\ s\ f\ \sigma\ R\ M$, a memory independent operation which was defined
at a node $d$ as $\mono{Iop}\ \mathit{op}\ \vec{a}\ x\ n$ assuming that $\sigma$
is dominated by $d$ ($p \le_{d} d$), then the following equation holds:

\begin{equation}\label{eq:equational-lemma}
  \left(\mathit{op}, \vec{a}\right) \Downarrow (R, M) = \left\lfloor R[x] \right\rfloor
\end{equation}

This is an important lemma as it essentially allows one to know the value of a
register as long as one knows that its assignment dominates the current node and
one knows what expressions it was assigned.

\subsection{Related Work to Scheduling}%
\label{sec:related-work}

The most closely related works to ours are those by
\textcite{tristan08_formal_verif_trans_valid} and by
\textcite{six22_formal_verif_super_sched}, so we begin this section by recapping
our main points of similarity and difference.

\textcite{tristan08_formal_verif_trans_valid} were the first to propose adding
scheduling to a verified compiler, and we adopt their method for validating
schedules -- running symbolic execution before and after scheduling and
comparing the resultant symbolic states. Their scheduler only \emph{reorders}
instructions, so syntactic equality suffices for comparing symbolic states,
whereas our scheduler can also \emph{modify} instructions (by manipulating
predicates). This means that our state comparisons are more involved, and we
turn to a SAT solver to help resolve them
(\cref{sec:value_summaries}). \citeauthor{tristan08_formal_verif_trans_valid}
also devised the use of constraints to prevent the scheduler introducing
undefined behaviour; we adopt this technique too, taking care to extend it to
handle predicates in such a way that valid schedules can still be validated
(\cref{sec:handling-discarded-expressions}). We remark that a direct empirical
comparison with \citeauthor{tristan08_formal_verif_trans_valid}'s work is not
feasible because their method was implemented for an old version of CompCert and
was not incorporated into its main branch.

\textcite{six22_formal_verif_super_sched} formalise superblock scheduling, which
is a restricted form of trace scheduling that is well-suited for VLIW
processors. Hyperblock scheduling is more general than superblock scheduling and
is well-suited to our application domain,
HLS. \citeauthor{six22_formal_verif_super_sched}'s scheduler reorders
instructions, and also combines them where it is advantageous to do so, so
comparing symbolic states is more involved than it was for
\citeauthor{tristan08_formal_verif_trans_valid}, but it still does not require a
SAT solver because there are no predicates to reason about. A direct empirical
comparison between our scheduler and
\citeauthor{six22_formal_verif_super_sched}'s is difficult because
\citeauthor{six22_formal_verif_super_sched}'s is based on an incompatible fork
of CompCert called CompCertKVX.

Outside of the realm of mechanised proof, most HLS tools implement some form of
static scheduling. For instance, AMD Vitis HLS~\cite{amd23_vitis_forum},
LegUp~\cite[][p.60]{canis15_legup}, Google XLS~\cite[line~112]{google23_xls} and
Bambu~\cite[line~304]{ferrandi21_bambu} all employ SDC-based hyperblock
scheduling. Other HLS tools, such as
Dynamatic~\cite{josipović18_dynam_sched_high_synth}, defer scheduling decisions
until runtime. It is notable that these unverified tools tend towards fewer,
larger passes, where several optimisations are packed into `scheduling'. This
minimises the number of times the solver needs to be invoked, and gives it the
best chance of finding a global optimum. Verified tools such as CompCert and
Vericert, on the other hand, tend towards more, smaller passes that are
individually feasible to prove correct.

Several HLS tools are associated with translation validators, either for an
individual scheduling
pass~\cite{chouksey20_verif_sched_condit_behav_high_level_synth,
  karfa06_formal_verif_method_sched_high_synth, kim04_autom_fsmd} or for the HLS
tool as a whole~\cite{mentor20_catap_high_level_synth, tiemeyer19_crest}. These
typically work by establishing equivalence at the level of state machines,
whereas we take the approach of comparing symbolic states, because
\textcite{tristan08_formal_verif_trans_valid} showed it to be viable in the
context of a verified compiler. Unlike our work, none of these translation
validators provide mechanised proofs of equivalence, so might produce false
positives.

% \subsection{Diffs w.r.t. superblock scheduling}

% \begin{itemize}

% \item Superblocks are well suited to VLIW processors [citation needed];
%  hyperblocks are well suited to HLS [citation needed].  Direct comparison is
%  tricky because the two tools are based on incompatible versions of CompCert.

%\item Both use translation validation, but we use a verified SAT solver as part
% of this.  (Superblock scheduling doesn't use predicates so doesn't need a SAT
% solver.)

% \item We introduce two intermediate languages (RTLBlock and RTLPar) that we
%   argue are easier to work with than the RTLPath language used in the superblock
%   scheduling work.

% \end{itemize}

% \JW{What's the link between hyperblocks and SDC-based scheduling? Are these
% orthogonal?}\YH{Well the original SDC papers only talk about basic blocks, and
% not SDC hyperblock scheduling.  But it is easy enough to adapt.  SDC just
% requires some kind of blocks without control-flow.} \JW{Ok cool, so you can
% have SDC scheduling without hyperblocks. Can you have hyperblocks without SDC
% scheduling? (I guess so, but just wanted to be completely sure.)}\YH{Yes
% exactly, you can perform any type of basic block scheduling on hyperblocks as
% well I think, as long as you mark the dependencies correctly.  But in the
% worst case you could even just see hyperblocks as basic blocks and ignore
% predicates.  It would still be correct but just less efficient.}
%
% \JW{It would be interesting to make the case for SDC-based scheduling. Is it
% superior to other scheduling mechanisms? Is it so much superior that it's
% worth the additional complexity, compared to, say, list scheduling? }\YH{SDC
% scheduling is more flexible with the type of constraint to optimise for
% (latency / throughput). I don't think it's too meaningful for compilers, where
% there isn't as much flexibility.}  \JW{Thanks. Does Vericert have any idea
% that the scheduling is being performed using SDC? Or is the SDC stuff entirely
% internal to your unverified OCaml code? That is, could you swap your scheduler
% with a different one that isn't based on SDC, and not have to change anything
% in Vericert?}\YH{The verified part doesn't know about SDC at all, so it could
% be changed to any other scheduler that acts on hyperblocks.}

\section{Summary}%
\label{sec:bg:summary}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% TeX-engine: luatex
%%% End:
