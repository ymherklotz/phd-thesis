\chapter{Hardware Generation}%
\label{sec:hardware-generation}

\begin{chapsummary}
  This chapter describes the final hardware generation step to go from
  software-like semantics to hardware Verilog semantics.
\end{chapsummary}

\noindent Until now the representation of the program has still been in the form
of a software program, with virtual, infinite registers, a program counter, as
well as a rich but abstract memory model.  This representation needs to be
transformed into a more suitable representation on which hardware specific
transformations can applied, and which is closer to th structure of the final
Verilog design.  The main transformation that takes place is converting a
control-flow graph into a state machine with data-path representation, which is
also the structure of the final design.  There are various steps involved in
this refinement because of the large gap between control-flow graph semantics
and state machine semantics.  In addition to that, additional components, such
as an implementation of a memory that can be efficiently implemented in
hardware, need to be added to the hardware to produce a useful design.  Finally,
until now programs have only been executing sequentially, whereas to produce the
final hardware one will have to transform the sequential execution of operations
within each state into parallel assignments.

This chapter describes the hardware generation process, starting from \rtlpar{}
and producing a final Verilog design.
\Cref{fig:hg:vericert-hardware-generation} shows the intermediate
transformations and in which section the transformation is described.
\Cref{sec:hg:hyperblock-destruction} describes the first step in the
transformation which separates each sequential block within a state in \rtlpar{}
into separate states that can be addressed using the program counter.  This
matches addressing that the state register would have to do in the hardware
design. Next, \cref{sec:hg:htl-generation} describes the generation of \htl{},
an intermediate language representing the execution of a state machine.  This
performs the main transformation from a software representation of the program
into hardware, making the execution of the program more explicit in the design
itself instead of as part of the semantics.  \Cref{sec:hg:bram-insertion} then
describes the first hardware-specific optimisation on the state-machine
representation of the hardware by adding a specification of a \gls{BRAM} to the
\htl{} semantics and replacing any explicit reads and writes to the array
representing memory by properly formed reads and writes to the \gls{BRAM}.
Until now, updates to registers have been specified sequentially, so a forward
substitution transformation is describe in
\cref{sec:hg:register-forward-substitution} to parallelise the updates to
registers.  Finally, \cref{sec:hg:verilog-generation} describes the generation
of the final Verilog design, which implements the state-machine that was
specified by \htl{}, in particular implementing the \gls{BRAM} that was
specified.

\begin{figure*}
\hypersetup{hidelinks}
\centering
\begin{tikzpicture}[font=\strut\sffamily]
\node[ir] (rtlpar) {\rtlpar{}};
\node[pass,right=of rtlpar] (hyperblock destruction) {Hyperblock \\
  Destruction};
\node[ir,right=of hyperblock destruction] (rtlsubpar) {\rtlsubpar{}};
\node[pass,right=of rtlsubpar] (htl generation) {\htl{} \\
  Generation};
\node[ir,right=of htl generation] (htl) {\htl{}};
\node[pass,below=of htl] (bram insertion) {BRAM \\ insertion};
\node[ir,below=of bram insertion] (htlmem) {\htl{}};
\node[pass,left=of htlmem] (forward substitution) {Forward \\ Substitution};
\node[ir,left=of forward substitution] (htlsubst) {\htl{}};
\node[pass,left=of htlsubst] (verilog generation) {Verilog \\ Generation};
\node[ir,left=of verilog generation] (verilog) {Verilog};

\draw[ed] (rtlpar) -- (hyperblock destruction) -- (rtlsubpar);
\draw[ed] (rtlsubpar) -- (htl generation) -- (htl);
\draw[ed] (htl) -- (bram insertion) -- (htlmem);
\draw[ed] (htlmem) -- (forward substitution) -- (htlsubst);
\draw[ed] (htlsubst) -- (verilog generation) -- (verilog);

\node[blacknumlarge] at (hyperblock destruction.north west) (bn)
{\ref{sec:hg:hyperblock-destruction}};
\node[blacknumlarge] at (htl generation.north west)
{\ref{sec:hg:htl-generation}};
\node[blacknumlarge] at (bram insertion.north west)
{\ref{sec:hg:bram-insertion}};
\node[blacknumlarge] at (forward substitution.north west)
{\ref{sec:hg:register-forward-substitution}};
\node[blacknumlarge] at (verilog generation.north west)
{\ref{sec:hg:verilog-generation}};

\begin{pgfonlayer}{background}
\node[bgbox, fill=bgbox4, fit={(rtlpar)(htlmem)(bn)(forward substitution)}, inner sep=3mm,minimum width=\linewidth] {};
\end{pgfonlayer}

 \end{tikzpicture}
  \caption{Hardware generation transformation passes introduced to convert
    \rtlpar{} to Verilog.}%
  \label{fig:hg:vericert-hardware-generation}
\end{figure*}

\section{Hyperblock Destruction}%
\label{sec:hg:hyperblock-destruction}

\rtlpar{} is a control-flow graph with nodes mapping to hyperblocks.  This is
useful for the scheduling proof, as each of these hyperblocks can be compared
individually.  However, in the hardware itself, the individual sequential blocks
have to be separated into different states, as within each state the
assignments will be performed in parallel.  This first hyperblock destruction
transformation separates operations that should execute in different clock
cycles into their own locations in the control-flow graph.

\begin{figure}
  \centering
  \begin{tikzpicture}[>=Latex,shorten >=1pt,
    label/.style={circle,draw,fill=white,inner sep=0.4mm,font=\footnotesize},
    bb/.style={align=left, draw=white, fill=black!5}]
    \node[bb] (initial block) {\rtlinline`[ [ [ r1 := r2 ] ]; `\\
                               \rtlinline`  [ [ r3 := r4 ];   `\\
                               \rtlinline`    [ goto 2   ] ] ]`};
    \node[bb, right=5cm of initial block, yshift=1cm] (first transf block)
      {\rtlinline`[ [ r1 := r2 ]; `\\
       \rtlinline`  [ goto 3   ] ]`};
    \node[bb, below=of first transf block] (second transf block)
                               {\rtlinline`  [ [ r3 := r4 ];   `\\
                                \rtlinline`    [ goto 2   ] ]`};
    \node[label] at (initial block.north west) {\texttt{1}};
    \node[label] at (first transf block.north west) {\texttt{1}};
    \node[label] at (second transf block.north west) {\texttt{3}};
    % \draw[->,very thick] ($(initial block.east) + (0.5,0)$)
    %   -- node [below, font=\footnotesize] {hyperblock destruction}
    %   ($(initial block.east) + (4,0)$);
   \path (initial block) -- node[pass] (fs) {hyperblock\\destruction} ($(initial block.east) + (4,0)$);
   \draw[ed] (initial block) -- (fs) -- ($(initial block.east) + (4,0)$);
  \end{tikzpicture}
  \caption{Hyperblock destruction transformation splitting up the hyperblock into
    multiple locations.}%
  \label{fig:hg:hyperblock-destruction}
\end{figure}

\Cref{fig:hg:hyperblock-destruction} shows an example hyperblock destruction
transformation, where a new block is added at location \diaglabel{$3$} with the
contents of the second sequential block, and a \rtlinline`goto` instruction is
added to the original block leading to this next block.  Fresh locations for new
blocks are chosen by keeping track of the greatest location in the current
function.

\subsection{Proof of hyperblock destruction}

The proof of hyperblock destruction is relatively simple using a
\gls{plus-forward-simulation}.  Then, for each input states, there are one or
more output states returned by the hyperblock destruction algorithm that should
be equivalent to the execution of the input state when executed sequentially.

\section{\htl{} Generation}%
\label{sec:hg:htl-generation}

The most important transformation of an HLS tool is then the generation of a
hardware description from the list of instructions.  Eventually the hardware
design will be described using Verilog, however, to make the transformation more
incremental, we first turn the program represented by a control-flow graph into
a program represented by an \gls{FSMD}.  This section will describe the
\gls{FSMD} language, called \htl{}, by showing the syntax and semantics of the
language.  Then, the transformation from \rtlsubpar{} to \htl{} is shown, together
with an overview of its correctness proof.

\subsection{\htl{} structure and semantics}%
\label{sec:hg:htl-structure-and-semantics}

At a high level, \htl{} is structured like many other \compcert{} languages,
mapping from locations to Verilog statements.  However, contrary to many other
intermediate languages in \compcert{}, \htl{} does not contain any instructions,
and its semantics use a smaller state to perform the execution.  For example,
the state does not have to contain the program counter because there is an
explicit state register that keeps track of it.

\htl{} comprises a a lot of metadata pointing to important registers, as well as
containing a map from program locations to Verilog statements.  The syntax of
\htl{} is shown in \cref{fig:hg:htl-syntax}.  First, \htl{} contains a list of
parameters, which are additional input registers to the current module.  Next,
the $\mathit{datapath}$ contains the code for the state machine, as well as the
main data path associated with the data path.  This is done by mapping program
locations, or states, to Verilog statements $\verilogstmnttype$, which updates
registers as part of the data path, but also updates to the state.  The
computation of the next state often relies on the state of registers, which is
why it needs to be performed as part of the data path.

\begin{figure}
\centering
\begin{tabular}{rr@{~}r@{~}l@{\hspace*{2mm}}l}
  \llabel{registers} & $\reg, \mbox{\rtlinline{r1}}, \mbox{\rtlinline{r2}}, \ldots \in \regtype$ & & & \\
  \llabel{CFG node labels} & $\location \in \locationtype$ & $::=$ & $\mathbb{N}$ & \\
  \llabel{Verilog statements} & $\verilogstmnt \in \verilogstmnttype$ & & & \\
  \llabel{Code} & $\htlcode \in \locationtype \rightarrow \verilogstmnttype$ & & & \\
  \llabel{\htl{}} & $\htlmodule$ & $::=$ & $\{\ \mathrm{params} : \regtype\
                                        \texttt{list}; $ \\
  & & & $\ \ \mathrm{datapath} : \locationtype \rightarrow \verilogstmnttype; $ \\
  & & & $\ \ \mathrm{entrypoint} : \mathbb{N};$ & \\
  & & & $\ \ \mathrm{state} : \regtype;$ & \\
  & & & $\ \ \mathrm{stack} : \regtype;$ & \\
  & & & $\ \ \mathrm{stack\_size} : \mathbb{N};$ & \\
  & & & $\ \ \mathrm{finish},\mathrm{return},\mathrm{start},\mathrm{reset},\mathrm{clk} :
        \regtype;$ & \\
  & & & $\ \ \mathrm{ram} : \optiontype{\mathrm{\gls{BRAM}}};$ & \\
  & & & $\ \ \mathrm{order\_wf} : \mathrm{state} < \mathrm{finish} < \mathrm{return}
        < \mathrm{stack}$ & \\
  & & & $\qquad\qquad{}\land \mathrm{stack} < \mathrm{reset} < \mathrm{clk};$ & \\
  & & & $\ \ \mathrm{ram\_wf} : \forall r\ldotp \mathrm{ram} = \some{r} \implies
        \mathrm{clk} < r.\mathrm{raddr}; $ & \\
  & & & $\ \ \mathrm{params\_wf} : \forall r \in \mathrm{params}\ldotp
        r < \mathrm{state} \ \}$
\end{tabular}
\caption{Syntax of \htl{}.}
\label{fig:hg:htl-syntax}
\end{figure}

Next, the \htl{} module contains an entry point, which is the initial starting
state of the $\mathit{state}$ register.  After the reset input wire is asserted,
the $\mathit{state}$ register will be reset to that value.  The $\mathit{state}$
register is read at every clock tick and determines the next statement that
should be executed from the $\mathit{datapath}$.  As mentioned before, the
$\mathit{state}$ register is a physical representation of the virtual program
counter from \rtlsubpar{} and other intermediate instruction languages.  We then
also have $\mathit{stack}$ register and an associated $\mathit{stack\_size}$,
which is a Verilog array storing the contents of the stack.  Initially, this
array will be accessed directly by operations in the data path, however,
\cref{sec:hg:bram-insertion} describes how these direct accesses to the array
are instead turned into accesses to a \gls{BRAM}.

We then have a list of input and output control signals, which are used to
return a result by setting the $\mathit{finish}$ flag and assigning the
$\mathit{return}$ register to the result.  Next, there are $\mathit{start}$ and
$\mathit{reset}$ signals that provide a way to reset the state of the internal
state machine, as well as start the execution of the module when ready.
Finally, there is a $\mathit{clk}$ input to provide the clock to the design.
This input is not yet used by the \htl{} design, as execution is still performed
using state transitions in the semantics, however, a register is already
allocated for the clock which will be needed by the final Verilog design.  The
module then also contains a $\mathit{ram}$ which will be further described in
\cref{sec:hg:bram-insertion}, because in the first translation pass it is
initialised to \cnone.

Finally, there are three \emph{well-formedness} criteria which are used to
enforce an ordering between the registers, mainly to be able to show that
registers are independent from each other.

\subsection{\htl{} generation algorithm}%
\label{sec:hg:htl-generation-algorithm}

The generation of \htl{} is relatively straight-forward, as most instructions
have a direct translation to a Verilog implementation.  The Verilog
implementation can therefore follow the semantics of each operation and
implement their arithmetic behaviour directly.  In addition to that, because of
the hyperblock destruction translation, each block in the control-flow graph
corresponds to a state in the final hardware.  First, the translation of
individual instructions is described in
\cref{sec:hg:translating-individual-arithmetic-instructions}, next the
translation of control-flow statements is described in
\cref{sec:hg:translating-control-flow-instructions}.

\subsubsection{Translating individual arithmetic instructions}%
\label{sec:hg:translating-individual-arithmetic-instructions}

The arithmetic operation is then assigned to the destination register using
\emph{blocking} assignment, so as to preserve the sequential nature of the
execution of the code and simplify the correctness proof.  One subtle aspect of
the proof is the translation of registers and predicates into Verilog, because
in \rtlsubpar{} these were contained in separate maps, whereas in Verilog and
\htl{} they need to be combined into one register association map.  This is done
by referring to register $r$ in \rtlsubpar{} as register $r' = 2r$ in Verilog.
Predicate $p$, on the other hand, is referred to as predicate $p' = 2p + 1$ in
Verilog, thereby combining both name spaces into one.  A short example of a
translation from \rtlsubpar{} to \htl{} is shown in
\cref{fig:hg:htl-generation}.

\begin{figure}
  \centering
  \begin{tikzpicture}[>=Latex,shorten >=1pt,
    label/.style={circle,draw,fill=white,inner sep=0.4mm,font=\footnotesize},
    bb/.style={align=left, draw=white, fill=black!5}]
    \node[bb] (initial block) {\rtlinline`[ [ r1 := r2 * r3     ] ]; `\\
                               \rtlinline`[ [ p1 => r4 := r5      ]; `\\
                               \rtlinline`[ [ p2 => Stack[r6] = 3 ]; `\\
                               \rtlinline`  [ goto 2              ] ]`};
    \node[bb, right=4.5cm of initial block] (transf block)
      {\veriloginline`r2 = r4 * r6;`\\
       \veriloginline`r8 = r3 ? r10 : r8;`\\
       \veriloginline`if (r5) stack[r12] = 32'd3;`\\
       \veriloginline`state = 32'd2;`};
    % \node[label] at (initial block.north west) {\texttt{1}};
    % \node[label] at (transf block.north west) {\texttt{1}};
    % \draw[->,very thick] ($(initial block.east) + (0.5,0)$)
    %   -- node [below, font=\footnotesize] {\htl{} generation}
    %   ($(initial block.east) + (3,0)$);
   \path (initial block) -- node[pass] (fs) {\htl{} generation} (transf block);
   \draw[ed] (initial block) -- (fs) -- (transf block);
  \end{tikzpicture}
  \caption{Simple translation from an \rtlsubpar{} block into an \htl{} block.}%
  \label{fig:hg:htl-generation}
\end{figure}

\paragraph{Implementing the \texttt{Oshrximm} instruction}%
\label{sec:algorithm:optimisation:oshrximm}

% Mention that this optimisation is not performed sometimes (clang -03).

Many of the \compcert{} instructions map well to hardware, but \texttt{Oshrximm}
(efficient signed division by a power of two using a logical shift) is expensive
if implemented na\"ively. The problem is that in \compcert{} it is specified as
a signed division:

\begin{equation*}
  \texttt{Oshrximm } x\ y = \text{round\_towards\_zero}\left(\frac{x}{2^{y}}\right)
\end{equation*}

(where $x, y \in \mathbb{Z}$, $0 \leq y < 31$, and $-2^{31} \leq x < 2^{31}$)
and instantiating divider circuits in hardware is well known to cripple
performance. Moreover, since \vericert{} requires the result of a divide
operation to be ready within a single clock cycle, the divide circuit needs to
be entirely combinational. This is inefficient in terms of area, but also in
terms of latency, because it means that the maximum frequency of the hardware
must be reduced dramatically so that the divide circuit has enough time to
finish.  It should therefore be implemented using a sequence of shifts.

\compcert{} eventually performs a translation from this representation into
assembly code which uses shifts to implement the division, however, the
specification of the instruction in \rtl{} itself still uses division instead of
shifts, meaning this proof of the translation cannot be reused.  In \vericert{},
the equivalence of the representation in terms of divisions and shifts is proven
over the integers and the specification, thereby making it simpler to prove the
correctness of the Verilog implementation in terms of shifts.

\subsubsection{Translating memory}
\label{sec:hg:translating-memory}

Translating memory operations and the memory itself is one of the trickiest part
of the translation, especially from a correctness point of view, because of the
large difference in behaviour between CompCert memories and their Verilog
implementation.  At the stage of \htl{} generation, a Verilog array is used to
represent the stack of the function in \rtlsubpar{}.  The verilog array is
defined as the following:

\begin{minted}{systemverilog}
logic [31:0] stack [STK_LEN-1:0];
\end{minted}

This is essentially an array of size \veriloginline`STK_LEN` of 32-bit integers.
This array is therefore word-addressable.  One big difference between C and
Verilog is how memory is represented.  Although Verilog arrays use similar
syntax to C arrays, they must be treated quite differently.  Eventually, this
array will have to be replaced by an actual \gls{BRAM}, which only has a limited
set of read and write ports (one of each in our case).  To make loads and stores
of words as efficient as possible, the \gls{BRAM} needs to be word-addressable,
which means that an entire integer can be loaded or stored in one clock cycle.
However, the memory model that \compcert{} uses for its intermediate languages
is byte\?addressable~\cite{blazy05_formal_verif_memor_model_c}.  If a
byte\?addressable memory was used in the target hardware, which is closer to
\compcert{}'s memory model, then a load and store would instead take four clock
cycles, the \gls{BRAM} implemented in hardware can only perform one read and
write per clock cycle.  It therefore has to be proven that the byte-addressable
memory behaves in the same way as the word-addressable memory in hardware.  Any
modifications of the bytes in the \compcert{} memory model also have to be shown
to modify the word-addressable memory in the same way.  Since only integer loads
and stores are currently supported in \vericert{}, it follows that the addresses
given to the loads and stores will be multiples of four.  Translating from
byte-addressed memory to word-addressed memory can then be done by dividing the
address by four.

As shown in \cref{fig:hg:htl-generation}, predicated instructions are translated
into blocking assignments of a ternary expression in Verilog.  This is the case
for all instructions except for the store instruction, which is translated to a
conditional statement.  This ensures that the memory is only modified when the
predicate is set.  If that were not the case, and the memory was translated
using a ternary statement:

\begin{center}
  \begin{tikzpicture}[>=Latex,shorten >=1pt]
    \node [] (rtlstack) {\rtlinline`p2 => Stack[r6] = 3`}; \node [right=4cm of
    rtlstack] (verilogstack)
    {\veriloginline`stack[r12] = r5 ? 32'd3 : stack[r12]`};
    \draw[->,very thick] ($(rtlstack.east) + (0.5,0)$) --
    ($(verilogstack.west) - (0.5,0)$);
  \end{tikzpicture}
\end{center}

\noindent Then, in the case where the predicate \rtlinline`p2` is $\lfalse$,
that would mean that the statement in \rtlsubpar{} would not be executed.
However, in the generated Verilog, one would have to execute the statement
\veriloginline`stack[r12]`, which might not be possible as the register
\rtlinline`r6` in \rtlsubpar{}, and the corresponding register
\veriloginline`r12` in Verilog could be out-of-range of the
\veriloginline`stack` array.  This would therefore mean that there is no way to
execute the Verilog in that case, as one cannot read from the stack when it is
out-of-range.  Gating it fully with an if-statement, as shown in
\cref{fig:hg:htl-generation}, ensures that the Verilog statement can always be
executed, assuming that the \rtlsubpar{} instruction can also be executed.

\subsubsection{Translating control flow instructions}%
\label{sec:hg:translating-control-flow-instructions}

Most control-flow instructions also map nicely to hardware, however, the way
predicated control-flow instructions are handled differs a bit between
\rtlsubpar{} and \htl{}.  The main problem is that when execution in a
\rtlsubpar{} block reaches a control-flow instruction that is executed, then it
will exit the block immediately.  This is not possible in the \htl{} block,
because the next state is determined based on the value of the
\veriloginline`state` register at the start of the next clock cycle.  Na\"ively
translating the control-flow instructions into assignments to the
\veriloginline`state` register would produce incorrect state transitions in
\htl{}, because a later control-flow instruction could overwrite a previous
instruction.  As a remedy, we keep track of the current negated exit condition,
which accumulates throughout the translation of a block.
\Cref{fig:hg:htl-generation:cf} demonstrates this on a chain of control-flow
operations.  First, the condition of the ternary statement in Verilog
corresponds to the predicate in the block, however, these quickly diverge.  For
the next control-flow instruction, the condition of the ternary statement is set
to be the the translated predicate associated with \rtlinline`p2`, anded
together with the current negated exit condition which is \veriloginline`~r3`.
The current negated exit condition is then updated to be
\veriloginline`~r3 & ~r5`, which is then the ternary expression condition
assigned to the last \veriloginline`state` update.

\begin{figure}
  \centering
  \begin{tikzpicture}[>=Latex,shorten >=1pt,
    label/.style={circle,draw,fill=white,inner sep=0.4mm,font=\footnotesize},
    bb/.style={align=left, draw=white, fill=black!5}]
    \node[bb] (initial block) {\rtlinline`[ [ p1 => goto 3;    `\\
                               \rtlinline`    p2 => goto 4;    `\\
                               \rtlinline`    goto 5;`\\
                               \rtlinline`    r1 <= r2 * r3 ] ]`};
    \node[bb, right=4.5cm of initial block] (transf block)
      {\veriloginline`state =  r3       ? 32'd3 : state;`\\
       \veriloginline`state = ~r3 & r5  ? 32'd4 : state;`\\
       \veriloginline`state = ~r3 & ~r5 ? 32'd4 : state;`\\
       \veriloginline`r2    = ~r3 & ~r5 ? r4 * r6 : r2; `};
    % \node[label] at (initial block.north west) {\texttt{1}};
    % \node[label] at (transf block.north west) {\texttt{1}};
    % \draw[->,very thick] ($(initial block.east) + (0.5,0)$)
    %   -- node [below, font=\footnotesize] {\htl{} generation}
    %   ($(initial block.east) + (3,0)$);
         \path (initial block) -- node[pass] (fs) {\htl{} generation} (transf block);
   \draw[ed] (initial block) -- (fs) -- (transf block);
  \end{tikzpicture}
  \caption[Describing the control flow translation from \rtlsubpar{} to
  \htl{}.]{Describing the control flow translation from \rtlsubpar{} to
    \htl{}.}%
  \label{fig:hg:htl-generation:cf}
\end{figure}

To avoid all side-effects after a gated control-flow instruction, regular
instructions will have to be gated by the same negated control-flow predicate,
but do not themselves modify the value of the negated control-flow predicate for
any following instructions.

Finally, \texttt{return} instructions are translated to an assignment of 1 to
the \htl{} finish register, signalling that the hardware has finished computing,
and the result that should be returned is assigned to the return register.

\subsubsection{Translating the top-level function}

Each \rtlsubpar{} function is translated separately, and within each function,
each block is translated to a Verilog statement.  In addition to that, new
registers are created for the various control signals and registers in \htl{},
ensuring that they follow the ordering present in the \htl{} specification.  In
addition to that, the top-level translation also needs to ensure that it is
compiling a translation unit with the \texttt{main} function, as linking with
other translation units is not supported by the Verilog semantics.

\subsection{\htl{} generation correctness proof}%
\label{sec:hg:htl-generation-correctness-proof}

\YH{TODO: Move this section earlier, or maybe remove it completely it I have
  already stated most of what is there.}

There is quite a large mismatch between the \htl{} semantics and the
\rtlsubpar{} semantics.  This is mainly due to the following two points:

\begin{itemize}
\item As already mentioned in \cref{sec:hg:translating-memory}, because the
  memory model in our \htl{} semantics is finite and concrete, but the CompCert
  memory model is more abstract and infinite with additional bounds, the
  equivalence of these models needs to be proven.  Moreover, our memory is
  word-addressed for efficiency reasons, whereas CompCert's memory is
  byte-addressed.

\item Second, the \htl{} semantics operates quite differently to the usual
  intermediate languages in CompCert.  All the CompCert intermediate languages
  use a map from control-flow nodes to instructions.  An instruction can
  therefore be selected using an abstract program pointer. Meanwhile, in the
  \htl{} semantics the whole design is executed at every clock cycle, because
  hardware is inherently parallel. The program pointer is part of the design as
  well, not just part of an abstract state. This makes the semantics of \htl{}
  simpler, but comparing it to the semantics of \rtl{} becomes more challenging,
  as one has to map the abstract notion of the state to concrete values in
  registers.
\end{itemize}

%\subsection{Forward Simulation from 3AC to \htl{}}\label{sec:proof:3ac_htl}

As \htl{} is quite far removed from \rtlsubpar{}, this first translation is the
most involved and therefore requires a larger proof, because the translation
from \rtlsubpar{} instructions to Verilog statements needs to be proven correct
in this step. Instead of defining small-step semantics for each construct in
Verilog, the semantics are defined over one clock cycle and mirror the semantics
defined for Verilog.  \Cref{lemma:htl} shows the result that needs to be proven
in this subsection.

\begin{lemma}[Forward simulation from \rtlsubpar{} to \htl{}]\label{lemma:htl}
  Writing \texttt{tr\_htl} for the translation from \rtlsubpar{} to \htl{}, we
  have:
  \begin{equation*}
    \forall c, h, B \in \texttt{Safe},\quad \yhfunction{tr\_htl} (c) = \yhconstant{OK} (h) \land c \Downarrow B \implies h \Downarrow B.
  \end{equation*}
\end{lemma}

\begin{proof}[Proof sketch]
  I prove this lemma by first establishing a specification of the translation
  function $\yhfunction{tr\_htl}$ that captures its important properties, and
  then splitting the proof into two parts: one to show that the translation
  function does indeed meet its specification, and one to show that the
  specification implies the desired simulation result. This strategy is in
  keeping with standard \compcert{} practice.

  % The forward simulation is then proven by showing that the initial states and
  % final states between the \rtlsubpar{} semantics and \htl{} semantics match, and then
  % showing that the simulation diagram in Lemma~\ref{lemma:simulation_diagram}
  % holds.
\end{proof}

%\subsubsection{From Implementation to Specification}\label{sec:proof:3ac_htl:specification}
%
%%To simplify the proof, instead of using the translation algorithm as an
%%assumption, as was done in Lemma~\ref{lemma:htl}, a specification of the
%%translation can be constructed instead which contains all the properties that
%%are needed to prove the correctness.  For example, for the translation from \rtlsubpar{}
%%to \htl{},
%
%The specification for the translation of \rtlsubpar{} instructions into \htl{} data-path and
%control logic can be defined by the following predicate:
%
%\begin{equation*}
%  \yhfunction{spec\_instr}\ \mathit{fin}\ \mathit{ret}\ \sigma\ \mathit{stk}\ i\ \mathit{data}
%\end{equation*}
%
%\noindent Here, the $\mathit{data}$ parameter is the statement that the current
%\rtlsubpar{} instruction $i$ should translate to. The other parameters are the
%special registers defined in Section~\ref{sec:verilog:integrating}. An example
%of a rule describing the translation of an arithmetic/logical operation from
%\rtlsubpar{} is the following:
%
%\begin{equation*}
%  \inferrule[Iop]{\yhfunction{tr\_op}\ \mathit{op}\ \vec{a} = \yhconstant{OK}\ e}{\yhfunction{spec\_instr}\ \mathit{fin}\ \mathit{ret}\ \sigma\ \mathit{stk}\ (\yhconstant{Iop}\ \mathit{op}\ \vec{a}\ d\ n)\ (d\ \yhkeyword{<=}\ e)\ (\sigma\ \yhkeyword{<=}\ n)}
%\end{equation*}
%
%\noindent Assuming that the translation of the operator $\mathit{op}$ with
%operands $\vec{a}$ is successful and results in expression $e$, the rule
%describes how the destination register $d$ is updated to $e$ via a non-blocking
%assignment in the data path, and how the program counter $\sigma$ is updated to
%point to the next CFG node $n$ via another non-blocking assignment in the
%control logic.
%
%In the following lemma, $\yhfunction{spec\_htl}$ is the top-level specification
%predicate, which is built using $\yhfunction{spec\_instr}$ at the level of
%instructions.
%
%\begin{lemma}\label{lemma:specification}
%  If a \rtlsubpar{} program $c$ is translated correctly to an \htl{} program $h$, then the
%  specification of the translation holds.
%  \begin{equation*}
%    \forall c, h,\quad \yhfunction{tr\_htl} (c) = \yhconstant{OK}(h) \implies \yhfunction{spec\_htl}\ c\ h.
%  \end{equation*}
%\end{lemma}
%
%%\begin{proof}
%%  Follows from the definition of the specification and therefore should match
%%  the implementation of the algorithm.
%%\end{proof}

\subsubsection{Forward simulation proof of translation}

To prove that the translation described in \cref{sec:hg:htl-generation} results
in the desired forward simulation, we must first define a relation that matches
each \rtlsubpar{} state to an equivalent \htl{} state.  This relation also
captures the assumptions made about the \rtlsubpar{} code that we receive from
\compcert{}.  These assumptions then have to be proven to always hold assuming
the \htl{} code was created by the translation algorithm.  Some of the
assumptions that need to be made about the \rtlsubpar{} and \htl{} code for a
pair of states to match are:

\begin{itemize}
\item The \rtlsubpar{} register file $R$ needs to be \enquote{less defined} than
  the \htl{} register map $\Gamma_{r}$ (written $R \le \Gamma_{r}$). This means
  that all entries should be equal to each other, unless a value in $R$ is
  undefined, in which case any value can match it.
\item There is a single allocation that was performed in \rtlsubpar{}, with the
  size of the allocation being equal to the stack size of the \texttt{main}
  function, which was performed when the \texttt{main} function was initially
  called; that is: $|M| \le \mathtt{main}.\mathit{stacksize}$.
\item The \gls{BRAM} values represented by each Verilog array in $\Gamma_{a}$ need to
  match the \rtlsubpar{} function's stack contents, which are part of the memory $M$;
  that is: $M \le \Gamma_{a}$.
\item The state is well formed, which means that the value of the state register
  matches the current value of the program counter; that is:
  $\mathit{pc} = \Gamma_{r}[\sigma]$.
\end{itemize}

We also define the following set $\mathcal{I}$ of invariants that must hold for
the current state to be valid:

\begin{itemize}
\item that all pointers in the program use the stack as a base pointer,
\item that any loads or stores to locations outside of the bounds of the stack
  result in undefined behaviour (and hence they do not need to be handled),
\item that $\mathit{rst}$ and $\mathit{fin}$ are not modified and therefore stay
  at a constant 0 throughout execution, and
\item that the stack frames match.  As no function calls are performed, as they
  are all inlined, the stack frames will always be empty.
\end{itemize}

We can now define the simulation diagram for the translation. The \rtlsubpar{} state can
be represented by the tuple $(R,M,\mathit{pc})$, which captures the register
file, memory, and program counter. The \htl{} state can be represented by the pair
$(\Gamma_{r}, \Gamma_{a})$, which captures the states of all the registers and
arrays in the module.  Finally, $\mathcal{I}$ stands for the other invariants
that need to hold for the states to match.

\begin{lemma}\label{lemma:simulation_diagram}
  Given the \rtlsubpar{} state $(R,M,\mathit{pc})$ and the matching \htl{} state
  $(\Gamma_{r}, \Gamma_{a})$, assuming one step in the \rtlsubpar{} semantics produces
  state $(R',M',\mathit{pc}')$, there exist one or more steps in the \htl{}
  semantics that result in matching states $(\Gamma_{r}', \Gamma_{a}')$.  This
  is all under the assumption that the specification $\yhfunction{spec\_{htl}}$
  holds for the translation.

  \YH{TODO: Maybe find a better way to describe that the memory was only
    allocated once, as it is a slightly different condition to the one that
    states that the contents of both memories are the same.  It touches more on
    the permissions of the CompCert memory and showing that they agree with the
    stack bounds.}

  \begin{center}
    \begin{tikzpicture}
      \begin{scope}
        \node[circle] (s1) at (0,2) {$R, M, \mathit{pc}$};
        \node[circle] (r1) at (14,2) {$\Gamma_{r}, \Gamma_{a}$};
        \node[circle] (s2) at (0,0) {$R', M', \mathit{pc}'$};
        \node[circle] (r2) at (14,0) {$\Gamma_{r}', \Gamma_{a}'$};
        %\node at (6.8,0.75) {+};
        \draw (s1) -- node[above] {$\mathcal{I} \land (R \le \Gamma_{r}) \land
          (M \le \Gamma_{a}) \land (\mathit{pc} = \Gamma_{r}[\sigma]) \land |M| \le \mathtt{main}.\mathit{stacksize}$} ++ (r1);
        \draw[-{Latex}] ($(s1.south) + (0,0.4)$) -- ($(s2.north) - (0,0.4)$);
        \draw[-{Latex},dashed] ($(r1.south) + (0,0.2)$) to[auto, pos=0.7] node {+} ($(r2.north) - (0,0.2)$);
        \draw[dashed] (s2) -- node[above] {$\mathcal{I} \land (R' \le
          \Gamma_{r}') \land (M' \le \Gamma_{a}') \land (\mathit{pc}' =
          \Gamma_{r}'[\sigma]) \land |M'| \le \mathtt{main}.\mathit{stacksize}$} ++ (r2);
      \end{scope}
    \end{tikzpicture}
  \end{center}
\end{lemma}

\begin{proof}[Proof sketch]
  This simulation diagram is proven by induction over the operational semantics
  of \rtlsubpar{}, which allows us to find one or more steps in the \htl{} semantics that
  will produce the same final matching state.
\end{proof}

\section{BRAM insertion}%
\label{sec:hg:bram-insertion}

The simplest way to implement loads and stores in \vericert{} would be to access
the Verilog array directly from within the data path as is currently the case
after the \htl{} generation. This would be correct, but when a Verilog array is
accessed at several program points, the synthesis tool is unlikely to detect
that it can be implemented as an \gls{BRAM}, and will resort to using registers
instead, increasing the circuit's area and affecting performance.  This is
because reads and writes to a \gls{BRAM} need to follow a certain pattern to be
suitable to be replaced by \gls{BRAM} reads and writes.  For example, the
synthesis tool will have to check that the array is only written to once per
clock cycle, and is only read from once as well.  To avert this, we arrange that
the data path does not access memory directly, but simply sets the address it
wishes to access and then toggles the \texttt{u\_en} flag. This activates the
\gls{BRAM} interface on the next falling clock edge, which performs the
requested load or store. By factoring all the memory accesses out into a
separate interface, we ensure that the underlying array is only accessed from a
single program point in the Verilog code, and thus ensure that the synthesis
tool will correctly infer a \gls{BRAM} block.\footnote{Interestingly, the
  Verilog code shown for the \gls{BRAM} interface must not be modified, because
  the synthesis tool will only generate a \gls{BRAM} when the code matches a
  small set of specific patterns.}

This transformation pass therefore translates direct accesses to the Verilog
array in \htl{} and replaces them by signals that access the \gls{BRAM}
interface in a separate always-block. The translation is performed by going
through all the instructions and replacing each load and store expression in
turn.  Stores can be replaced by the necessary wires to the \gls{BRAM}
directly. Loads are a little more subtle: loads that use the \gls{BRAM}
interface take two clock cycles where a direct load from an array takes only
one, so this pass inserts an extra state after each load.  The scheduling
algorithm described in \cref{sec:hs:implementing-scheduling} can already take
this into account as well, and can ensure that the next clock cycle after a load
does not perform a load or a store, however, some additional proofs would have
to be added to this transformation pass to check that this is actually the case,
and that the load will proceed as expected.

%\JW{I've called that negedge always-block the `\gls{BRAM} driver' in my proposed text above -- that feels like quite a nice a word for it to my mind -- what do you think?}\YH{Yes I quite like it!}
%Verilog arrays can be used in a variety of ways, however, these do not all
%produce optimal hardware designs.  If, for example, arrays in Verilog are
%accessed immediately in the data-path, then the synthesis tool is not be able
%to identify it as having the right properties for a \gls{BRAM}, and would
%instead implement the array using registers.  This is extremely expensive, and
%for large memories this can easily blow up the area usage of the FPGA, and
%because of the longer wires that are needed, it would also affect the
%performance of the circuit.  The synthesis tools therefore provide code
%snippets that they know how to transform into various constructs, including
%snippets that will generate proper RAMs in the final hardware.  This process is
%called memory inference.  The initial translation from 3AC to \htl{} converts
%loads and stores to direct accesses to the memory, as this preserves the same
%behaviour without having to insert more registers and logic.  We therefore have
%another pass from \htl{} to itself which performs the translation from this
%na\"ive use of arrays to a representation which always allows for memory
%inference.  This pass creates a separate always-block to perform the loads and
%stores to the memory, and adds the necessary data, address and enable signals
%to communicate with that always-block from other always-blocks.  This
%always-block is shown between lines 10-15 in Fig.~\ref{fig:accumulator_v}.

\begin{figure}
  \centering
  \begin{minipage}{6cm}
\begin{minted}{systemverilog}
// BRAM interface
(* ram_style = "block" *)
logic [31:0] stack [1:0];
always @(negedge clk)
  if ({u_en != en}) begin
    if (wr_en) stack[addr] <= d_in;
    else d_out <= stack[addr];
    en <= u_en;
  end
\end{minted}
  \end{minipage}
  \caption{Verilog implementation of the BRAM interface generated by Vericert.}
  \label{fig:hg:bram-interface}
\end{figure}

There are two interesting parts to the inserted \gls{BRAM} interface, shown in
\cref{fig:hg:bram-interface}.  Firstly, the memory updates are triggered on the
negative (falling) edge of the clock, out of phase with the rest of the design
which is triggered on the positive (rising) edge of the clock.  The advantage of
this is that instead of loads and stores taking three clock cycles and two clock
cycles respectively, they only take two clock cycles and one clock cycle
instead, greatly improving their performance.  Ideally, this translation would
take advantage of the scheduler, which can already ensure that the clock cycle
after a load remains free to wait for the data from the \gls{BRAM}, however,
this would require additional checking by this transformation to double check
that the read result is not used until it is ready.  Using the negative edge of
the clock is supported by synthesis tools and FPGAs, but in general it does
reduces the time that's available for arithmetic operations in the positive edge
of the clock, making it harder to schedule operations.

Secondly, the logic in the enable signal of the \gls{BRAM} (\texttt{en !=
  u\_en}) is also atypical in hardware designs.  Enable signals are normally
manually controlled and inserted into the appropriate states, by using a check
like the following in the \gls{BRAM}:\@ \texttt{en == 1}.  This means that the
\gls{BRAM} only turns on when the enable signal is set.  However, to make the
proof simpler and avoid reasoning about possible side effects introduced by the
\gls{BRAM} being enabled but not used, a \gls{BRAM} which disables itself after
every use would be ideal.  One method for implementing this would be to insert
an extra state after each load or store that disables the \gls{BRAM}, but this
extra state would eliminate the speed advantage of the negative-edge-triggered
\gls{BRAM}. Another method would be to determine the next state after each load
or store and disable the \gls{BRAM} in that state, but this could quickly become
complicated, especially in the case where the next state also contains a memory
operation, and hence the disable signal should not be added. The method I
ultimately chose was to have the \gls{BRAM} become enabled not when the enable
signal is high, but when it \emph{toggles} its value.  This can be arranged by
keeping track of the old value of the enable signal in \texttt{en} and comparing
it to the current value \texttt{u\_en} set by the data path.  When the values
are different, the \gls{BRAM} gets enabled, and then \texttt{en} is set to the
value of \texttt{u\_en}. This ensures that the \gls{BRAM} will always be
disabled straight after it was used, without having to insert or modify any
other states.

%We can instead generate a second enable signal that is set by the user, and the original enable signal is then updated by the \gls{BRAM} to be equal to the value that the user set.  This means that the \gls{BRAM} should be enabled whenever the two signals are different, and disabled otherwise.

\subsection{BRAM model semantics}%
\label{sec:hg:bram-model-semantics}

\Cref{fig:ram_load_store} gives an example of how the \gls{BRAM} interface
behaves when values are loaded and stored.  There is a \texttt{wr\_en} signal
that determines if a load or store is being performed.  Then, if at the falling
edge \texttt{u\_en} and \texttt{en} are different, the read or write at address
\texttt{addr} is executed.  At the same time, the value of \texttt{u\_en} is
then assigned to \texttt{en}, which would disable it if there is no other action
by the data path on the next clock cycle.  However, if the data path toggles the
value of \texttt{u\_en} on the next clock cycle, the \gls{BRAM} would be enabled
again.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.48\linewidth}
    \begin{tikztimingtable}[timing/d/background/.style={fill=white}]
      \small clk & 2L 3{6C} \\
      \small u\_en & 2D{u\_en} 18D{$\overline{\text{u\_en}}$}\\
      \small addr & 2U 18D{3} \\
      \small wr\_en & 2U 18L \\
      \small en & 8D{u\_en} 12D{$\overline{\text{u\_en}}$}\\
      \small d\_out & 8U 12D{0xDEADBEEF} \\
      \small r & 14U 6D{0xDEADBEEF} \\
      \extracode
      \node[help lines] at (2,2.25) {\tiny 1};
      \node[help lines] at (8,2.25) {\tiny 2};
      \node[help lines] at (14,2.25) {\tiny 3};
      \begin{pgfonlayer}{background}
        \vertlines[help lines]{2,8,14}
      \end{pgfonlayer}
    \end{tikztimingtable}
    \caption[Timing diagrams for loads.]{Timing diagram for loads. At time 1, the \texttt{u\_en} signal is toggled to enable the \gls{BRAM}. At time 2, \texttt{d\_out} is set to the value stored at the address in the \gls{BRAM}, which is finally assigned to the register \texttt{r} at time 3.}\label{fig:ram_load}
  \end{subfigure}\hfill%
  \begin{subfigure}[b]{0.48\linewidth}
    \begin{tikztimingtable}[timing/d/background/.style={fill=white}]
      \small clk & 2L 2{7C} \\
      \small u\_en & 2D{u\_en} 14D{$\overline{\text{u\_en}}$}\\
      \small addr & 2U 14D{3} \\
      \small wr\_en & 2U 14H \\
      \small d\_in & 2U 14D{0xDEADBEEF} \\
      \small en & 9D{u\_en} 7D{$\overline{\text{u\_en}}$}\\
      \small stack[addr] & 9U 7D{0xDEADBEEF} \\
      \extracode
      \node[help lines] at (2,2.25) {\tiny 1};
      \node[help lines] at (9,2.25) {\tiny 2};
      \begin{pgfonlayer}{background}
        \vertlines[help lines]{2,9}
      \end{pgfonlayer}
    \end{tikztimingtable}
    \caption[Timing diagram for stores.]{Timing diagram for stores. At time 1,
      the \texttt{u\_en} signal is toggled to enable the \gls{BRAM}, and the
      address \texttt{addr} and the data to store \texttt{d\_in} are set. On the
      negative edge at time 2, the data is stored into the
      \gls{BRAM}.}\label{fig:ram_store}
  \end{subfigure}
%  \alt{Timing diagrams of loads and stores, showing which signals are modified at which time step.}
  \caption{Timing diagrams showing the execution of loads and stores over
    multiple clock cycles.}\label{fig:ram_load_store}
\end{figure}

\subsection{BRAM insertion and correctness proof}%
\label{sec:hg:bram-insertion-and-correctness-proof}

\begin{figure}
  \centering
  \begin{minipage}{1.0\linewidth}
    \begin{gather*}
      \inferrule[Idle]{\Gamma_{\mathrm{r}}[r.\mathrm{en}] = \Gamma_{\mathrm{r}}[r.\mathrm{u\_en}]}{((\Gamma_{\mathrm{r}}, \Gamma_{\mathrm{a}}), \Delta, r) \downarrow_{\mathrm{ram}} \Delta}\\
%
      \inferrule[Load]{\Gamma_{\mathrm{r}}[r.\mathrm{en}] \ne \Gamma_{\mathrm{r}}[r.\mathrm{u\_en}] \\ \Gamma_{\mathrm{r}}[r.\mathrm{wr\_en}] = 0}{((\Gamma_{\mathrm{r}}, \Gamma_{\mathrm{a}}), (\Delta_{\mathrm{r}}, \Delta_{\mathrm{a}}), r) \downarrow_{\mathrm{ram}} (\Delta_{\mathrm{r}}[r.\mathrm{en} \mapsto r.\mathrm{u\_en}, r.\mathrm{d\_out} \mapsto (\Gamma_{\mathrm{a}}[r.\mathrm{mem}])[ r.\mathrm{addr}]], \Delta_{\mathrm{a}}) }\\
%
      \inferrule[Store]{\Gamma_{\mathrm{r}}[r.\mathrm{en}] \ne \Gamma_{\mathrm{r}}[r.\mathrm{u\_en}] \\ \Gamma_{\mathrm{r}}[r.\mathrm{wr\_en}] = 1}{((\Gamma_{\mathrm{r}}, \Gamma_{\mathrm{a}}), (\Delta_{\mathrm{r}}, \Delta_{\mathrm{a}}), r) \downarrow_{\mathrm{ram}} (\Delta_{\mathrm{r}}[r.\mathrm{en} \mapsto r.\mathrm{u\_en}], \Delta_{\mathrm{a}}[r.\mathrm{mem} \mapsto (\Gamma_{\mathrm{a}}[ r.\mathrm{mem}])[r.\mathrm{addr} \mapsto r.\mathrm{d\_in}]]) }
    \end{gather*}
  \end{minipage}
  \caption[Specification for the memory implementation in \htl{}.]{Specification
    for the memory implementation in \htl{}, where $r$ is the \gls{BRAM}, which
    is then implemented by equivalent Verilog code.}\label{fig:htl_ram_spec}
\end{figure}

\YH{TODO: Link the \gls{BRAM} from \htl{} to the ram instantiated here.}

\htl{} can only represent a single state machine, so we must model the
\gls{BRAM} abstractly to reason about the correctness of replacing the direct
read and writes to the array by loads and stores to a \gls{BRAM}.  The
specification for the \gls{BRAM} is shown in \cref{fig:htl_ram_spec}, which
defines how the \gls{BRAM} $r$ will behave for all the possible combinations of
the input signals.  This specification is part of the \htl{} semantics and runs
in parallel to the state machine.  However, as the \gls{BRAM} is triggered by
the falling edge of the clock, it will execute in between standard clock cycles,
and a merge of the association maps is performed in between each.

\subsubsection{From implementation to specification}

The first step in proving the simulation correct is to build a specification of
the translation algorithm.  There are five possibilities for the transformation
of an instruction. For each Verilog statement in the map at location $i$, the
statement is either a load, a store, a predicated load, a predicated store, or
neither. The load, store, predicated load and predicated store is translated to
the equivalent representation using the \gls{BRAM} specification and all other
instructions are left intact.  The specification of the translation is shown in
\cref{sec:hg:memory-transformation-specification}, where $\sigma$ is state
register, $r$ is the \gls{BRAM}, $d$ are the input data-path and control logic maps,
and $i$ is the current state.  ($n$ is the newly inserted state, which only
applies to the translation of loads.)

\newcommand\nonblockasgn{\mathrel{\texttt{<=}}}
\newcommand\blockasgn{\mathrel{\texttt{=}}}
\newcommand\msemi{\texttt{;}\ }
\newcommand\mternary[3]{#1\mathbin{\texttt{?}}#2\mathbin{\texttt{:}}#3}
\newcommand\mxor{\oplus}
\newcommand\verilogeq{\mathbin{\texttt{==}}}

\begin{figure}
\centering
\begin{mathpar}
  \inferrule[Store Transl]{ d[i] = \left(\begin{array}{l}
    r.\mathrm{mem}\texttt{[}e_{1}\texttt{]} \blockasgn
    e_{2}\msemi \arcr
    \sigma \blockasgn e_3\msemi
  \end{array}\right) \\
  t = \left(\begin{array}{l}
    r.\mathrm{u\_en} \blockasgn
    \neg r.\mathrm{u\_en}\msemi \arcr
    r.\mathrm{wr\_en} \blockasgn 1\msemi \arcr r.\mathrm{d\_in} \blockasgn
    e_{2}\msemi \arcr
    r.\mathrm{addr} \blockasgn e_{1} \msemi \arcr
    \sigma \blockasgn e_3 \msemi
  \end{array}\right)}%
  {\yhfunction{spec\_ram\_tr}\ \sigma\ r\ d\ d[i \mapsto t]\ i\ n}
  \and
  \inferrule[Predicated Store Transl]{ d[i] = \left(\begin{array}{l}
    \texttt{if(} c\texttt{)}\ r.\mathrm{mem}\texttt{[}e_{1}\texttt{]} \blockasgn
    e_{2}\msemi \arcr
    \sigma \blockasgn e_3\msemi\end{array}\right) \\ t = \left(\begin{array}{l}
      r.\mathrm{u\_en} \blockasgn
    (c \verilogeq \texttt{32'b0})\mxor r.\mathrm{u\_en}\msemi\arcr
      r.\mathrm{wr\_en} \blockasgn \texttt{1'b1}\msemi\arcr
      r.\mathrm{d\_in} \blockasgn \mternary{c}{e_{2}}{r.\mathrm{d\_in}}\msemi\arcr
      r.\mathrm{addr} \blockasgn \mternary{c}{e_{1}}{r.\mathrm{addr}}\msemi
      \arcr
      \sigma \blockasgn e_3 \msemi
    \end{array}\right)}%
  {\yhfunction{spec\_ram\_tr}\ \sigma\ r\ d\ d[i \mapsto t]\ i\ n}
  \and
  \inferrule[Load Transl]{ d[i] = \left(\begin{array}{l}
    d \blockasgn r.\mathrm{mem}\texttt{[}e_{1}\texttt{]} \arcr
    \sigma \blockasgn e_2\msemi
  \end{array}\right) \\
  \begin{array}{l}
  t_1 = \left(\begin{array}{l}
    r.\mathrm{u\_en} \blockasgn
    \neg r.\mathrm{u\_en}\msemi \arcr
    r.\mathrm{wr\_en} \blockasgn \texttt{1'b0}\msemi \arcr
    r.\mathrm{addr} \blockasgn e_{1} \msemi \arcr
    \sigma \blockasgn n \msemi
  \end{array}\right) \arcr
  t_2 = \left(\begin{array}{l}
    d \blockasgn r.\mathrm{d\_out} \msemi \arcr
    \sigma \blockasgn e_2 \msemi
  \end{array}\right)
  \end{array}}%
  {\yhfunction{spec\_ram\_tr}\ \sigma\ r\ d\ d[i \mapsto t_1; n \mapsto t_2]\ i\ n}
  \and
  \inferrule[Predicated Load Transl]{ d[i] = \left(\begin{array}{l}
    d \blockasgn \mternary{c}{r.\mathrm{mem}\texttt{[}e_{1}\texttt{]}}{d}\msemi \arcr
    \sigma \blockasgn e_2\msemi
  \end{array}\right) \\
  \begin{array}{l}
    t_1 = \left(\begin{array}{l}
    r.\mathrm{u\_en} \blockasgn (c \verilogeq \texttt{32'b0})\mxor r.\mathrm{u\_en}\msemi \arcr
    r.\mathrm{wr\_en} \blockasgn \texttt{1'b0}\msemi \arcr
    r.\mathrm{addr} \blockasgn \mternary{c}{e_{1}}{r.\mathrm{addr}} \msemi \arcr
    \sigma \blockasgn n \msemi
  \end{array}\right) \arcr
  t_2 = \left(\begin{array}{l}
    d \blockasgn \mternary{c}{e_{1}}{r.\mathrm{d\_out}} \msemi \arcr
    \sigma \blockasgn e_2 \msemi
  \end{array}\right)
    \end{array}}%
  {\yhfunction{spec\_ram\_tr}\ \sigma\ r\ d\ d[i \mapsto t_1; n \mapsto t_2]\ i\ n}
  \and
  \inferrule[Default Transl]{(\forall e_1\ e_2\ e_3\ldotp d[i] \neq (r.\mathrm{mem}\texttt{[}e_{1}\texttt{]} \blockasgn
    e_{2}\msemi \sigma \blockasgn e_3)) \\
    (\forall c\ e_1\ e_2\ e_3\ldotp d[i] \neq (\texttt{if(}c\texttt{)}\ r.\mathrm{mem}\texttt{[}e_{1}\texttt{]} \blockasgn
    e_{2}\msemi \sigma \blockasgn e_3)) \\
  (\forall d\ e_1\ e_2\ldotp d[i] \neq (d \blockasgn
  r.\mathrm{mem}\texttt{[}e_{1}\texttt{]}\msemi \sigma \blockasgn
  e_2))\\
(\forall c\ d\ e_1\ e_2\ldotp d[i] \neq (d \blockasgn \mternary{c}{r.\mathrm{mem}\texttt{[}e_{1}\texttt{]}}{d}\msemi \sigma \blockasgn e_2))}%
  {\yhfunction{spec\_ram\_tr}\ \sigma\ r\ d\ d\ i\ n}
\end{mathpar}
\caption[Memory transformation specification.]{Memory transformation
  specification.\YH{TODO: replace register name $d$ because it's also used for
    the data path.}}%
\label{sec:hg:memory-transformation-specification}
\end{figure}

\subsubsection{From specification to simulation}

Another simulation proof is performed to prove that the insertion of the
\gls{BRAM} is semantics preserving.  As in \cref{lemma:simulation_diagram}, we
require some invariants that always hold at the start and end of the simulation.
The invariants needed for the simulation of the \gls{BRAM} insertion are quite
different to the previous ones, so we can define these invariants
$\mathcal{I}_{\mathrm{r}}$ to be the following:

\begin{itemize}
\item The association map for arrays $\Gamma_{\mathrm{a}}$ always needs to have
  the same arrays present, and these arrays should never change in size.
\item The \gls{BRAM} should always be disabled at the start of each simulation
  step. (This is why self-disabling \gls{BRAM} is needed.)
\end{itemize}

The other invariants and assumptions for defining two matching states in \htl{}
are quite similar to the simulation performed in
\cref{lemma:simulation_diagram}, such as ensuring that the states have the same
value, and that the values in the registers are less defined.  In particular,
the less defined relation matches up all the registers, except for the new
registers introduced by the \gls{BRAM}.

\begin{lemma}[Forward simulation from \htl{} to \htl{} after inserting the \gls{BRAM}]\label{lemma:htl_ram}
  Given an \htl{} program, the forward-simulation relation should hold after
  inserting the \gls{BRAM} and wiring the load, store, and control signals.

  \begin{align*}
    \forall h\ h'\ B \in \texttt{Safe}\ldotp\quad \yhfunction{tr\_ram\_ins}(h) = h' \land h \Downarrow B \implies h' \Downarrow B.
  \end{align*}
\end{lemma}

\section{Register Forward Substitution}%
\label{sec:hg:register-forward-substitution}

Until now, only blocking assignment has been used to assign variables
sequentially, being more faithful to the instructions provided as the input
language.  The transformation is shown in \cref{fig:hg:forward-substitution},
and it turns sequential, blocking assignment into parallel, nonblocking
assignment by substituting register definitions within a state.  Each assignment
in the translated block is independent from the other blocks, meaning each
assignment can be executed in parallel.  Note that the \texttt{state} is
assigned twice using nonblocking assignment.  In that case, only the last
nonblocking assignment is kept.  In addition to that, \texttt{r2} is no longer
used in the block, as it has been replaced by its definition in the assignment
to \texttt{r8}.  It is likely that \texttt{r2} is not used in any other block
either, in which case it should be removed.  This is not performed by this
transformation pass, as in practice synthesis tools can reliably remove a
register that is assigned and never referenced in the design.  There are two
reasons why this transformation is needed.

\begin{enumerate}
\item In a clocked always-block, nonblocking assignment should be used for all
  registers that could interact with other always-blocks.  As we have a \gls{BRAM}
  block in a separate always block, we should at least use nonblocking
  assignment for the registers that interact with memory.  In this case, it is
  technically not required, because the \gls{BRAM} is executing on the negative edge of
  the clock, however, if it is ever replaced by a \gls{BRAM} that executes on the
  positive edge of the clock, or supplemented with other functional units that
  execute on the positive edge of the clock, then any registers communicating
  with those blocks will need to use nonblocking assignment.
\item Some synthesis tools do not seem to be able to optimise the designs
  generated by Vericert with blocking assignments, as they seem to remove less
  unused registers, resulting in slightly lower maximum frequency and slightly
  larger area.\footnote{This seems to be the case with Vivado 2017.1 and seems
    to have been fixed by Vivado 2023.1.}  In the case of the example given in
  \cref{fig:hg:forward-substitution}, it seems like some synthesis tool versions
  do not remove \texttt{r2} when it is referenced in the assignment to
  \texttt{r8} and not referenced anywhere else in the design.  This should not
  be the case as in this example \texttt{r2} should just become a wire.
\end{enumerate}

\definecolor{mathcommentcolor}{HTML}{0099FF}
\newcommand\mathstate[1]{\veriloginline`//` \textcolor{mathcommentcolor}{#1}}
\makeatletter
\newcommand\mathstateln[2]{{\tiny #1 \quad}\my@name@mathstateln{#1}}
\NewDocumentCommand \my@name@mathstateln { m }{%
  \def\@currentlabelname{#1}%
}
\makeatother

\newcommand\asgnblockingbefore{\Gamma}
\newcommand\asgnblockingafter{\Gamma'}
\newcommand\asgnnonblockingbefore{\Delta}
\newcommand\asgnnonblockingafter{\Delta'}

\newcommand\interstate[2]{{#2}^{#1}_{\mathrm r}}
\newcommand\initialstate[1]{{#1}^{0}_{\mathrm r}}

\begin{figure}
  \centering
  \begin{tikzpicture}[>=Latex,shorten >=1pt,
    label/.style={circle,draw,fill=white,inner sep=0.4mm,font=\footnotesize},
    bb/.style={align=left, draw=white, fill=black!5}]
    \node[bb] (initial block) {
      \veriloginline`state = 32'd2;`\\
      \veriloginline`r2 = r4 * r6;`\\
      \veriloginline`r8 = r2 + r10;`\\
      \veriloginline`state = r3 ? 32'd3 : state;`};

    \node[bb, right=4.5cm of
    initial block] (transf block)
    {\veriloginline`state <= 32'd2;`\\
     \veriloginline`r2 <= r4 * r6;`\\
     \veriloginline`r8 <= {r4 * r6} + r10;`\\
     \veriloginline`state <= r3 ? 32'd3 : 32'd2;`};
    % \node[label] at (initial
    % block.north west) {\texttt{1}};
    % \node[label] at (transf block.north west)
    % {\texttt{1}};
    %\draw[->,very thick] ($(initial block.east) + (0.5,0)$) --
    %node [below, font=\footnotesize] {forward substitution}
    %($(initial block.east) + (4,0)$);
   \path (initial block) -- node[pass] (fs) {forward\\substitution} (transf block);
   \draw[ed] (initial block) -- (fs) -- (transf block);
  \end{tikzpicture}
  \caption{Simple example of the forward substitution transformation.}%
  \label{fig:hg:forward-substitution-simple}
\end{figure}

\begin{figure}
  \centering
  \begin{tikzpicture}[>=Latex,shorten >=1pt,
    label/.style={circle,draw,fill=white,inner sep=0.4mm,font=\footnotesize},
    bb/.style={align=left, draw=white, fill=black!5}]
    \node[bb] (initial block) {
      \mathstateln{1}\phantomsection\label{msln:1}\mathstate{\texttt{assume:} $\at{\initialstate{\asgnblockingbefore}}{\texttt{state} \mapsto \varnothing; \texttt{r2} \mapsto
          \varnothing; \texttt{r8} \mapsto \varnothing}$}\\[0.5em]
      \mathstateln{2}\phantomsection\label{msln:2}\label{msln:3}\label{msln:4}
      \veriloginline`state = 32'd2; `\mathstate{$\interstate{1}{\asgnblockingbefore} = \at{\initialstate{\asgnblockingbefore}}{\texttt{state} \mapsto
          \texttt{32'd2}}, \interstate{1}{\asgnnonblockingbefore} = \initialstate{\asgnnonblockingbefore}$}\\[0.5em]
%      \mathstateln{3}\phantomsection\label{msln:3}\mathstate{$\interstate{1}{\asgnblockingbefore} = \at{\initialstate{\asgnblockingbefore}}{\texttt{state} \mapsto
%          \texttt{32'd2}}$}\\[0.5em]
%      \mathstateln{4}\phantomsection\label{msln:4}\mathstate{$\interstate{1}{\asgnnonblockingbefore} = \initialstate{\asgnnonblockingbefore}$}\\[0.5em]
      \mathstateln{3}\phantomsection\label{msln:5}\label{msln:6}\label{msln:7}
      \veriloginline`r2 = r4 * r6; `\mathstate{$\interstate{2}{\asgnblockingbefore} = \at{\interstate{1}{\asgnblockingbefore}}{\texttt{r2} \mapsto
          \at{\initialstate{\asgnblockingbefore}}{\texttt{r4}} \mathbin{\texttt{*}}
          \at{\initialstate{\asgnblockingbefore}}{\texttt{r6}}}, \interstate{2}{\asgnnonblockingbefore} = \initialstate{\asgnnonblockingbefore}$}\\[0.5em]
      % {\tiny 6 $\quad$}\mathstate{$\interstate{2}{\asgnblockingbefore} = \at{\interstate{1}{\asgnblockingbefore}}{\texttt{r2} \mapsto
      %     \at{\interstate{1}{\asgnblockingbefore}}{\texttt{r4}} \mathbin{\texttt{*}}
      %     \at{\interstate{1}{\asgnblockingbefore}}{\texttt{r6}}}$}\\[0.5em]
      % {\tiny 7 $\quad$}\mathstate{$\interstate{2}{\asgnnonblockingbefore} = \interstate{1}{\asgnnonblockingbefore}$}\\[0.5em]
%      \mathstateln{6}\phantomsection\label{msln:6}\mathstate{$\interstate{2}{\asgnblockingbefore} = \at{\interstate{1}{\asgnblockingbefore}}{\texttt{r2} \mapsto
%          \at{\initialstate{\asgnblockingbefore}}{\texttt{r4}} \mathbin{\texttt{*}}
%          \at{\initialstate{\asgnblockingbefore}}{\texttt{r6}}}$}\\[0.5em]
%      \mathstateln{7}\phantomsection\label{msln:7}\mathstate{$\interstate{2}{\asgnnonblockingbefore} = \initialstate{\asgnnonblockingbefore}$}\\[0.5em]
      \mathstateln{4}\phantomsection\label{msln:8}\label{msln:9}\label{msln:10}
      \veriloginline`r8 = r2 + r10; `\mathstate{$\interstate{3}{\asgnblockingbefore} = \at{\interstate{2}{\asgnblockingbefore}}{\texttt{r8} \mapsto
          (\at{\initialstate{\asgnblockingbefore}}{\texttt{r4}} \mathbin{\texttt{*}}
          \at{\initialstate{\asgnblockingbefore}}{\texttt{r6}}) \mathbin{\texttt{+}}
          \at{\initialstate{\asgnblockingbefore}}{\texttt{r10}}}, \interstate{3}{\asgnnonblockingbefore} = \initialstate{\asgnnonblockingbefore}$}\\[0.5em]
      % {\tiny 11 $\quad$}\mathstate{$\interstate{3}{\asgnblockingbefore} = \at{\interstate{2}{\asgnblockingbefore}}{\texttt{r8} \mapsto
      %     \at{\interstate{2}{\asgnblockingbefore}}{\texttt{r2}} \mathbin{\texttt{+}}
      %     \at{\interstate{2}{\asgnblockingbefore}}{\texttt{r10}}}$}\\[0.5em]
      % {\tiny 12 $\quad$}\mathstate{$\interstate{3}{\asgnnonblockingbefore} = \interstate{2}{\asgnnonblockingbefore}$}\\[0.5em]
%\mathstateln{9}\phantomsection\label{msln:9}\mathstate{$\interstate{3}{\asgnblockingbefore} = \at{\interstate{2}{\asgnblockingbefore}}{\texttt{r8} \mapsto
%          (\at{\initialstate{\asgnblockingbefore}}{\texttt{r4}} \mathbin{\texttt{*}}
%          \at{\initialstate{\asgnblockingbefore}}{\texttt{r6}}) \mathbin{\texttt{+}}
%          \at{\initialstate{\asgnblockingbefore}}{\texttt{r10}}}$}\\[0.5em]
%\mathstateln{10}\phantomsection\label{msln:10}\mathstate{$\interstate{3}{\asgnnonblockingbefore} = \initialstate{\asgnnonblockingbefore}$}\\[0.5em]
     \mathstateln{5}\phantomsection\label{msln:11}\label{msln:12}\label{msln:13}
     \veriloginline`state = r3 ? 32'd3 : state; `\mathstate{$\interstate{4}{\asgnblockingbefore} =
  \at{\interstate{3}{\asgnblockingbefore}}{\texttt{state} \mapsto
    \mternary{\at{\initialstate{\asgnblockingbefore}}{\texttt{r3}}}{\texttt{32'd3}}{\texttt{32'd2}}},
  \interstate{4}{\asgnnonblockingbefore} =
  \initialstate{\asgnnonblockingbefore}$}
% {\tiny 16 $\quad$}\mathstate{$\interstate{4}{\asgnblockingbefore} =
%   \at{\interstate{3}{\asgnblockingbefore}}{\texttt{state} \mapsto
%     \mternary{\at{\interstate{3}{\asgnblockingbefore}}{\texttt{r3}}}{\texttt{32'd3}}{\at{\interstate{3}{\asgnblockingbefore}}{\texttt{state}}}}$}\\[0.5em]
% {\tiny 17 $\quad$}\mathstate{$\interstate{4}{\asgnnonblockingbefore} = \interstate{3}{\asgnnonblockingbefore}$}\\[0.5em]
% \mathstateln{12}\phantomsection\label{msln:12}\mathstate{$\interstate{4}{\asgnblockingbefore} =
%   \at{\interstate{3}{\asgnblockingbefore}}{\texttt{state} \mapsto
%     \mternary{\at{\initialstate{\asgnblockingbefore}}{\texttt{r3}}}{\texttt{32'd3}}{\texttt{32'd2}}}$}\\[0.5em]
% \mathstateln{13}\phantomsection\label{msln:13}\mathstate{$\interstate{4}{\asgnnonblockingbefore} =
%   \initialstate{\asgnnonblockingbefore}$}
};

    \node[bb, below=1.5cm of
    initial block] (transf block)
    {\veriloginline`state <= 32'd2; `
\mathstate{$\interstate{1}{\asgnblockingafter} =
        \initialstate{\asgnblockingafter}, \interstate{1}{\asgnnonblockingafter} = \at{\initialstate{\asgnnonblockingafter}}{\texttt{state} \mapsto \texttt{32'd2}}$}\\[0.5em]
     % \mathstate{$\interstate{1}{\asgnblockingafter} = \initialstate{\asgnblockingafter}$}\\[0.5em]
     % \mathstate{$\interstate{1}{\asgnnonblockingafter} = \at{\initialstate{\asgnnonblockingafter}}{\texttt{state} \mapsto \texttt{32'd2}}$}\\[0.5em]
      \veriloginline`r2 <= r4 * r6; `
      \mathstate{$\interstate{2}{\asgnblockingafter} =
        \initialstate{\asgnblockingafter}, \interstate{2}{\asgnnonblockingafter} = \at{\interstate{1}{\asgnnonblockingafter}}{\texttt{r2} \mapsto \at{\initialstate{\asgnblockingafter}}{\texttt{r4}} \mathbin{\texttt{*}} \at{\initialstate{\asgnblockingafter}}{\texttt{r6}}}$}\\[0.5em]
     % \mathstate{$\interstate{2}{\asgnblockingafter} = \interstate{1}{\asgnblockingafter}$}\\[0.5em]
     % \mathstate{$\interstate{2}{\asgnnonblockingafter} = \at{\interstate{1}{\asgnnonblockingafter}}{\texttt{r2} \mapsto \at{\interstate{1}{\asgnblockingafter}}{\texttt{r4}} \mathbin{\texttt{*}} \at{\interstate{1}{\asgnblockingafter}}{\texttt{r6}}}$}\\[0.5em]
     % \mathstate{$\interstate{2}{\asgnblockingafter} = \initialstate{\asgnblockingafter}$}\\[0.5em]
     % \mathstate{$\interstate{2}{\asgnnonblockingafter} = \at{\interstate{1}{\asgnnonblockingafter}}{\texttt{r2} \mapsto \at{\initialstate{\asgnblockingafter}}{\texttt{r4}} \mathbin{\texttt{*}} \at{\initialstate{\asgnblockingafter}}{\texttt{r6}}}$}\\[0.5em]
     \veriloginline`r8 <= {r4 * r6} + r10; `
     \mathstate{$\interstate{3}{\asgnblockingafter} =
       \initialstate{\asgnblockingafter}, \interstate{3}{\asgnnonblockingafter}
       = \at{\interstate{2}{\asgnnonblockingafter}}{\texttt{r8} \mapsto
         (\at{\initialstate{\asgnblockingafter}}{\texttt{r4}}
         \mathbin{\texttt{*}}
         \at{\initialstate{\asgnblockingafter}}{\texttt{r6}})
         \mathbin{\texttt{+}}
         \at{\initialstate{\asgnblockingafter}}{\texttt{r10}}}$}\\[0.5em]
     % \mathstate{$\interstate{3}{\asgnblockingafter} = \interstate{2}{\asgnblockingafter}$}\\[0.5em]
     % \mathstate{$\interstate{3}{\asgnnonblockingafter} = \at{\interstate{2}{\asgnnonblockingafter}}{\texttt{r8} \mapsto
     %      (\at{\interstate{2}{\asgnblockingafter}}{\texttt{r4}} \mathbin{\texttt{*}}
     %      \at{\interstate{2}{\asgnblockingafter}}{\texttt{r6}}) \mathbin{\texttt{+}}
     %      \at{\interstate{2}{\asgnblockingafter}}{\texttt{r10}}}$}\\[0.5em]
     % \mathstate{$\interstate{3}{\asgnblockingafter} = \initialstate{\asgnblockingafter}$}\\[0.5em]
     % \mathstate{$\interstate{3}{\asgnnonblockingafter} = \at{\interstate{2}{\asgnnonblockingafter}}{\texttt{r8} \mapsto
     %      (\at{\initialstate{\asgnblockingafter}}{\texttt{r4}} \mathbin{\texttt{*}}
     %      \at{\initialstate{\asgnblockingafter}}{\texttt{r6}}) \mathbin{\texttt{+}}
     %      \at{\initialstate{\asgnblockingafter}}{\texttt{r10}}}$}\\[0.5em]
     \veriloginline`state <= r3 ? 32'd3 : 32'd2; `\mathstate{$\interstate{4}{\asgnblockingafter} = \initialstate{\asgnblockingafter},\interstate{4}{\asgnnonblockingafter} =
  \at{\interstate{3}{\asgnnonblockingafter}}{\texttt{state} \mapsto
    \mternary{\at{\initialstate{\asgnblockingafter}}{\texttt{r3}}}{\texttt{32'd3}}{\texttt{32'd2}}}$}
%      \mathstate{$\interstate{4}{\asgnblockingafter} = \interstate{3}{\asgnblockingafter}$}\\[0.5em]
% \mathstate{$\interstate{4}{\asgnnonblockingafter} =
%   \at{\interstate{3}{\asgnnonblockingafter}}{\texttt{state} \mapsto
%     \mternary{\at{\interstate{3}{\asgnblockingafter}}{\texttt{r3}}}{\texttt{32'd3}}{\texttt{32'd2}}}$}\\[0.5em]
%      \mathstate{$\interstate{4}{\asgnblockingafter} = \initialstate{\asgnblockingafter}$}\\[0.5em]
% \mathstate{$\interstate{4}{\asgnnonblockingafter} =
%   \at{\interstate{3}{\asgnnonblockingafter}}{\texttt{state} \mapsto
%     \mternary{\at{\initialstate{\asgnblockingafter}}{\texttt{r3}}}{\texttt{32'd3}}{\texttt{32'd2}}}$}
};
    % \node[label] at (initial
    % block.north west) {\texttt{1}};
    % \node[label] at (transf block.north west)
    % {\texttt{1}};
    %\draw[->,very thick] ($(initial block.east) + (0.5,0)$) --
    %node [below, font=\footnotesize] {forward substitution}
    %($(initial block.east) + (4,0)$);
   \path (initial block) -- node[pass] (fs) {forward substitution} (transf block);
   \draw[ed] (initial block) -- (fs) -- (transf block);
  \end{tikzpicture}
  \caption[Simple forward substitution transformation with the runtime
  association maps.]{Simple forward substitution transformation together with
    the runtime value of the blocking assignment association map
    ($\Gamma_{\mathrm r}$) and the nonblocking assignment association map
    ($\Delta_{\mathrm r}$).}%
  \label{fig:hg:forward-substitution}
\end{figure}

\newcommand\substmap{t}
\newcommand\substexpr[2]{\texttt{subst\_expr}\ #1\ #2}
\newcommand\subststmnt[2]{\texttt{subst\_stmnt}\ #1\ #2}
\newcommand\invinsubstmap[4]{\texttt{in\_subst\_map}\ #1\ #2\ #3\ #4}
\newcommand\invnotinsubstmap[3]{\texttt{not\_in\_subst\_map}\ #1\ #2\ #3}

The transformation is performed by traversing each block and storing for each
register the expression that is being assigned to it.  If the same register is
encountered multiple times, the expression being assigned is always substituted
first and then replaces the current mapping from register to expression.  As the
BRAM insertion already removed all the load and store operations in the data
path, only regular register assignments need to be accounted for.

First, I describe the substitution of expressions in
\cref{def:substitute-expressions}, and then I describe the substitution of
statements in \cref{def:substitute-statements}.

\begin{definition}[Substitute expressions]%
  \label{def:substitute-expressions}

  Expressions are substituted based on a map $\substmap$ from registers to
  expressions.  Each register within the expression is replaced by the
  expression in the map.

  \begin{mathpar}
    \inferrule[SubstReg]{\at{t}{r} = \some{e'}}{\substexpr{t}{r} = e'}
    \and
    \inferrule[SubstRegNotIn]{\at{t}{r} = \cnone}{\substexpr{t}{r} = r}
    \and
    \inferrule[SubstBinaryOp]{ }{\substexpr{t}{(e_1
        \mathbin{\texttt{+}} e_2)} = \substexpr{t}{e_1} \mathbin{\texttt{+}}
      \substexpr{t}{e_2}}
    \and \cdots
  \end{mathpar}
\end{definition}

\begin{definition}[Substitute statements]%
  \label{def:substitute-statements}

  Only two types of statements need to be substituted, blocking assignments and
  sequential composition of statements.  For blocking assignments, substitution
  means that it is turned into nonblocking assignment by substituting the
  expression with the current expression substitution map $\substmap$, in
  addition to updating the map itself with a new expression mapping for the
  register $r$.

  \begin{mathpar}
    \inferrule[SubstBlocking]{e' = \substexpr{\substmap}{e}}{\subststmnt{\substmap}{(r
        \blockasgn e)} = \some{r \nonblockasgn e', \at{\substmap}{r \mapsto e'}}}
    \and
    \inferrule[SubstSeq]{\subststmnt{\substmap}{s_1} = \some{s_1', \substmap'}
      \\ \subststmnt{\substmap'}{s_2} = \some{s_2',
        \substmap''}}{\subststmnt{\substmap}{(s_1\msemi s_2)} = \some{s_1'\msemi s_2', \substmap''}}
    \and
    \inferrule[SubstOther]{(\forall s_1\ s_2\ldotp s \neq s_1\msemi s_2) \\
      (\forall r\ e\ldotp s \neq r \blockasgn e)}{\subststmnt{\substmap}{s} = \cnone}
  \end{mathpar}
\end{definition}

It might seem restrictive to only support sequential composition of statements
and blocking assignment, but those are the only constructs that are generated by
the \htl{} generation.  Supporting additional statements would not require a
significant change to the transformation.  Even adding support for nonblocking
assignment in the input statement should be safe, as long as the expressions are
correctly substituted, and are \emph{not} added to the expression substitution
map $\substmap$.  The main property one would have to check about the input
statement is that a register is not assigned using blocking assignment after a
nonblocking assignment, as otherwise the input and output statement would have
different results.

\subsection{Forward substitution correctness proof}%
\label{sec:hg:forward-substitution-correctness-proof}

The main sub-proof that is needed to prove the forward simulation correct is
that the execution of statements in each state results in equivalent
\emph{merged} association maps from the blocking and nonblocking assignments.
In \cref{fig:hg:forward-substitution} this corresponds to showing that
$\interstate{4}{\asgnblockingbefore} \verilogmerge \Delta^{0}_{\mathrm r} =
{\Gamma'}^{0}_{\mathrm r} \verilogmerge {\Delta'}^4_{\mathrm r}$.
\YH{TODO: Technically it shouldn't be equality but equivalence.}

\begin{lemma}[Equivalence of statement substitution]%
  \label{lem:hg:equivalence-of-statement-subst}

  After merging the maps of a statement $s$ and the substituted statements $s'$,
  the contents of the merged blocking association map $\Gamma'$ and nonblocking
  association map $\Delta'$ after executing $s$ should be equivalent to merging
  the maps $\Gamma''$ and $\Delta''$ after executing $s'$.

  {\normalfont
    \begin{equation*}
      \begin{aligned}
        &\subststmnt{\substmap}{s} = \some{s', \substmap'}\implies\\
        &((\Gamma, \Delta), s) \downarrow_{\text{stmnt}} (\Gamma', \Delta')
          \implies\\
        &((\Gamma, \Delta), s') \downarrow_{\text{stmnt}} (\Gamma'', \Delta'')
          \implies\\
        &\Gamma' \verilogmerge \Delta' = \Gamma'' \verilogmerge \Delta''
      \end{aligned}
    \end{equation*}}

  \begin{proof}[Proof sketch]
    By induction on the definition of a statement, by applying
    \cref{lem:hg:forward-substitution-of-expressions} to show equivalence of
    expressions within the statements.
  \end{proof}
\end{lemma}

There are two main invariants that have to be maintained when proving the lemma
above, which relates the run time association map used to execute expressions
encountered during the execution of the original statement with the contents of
the expression substitution map $\substmap$.  The first invariant, described in
\cref{def:in-substitution-map}, states that the evaluation of an expression in
the substitution map $\substmap$ should correspond to the value associated with
that register in the current execution of the block.  For example, in
\cref{fig:hg:forward-substitution} after line~\nameref{msln:5}, the substitution
map would contain the entry:
$\at{\substmap}{\texttt{r2} \mapsto \at{\substmap}{\texttt{r4}}
  \mathbin{\texttt{*}} \at{\substmap}{\texttt{r6}}}$.  Executing this expression
$\at{\substmap}{\texttt{r4}} \mathbin{\texttt{*}} \at{\substmap}{\texttt{r6}}$
with the initial blocking assignment at the start of the statement
$\initialstate{\asgnblockingbefore}$ should be the same as indexing the current
run time association map at the register \texttt{r2},
i.e. $\at{\interstate{2}{\asgnblockingbefore}}{\texttt{r2}}$, which is the case.

\begin{definition}[In substitution map]%
  \label{def:in-substitution-map}

  If the register $r$ is in the substitution map $\substmap$ and it maps to
  expression $e$, then the value in the current association map
  $\Gamma_{\mathrm r}$ should be the same as the value obtained from evaluating
  expression $e$ with the initial association map
  $\Gamma^{0}_{\mathrm r}$.

  \begin{equation*}
    \begin{aligned}
      &\invinsubstmap{\Gamma^{0}_{\mathrm r}}{\Gamma_{\mathrm r}}{\Gamma_{\mathrm a}}{\substmap} \defeq \\
      &\qquad \forall r\ e\ldotp \at{t}{r} = \some{e} \implies \exists v\ldotp
      ((\Gamma^{0}_{\mathrm r}, \Gamma_{\mathrm a}), e) \downarrow_{\text{expr}} v
      \land \at{\Gamma_{\mathrm r}}{r} = \some{v}
    \end{aligned}
  \end{equation*}
\end{definition}

\Cref{def:hg:not-in-substitution-map} describes what the relationship is between
the initial map and the current run time map is when a register is not in the
expression substitution map.  For example, this would be the case for register
\texttt{r2} before line~\nameref{msln:5}.  In that case, \texttt{r2} is not in
$\substmap$ and so
$\at{\interstate{1}{\asgnblockingbefore}}{\texttt{r2}} =
\at{\initialstate{\asgnblockingbefore}}{\texttt{r2}}$.

\begin{definition}[Not in substitution map]%
  \label{def:hg:not-in-substitution-map}

  If the register $r$ is not in the substitution map $\substmap$, then the value
  for that register in the blocking assignment association map
  $\Gamma_{\mathrm r}$ should be the same as the value of the register in the
  initial association map $\Gamma^{0}_{\mathrm r}$ at the start of the execution
  of the statement.

  \begin{equation*}
    \begin{aligned}
      &\invnotinsubstmap{\Gamma^{0}_{\mathrm r}}{\Gamma_{\mathrm r}}{\substmap}
      \defeq \\
      &\qquad\forall r\ldotp \at{t}{r} = \cnone \implies \at{\Gamma^{0}_{\mathrm r}}{r} =
      \at{\Gamma_{\mathrm r}}{r}
    \end{aligned}
  \end{equation*}
\end{definition}

The main lemma that needs to be proven for the forward substitution is the
following.

\begin{lemma}[Forward substitution of expressions]%
  \label{lem:hg:forward-substitution-of-expressions}

  Given a map from registers to expression $t$, an expression before forward
  substitution $e$, and the result of forward substituting $e$ with map $t$
  resulting in expression $e'$, then executing $e$ with the dynamically updated
  association map $\Gamma_{\mathrm r}$ should be equivalent to executing $e'$
  with the initial state of all the registers $\Gamma^{0}_{\mathrm r}$.

  {\normalfont
    \begin{equation*}
      \begin{aligned}
        &\substexpr{\substmap}{e} = \some{e'} \implies \\
        &\invinsubstmap{\Gamma^{0}_{\mathrm r}}{\Gamma_{\mathrm r}}{\Gamma_{\mathrm a}}{\substmap} \implies\\
        &\invnotinsubstmap{\Gamma^{0}_{\mathrm r}}{\Gamma_{\mathrm r}}{\substmap} \implies\\
        &\forall v\ldotp ((\Gamma_{\mathrm r}, \Gamma_{\mathrm a}), e) \downarrow_{\text{expr}} v
          \implies ((\Gamma^{0}_{\mathrm r}, \Gamma_{\mathrm a}), e') \downarrow_{\text{expr}} v
      \end{aligned}
    \end{equation*}}

  \begin{proof}[Proof sketch]
    Using the invariants, one can show that the modified expression $e'$ will
    behave like the original expression $e$ at the current context
    $(\Gamma_{\mathrm r}, \Gamma_{\mathrm a})$, when it is evaluated in the
    initial context at the start of the state, i.e.
    $(\Gamma^{0}_{\mathrm r}, \Gamma_{\mathrm a})$.
  \end{proof}
\end{lemma}

% \YH{TODO: Talk a bit more about the top-level proof.  Especially the
%   invariants.}

\section{Verilog Generation}%
\label{sec:hg:verilog-generation}

Finally, Verilog generation produces proper Verilog from \htl{}.  The main two
transformations that take place are:

\begin{enumerate}
\item Converting the mapping from states to Verilog statements into a case
  statement, with reset logic to reset the state.
\item Instantiating the \gls{BRAM} specification as a Verilog always block.
\end{enumerate}

\begin{figure}
  \centering
  \begin{subfigure}{0.45\linewidth}
\begin{minted}[fontsize=\footnotesize]{systemverilog}
main() {
  8: {
    state <= 32'd18;
    u_en <= ( ~ u_en);
    wr_en <= 32'd0;
    addr <= {{{reg_6 + 32'd0}
              + {reg_2 * 32'd4}}
             / 32'd4};
  }
}
\end{minted}
    \caption{Original \htl{} representation of the design, with the starting
      state set to 8.}%
    \label{fig:hg:ram-instantiation:htl}
  \end{subfigure}\hfill%
  \begin{subfigure}{0.5\linewidth}
\begin{minted}[fontsize=\footnotesize]{systemverilog}
module main(reset, clk, finish, return_val);
  // Register declarations
  // ...

  // BRAM interface
  (* ram_style = "block" *)
  logic [31:0] stack [1:0];
  always @(negedge clk)
    if ({u_en != en}) begin
      if (wr_en) stack[addr] <= d_in;
      else d_out <= stack[addr];
      en <= u_en;
    end

  // Finite-state machine with data path
  always @(posedge clk)
    if ({reset == 32'd1}) state <= 32'd8;
    else
      case (state)
        32'd8: begin
          state <= 32'd18;
          u_en <= ( ~ u_en);
          wr_en <= 32'd0;
          addr <= {{{reg_6 + 32'd0}
                    + {reg_2 * 32'd4}}
                   / 32'd4};
        end
        default:;
      endcase
endmodule
\end{minted}
    \caption{Translated Verilog design with the explicit reset and the explicit
      memory instantiation.}%
    \label{fig:hg:ram-instantiation:verilog}
  \end{subfigure}
  \caption[Instantiation of BRAM specification with Verilog
  implementation.]{Instantiation of \gls{BRAM} specification with Verilog
    implementation.}%
  \label{fig:hg:ram-instantiation}
\end{figure}

This translation is shown in \cref{fig:hg:ram-instantiation}, where the \htl{}
design shown in \cref{fig:hg:ram-instantiation:htl} is translated to the Verilog
shown in \cref{fig:hg:ram-instantiation:verilog}.  The main state machine is
then turned into a single always block with an explicit \texttt{reset}, setting
the state to the starting state present in \htl{}.  If the \texttt{reset} is not
set, then there is a case statement that implements the state machine from
\htl{}, where each state has the same Verilog statements as the corresponding
state in \htl{}.

In addition to that, the implicit memory that is part of the \htl{} semantics is
made explicit by a separate always block implementing the memory interface.
This interface follows a standard \gls{BRAM} memory template, and can therefore
be detected by the synthesis tool and will become a proper memory in the
synthesised netlist.

\subsection{Forward simulation from \htl{} to Verilog}%
\label{sec:proof:htl_verilog}

The \htl{}-to-Verilog simulation is conceptually simple, as the only
transformation is from the map representation of the code to the case-statement
representation.  The proof is more involved, as the semantics of a map structure
is quite different to that of the case-statement to which it is converted.

%\YH{Maybe want to split this up into two lemmas?  One which states the proof about the map property of uniqueness of keys, and another proving the final theorem?}
\begin{lemma}[Forward simulation from \htl{} to Verilog]\label{lemma:verilog}
  In the following, I write $\yhfunction{tr\_verilog}$ for the translation from
  \htl{} to Verilog. (Note that this translation cannot fail, so we do not need
  the \yhconstant{OK} constructor here.)  {\normalfont
    \begin{align*}
      \forall h\ V\ B \in \texttt{Safe}\ldotp\quad \yhfunction{tr\_verilog} (h) = V \land h \Downarrow B \implies V \Downarrow B.
    \end{align*}}
\end{lemma}

\begin{proof}[Proof sketch]
  The translation from maps to case-statements is done by turning each node of
  the tree into a case-expression containing the same statements.  The main
  difficulty is that a random-access structure is being transformed into an
  inductive structure where a certain number of constructors need to be called
  to get to the correct case.
  % \JW{I would chop from here.}\YH{Looks good to me.}  The proof of the
  % translation from maps to case-statements follows by induction over the list
  % of elements in the map and the fact that each key will be unique.  In
  % addition to that, the statement that is currently being evaluated is
  % guaranteed by the correctness of the list of elements to be in that list.
  % The latter fact therefore eliminates the base case, as an empty list does
  % not contain the element we know is in the list.  The other two cases follow
  % by the fact that either the key is equal to the evaluated value of the case
  % expression, or it isn't.  In the first case we can then evaluate the
  % statement and get the state after the case expression, as the uniqueness of
  % the key tells us that the key cannot show up in the list anymore.  In the
  % other case we can just apply the inductive hypothesis and remove the current
  % case from the case statement, as it did not match.
\end{proof}

\section{Summary}

This chapter described the translation from the scheduled code, which was still
represented in a language with software semantics, down to Verilog with faithful
hardware semantics.  On the way, some hardware specific optimisations and
transformations had to be performed.  First, any direct assignments to the
memory had to be translated to a more standard interaction with memory, using
read and write ports of a \gls{BRAM}.  Next, any blocking assignment had to be
turned into nonblocking assignment to parallelise the Verilog design, helping
improve synthesis results in some cases.  Finally, proper Verilog had to be
generated, implementing the implicit behaviour present in \htl{} as explicit
behaviour in Verilog.

%\subsection{Coq Mechanisation}

%\JW{Would be nice to include a few high-level metrics here. How many person-years of effort was the proof (very roughly)? How many lines of Coq? How many files, how many lemmas? How long does it take for the Coq proof to execute?}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% TeX-engine: luatex
%%% End:
