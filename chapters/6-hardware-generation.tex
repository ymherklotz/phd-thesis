\chapter{Hardware Generation}

Until now the representation of the program has still been in the form of a
software program, with virtual, infinite registers, a program counter, as well
as a rich but abstract memory model.  This representation needs to be
transformed into a more suitable representation on which hardware specific
transformations can applied, and which is closer to th structure of the final
Verilog design.  The main transformation that takes place is converting a
control-flow graph into a state machine with data-path representation, which is
also the structure of the final design.  There are various steps involved in
this refinement because of the large gap between control-flow graph semantics
and state machine semantics.  In addition to that, additional components, such
as an implementation of a memory that can be efficiently implemented in
hardware, need to be added to the hardware to produce a useful design.  Finally,
until now programs have only been executing sequentially, whereas to produce the
final hardware one will have to transform the sequential execution of operations
within each state into parallel assignments.

This chapter describes the hardware generation process, starting from \rtlpar{}
and producing a final Verilog design.
\Cref{fig:hg:vericert-hardware-generation} shows the intermediate
transformations and in which section the transformation is described.
\Cref{sec:hg:hyperblock-destruction} describes the first step in the
transformation which separates each sequential block within a state in \rtlpar{}
into separate states that can be addressed using the program counter.  This
matches addressing that the state register would have to do in the hardware
design. Next, \cref{sec:hg:htl-generation} describes the generation of \htl{},
an intermediate language representing the execution of a state machine.  This
performs the main transformation from a software representation of the program
into hardware, making the execution of the program more explicit in the design
itself instead of as part of the semantics.  \Cref{sec:hg:bram-insertion} then
describes the first hardware-specific optimisation on the state-machine
representation of the hardware by adding a specification of a \gls{BRAM} to the
\htl{} semantics and replacing any explicit reads and writes to the array
representing memory by properly formed reads and writes to the \gls{BRAM}.
Until now, updates to registers have been specified sequentially, so a forward
substitution transformation is describe in
\cref{sec:hg:register-forward-substitution} to parallelise the updates to
registers.  Finally, \cref{sec:hg:verilog-generation} describes the generation
of the final Verilog design, which implements the state-machine that was
specified by \htl{}, in particular implementing the \gls{BRAM} that was
specified.

\definecolor{bgbox1}{HTML}{b3e2cd}
\definecolor{bgbox2}{HTML}{cbd5e8}
\definecolor{bgbox3}{HTML}{fdcdac}
\definecolor{ircolor}{HTML}{e78ac3}

\tikzset{
  numlabel/.style={draw,circle,inner sep=0.5mm,fill=white},
  ir/.style={draw,very thick,black, fill=ircolor!70, align=center},
  pass/.style={draw, very thick, rounded corners, fill=white, align=center},
  extpass/.style={draw, dotted, very thick, rounded corners, fill=white, align=center},
  bgbox/.style={draw=none},
  ed/.style={->, very thick, >=stealth, shorten >=1pt},
blacknumlarge/.style={
  circle, draw=none,
  fill=black, inner sep=1pt,
  outer sep=0pt, minimum size=0.9em, text=white, font=\scriptsize\sffamily\bfseries}
}

\begin{figure*}

\begin{center}
\begin{tikzpicture}[
yscale=-1
]

\node[ir] (rtlpar) {\rtlpar{}};
\node[pass,right=of rtlpar] (hyperblock destruction) {Hyperblock \\
  Destruction};
\node[ir,right=of hyperblock destruction] (rtlsubpar) {\rtlsubpar{}};
\node[pass,right=of rtlsubpar] (htl generation) {\htl{} \\
  Generation};
\node[ir,right=of htl generation] (htl) {\htl{}};
\node[pass,below=of htl] (bram insertion) {BRAM \\ insertion};
\node[ir,below=of bram insertion] (htlmem) {\htl{}};
\node[pass,left=of htlmem] (forward substitution) {Forward \\ Substitution};
\node[ir,left=of forward substitution] (htlsubst) {\htl{}};
\node[pass,left=of htlsubst] (verilog generation) {Verilog \\ Generation};
\node[ir,left=of verilog generation] (verilog) {Verilog};

\draw[ed] (rtlpar) -- (hyperblock destruction);
\draw[ed] (hyperblock destruction) -- (rtlsubpar);
\draw[ed] (rtlsubpar) -- (htl generation);
\draw[ed] (htl generation) -- (htl);
\draw[ed] (htl) -- (bram insertion);
\draw[ed] (bram insertion) -- (htlmem);
\draw[ed] (htlmem) -- (forward substitution);
\draw[ed] (forward substitution) -- (htlsubst);
\draw[ed] (htlsubst) -- (verilog generation);
\draw[ed] (verilog generation) -- (verilog);

\node[blacknumlarge] at (hyperblock destruction.north west)
{\ref*{sec:hg:hyperblock-destruction}};
\node[blacknumlarge] at (htl generation.north west)
{\ref*{sec:hg:htl-generation}};
\node[blacknumlarge] at (bram insertion.north west)
{\ref*{sec:hg:bram-insertion}};
\node[blacknumlarge] at (forward substitution.north west)
{\ref*{sec:hg:register-forward-substitution}};
\node[blacknumlarge] at (verilog generation.north west)
{\ref*{sec:hg:verilog-generation}};

\begin{pgfonlayer}{background}
\node[bgbox, fill=bgbox2, fit=(rtlpar)(htlmem), inner sep=1cm] {};
\end{pgfonlayer}

 \end{tikzpicture}
\end{center}
  \caption{Hardware generation transformation passes introduced to convert
    \rtlpar{} to Verilog.}%
  \label{fig:hg:vericert-hardware-generation}
\end{figure*}

\section{Hyperblock Destruction}%
\label{sec:hg:hyperblock-destruction}

\rtlpar{} is a control-flow graph with nodes mapping to hyperblocks.  This is
useful for the scheduling proof, as each of these hyperblocks can be compared
individually.  However, in the hardware itself, the individual sequential blocks
have to be separated into different states, as within each state the
assignments will be performed in parallel.  This first hyperblock destruction
transformation separates operations that should execute in different clock
cycles into their own locations in the control-flow graph.

\begin{figure}
  \centering
  \begin{tikzpicture}[>=Latex,shorten >=1pt,
    label/.style={circle,draw,fill=white,inner sep=0.4mm,font=\footnotesize},
    bb/.style={align=left, draw=white, fill=black!5}]
    \node[bb] (initial block) {\rtlinline`[ [ [ r1 := r2 ] ]; `\\
                               \rtlinline`  [ [ r3 := r4 ];   `\\
                               \rtlinline`    [ goto 2   ] ] ]`};
    \node[bb, right=5cm of initial block, yshift=1cm] (first transf block)
      {\rtlinline`[ [ r1 := r2 ]; `\\
       \rtlinline`  [ goto 3   ] ]`};
    \node[bb, below=of first transf block] (second transf block)
                               {\rtlinline`  [ [ r3 := r4 ];   `\\
                                \rtlinline`    [ goto 2   ] ]`};
    \node[label] at (initial block.north west) {\texttt{1}};
    \node[label] at (first transf block.north west) {\texttt{1}};
    \node[label] at (second transf block.north west) {\texttt{3}};
    \draw[->,very thick] ($(initial block.east) + (0.5,0)$)
      -- node [below, font=\footnotesize] {hyperblock destruction}
      ($(initial block.east) + (4,0)$);
  \end{tikzpicture}
  \caption{Hyperblock destruction transformation splitting up the hyperblock into
    multiple locations.}%
  \label{fig:hg:hyperblock-destruction}
\end{figure}

\Cref{fig:hg:hyperblock-destruction} shows an example hyperblock destruction
transformation, where a new block is added at location \diaglabel{$3$} with the
contents of the second sequential block, and a \rtlinline`goto` instruction is
added to the original block leading to this next block.  Fresh locations for new
blocks are chosen by keeping track of the greatest location in the current
function.

\subsection{Proof of Hyperblock Destruction}

The proof of hyperblock destruction is relatively simple using a
\gls{plus-forward-simulation}.  Then, for each input states, there are one or
more output states returned by the hyperblock destruction algorithm that should
be equivalent to the execution of the input state when executed sequentially.

\section{\htl{} Generation}%
\label{sec:hg:htl-generation}

The most important transformation of an HLS tool is then the generation of a
hardware description from the list of instructions.  Eventually the hardware
design will be described using Verilog, however, to make the transformation more
incremental, we first turn the program represented by a control-flow graph into
a program represented by an \gls{FSMD}.  This section will describe the
\gls{FSMD} language, called \htl{}, by showing the syntax and semantics of the
language.  Then, the transformation from \rtlsubpar{} to \htl{} is shown, together
with an overview of its correctness proof.

\subsection{\htl{} Structure and Semantics}%
\label{sec:hg:htl-structure-and-semantics}

At a high level, \htl{} is structured like many other \compcert{} languages,
mapping from locations to Verilog statements.  However, contrary to many other
intermediate languages in \compcert{}, \htl{} does not contain any instructions,
and its semantics use a smaller state to perform the execution.  For example,
the state does not have to contain the program counter because there is an
explicit state register that keeps track of it.

\htl{} comprises a a lot of metadata pointing to important registers, as well as
containing a map from program locations to Verilog statements.  The syntax of
\htl{} is shown in \cref{fig:hg:htl-syntax}.  First, \htl{} contains a list of
parameters, which are additional input registers to the current module.  Next,
the $\mathit{datapath}$ contains the code for the state machine, as well as the
main data path associated with the data path.  This is done by mapping program
locations, or states, to Verilog statements $\verilogstmnttype$, which updates
registers as part of the data path, but also updates to the state.  The
computation of the next state often relies on the state of registers, which is
why it needs to be performed as part of the data path.

\begin{figure}
\centering
\begin{tabular}{rr@{~}r@{~}l@{\hspace*{2mm}}l}
  \llabel{registers} & $\reg, \mbox{\rtlinline{r1}}, \mbox{\rtlinline{r2}}, \ldots \in \regtype$ & & & \\
  \llabel{CFG node labels} & $\location \in \locationtype$ & $::=$ & $\mathbb{N}$ & \\
  \llabel{Verilog statements} & $\verilogstmnt \in \verilogstmnttype$ & & & \\
  \llabel{Code} & $\htlcode \in \locationtype \rightarrow \verilogstmnttype$ & & & \\
  \llabel{\htl{}} & $\htlmodule$ & $::=$ & $\{\ \mathrm{params} : \regtype\
                                        \texttt{list}; $ \\
  & & & $\ \ \mathrm{datapath} : \locationtype \rightarrow \verilogstmnttype; $ \\
  & & & $\ \ \mathrm{entrypoint} : \mathbb{N};$ & \\
  & & & $\ \ \mathrm{state} : \regtype;$ & \\
  & & & $\ \ \mathrm{stack} : \regtype;$ & \\
  & & & $\ \ \mathrm{stack\_size} : \mathbb{N};$ & \\
  & & & $\ \ \mathrm{finish},\mathrm{return},\mathrm{start},\mathrm{reset},\mathrm{clk} :
        \regtype;$ & \\
  & & & $\ \ \mathrm{ram} : \optiontype{\mathrm{RAM}};$ & \\
  & & & $\ \ \mathrm{order\_wf} : \mathrm{state} < \mathrm{finish} < \mathrm{return}
        < \mathrm{stack} < \mathrm{reset} < \mathrm{clk};$ & \\
  & & & $\ \ \mathrm{ram\_wf} : \forall r\ldotp \mathrm{ram} = \some{r} \implies
        \mathrm{clk} < r.\mathrm{raddr}; $ & \\
  & & & $\ \ \mathrm{params\_wf} : \forall r \in \mathrm{params}\ldotp
        r < \mathrm{state} \ \}$
\end{tabular}
\caption{Syntax of \htl{}.}
\label{fig:hg:htl-syntax}
\end{figure}

Next, the \htl{} module contains an entry point, which is the initial starting
state of the $\mathit{state}$ register.  After the reset input wire is asserted,
the $\mathit{state}$ register will be reset to that value.  The $\mathit{state}$
register is read at every clock tick and determines the next statement that
should be executed from the $\mathit{datapath}$.  As mentioned before, the
$\mathit{state}$ register is a physical representation of the virtual program
counter from \rtlsubpar{} and other intermediate instruction languages.  We then
also have $\mathit{stack}$ register and an associated $\mathit{stack\_size}$,
which is a Verilog array storing the contents of the stack.  Initially, this
array will be accessed directly by operations in the data path, however,
\cref{sec:hg:bram-insertion} describes how these direct accesses to the array
are instead turned into accesses to a \gls{BRAM}.

We then have a list of input and output control signals, which are used to
return a result by setting the $\mathit{finish}$ flag and assigning the
$\mathit{return}$ register to the result.  Next, there are $\mathit{start}$ and
$\mathit{reset}$ signals that provide a way to reset the state of the internal
state machine, as well as start the execution of the module when ready.
Finally, there is a $\mathit{clk}$ input to provide the clock to the design.
This input is not yet used by the \htl{} design, as execution is still performed
using state transitions in the semantics, however, a register is already
allocated for the clock which will be needed by the final Verilog design.  The
module then also contains a $\mathit{ram}$ which will be further described in
\cref{sec:hg:bram-insertion}, because in the first translation pass it is
initialised to \cnone.

Finally, there are three \emph{well-formedness} criteria which are used to
enforce an ordering between the registers, mainly to be able to show that
registers are independent from each other.

\subsection{\htl{} Generation Algorithm}%
\label{sec:hg:htl-generation-algorithm}

The generation of \htl{} is relatively straight-forward, as most instructions
have a direct translation to a Verilog implementation.  The Verilog
implementation can therefore follow the semantics of each operation and
implement their arithmetic behaviour directly.  In addition to that, because of
the hyperblock destruction translation, each block in the control-flow graph
corresponds to a state in the final hardware.  First, the translation of
individual instructions is described in
\cref{sec:hg:translating-individual-arithmetic-instructions}, next the
translation of control-flow statements is described in
\cref{sec:hg:translating-control-flow-instructions}.

\subsubsection{Translating Individual Arithmetic Instructions}%
\label{sec:hg:translating-individual-arithmetic-instructions}

The arithmetic operation is then assigned to the destination register using
\emph{blocking} assignment, so as to preserve the sequential nature of the
execution of the code and simplify the correctness proof.  One subtle aspect of
the proof is the translation of registers and predicates into Verilog, because
in \rtlsubpar{} these were contained in separate maps, whereas in Verilog and
\htl{} they need to be combined into one register association map.  This is done
by referring to register $r$ in \rtlsubpar{} as register $r' = 2r$ in Verilog.
Predicate $p$, on the other hand, is referred to as predicate $p' = 2p + 1$ in
Verilog, thereby combining both name spaces into one.  A short example of a
translation from \rtlsubpar{} to \htl{} is shown in
\cref{fig:hg:htl-generation}.

\begin{figure}
  \centering
  \begin{tikzpicture}[>=Latex,shorten >=1pt,
    label/.style={circle,draw,fill=white,inner sep=0.4mm,font=\footnotesize},
    bb/.style={align=left, draw=white, fill=black!5}]
    \node[bb] (initial block) {\rtlinline`[ [ r1 := r2 * r3     ] ]; `\\
                               \rtlinline`[ [ p1 => r4 := r5      ]; `\\
                               \rtlinline`[ [ p2 => Stack[r6] = 3 ]; `\\
                               \rtlinline`  [ goto 2              ] ]`};
    \node[bb, right=4cm of initial block] (transf block)
      {\veriloginline`r2 = r4 * r6;`\\
       \veriloginline`r8 = r3 ? r10 : r8;`\\
       \veriloginline`if (r5) stack[r12] = 32'd3;`\\
       \veriloginline`state = 32'd2;`};
    \node[label] at (initial block.north west) {\texttt{1}};
    \node[label] at (transf block.north west) {\texttt{1}};
    \draw[->,very thick] ($(initial block.east) + (0.5,0)$)
      -- node [below, font=\footnotesize] {\htl{} generation}
      ($(initial block.east) + (3,0)$);
  \end{tikzpicture}
  \caption{Simple translation from an \rtlsubpar{} block into an \htl{} block.}%
  \label{fig:hg:htl-generation}
\end{figure}

\paragraph{Implementing the \texttt{Oshrximm} Instruction}%
\label{sec:algorithm:optimisation:oshrximm}

% Mention that this optimisation is not performed sometimes (clang -03).

Many of the \compcert{} instructions map well to hardware, but \texttt{Oshrximm}
(efficient signed division by a power of two using a logical shift) is expensive
if implemented na\"ively. The problem is that in \compcert{} it is specified as
a signed division:

\begin{equation*}
  \texttt{Oshrximm } x\ y = \text{round\_towards\_zero}\left(\frac{x}{2^{y}}\right)
\end{equation*}

(where $x, y \in \mathbb{Z}$, $0 \leq y < 31$, and $-2^{31} \leq x < 2^{31}$)
and instantiating divider circuits in hardware is well known to cripple
performance. Moreover, since \vericert{} requires the result of a divide
operation to be ready within a single clock cycle, the divide circuit needs to
be entirely combinational. This is inefficient in terms of area, but also in
terms of latency, because it means that the maximum frequency of the hardware
must be reduced dramatically so that the divide circuit has enough time to
finish.  It should therefore be implemented using a sequence of shifts.

\compcert{} eventually performs a translation from this representation into
assembly code which uses shifts to implement the division, however, the
specification of the instruction in 3AC itself still uses division instead of
shifts, meaning this proof of the translation cannot be reused.  In \vericert{},
the equivalence of the representation in terms of divisions and shifts is proven
over the integers and the specification, thereby making it simpler to prove the
correctness of the Verilog implementation in terms of shifts.

\subsubsection{Translating Memory}
\label{sec:hg:translating-memory}

Translating memory operations and the memory itself is one of the trickiest part
of the translation, especially from a correctness point of view, because of the
large difference in behaviour between CompCert memories and their Verilog
implementation.  At the stage of \htl{} generation, we use a Verilog array to
represent the stack of the function in \rtlsubpar{}.  The verilog array is
defined as the following:

\begin{minted}{systemverilog}
logic [31:0] stack [STK_LEN-1:0];
\end{minted}

This is essentially an array of size \veriloginline`STK_LEN` of 32-bit integers.
This array is therefore word-addressable.  One big difference between C and
Verilog is how memory is represented.  Although Verilog arrays use similar
syntax to C arrays, they must be treated quite differently.  Eventually, this
array will have to be replaced by an actual RAM, which only has a limited set of
read and write ports (one of each in our case).  To make loads and stores of
words as efficient as possible, the RAM needs to be word-addressable, which
means that an entire integer can be loaded or stored in one clock cycle.
However, the memory model that \compcert{} uses for its intermediate languages
is byte\?addressable~\cite{blazy05_formal_verif_memor_model_c}.  If a
byte\?addressable memory was used in the target hardware, which is closer to
\compcert{}'s memory model, then a load and store would instead take four clock
cycles, the RAM we implement in hardware can only perform one read and write per
clock cycle.  It therefore has to be proven that the byte-addressable memory
behaves in the same way as the word-addressable memory in hardware.  Any
modifications of the bytes in the \compcert{} memory model also have to be shown
to modify the word-addressable memory in the same way.  Since only integer loads
and stores are currently supported in \vericert{}, it follows that the addresses
given to the loads and stores will be multiples of four.  Translating from
byte-addressed memory to word-addressed memory can then be done by dividing the
address by four.

As shown in \cref{fig:hg:htl-generation}, predicated instructions are translated
into blocking assignments of a ternary expression in Verilog.  This is the case
for all instructions except for the store instruction, which is translated to a
conditional statement.  This ensures that the memory is only modified when the
predicate is set.  If that were not the case, and the memory was translated
using a ternary statement:

\begin{center}
  \begin{tikzpicture}[>=Latex,shorten >=1pt]
    \node [] (rtlstack) {\rtlinline`p2 => Stack[r6] = 3`}; \node [right=4cm of
    rtlstack] (verilogstack)
    {\veriloginline`stack[r12] = r5 ? 32'd3 : stack[r12]`};
    \draw[->,very thick] ($(rtlstack.east) + (0.5,0)$) --
    ($(verilogstack.west) - (0.5,0)$);
  \end{tikzpicture}
\end{center}

\noindent Then, in the case where the predicate \rtlinline`p2` is $\lfalse$,
that would mean that the statement in \rtlsubpar{} would not be executed.
However, in the generated Verilog, one would have to execute the statement
\veriloginline`stack[r12]`, which might not be possible as the register
\rtlinline`r6` in \rtlsubpar{}, and the corresponding register
\veriloginline`r12` in Verilog could be out-of-range of the
\veriloginline`stack` array.  This would therefore mean that there is no way to
execute the Verilog in that case, as one cannot read from the stack when it is
out-of-range.  Gating it fully with an if-statement, as shown in
\cref{fig:hg:htl-generation}, ensures that the Verilog statement can always be
executed, assuming that the \rtlsubpar{} instruction can also be executed.

\subsubsection{Translating Control-Flow Instructions}%
\label{sec:hg:translating-control-flow-instructions}

Most control-flow instructions also map nicely to hardware, however, the way
predicated control-flow instructions are handled differs a bit between
\rtlsubpar{} and \htl{}.  The main problem is that when execution in a
\rtlsubpar{} block reaches a control-flow instruction that is executed, then it
will exit the block immediately.  This is not possible in the \htl{} block,
because the next state is determined based on the value of the
\veriloginline`state` register at the start of the next clock cycle.  Na\"ively
translating the control-flow instructions into assignments to the
\veriloginline`state` register would produce incorrect state transitions in
\htl{}, because a later control-flow instruction could overwrite a previous
instruction.  As a remedy, we keep track of the current negated exit condition,
which accumulates throughout the translation of a block.
\Cref{fig:hg:htl-generation} demonstrates this on a chain of control-flow
operations.  First, the condition of the ternary statement in Verilog
corresponds to the predicate in the block, however, these quickly diverge.  For
the next control-flow instruction, the condition of the ternary statement is set
to be the the translated predicate associated with \rtlinline`p2`, anded
together with the current negated exit condition which is \veriloginline`~r3`.
The current negated exit condition is then updated to be
\veriloginline`~r3 & ~r5`, which is then the ternary expression condition
assigned to the last \veriloginline`state` update.

\begin{figure}
  \centering
  \begin{tikzpicture}[>=Latex,shorten >=1pt,
    label/.style={circle,draw,fill=white,inner sep=0.4mm,font=\footnotesize},
    bb/.style={align=left, draw=white, fill=black!5}]
    \node[bb] (initial block) {\rtlinline`[ [ p1 => goto 3;    `\\
                               \rtlinline`    p2 => goto 4;    `\\
                               \rtlinline`    goto 5;`\\
                               \rtlinline`    r1 <= r2 * r3 ] ]`};
    \node[bb, right=4cm of initial block] (transf block)
      {\veriloginline`state =  r3       ? 32'd3 : state;`\\
       \veriloginline`state = ~r3 & r5  ? 32'd4 : state;`\\
       \veriloginline`state = ~r3 & ~r5 ? 32'd4 : state;`\\
       \veriloginline`r2    = ~r3 & ~r5 ? r4 * r6 : r2; `};
    \node[label] at (initial block.north west) {\texttt{1}};
    \node[label] at (transf block.north west) {\texttt{1}};
    \draw[->,very thick] ($(initial block.east) + (0.5,0)$)
      -- node [below, font=\footnotesize] {\htl{} generation}
      ($(initial block.east) + (3,0)$);
  \end{tikzpicture}
  \caption{Simple translation from an \rtlsubpar{} block into an \htl{} block.}%
  \label{fig:hg:htl-generation}
\end{figure}

To avoid all side-effects after a gated control-flow instruction, regular
instructions will have to be gated by the same negated control-flow predicate,
but do not themselves modify the value of the negated control-flow predicate for
any following instructions.

\YH{TODO: Describe return instruction translation.}

\subsubsection{Translating the Top-Level Function}

Each \rtlsubpar{} function is translated separately, and within each function,
each block is translated to a Verilog statement.  In addition to that, new
registers are created for the various control signals and registers in \htl{},
ensuring that they follow the ordering present in the \htl{} specification.  In
addition to that, the top-level translation also needs to ensure that it is
compiling a translation unit with the \texttt{main} function, as linking with
other translation units is not supported by the Verilog semantics.

\subsection{\htl{} Generation Correctness Proof}%
\label{sec:hg:htl-generation-correctness-proof}

\YH{TODO: Move this section earlier, or maybe remove it completely it I have
  already stated most of what is there.}

There is quite a large mismatch between the \htl{} semantics and the
\rtlsubpar{} semantics.  This is mainly due to the following two points:

\begin{itemize}
\item As already mentioned in Section~\ref{sec:hg:translating-memory}, because
  the memory model in our \htl{} semantics is finite and concrete, but the
  CompCert memory model is more abstract and infinite with additional bounds,
  the equivalence of these models needs to be proven.  Moreover, our memory is
  word-addressed for efficiency reasons, whereas CompCert's memory is
  byte-addressed.

\item Second, the \htl{} semantics operates quite differently to the usual
  intermediate languages in CompCert.  All the CompCert intermediate languages
  use a map from control-flow nodes to instructions.  An instruction can
  therefore be selected using an abstract program pointer. Meanwhile, in the
  \htl{} semantics the whole design is executed at every clock cycle, because
  hardware is inherently parallel. The program pointer is part of the design as
  well, not just part of an abstract state. This makes the semantics of \htl{}
  simpler, but comparing it to the semantics of 3AC becomes more challenging, as
  one has to map the abstract notion of the state to concrete values in
  registers.
\end{itemize}

%\subsection{Forward Simulation from 3AC to \htl{}}\label{sec:proof:3ac_htl}

As \htl{} is quite far removed from \rtlsubpar{}, this first translation is the
most involved and therefore requires a larger proof, because the translation
from \rtlsubpar{} instructions to Verilog statements needs to be proven correct
in this step.  In addition to that, the semantics of \htl{} are also quite
different to the \rtlsubpar{} semantics. Instead of defining small-step
semantics for each construct in Verilog, the semantics are defined over one
clock cycle and mirror the semantics defined for Verilog.  Lemma~\ref{lemma:htl}
shows the result that needs to be proven in this subsection.

\begin{lemma}[Forward simulation from \rtlsubpar{} to \htl{}]\label{lemma:htl}
  Writing \texttt{tr\_htl} for the translation from \rtlsubpar{} to \htl{}, we
  have:
  \begin{equation*}
    \forall c, h, B \in \texttt{Safe},\quad \yhfunction{tr\_htl} (c) = \yhconstant{OK} (h) \land c \Downarrow B \implies h \Downarrow B.
  \end{equation*}
\end{lemma}

\begin{proof}[Proof sketch]
  We prove this lemma by first establishing a specification of the translation
  function $\yhfunction{tr\_htl}$ that captures its important properties, and
  then splitting the proof into two parts: one to show that the translation
  function does indeed meet its specification, and one to show that the
  specification implies the desired simulation result. This strategy is in
  keeping with standard \compcert{} practice.

  % The forward simulation is then proven by showing that the initial states and
  % final states between the \rtlsubpar{} semantics and \htl{} semantics match, and then
  % showing that the simulation diagram in Lemma~\ref{lemma:simulation_diagram}
  % holds.
\end{proof}

%\subsubsection{From Implementation to Specification}\label{sec:proof:3ac_htl:specification}
%
%%To simplify the proof, instead of using the translation algorithm as an
%%assumption, as was done in Lemma~\ref{lemma:htl}, a specification of the
%%translation can be constructed instead which contains all the properties that
%%are needed to prove the correctness.  For example, for the translation from \rtlsubpar{}
%%to \htl{},
%
%The specification for the translation of \rtlsubpar{} instructions into \htl{} data-path and
%control logic can be defined by the following predicate:
%
%\begin{equation*}
%  \yhfunction{spec\_instr}\ \mathit{fin}\ \mathit{ret}\ \sigma\ \mathit{stk}\ i\ \mathit{data}
%\end{equation*}
%
%\noindent Here, the $\mathit{data}$ parameter is the statement that the current
%\rtlsubpar{} instruction $i$ should translate to. The other parameters are the
%special registers defined in Section~\ref{sec:verilog:integrating}. An example
%of a rule describing the translation of an arithmetic/logical operation from
%\rtlsubpar{} is the following:
%
%\begin{equation*}
%  \inferrule[Iop]{\yhfunction{tr\_op}\ \mathit{op}\ \vec{a} = \yhconstant{OK}\ e}{\yhfunction{spec\_instr}\ \mathit{fin}\ \mathit{ret}\ \sigma\ \mathit{stk}\ (\yhconstant{Iop}\ \mathit{op}\ \vec{a}\ d\ n)\ (d\ \yhkeyword{<=}\ e)\ (\sigma\ \yhkeyword{<=}\ n)}
%\end{equation*}
%
%\noindent Assuming that the translation of the operator $\mathit{op}$ with
%operands $\vec{a}$ is successful and results in expression $e$, the rule
%describes how the destination register $d$ is updated to $e$ via a non-blocking
%assignment in the data path, and how the program counter $\sigma$ is updated to
%point to the next CFG node $n$ via another non-blocking assignment in the
%control logic.
%
%In the following lemma, $\yhfunction{spec\_htl}$ is the top-level specification
%predicate, which is built using $\yhfunction{spec\_instr}$ at the level of
%instructions.
%
%\begin{lemma}\label{lemma:specification}
%  If a \rtlsubpar{} program $c$ is translated correctly to an \htl{} program $h$, then the
%  specification of the translation holds.
%  \begin{equation*}
%    \forall c, h,\quad \yhfunction{tr\_htl} (c) = \yhconstant{OK}(h) \implies \yhfunction{spec\_htl}\ c\ h.
%  \end{equation*}
%\end{lemma}
%
%%\begin{proof}
%%  Follows from the definition of the specification and therefore should match
%%  the implementation of the algorithm.
%%\end{proof}

\subsubsection{Forward Simulation Proof of Translation}

To prove that the translation described in \cref{sec:hg:htl-generation} results
in the desired forward simulation, we must first define a relation that matches
each \rtlsubpar{} state to an equivalent \htl{} state.  This relation also
captures the assumptions made about the \rtlsubpar{} code that we receive from
\compcert{}.  These assumptions then have to be proven to always hold assuming
the \htl{} code was created by the translation algorithm.  Some of the
assumptions that need to be made about the \rtlsubpar{} and \htl{} code for a
pair of states to match are:

\begin{itemize}
\item The \rtlsubpar{} register file $R$ needs to be `less defined' than the \htl{} register
  map $\Gamma_{r}$ (written $R \le \Gamma_{r}$). This means that all entries
  should be equal to each other, unless a value in $R$ is undefined, in which
  case any value can match it.
\item There is a single allocation that was performed in \rtlsubpar{}, with the
  size of the allocation being equal to the stack size of the \texttt{main}
  function, which was performed when the \texttt{main} function was initially
  called; that is: $|M| \le \mathtt{main}.\mathit{stacksize}$.
\item The RAM values represented by each Verilog array in $\Gamma_{a}$ need to
  match the \rtlsubpar{} function's stack contents, which are part of the memory $M$;
  that is: $M \le \Gamma_{a}$.
\item The state is well formed, which means that the value of the state register
  matches the current value of the program counter; that is:
  $\mathit{pc} = \Gamma_{r}[\sigma]$.
\end{itemize}

We also define the following set $\mathcal{I}$ of invariants that must hold for
the current state to be valid:

\begin{itemize}
\item that all pointers in the program use the stack as a base pointer,
\item that any loads or stores to locations outside of the bounds of the stack
  result in undefined behaviour (and hence we do not need to handle them),
\item that $\mathit{rst}$ and $\mathit{fin}$ are not modified and therefore stay
  at a constant 0 throughout execution, and
\item that the stack frames match.  As no function calls are performed, as they
  are all inlined, the stack frames will always be empty.
\end{itemize}

We can now define the simulation diagram for the translation. The \rtlsubpar{} state can
be represented by the tuple $(R,M,\mathit{pc})$, which captures the register
file, memory, and program counter. The \htl{} state can be represented by the pair
$(\Gamma_{r}, \Gamma_{a})$, which captures the states of all the registers and
arrays in the module.  Finally, $\mathcal{I}$ stands for the other invariants
that need to hold for the states to match.

\begin{lemma}\label{lemma:simulation_diagram}
  Given the \rtlsubpar{} state $(R,M,\mathit{pc})$ and the matching \htl{} state
  $(\Gamma_{r}, \Gamma_{a})$, assuming one step in the \rtlsubpar{} semantics produces
  state $(R',M',\mathit{pc}')$, there exist one or more steps in the \htl{}
  semantics that result in matching states $(\Gamma_{r}', \Gamma_{a}')$.  This
  is all under the assumption that the specification $\yhfunction{spec\_{htl}}$
  holds for the translation.

  \YH{TODO: Maybe find a better way to describe that the memory was only
    allocated once, as it is a slightly different condition to the one that
    states that the contents of both memories are the same.  It touches more on
    the permissions of the CompCert memory and showing that they agree with the
    stack bounds.}

  \begin{center}
    \begin{tikzpicture}
      \begin{scope}
        \node[circle] (s1) at (0,2) {$R, M, \mathit{pc}$};
        \node[circle] (r1) at (14,2) {$\Gamma_{r}, \Gamma_{a}$};
        \node[circle] (s2) at (0,0) {$R', M', \mathit{pc}'$};
        \node[circle] (r2) at (14,0) {$\Gamma_{r}', \Gamma_{a}'$};
        %\node at (6.8,0.75) {+};
        \draw (s1) -- node[above] {$\mathcal{I} \land (R \le \Gamma_{r}) \land
          (M \le \Gamma_{a}) \land (\mathit{pc} = \Gamma_{r}[\sigma]) \land |M| \le \mathtt{main}.\mathit{stacksize}$} ++ (r1);
        \draw[-{Latex}] ($(s1.south) + (0,0.4)$) -- ($(s2.north) - (0,0.4)$);
        \draw[-{Latex},dashed] ($(r1.south) + (0,0.2)$) to[auto, pos=0.7] node {+} ($(r2.north) - (0,0.2)$);
        \draw[dashed] (s2) -- node[above] {$\mathcal{I} \land (R' \le
          \Gamma_{r}') \land (M' \le \Gamma_{a}') \land (\mathit{pc}' =
          \Gamma_{r}'[\sigma]) \land |M'| \le \mathtt{main}.\mathit{stacksize}$} ++ (r2);
      \end{scope}
    \end{tikzpicture}
  \end{center}
\end{lemma}

\begin{proof}[Proof sketch]
  This simulation diagram is proven by induction over the operational semantics
  of \rtlsubpar{}, which allows us to find one or more steps in the \htl{} semantics that
  will produce the same final matching state.
\end{proof}

\section{BRAM insertion}%
\label{sec:hg:bram-insertion}

The simplest way to implement loads and stores in \vericert{} would be to access
the Verilog array directly from within the data path as is currently the case
after the \htl{} generation. This would be correct, but when a Verilog array is
accessed at several program points, the synthesis tool is unlikely to detect
that it can be implemented as an RAM, and will resort to using lots of registers
instead, ruining the circuit's area and performance.  This is because reads and
writes to a RAM need to follow a certain pattern to be suitable to be replaced
by RAM reads and writes.  For example, the synthesis tool will have to check
that the array is only written to once per clock cycle, and is only read from
once as well.  To avert this, we arrange that the data-path does not access
memory directly, but simply sets the address it wishes to access and then
toggles the \texttt{u\_en} flag. This activates the RAM interface (lines 9--15
of Fig.~\ref{fig:accumulator_v}) on the next falling clock edge, which performs
the requested load or store. By factoring all the memory accesses out into a
separate interface, we ensure that the underlying array is only accessed from a
single program point in the Verilog code, and thus ensure that the synthesis
tool will correctly infer a RAM block.\footnote{Interestingly, the Verilog code
  shown for the RAM interface must not be modified, because the synthesis tool
  will only generate a RAM when the code matches a small set of specific
  patterns.}

This transformation pass therefore translates direct accesses to the Verilog
array in \htl{} and replaces them by signals that access the RAM interface in a
separate always-block. The translation is performed by going through all the
instructions and replacing each load and store expression in turn.  Stores can
be replaced by the necessary wires to the RAM directly. Loads are a little more
subtle: loads that use the RAM interface take two clock cycles where a direct
load from an array takes only one, so this pass inserts an extra state after
each load.  The scheduling algorithm described in
\cref{sec:hs:scheduling-algorithm} can already take this into account as well,
and can ensure that the next clock cycle after a load does not perform a load or
a store, however, some additional proofs would have to be added to this
transformation pass to check that this is actually the case, and that the load
will proceed as expected.

%\JW{I've called that negedge always-block the `RAM driver' in my proposed text above -- that feels like quite a nice a word for it to my mind -- what do you think?}\YH{Yes I quite like it!}
%Verilog arrays can be used in a variety of ways, however, these do not all produce optimal hardware designs.  If, for example, arrays in Verilog are accessed immediately in the data-path, then the synthesis tool is not be able to identify it as having the right properties for a RAM, and would instead implement the array using registers.  This is extremely expensive, and for large memories this can easily blow up the area usage of the FPGA, and because of the longer wires that are needed, it would also affect the performance of the circuit.  The synthesis tools therefore provide code snippets that they know how to transform into various constructs, including snippets that will generate proper RAMs in the final hardware.  This process is called memory inference.  The initial translation from 3AC to \htl{} converts loads and stores to direct accesses to the memory, as this preserves the same behaviour without having to insert more registers and logic.  We therefore have another pass from \htl{} to itself which performs the translation from this na\"ive use of arrays to a representation which always allows for memory inference.  This pass creates a separate always-block to perform the loads and stores to the memory, and adds the necessary data, address and enable signals to communicate with that always-block from other always-blocks.  This always-block is shown between lines 10-15 in Fig.~\ref{fig:accumulator_v}.

There are two interesting parts to the inserted RAM interface.  Firstly, the
memory updates are triggered on the negative (falling) edge of the clock, out of
phase with the rest of the design which is triggered on the positive (rising)
edge of the clock.  The advantage of this is that instead of loads and stores
taking three clock cycles and two clock cycles respectively, they only take two
clock cycles and one clock cycle instead, greatly improving their performance.
Ideally, this translation would take advantage of the scheduler, which can
already ensure that the clock cycle after a load remains free to wait for the
data from the RAM, however, this would require additional checking by this
transformation to double check that the read result is not used until it is
ready.
% \JW{Is this a standard `trick' in hardware design? If so it might be nice to
% cite it.}\YH{Hmm, not really, because it has the downside of kind of halving
% your available clock period. However, RAMs normally come in both forms on the
% FPGA (Page 12, Fig. 2,
% \url{https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/ug/ug_ram_rom.pdf})}
% JW: thanks!
Using the negative edge of the clock is supported by synthesis tools and FPGAs,
but in general it does reduces the time that's available for arithmetic
operations in the positive edge of the clock, making it harder to schedule
operations.

Secondly, the logic in the enable signal of the RAM (\texttt{en != u\_en}) is
also atypical in hardware designs.  Enable signals are normally manually
controlled and inserted into the appropriate states, by using a check like the
following in the RAM:\@ \texttt{en == 1}.  This means that the RAM only turns on
when the enable signal is set.  However, to make the proof simpler and avoid
reasoning about possible side effects introduced by the RAM being enabled but
not used, a RAM which disables itself after every use would be ideal.  One
method for implementing this would be to insert an extra state after each load
or store that disables the RAM, but this extra state would eliminate the speed
advantage of the negative-edge-triggered RAM. Another method would be to
determine the next state after each load or store and disable the RAM in that
state, but this could quickly become complicated, especially in the case where
the next state also contains a memory operation, and hence the disable signal
should not be added. The method we ultimately chose was to have the RAM become
enabled not when the enable signal is high, but when it \emph{toggles} its
value.  This can be arranged by keeping track of the old value of the enable
signal in \texttt{en} and comparing it to the current value \texttt{u\_en} set
by the data path.  When the values are different, the RAM gets enabled, and then
\texttt{en} is set to the value of \texttt{u\_en}. This ensures that the RAM
will always be disabled straight after it was used, without having to insert or
modify any other states.

%We can instead generate a second enable signal that is set by the user, and the original enable signal is then updated by the RAM to be equal to the value that the user set.  This means that the RAM should be enabled whenever the two signals are different, and disabled otherwise.

\subsection{BRAM Model Semantics}%
\label{sec:hg:bram-model-semantics}

Fig.~\ref{fig:ram_load_store} gives an example of how the RAM interface behaves
when values are loaded and stored.  There is a \texttt{wr\_en} signal that
determines if a load or store is being performed.  Then, if at the falling edge
\texttt{u\_en} and \texttt{en} are different, the read or write at address
\texttt{addr} is executed.  At the same time, the value of \texttt{u\_en} is
then assigned to \texttt{en}, which would disable it if there is no other action
by the data path on the next clock cycle.  However, if the data path toggles the
value of \texttt{u\_en} on the next clock cycle, the RAM would be enabled again.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.48\linewidth}
    \begin{tikztimingtable}[timing/d/background/.style={fill=white}]
      \small clk & 2L 3{6C} \\
      \small u\_en & 2D{u\_en} 18D{$\overline{\text{u\_en}}$}\\
      \small addr & 2U 18D{3} \\
      \small wr\_en & 2U 18L \\
      \small en & 8D{u\_en} 12D{$\overline{\text{u\_en}}$}\\
      \small d\_out & 8U 12D{0xDEADBEEF} \\
      \small r & 14U 6D{0xDEADBEEF} \\
      \extracode
      \node[help lines] at (2,2.25) {\tiny 1};
      \node[help lines] at (8,2.25) {\tiny 2};
      \node[help lines] at (14,2.25) {\tiny 3};
      \begin{pgfonlayer}{background}
        \vertlines[help lines]{2,8,14}
      \end{pgfonlayer}
    \end{tikztimingtable}
    \caption{Timing diagram for loads. At time 1, the \texttt{u\_en} signal is toggled to enable the RAM. At time 2, \texttt{d\_out} is set to the value stored at the address in the RAM, which is finally assigned to the register \texttt{r} at time 3.}\label{fig:ram_load}
  \end{subfigure}\hfill%
  \begin{subfigure}[b]{0.48\linewidth}
    \begin{tikztimingtable}[timing/d/background/.style={fill=white}]
      \small clk & 2L 2{7C} \\
      \small u\_en & 2D{u\_en} 14D{$\overline{\text{u\_en}}$}\\
      \small addr & 2U 14D{3} \\
      \small wr\_en & 2U 14H \\
      \small d\_in & 2U 14D{0xDEADBEEF} \\
      \small en & 9D{u\_en} 7D{$\overline{\text{u\_en}}$}\\
      \small stack[addr] & 9U 7D{0xDEADBEEF} \\
      \extracode
      \node[help lines] at (2,2.25) {\tiny 1};
      \node[help lines] at (9,2.25) {\tiny 2};
      \begin{pgfonlayer}{background}
        \vertlines[help lines]{2,9}
      \end{pgfonlayer}
    \end{tikztimingtable}
    \caption{Timing diagram for stores. At time 1, the \texttt{u\_en} signal is toggled to enable the RAM, and the address \texttt{addr} and the data to store \texttt{d\_in} are set. On the negative edge at time 2, the data is stored into the RAM.}\label{fig:ram_store}
  \end{subfigure}
%  \alt{Timing diagrams of loads and stores, showing which signals are modified at which time step.}
  \caption{Timing diagrams showing the execution of loads and stores over multiple clock cycles.}\label{fig:ram_load_store}
\end{figure}

\subsection{BRAM Insertion and Correctness Proof}%
\label{sec:hg:bram-insertion-and-correctness-proof}

\begin{figure}
  \centering
  \begin{minipage}{1.0\linewidth}
    \begin{gather*}
      \inferrule[Idle]{\Gamma_{\mathrm{r}}[r.\mathrm{en}] = \Gamma_{\mathrm{r}}[r.\mathrm{u\_en}]}{((\Gamma_{\mathrm{r}}, \Gamma_{\mathrm{a}}), \Delta, r) \downarrow_{\mathrm{ram}} \Delta}\\
%
      \inferrule[Load]{\Gamma_{\mathrm{r}}[r.\mathrm{en}] \ne \Gamma_{\mathrm{r}}[r.\mathrm{u\_en}] \\ \Gamma_{\mathrm{r}}[r.\mathrm{wr\_en}] = 0}{((\Gamma_{\mathrm{r}}, \Gamma_{\mathrm{a}}), (\Delta_{\mathrm{r}}, \Delta_{\mathrm{a}}), r) \downarrow_{\mathrm{ram}} (\Delta_{\mathrm{r}}[r.\mathrm{en} \mapsto r.\mathrm{u\_en}, r.\mathrm{d\_out} \mapsto (\Gamma_{\mathrm{a}}[r.\mathrm{mem}])[ r.\mathrm{addr}]], \Delta_{\mathrm{a}}) }\\
%
      \inferrule[Store]{\Gamma_{\mathrm{r}}[r.\mathrm{en}] \ne \Gamma_{\mathrm{r}}[r.\mathrm{u\_en}] \\ \Gamma_{\mathrm{r}}[r.\mathrm{wr\_en}] = 1}{((\Gamma_{\mathrm{r}}, \Gamma_{\mathrm{a}}), (\Delta_{\mathrm{r}}, \Delta_{\mathrm{a}}), r) \downarrow_{\mathrm{ram}} (\Delta_{\mathrm{r}}[r.\mathrm{en} \mapsto r.\mathrm{u\_en}], \Delta_{\mathrm{a}}[r.\mathrm{mem} \mapsto (\Gamma_{\mathrm{a}}[ r.\mathrm{mem}])[r.\mathrm{addr} \mapsto r.\mathrm{d\_in}]]) }
    \end{gather*}
  \end{minipage}
  \caption{Specification for the memory implementation in \htl{}, where $r$ is the RAM, which is then implemented by equivalent Verilog code.}\label{fig:htl_ram_spec}
\end{figure}

\YH{TODO: Link the RAM from \htl{} to the ram instantiated here.}

\htl{} can only represent a single state machine, so we must model the RAM
abstractly to reason about the correctness of replacing the direct read and
writes to the array by loads and stores to a RAM.  The specification for the RAM
is shown in Fig.~\ref{fig:htl_ram_spec}, which defines how the RAM $r$ will
behave for all the possible combinations of the input signals.  This
specification is part of the \htl{} semantics and runs in parallel to the state
machine.  However, as the RAM is triggered by the falling edge of the clock, it
will execute in between standard clock cycles, and a merge of the association
maps is performed in between each.

\subsubsection{From Implementation to Specification}

The first step in proving the simulation correct is to build a specification of
the translation algorithm.  There are five possibilities for the transformation
of an instruction. For each Verilog statement in the map at location $i$, the
statement is either a load, a store, a predicated load, a predicated store, or
neither. The load, store, predicated load and predicated store is translated to
the equivalent representation using the RAM specification and all other
instructions are left intact.  The specification of the translation is shown in
\cref{sec:hg:memory-transformation-specification}, where $\sigma$ is state
register, $r$ is the RAM, $d$ are the input data-path and control logic maps,
and $i$ is the current state.  ($n$ is the newly inserted state, which only
applies to the translation of loads.)

\newcommand\nonblockasgn{\mathrel{\texttt{<=}}}
\newcommand\blockasgn{\mathrel{\texttt{=}}}
\newcommand\msemi{\texttt{;}\ }
\newcommand\mternary[3]{#1\mathbin{\texttt{?}}#2\mathbin{\texttt{:}}#3}
\newcommand\mxor{\oplus}
\newcommand\verilogeq{\mathbin{\texttt{==}}}

\begin{figure}
\begin{center}
\begin{mathpar}
  \inferrule[Store Transl]{ d[i] = \left(\begin{array}{l}
    r.\mathrm{mem}\texttt{[}e_{1}\texttt{]} \blockasgn
    e_{2}\msemi \arcr
    \sigma \blockasgn e_3\msemi
  \end{array}\right) \\
  t = \left(\begin{array}{l}
    r.\mathrm{u\_en} \blockasgn
    \neg r.\mathrm{u\_en}\msemi \arcr
    r.\mathrm{wr\_en} \blockasgn 1\msemi \arcr r.\mathrm{d\_in} \blockasgn
    e_{2}\msemi \arcr
    r.\mathrm{addr} \blockasgn e_{1} \msemi \arcr
    \sigma \blockasgn e_3 \msemi
  \end{array}\right)}%
  {\yhfunction{spec\_ram\_tr}\ \sigma\ r\ d\ d[i \mapsto t]\ i\ n}
  \and
  \inferrule[Predicated Store Transl]{ d[i] = \left(\begin{array}{l}
    \texttt{if(} c\texttt{)}\ r.\mathrm{mem}\texttt{[}e_{1}\texttt{]} \blockasgn
    e_{2}\msemi \arcr
    \sigma \blockasgn e_3\msemi\end{array}\right) \\ t = \left(\begin{array}{l}
      r.\mathrm{u\_en} \blockasgn
    (c \verilogeq \texttt{32'b0})\mxor r.\mathrm{u\_en}\msemi\arcr
      r.\mathrm{wr\_en} \blockasgn \texttt{1'b1}\msemi\arcr
      r.\mathrm{d\_in} \blockasgn \mternary{c}{e_{2}}{r.\mathrm{d\_in}}\msemi\arcr
      r.\mathrm{addr} \blockasgn \mternary{c}{e_{1}}{r.\mathrm{addr}}\msemi
      \arcr
      \sigma \blockasgn e_3 \msemi
    \end{array}\right)}%
  {\yhfunction{spec\_ram\_tr}\ \sigma\ r\ d\ d[i \mapsto t]\ i\ n}
  \and
  \inferrule[Load Transl]{ d[i] = \left(\begin{array}{l}
    d \blockasgn r.\mathrm{mem}\texttt{[}e_{1}\texttt{]} \arcr
    \sigma \blockasgn e_2\msemi
  \end{array}\right) \\
  t_1 = \left(\begin{array}{l}
    r.\mathrm{u\_en} \blockasgn
    \neg r.\mathrm{u\_en}\msemi \arcr
    r.\mathrm{wr\_en} \blockasgn \texttt{1'b0}\msemi \arcr
    r.\mathrm{addr} \blockasgn e_{1} \msemi \arcr
    \sigma \blockasgn n \msemi
  \end{array}\right) \\
  t_2 = \left(\begin{array}{l}
    d \blockasgn r.\mathrm{d\_out} \msemi \arcr
    \sigma \blockasgn e_2 \msemi
  \end{array}\right)}%
  {\yhfunction{spec\_ram\_tr}\ \sigma\ r\ d\ d[i \mapsto t_1; n \mapsto t_2]\ i\ n}
  \and
  \inferrule[Predicated Load Transl]{ d[i] = \left(\begin{array}{l}
    d \blockasgn \mternary{c}{r.\mathrm{mem}\texttt{[}e_{1}\texttt{]}}{d}\msemi \arcr
    \sigma \blockasgn e_2\msemi
  \end{array}\right) \\
  t_1 = \left(\begin{array}{l}
    r.\mathrm{u\_en} \blockasgn (c \verilogeq \texttt{32'b0})\mxor r.\mathrm{u\_en}\msemi \arcr
    r.\mathrm{wr\_en} \blockasgn \texttt{1'b0}\msemi \arcr
    r.\mathrm{addr} \blockasgn \mternary{c}{e_{1}}{r.\mathrm{addr}} \msemi \arcr
    \sigma \blockasgn n \msemi
  \end{array}\right) \\
  t_2 = \left(\begin{array}{l}
    d \blockasgn \mternary{c}{e_{1}}{r.\mathrm{d\_out}} \msemi \arcr
    \sigma \blockasgn e_2 \msemi
  \end{array}\right)}%
  {\yhfunction{spec\_ram\_tr}\ \sigma\ r\ d\ d[i \mapsto t_1; n \mapsto t_2]\ i\ n}
  \and
  \inferrule[Default Transl]{(\forall e_1\ e_2\ e_3\ldotp d[i] \neq (r.\mathrm{mem}\texttt{[}e_{1}\texttt{]} \blockasgn
    e_{2}\msemi \sigma \blockasgn e_3)) \\
    (\forall c\ e_1\ e_2\ e_3\ldotp d[i] \neq (\texttt{if(}c\texttt{)}\ r.\mathrm{mem}\texttt{[}e_{1}\texttt{]} \blockasgn
    e_{2}\msemi \sigma \blockasgn e_3)) \\
  (\forall d\ e_1\ e_2\ldotp d[i] \neq (d \blockasgn
  r.\mathrm{mem}\texttt{[}e_{1}\texttt{]}\msemi \sigma \blockasgn
  e_2))\\
(\forall c\ d\ e_1\ e_2\ldotp d[i] \neq (d \blockasgn \mternary{c}{r.\mathrm{mem}\texttt{[}e_{1}\texttt{]}}{d}\msemi \sigma \blockasgn e_2))}%
  {\yhfunction{spec\_ram\_tr}\ \sigma\ r\ d\ d\ i\ n}
\end{mathpar}
\end{center}
\caption{Memory transformation specification.\YH{TODO: replace register name $d$
  because it's also used for the data path.}}%
\label{sec:hg:memory-transformation-specification}
\end{figure}

\subsubsection{From Specification to Simulation}

Another simulation proof is performed to prove that the insertion of the RAM is
semantics preserving.  As in Lemma~\ref{lemma:simulation_diagram}, we require
some invariants that always hold at the start and end of the simulation.  The
invariants needed for the simulation of the RAM insertion are quite different to
the previous ones, so we can define these invariants $\mathcal{I}_{\mathrm{r}}$ to be the
following:

\begin{itemize}
\item The association map for arrays $\Gamma_{\mathrm{a}}$ always needs to have
  the same arrays present, and these arrays should never change in size.
\item The RAM should always be disabled at the start of each simulation
  step. (This is why self-disabling RAM is needed.)
\end{itemize}

The other invariants and assumptions for defining two matching states in \htl{} are
quite similar to the simulation performed in
Lemma~\ref{lemma:simulation_diagram}, such as ensuring that the states have the
same value, and that the values in the registers are less defined.  In
particular, the less defined relation matches up all the registers, except for
the new registers introduced by the RAM.

\begin{lemma}[Forward simulation from \htl{} to \htl{} after inserting the RAM]\label{lemma:htl_ram}
  Given an \htl{} program, the forward-simulation relation should hold after
  inserting the RAM and wiring the load, store, and control signals.

  \begin{align*}
    \forall h, h', B \in \texttt{Safe},\quad \yhfunction{tr\_ram\_ins}(h) = h' \land h \Downarrow B \implies h' \Downarrow B.
  \end{align*}
\end{lemma}

\section{Register Forward Substitution}%
\label{sec:hg:register-forward-substitution}

Until now, only blocking assignment has been used to assign variables
sequentially, being more faithful to the instructions provided as the input
language.  The transformation is shown in \cref{fig:hg:forward-substitution},
and it turns sequential, blocking assignment into parallel, nonblocking
assignment by substituting register definitions within a state.  Each assignment
in the translated block is independent from the other blocks, meaning each
assignment can be executed in parallel.  Note that the \texttt{state} is
assigned twice using nonblocking assignment.  In that case, only the last
nonblocking assignment is kept.  In addition to that, \texttt{r2} is no longer
used in the block, as it has been replaced by its definition in the assignment
to \texttt{r8}.  It is likely that \texttt{r2} is not used in any other block
either, in which case it should be removed.  This is not performed by this
transformation pass, as in practice synthesis tools can reliably remove a
register that is assigned and never referenced in the design.  There are two
reasons why this transformation is needed.

\begin{enumerate}
\item In a clocked always-block, nonblocking assignment should be used for all
  registers that could interact with other always-blocks.  As we have a RAM
  block in a separate always block, we should at least use nonblocking
  assignment for the registers that interact with memory.  In this case, it is
  technically not required, because the RAM is executing on the negative edge of
  the clock, however, if it is ever replaced by a RAM that executes on the
  positive edge of the clock, or supplemented with other functional units that
  execute on the positive edge of the clock, then any registers communicating
  with those blocks will need to use nonblocking assignment.
\item Some synthesis tools do not seem to be able to optimise the designs
  generated by Vericert with blocking assignments, as they seem to remove less
  unused registers, resulting in slightly lower maximum frequency and slightly
  larger area.\footnote{This seems to be the case with Vivado 2017.1 and seems
    to have been fixed by Vivado 2023.1.}  In the case of the example given in
  \cref{fig:hg:forward-substitution}, it seems like some synthesis tool versions
  do not remove \texttt{r2} when it is referenced in the assignment to
  \texttt{r8} and not referenced anywhere else in the design.  This should not
  be the case as in this example \texttt{r2} should just become a wire.
\end{enumerate}

\begin{figure}
  \centering
  \begin{tikzpicture}[>=Latex,shorten >=1pt,
    label/.style={circle,draw,fill=white,inner sep=0.4mm,font=\footnotesize},
    bb/.style={align=left, draw=white, fill=black!5}]
    \node[bb] (initial block) {\veriloginline`state = 32'd2`\\
                               \veriloginline`r2 = r4 * r6`\\
                               \veriloginline`r8 = r2 + r10`\\
                               \veriloginline`state = r3 ? 32'd3 : state`};
    \node[bb, right=5cm of initial block] (transf block)
      {\veriloginline`state <= 32'd2`\\
       \veriloginline`r2 <= r4 * r6`\\
       \veriloginline`r8 <= {r4 * r6} + r10`\\
       \veriloginline`state <= r3 ? 32'd3 : 32'd2`};
    \node[label] at (initial block.north west) {\texttt{1}};
    \node[label] at (transf block.north west) {\texttt{1}};
    \draw[->,very thick] ($(initial block.east) + (0.5,0)$)
      -- node [below, font=\footnotesize] {forward substitution}
      ($(initial block.east) + (4,0)$);
  \end{tikzpicture}
  \caption{Simple forward substitution transformation.}%
  \label{fig:hg:forward-substitution}
\end{figure}

The transformation is performed by traversing each block and storing for each
register the expression that is being assigned to it.  If the same register is
encountered multiple times, the expression being assigned is always substituted
first and then replaces the current mapping from register to expression.  As the
BRAM insertion already removed all the load and store operations in the data
path, only regular register assignments need to be accounted for.

\subsection{Forward Substitution Correctness Proof}%
\label{sec:hg:forward-substitution-correctness-proof}

The main lemma that needs to be proven for the forward substitution is the
following.

\begin{lemma}[Forward substitution of expressions]

  Given a map from registers to expression $t$, an expression before forward
  substitution $e$, and the result of forward substituting $e$ with map $t$
  resulting in expression $e'$, then executing $e$ with the dynamically updated
  association map $\Gamma^2_{\mathrm r}$ should be equivalent to executing $e'$
  with the initial state of all the registers $\Gamma^1_{\mathrm r}$.\YH{TODO:
    Explain the two starting hypotheses in the following equation.}

  \begin{equation*}
    \begin{aligned}
    &(\forall r\ e\ldotp \at{t}{r} = \some{e} \implies \exists v\ldotp
    ((\Gamma^1_{\mathrm r}, \Gamma_{\mathrm a}), e) \downarrow_{\text{expr}} v
    \land \at{\Gamma^2_{\mathrm r}}{r} = \some{v}) \implies\\
    &(\forall r\ldotp \at{t}{r} = \cnone \implies \at{\Gamma^1_{\mathrm r}}{r} =
    \at{\Gamma^2_{\mathrm r}}{r}) \implies\\
    &\texttt{forward\_substitute}(t, e) = \some{e'} \implies \\
    &((\Gamma^2_{\mathrm r}, \Gamma_{\mathrm a}), e) \downarrow_{\text{expr}} v
      \implies\\
    &((\Gamma^1_{\mathrm r}, \Gamma_{\mathrm a}), e') \downarrow_{\text{expr}} v
    \end{aligned}
  \end{equation*}
\end{lemma}

Using this, the rest of the proof can be built up inductively.\YH{TODO: Talk
  a bit more about the top-level proof.  Especially the invariants.}

\section{Verilog Generation}%
\label{sec:hg:verilog-generation}

Finally, Verilog generation produces proper Verilog from \htl{}.  The main two
transformations that take place are:

\begin{enumerate}
\item Converting the mapping from states to Verilog statements into a case
  statement.
\item Instantiating the RAM specification as a Verilog always block.
\end{enumerate}

\begin{figure}
  \centering
\begin{minted}{systemverilog}
always @(negedge clk) begin
  if ({u_en != en}) begin
    if (wr_en) begin
      stack[addr] <= d_in;
    end else begin
      d_out <= stack[addr];
    end
    en <= u_en;
  end else begin
  end
end
\end{minted}
  \caption{Instantiation of RAM specification with Verilog implementation.}%
  \label{fig:hg:ram-instantiation}
\end{figure}

\subsection{Forward Simulation from \htl{} to Verilog}%
\label{sec:proof:htl_verilog}

The \htl{}-to-Verilog simulation is conceptually simple, as the only
transformation is from the map representation of the code to the case-statement
representation.  The proof is more involved, as the semantics of a map structure
is quite different to that of the case-statement to which it is converted.

%\YH{Maybe want to split this up into two lemmas?  One which states the proof about the map property of uniqueness of keys, and another proving the final theorem?}
\begin{lemma}[Forward simulation from \htl{} to Verilog]\label{lemma:verilog}
  In the following, we write $\yhfunction{tr\_verilog}$ for the translation from
  \htl{} to Verilog. (Note that this translation cannot fail, so we do not need the
  \yhconstant{OK} constructor here.)
  \begin{align*}
    \forall h, V, B \in \texttt{Safe},\quad \yhfunction{tr\_verilog} (h) = V \land h \Downarrow B \implies V \Downarrow B.
  \end{align*}
\end{lemma}

\begin{proof}[Proof sketch]
  The translation from maps to case-statements is done by turning each node of
  the tree into a case-expression containing the same statements.  The main
  difficulty is that a random-access structure is being transformed into an
  inductive structure where a certain number of constructors need to be called
  to get to the correct case.
  % \JW{I would chop from here.}\YH{Looks good to me.}  The proof of the
  % translation from maps to case-statements follows by induction over the list
  % of elements in the map and the fact that each key will be unique.  In
  % addition to that, the statement that is currently being evaluated is
  % guaranteed by the correctness of the list of elements to be in that list.
  % The latter fact therefore eliminates the base case, as an empty list does
  % not contain the element we know is in the list.  The other two cases follow
  % by the fact that either the key is equal to the evaluated value of the case
  % expression, or it isn't.  In the first case we can then evaluate the
  % statement and get the state after the case expression, as the uniqueness of
  % the key tells us that the key cannot show up in the list anymore.  In the
  % other case we can just apply the inductive hypothesis and remove the current
  % case from the case statement, as it did not match.
\end{proof}

%\subsection{Coq Mechanisation}

%\JW{Would be nice to include a few high-level metrics here. How many person-years of effort was the proof (very roughly)? How many lines of Coq? How many files, how many lemmas? How long does it take for the Coq proof to execute?}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% TeX-engine: luatex
%%% End:
