@Comment{
ebib-main-file: /home/ymh/Dropbox/bibliography/references.bib
}


@inproceedings{aagaard91_fvsls,
	author = {Aagaard, Mark and Leeser, Miriam},
	organization = {IEEE},
	booktitle = {[1991 Proceedings] IEEE International Conference on Computer Design: VLSI in Computers and Processors},
	keywords = {verification,synthesis},
	pages = {346--350},
	title = {A Formally Verified System for Logic Synthesis},
	year = {1991}
}

@article{abdulla21_decid_reach_persis_x86_tso,
	abstract = {We address the problem of verifying the reachability problem in programs running under the formal model Px86 defined recently by Raad et al. in POPL'20 for the persistent Intel x86 architecture. We prove that this problem is decidable. To achieve that, we provide a new formal model that is equivalent to Px86 and that has the feature of being a well structured system. Deriving this new model is the result of a deep investigation of the properties of Px86 and the interplay of its components.},
	author = {Abdulla, Parosh Aziz and Atig, Mohamed Faouzi and Bouajjani, Ahmed and Kumar, K. Narayan and Saivasan, Prakash},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434337},
	doi = {10.1145/3434337},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {model checking,persistent memories,TSO memory model,program verification},
	month = jan,
	number = {POPL},
	title = {Deciding Reachability under Persistent X86-TSO},
	volume = {5},
	year = {2021}
}

@inproceedings{abir18_towar,
	author = {Abir, B. and Salah, M.},
	url = {https://doi.org/10.1109/PAIS.2018.8598527},
	booktitle = {2018 3rd International Conference on Pattern Analysis and Intelligent Systems (PAIS)},
	doi = {10.1109/PAIS.2018.8598527},
	keywords = {verification,cryptography,high-level synthesis},
	month = oct,
	pages = {1--6},
	title = {Towards formal verification of cryptographic circuits: A functional approach},
	year = {2018}
}

@article{abreu23_type_based_approac_divid_conquer_recur_coq,
	author = {Abreu, Pedro and Delaware, Benjamin and Hubers, Alex and Jenkins, Christa and Morris, J. Garrett and Stump, Aaron},
	title = {A Type-Based Approach to Divide-and-Conquer Recursion in Coq},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571196},
	doi = {10.1145/3571196},
	abstract = {This paper proposes a new approach to writing and verifying divide-and-conquer programs in Coq. Extending the rich line of previous work on algebraic approaches to recursion schemes, we present an algebraic approach to divide-and-conquer recursion: recursions are represented as a form of algebra, and from outer recursions, one may initiate inner recursions that can construct data upon which the outer recursions may legally recurse. Termination is enforced entirely by the typing discipline of our recursion schemes. Despite this, our approach requires little from the underlying type system, and can be implemented in System Fω plus a limited form of positive-recursive types. Our implementation of the method in Coq does not rely on structural recursion or on dependent types. The method is demonstrated on several examples, including mergesort, quicksort, Harper’s regular-expression matcher, and others. An indexed version is also derived, implementing a form of divide-and-conquer induction that can be used to reason about functions defined via our method.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {3},
	numpages = {30},
	keywords = {strong functional programming, well-founded recursion, Divide-and-conquer recursion}
}

@Software{absint19_compc,
	year = {2019},
	title = {{CompCert} release 19.10},
	author = {AbsInt},
	url = {https://www.absint.com/releasenotes/compcert/19.10/}
}

@Software{absint22_compc,
	url = {https://www.absint.com/releasenotes/compcert/22.10/},
	year = {2022},
	title = {{CompCert} release 22.10},
	author = {AbsInt}
}

@article{accattoli21_in_effic_inter,
	abstract = {Evaluating higher-order functional programs through abstract machines inspired by the geometry of the interaction is known to induce space efficiencies, the price being time performances often poorer than those obtainable with traditional, environment-based, abstract machines. Although families of lambda-terms for which the former is exponentially less efficient than the latter do exist, it is currently unknown how general this phenomenon is, and how far the inefficiencies can go, in the worst case. We answer these questions formulating four different well-known abstract machines inside a common definitional framework, this way being able to give sharp results about the relative time efficiencies. We also prove that non-idempotent intersection type theories are able to precisely reflect the time performances of the interactive abstract machine, this way showing that its time-inefficiency ultimately descends from the presence of higher-order types.},
	author = {Accattoli, Beniamino and Dal Lago, Ugo and Vanoni, Gabriele},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434332},
	doi = {10.1145/3434332},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {lambda-calculus,geometry of interaction,abstract machines},
	month = jan,
	number = {POPL},
	title = {The (In)Efficiency of Interaction},
	volume = {5},
	year = {2021}
}

@article{accattoli22_multi_types_reason_space,
	author = {Accattoli, Beniamino and Dal Lago, Ugo and Vanoni, Gabriele},
	title = {Multi Types and Reasonable Space},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3547650},
	doi = {10.1145/3547650},
	abstract = {Accattoli, Dal Lago, and Vanoni have recently proved that the space used by the Space KAM, a variant of the Krivine abstract machine, is a reasonable space cost model for the λ-calculus accounting for logarithmic space, solving a longstanding open problem. In this paper, we provide a new system of multi types (a variant of intersection types) and extract from multi type derivations the space used by the Space KAM, capturing into a type system the space complexity of the abstract machine. Additionally, we show how to capture also the time of the Space KAM, which is a reasonable time cost model, via minor changes to the type system.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {119},
	numpages = {27},
	keywords = {cost models, abstract machines, lambda calculus, space complexity, intersection types}
}

@article{accattoli22_theor_call_value_solvab,
	author = {Accattoli, Beniamino and Guerrieri, Giulio},
	title = {The Theory of Call-by-Value Solvability},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3547652},
	doi = {10.1145/3547652},
	abstract = {The semantics of the untyped (call-by-name) lambda-calculus is a well developed field built around the concept of solvable terms, which are elegantly characterized in many different ways. In particular, unsolvable terms provide a consistent notion of meaningless term. The semantics of the untyped call-by-value lambda-calculus (CbV) is instead still in its infancy, because of some inherent difficulties but also because CbV solvable terms are less studied and understood than in call-by-name. On the one hand, we show that a carefully crafted presentation of CbV allows us to recover many of the properties that solvability has in call-by-name, in particular qualitative and quantitative characterizations via multi types. On the other hand, we stress that, in CbV, solvability plays a different role: identifying unsolvable terms as meaningless induces an inconsistent theory.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {121},
	numpages = {31},
	keywords = {operational semantics, solvability, call-by-value, denotational semantics, intersection types, lambda calculus}
}

@article{adithya21_static_version_polyh_model,
	author = {Adithya, Dattari and Meister, Benoit},
	title = {{Static Versioning in the Polyhedral Model}},
	year = {2021}
}

@inproceedings{affeldt23_seman_probab_progr_using_s,
	author = {Affeldt, Reynald and Cohen, Cyril and Saito, Ayumu},
	title = {Semantics of Probabilistic Programs Using S-Finite Kernels in Coq},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575691},
	doi = {10.1145/3573105.3575691},
	abstract = {Probabilistic programming languages are used to write probabilistic models to make probabilistic inferences. A number of rigorous semantics have recently been proposed that are now available to carry out formal verification of probabilistic programs. In this paper, we extend an existing formalization of measure and integration theory with s-finite kernels, a mathematical structure to interpret typing judgments in the semantics of a probabilistic programming language. The resulting library makes it possible to reason formally about transformations of probabilistic programs and their execution.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {3–16},
	numpages = {14},
	keywords = {probabilistic programming language, measure theory, integration theory, Coq},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@inproceedings{agarwal02_specul_vliw,
	abstract = {VLIW processors are statically scheduled processors and their performance depends on the quality of schedules generated by the compiler's scheduler. We propose a new scheduling scheme where the application is first divided into decision trees and then further split into traces. Traces are speculatively scheduled on the processor based on their probability of execution. We have developed a tool "SpliTree" to generate traces automatically. By using dynamic branch prediction for scheduling traces our scheme achieves approximately 1.4/spl times/ performance improvement over that using decision trees for Spec92 benchmarks simulated on TriMedia/spl trade/.},
	author = {Agarwal, M. and Nandy, S. K.},
	booktitle = {Proceedings. IEEE International Conference on Computer Design: VLSI in Computers and Processors},
	doi = {10.1109/ICCD.2002.1106803},
	issn = {1063-6404},
	keywords = {hyperblocks,trace scheduling,static scheduling},
	month = sep,
	pages = {408--413},
	title = {Speculative trace scheduling in VLIW processors},
	year = {2002}
}

@article{aguirre21_pre_expec_calcul_probab_sensit,
	abstract = {Sensitivity properties describe how changes to the input of a program affect the output, typically by upper bounding the distance between the outputs of two runs by a monotone function of the distance between the corresponding inputs. When programs are probabilistic, the distance between outputs is a distance between distributions. The Kantorovich lifting provides a general way of defining a distance between distributions by lifting the distance of the underlying sample space; by choosing an appropriate distance on the base space, one can recover other usual probabilistic distances, such as the Total Variation distance. We develop a relational pre-expectation calculus to upper bound the Kantorovich distance between two executions of a probabilistic program. We illustrate our methods by proving algorithmic stability of a machine learning algorithm, convergence of a reinforcement learning algorithm, and fast mixing for card shuffling algorithms. We also consider some extensions: using our calculus to show convergence of Markov chains to the uniform distribution over states and an asynchronous extension to reason about pairs of program executions with different control flow.},
	author = {Aguirre, Alejandro and Barthe, Gilles and Hsu, Justin and Kaminski, Benjamin Lucien and Katoen, Joost-Pieter and Matheja, Christoph},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434333},
	doi = {10.1145/3434333},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {probabilistic programming,verification},
	month = jan,
	number = {POPL},
	title = {A Pre-Expectation Calculus for Probabilistic Sensitivity},
	volume = {5},
	year = {2021}
}

@article{aguirre23_step_index_logic_relat_count,
	author = {Aguirre, Alejandro and Birkedal, Lars},
	title = {Step-Indexed Logical Relations for Countable Nondeterminism and Probabilistic Choice},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571195},
	doi = {10.1145/3571195},
	abstract = {Developing denotational models for higher-order languages that combine probabilistic and nondeterministic choice is known to be very challenging. In this paper, we propose an alternative approach based on operational techniques. We study a higher-order language combining parametric polymorphism, recursive types, discrete probabilistic choice and countable nondeterminism. We define probabilistic generalizations of may- and must-termination as the optimal and pessimal probabilities of termination. Then we define step-indexed logical relations and show that they are sound and complete with respect to the induced contextual preorders. For may-equivalence we use step-indexing over the natural numbers whereas for must-equivalence we index over the countable ordinals. We then show than the probabilities of may- and must-termination coincide with the maximal and minimal probabilities of termination under all schedulers. Finally we derive the equational theory induced by contextual equivalence and show that it validates the distributive combination of the algebraic theories for probabilistic and nondeterministic choice.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {2},
	numpages = {28},
	keywords = {Logical Relations, Probabilistic programming, Functional Languages}
}

@article{ahman21_async_effec,
	abstract = {We explore asynchronous programming with algebraic effects. We complement their conventional synchronous treatment by showing how to naturally also accommodate asynchrony within them, namely, by decoupling the execution of operation calls into signalling that an operation’s implementation needs to be executed, and interrupting a running computation with the operation’s result, to which the computation can react by installing interrupt handlers. We formalise these ideas in a small core calculus, called λæ. We demonstrate the flexibility of λæ&nbsp;using examples ranging from a multi-party web application, to preemptive multi-threading, to remote function calls, to a parallel variant of runners of algebraic effects. In addition, the paper is accompanied by a formalisation of λæ’s type safety proofs in Agda, and a prototype implementation of λæ&nbsp;in OCaml.},
	author = {Ahman, Danel and Pretnar, Matija},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434305},
	doi = {10.1145/3434305},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {algebraic effects,asynchrony,signals,concurrency,interrupt handling},
	month = jan,
	number = {POPL},
	title = {Asynchronous Effects},
	volume = {5},
	year = {2021}
}

@inproceedings{ahrens22_implem_categ_theor_framew_typed_abstr_syntax,
	author = {Ahrens, Benedikt and Matthes, Ralph and M\"{o}rtberg, Anders},
	title = {Implementing a Category-Theoretic Framework for Typed Abstract Syntax},
	year = {2022},
	isbn = {9781450391825},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497775.3503678},
	doi = {10.1145/3497775.3503678},
	abstract = {In previous work ("From signatures to monads in UniMath"),we described a category-theoretic construction of abstract syntax from a signature, mechanized in the UniMath library based on the Coq proof assistant. In the present work, we describe what was necessary to generalize that work to account for simply-typed languages. First, some definitions had to be generalized to account for the natural appearance of non-endofunctors in the simply-typed case. As it turns out, in many cases our mechanized results carried over to the generalized definitions without any code change. Second, an existing mechanized library on 𝜔-cocontinuous functors had to be extended by constructions and theorems necessary for constructing multi-sorted syntax. Third, the theoretical framework for the semantical signatures had to be generalized from a monoidal to a bicategorical setting, again to account for non-endofunctors arising in the typed case. This uses actions of endofunctors on functors with given source, and the corresponding notion of strong functors between actions, all formalized in UniMath using a recently developed library of bicategory theory. We explain what needed to be done to plug all of these ingredients together, modularly. The main result of our work is a general construction that, when fed with a signature for a simply-typed language, returns an implementation of that language together with suitable boilerplate code, in particular, a certified monadic substitution operation.},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {307–323},
	numpages = {17},
	keywords = {monad, signature, typed abstract syntax, computer-checked proof, formalization},
	location = {Philadelphia, PA, USA},
	series = {CPP 2022}
}

@inbook{aiken16_trace_sched,
	abstract = {Since its introduction by Joseph A. Fisher in 1979, trace scheduling has influenced much of the work on compile-time ILP. Initially developed for use in microcode compaction, trace scheduling quickly became the main technique for machine-level compile-time parallelism exploitation. Trace scheduling has been used since the 1980s in many state-of-the-art compilers (e.g., Intel, Fujitsu, HP).},
	author = {Aiken, Alex and Banerjee, Utpal and Kejariwal, Arun and Nicolau, Alexandru},
	location = {Boston, MA},
	publisher = {Springer US},
	url = {https://doi.org/10.1007/978-1-4899-7797-7_4},
	booktitle = {Instruction Level Parallelism},
	doi = {10.1007/978-1-4899-7797-7_4},
	isbn = {978-1-4899-7797-7},
	keywords = {trace scheduling,static scheduling},
	pages = {79--116},
	title = {Trace Scheduling},
	year = {2016}
}

@ARTICLE{akers78_binar_decis_diagr,
	keywords = {boolean simplification, boolean logic},
	author = {Akers},
	journal = {IEEE Transactions on Computers},
	title = {Binary Decision Diagrams},
	year = {1978},
	volume = {C-27},
	number = {6},
	pages = {509-516},
	doi = {10.1109/TC.1978.1675141}
}

@article{alam21_formal_verif_datab_applic_using_predic_abstr,
	keywords = {abstract interpretation, predicate abstraction, coq},
	doi = {10.1007/s42979-020-00426-2},
	url = {https://doi.org/10.1007%2Fs42979-020-00426-2},
	year = 2021,
	month = {mar},
	publisher = {Springer Science and Business Media {LLC}},
	volume = {2},
	number = {3},
	author = {Md Imran Alam and Raju Halder},
	title = {Formal Verification of Database Applications Using Predicate Abstraction},
	journal = {{SN} Computer Science}
}

@inproceedings{ali17_explor_hls_optim_effic_stereo,
	abstract = {Nowadays, FPGA technology offers a tremendous number of logic cells on a single chip. Digital design for such huge hardware resources under time-to-market constraint urged the evolution of High Level Synthesis (HLS) tools. In this work, we will explore several HLS optimization steps in order to improve the system performance. Different design choices are obtained from our exploration such that an efficient implementation is selected based on given system constraints (resource utilization, power consumption, execution time, ...). Our exploration methodology is illustrated through a case study considering a Multi-Window Sum of Absolute Difference stereo matching algorithm. We implemented our design using Xilinx Zynq ZC706 FPGA evaluation board for gray images of size {\$}{\$}640{\backslash}times 480{\$}{\$}.},
	author = {Ali, Karim M. A. and Ben Atitallah, Rabie and Fakhfakh, Nizar and Dekeyser, Jean-Luc},
	editor = {Wong, Stephan and Beck, Antonio Carlos and Bertels, Koen and Carro, Luigi},
	location = {Cham},
	publisher = {Springer International Publishing},
	booktitle = {Applied Reconfigurable Computing},
	isbn = {978-3-319-56258-2},
	keywords = {high-level synthesis,compiler optimisation},
	pages = {168--176},
	title = {Exploring HLS Optimizations for Efficient Stereo Matching Hardware Implementation},
	year = {2017}
}

@article{allamanis17_learn_repres_progr_graph,
	author = {Allamanis, Miltiadis and Brockschmidt, Marc and Khademi, Mahmoud},
	url = {http://arxiv.org/abs/1711.00740},
	eprint = {1711.00740},
	eprinttype = {arXiv},
	journaltitle = {CoRR},
	title = {Learning to Represent Programs with Graphs},
	volume = {abs/1711.00740},
	year = {2017}
}

@report{allamanis18_survey_machin_learn_big_code_natur,
	abstract = {Research at the intersection of machine learning, programming languages, and software engineering has recently taken important steps in proposing learnable probabilistic models of source code that exploit code's abundance of patterns. In this article, we survey this work. We contrast programming languages against natural languages and discuss how these similarities and differences drive the design of probabilistic models. We present a taxonomy based on the underlying design principles of each model and use it to navigate the literature. Then, we review how researchers have adapted these models to application areas and discuss cross-cutting and application-specific challenges and opportunities.},
	author = {Allamanis, Miltiadis and Sutton, Charles},
	eprint = {1709.06182v2},
	eprinttype = {arXiv},
	keywords = {Additional Key Words and Phrases: Big Code,Code Naturalness,Software Engineering Tools,Machine Learn-ing,CCS Concepts: • Computing methodologies → Machine learning,Natural language processing,• General and reference → Surveys and overviews,• Soft-ware and its engineering → Software notations and tools},
	title = {{A Survey of Machine Learning for Big Code and Naturalness}},
	type = {techreport},
	year = {2018}
}

@inproceedings{allamigeon23_formal_dispr_hirsc_conjec,
	author = {Allamigeon, Xavier and Canu, Quentin and Strub, Pierre-Yves},
	title = {A Formal Disproof of Hirsch Conjecture},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575678},
	doi = {10.1145/3573105.3575678},
	abstract = {The purpose of this paper is the formal verification of a counterexample of Santos et al. to the so-called Hirsch Conjecture on the diameter of polytopes (bounded convex polyhedra). In contrast with the pen-and-paper proof, our approach is entirely computational: we have implemented in Coq&nbsp;and proved correct an algorithm that explicitly computes, within the proof assistant, vertex-edge graphs of polytopes as well as their diameter. The originality of this certificate-based algorithm is to achieve a tradeoff between simplicity and efficiency. Simplicity is crucial in obtaining the proof of correctness of the algorithm. This proof splits into the correctness of an abstract algorithm stated over proof-oriented data types and the correspondence with a low-level implementation over computation-oriented data types. A special effort has been made to reduce the algorithm to a small sequence of elementary operations (e.g. matrix multiplications, basic routines on graphs), in order to make the derivation of the correctness of the low-level implementation more transparent. Efficiency allows us to scale up to polytopes with a challenging combinatorics. For instance, we formally check the two counterexamples to Hirsch conjecture due to Matschke, Santos and Weibel, respectively 20- and 23-dimensional polytopes with 36425 and 73224 vertices involving rational coefficients with up to 20 digits. We also illustrate the performance of the method by computing the list of vertices or the diameter of well-known classes of polytopes, such as (polars of) cyclic polytopes involved in McMullen's Upper Bound Theorem.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {17–29},
	numpages = {13},
	keywords = {certified computation, Hirsch Conjecture, proof assistants, polytopes, polyhedra},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@inproceedings{allara97_flexib_model_evaluat_behav_hardw_softw_system,
	author = {Allara, A. and Filipponi, S. and Salice, F. and Fornaciari, W. and Sciuto, D.},
	location = {Washington, DC, USA},
	publisher = {IEEE Computer Society},
	url = {http://dl.acm.org/citation.cfm?id=792768.793516},
	booktitle = {Proceedings of the 5th International Workshop on Hardware/Software Co-Design},
	isbn = {0-8186-7895-X},
	keywords = {model,hardware/software co-simulation},
	pages = {109--},
	series = {CODES '97},
	title = {A Flexible Model for Evaluating the Behavior of Hardware/Software Systems},
	year = {1997}
}

@inproceedings{allen70_cfa,
	keywords = {control-flow},
	author = {Allen, Frances E.},
	title = {Control Flow Analysis},
	year = {1970},
	isbn = {9781450373869},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/800028.808479},
	abstract = {Any static, global analysis of the expression and data relationships in a program requires a knowledge of the control flow of the program. Since one of the primary reasons for doing such a global analysis in a compiler is to produce optimized programs, control flow analysis has been embedded in many compilers and has been described in several papers. An early paper by Prosser [5] described the use of Boolean matrices (or, more particularly, connectivity matrices) in flow analysis. The use of “dominance” relationships in flow analysis was first introduced by Prosser and much expanded by Lowry and Medlock [6]. References [6,8,9] describe compilers which use various forms of control flow analysis for optimization. Some recent developments in the area are reported in [4] and in [7].The underlying motivation in all the different types of control flow analysis is the need to codify the flow relationships in the program. The codification may be in connectivity matrices, in predecessor-successor tables, in dominance lists, etc. Whatever the form, the purpose is to facilitate determining what the flow relationships are; in other words to facilitate answering such questions as: is this an inner loop?, if an expression is removed from the loop where can it be correctly and profitably placed?, which variable definitions can affect this use?In this paper the basic control flow relationships are expressed in a directed graph. Various graph constructs are then found and shown to codify interesting global relationships.},
	booktitle = {Proceedings of a Symposium on Compiler Optimization},
	pages = {1–19},
	numpages = {19},
	location = {Urbana-Champaign, Illinois}
}

@inproceedings{allen83_conver_contr_depen_data_depen,
	abstract = {Program analysis methods, especially those which support automatic vectorization, are based on the concept of interstatement dependence where a dependence holds between two statements when one of the statements computes values needed by the other. Powerful program transformation systems that convert sequential programs to a form more suitable for vector or parallel machines have been developed using this concept [AllK 82, KKLW 80].The dependence analysis in these systems is based on data dependence. In the presence of complex control flow, data dependence is not sufficient to transform programs because of the introduction of control dependences. A control dependence exists between two statements when the execution of one statement can prevent the execution of the other. Control dependences do not fit conveniently into dependence-based program translators.One solution is to convert all control dependences to data dependences by eliminating goto statements and introducing logical variables to control the execution of statements in the program. In this scheme, action statements are converted to IF statements. The variables in the conditional expression of an IF statement can be viewed as inputs to the statement being controlled. The result is that control dependences between statements become explicit data dependences expressed through the definitions and uses of the controlling logical variables.This paper presents a method for systematically converting control dependences to data dependences in this fashion. The algorithms presented here have been implemented in PFC, an experimental vectorizer written at Rice University.},
	author = {Allen, J. R. and Kennedy, Ken and Porterfield, Carrie and Warren, Joe},
	location = {Austin, Texas},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 10th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
	doi = {10.1145/567067.567085},
	isbn = {0897910907},
	keywords = {if-conversion},
	pages = {177--189},
	series = {POPL '83},
	title = {Conversion of Control Dependence to Data Dependence},
	year = {1983}
}

@article{almagor21_decid_regul_proper_linear_recur_sequen,
	abstract = {We consider the problem of deciding ω-regular properties on infinite traces produced by linear loops. Here we think of a given loop as producing a single infinite trace that encodes information about the signs of program variables at each time step. Formally, our main result is a procedure that inputs a prefix-independent ω-regular property and a sequence of numbers satisfying a linear recurrence, and determines whether the sign description of the sequence (obtained by replacing each positive entry with “+”, each negative entry with “−”, and each zero entry with “0”) satisfies the given property. Our procedure requires that the recurrence be simple, i.e., that the update matrix of the underlying loop be diagonalisable. This assumption is instrumental in proving our key technical lemma: namely that the sign description of a simple linear recurrence sequence is almost periodic in the sense of Muchnik, Sem'enov, and Ushakov. To complement this lemma, we give an example of a linear recurrence sequence whose sign description fails to be almost periodic. Generalising from sign descriptions, we also consider the verification of properties involving semi-algebraic predicates on program variables.},
	author = {Almagor, Shaull and Karimov, Toghrul and Kelmendi, Edon and Ouaknine, Joël and Worrell, James},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434329},
	doi = {10.1145/3434329},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {linear loops,almost periodic words,linear recurrence sequences,omega-regular properties},
	month = jan,
	number = {POPL},
	title = {Deciding ω-Regular Properties on Linear Recurrence Sequences},
	volume = {5},
	year = {2021}
}

@inproceedings{almeida17_jasmin,
	author = {Almeida, Jos\'{e} Bacelar and Barbosa, Manuel and Barthe, Gilles and Blot, Arthur and Gr\'{e}goire, Benjamin and Laporte, Vincent and Oliveira, Tiago and Pacheco, Hugo and Schmidt, Benedikt and Strub, Pierre-Yves},
	title = {Jasmin: High-Assurance and High-Speed Cryptography},
	year = {2017},
	isbn = {9781450349468},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3133956.3134078},
	doi = {10.1145/3133956.3134078},
	abstract = {Jasmin is a framework for developing high-speed and high-assurance cryptographic software. The framework is structured around the Jasmin programming language and its compiler. The language is designed for enhancing portability of programs and for simplifying verification tasks. The compiler is designed to achieve predictability and efficiency of the output code (currently limited to x64 platforms), and is formally verified in the Coq proof assistant. Using the supercop framework, we evaluate the Jasmin compiler on representative cryptographic routines and conclude that the code generated by the compiler is as efficient as fast, hand-crafted, implementations. Moreover, the framework includes highly automated tools for proving memory safety and constant-time security (for protecting against cache-based timing attacks). We also demonstrate the effectiveness of the verification tools on a large set of cryptographic routines.},
	booktitle = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
	pages = {1807–1823},
	numpages = {17},
	keywords = {verification, cryptography, predictable execution, constant-time},
	location = {Dallas, Texas, USA},
	series = {CCS '17}
}

@inproceedings{alpern88_detec_equal_variab_progr,
	author = {Alpern, B. and Wegman, M. N. and Zadeck, F. K.},
	location = {San Diego, California, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/73560.73561},
	booktitle = {Proceedings of the 15th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
	doi = {10.1145/73560.73561},
	isbn = {0897912527},
	keywords = {gated-SSA,SSA},
	pages = {1--11},
	series = {POPL '88},
	title = {Detecting Equality of Variables in Programs},
	year = {1988}
}

@article{alur23_robus_theor_series_paral_graph,
	author = {Alur, Rajeev and Stanford, Caleb and Watson, Christopher},
	title = {A Robust Theory of Series Parallel Graphs},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571230},
	doi = {10.1145/3571230},
	abstract = {Motivated by distributed data processing applications, we introduce a class of labeled directed acyclic graphs constructed using sequential and parallel composition operations, and study automata and logics over them. We show that deterministic and non-deterministic acceptors over such graphs have the same expressive power, which can be equivalently characterized by Monadic Second-Order logic and the graded µ-calculus. We establish closure under composition operations and decision procedures for membership, emptiness, and inclusion. A key feature of our graphs, called synchronized series-parallel graphs (SSPG), is that parallel composition introduces a synchronization edge from the newly introduced source vertex to the sink. The transfer of information enabled by such edges is crucial to the determinization construction, which would not be possible for the traditional definition of series-parallel graphs. SSPGs allow both ordered ranked parallelism and unordered unranked parallelism. The latter feature means that in the corresponding automata, the transition function needs to account for an arbitrary number of predecessors by counting each type of state only up to a specified constant, thus leading to a notion of counting complexity that is distinct from the classical notion of state complexity. The determinization construction translates a nondeterministic automaton with n states and k counting complexity to a deterministic automaton with 2n2 states and kn counting complexity, and both these bounds are shown to be tight. Furthermore, for nondeterministic automata a bound of 2 on counting complexity suffices without loss of expressiveness.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {37},
	numpages = {31},
	keywords = {logic in computer science, regular languages, series-parallel graphs, distributed stream processing}
}

@InProceedings{amadini20_abstr_inter_symbol_execut_const,
	author = {Roberto Amadini and Graeme Gange and Peter Schachte and Harald S{\o}ndergaard and Peter J. Stuckey},
	title = {{Abstract Interpretation, Symbolic Execution and Constraints}},
	booktitle = {Recent Developments in the Design and Implementation of Programming Languages},
	pages = {7:1--7:19},
	series = {OpenAccess Series in Informatics (OASIcs)},
	ISBN = {978-3-95977-171-9},
	ISSN = {2190-6807},
	year = {2020},
	volume = {86},
	editor = {Frank S. de Boer and Jacopo Mauro},
	publisher = {Schloss Dagstuhl--Leibniz-Zentrum f{\"u}r Informatik},
	address = {Dagstuhl, Germany},
	URL = {https://drops.dagstuhl.de/opus/volltexte/2020/13229/pdf/OASIcs-Gabbrielli-7.pdf},
	URN = {urn:nbn:de:0030-drops-132294},
	doi = {10.4230/OASIcs.Gabbrielli.7},
	annote = {Keywords: Abstract interpretation, symbolic execution, constraint solving, dynamic analysis, static analysis}
}

@inproceedings{ambal22_certif_abstr_machin_skelet_seman,
	author = {Ambal, Guillaume and Lenglet, Sergue\"{\i} and Schmitt, Alan},
	title = {Certified Abstract Machines for Skeletal Semantics},
	year = {2022},
	isbn = {9781450391825},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497775.3503676},
	doi = {10.1145/3497775.3503676},
	abstract = {Skeletal semantics is a framework to describe semantics of programming languages. We propose an automatic generation of a certified OCaml interpreter for any language written in skeletal semantics. To this end, we introduce two new interpretations, i.e., formal meanings, of skeletal semantics, in the form of non-deterministic and deterministic abstract machines. These machines are derived from the usual big-step interpretation of skeletal semantics using functional correspondence, a standard transformation from big-step evaluators to abstract machines. All these interpretations are formalized in the Coq proof assistant and we certify their soundness. We finally use the extraction from Coq to OCaml to obtain the certified interpreter.},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {55–67},
	numpages = {13},
	keywords = {Operational Semantics, Abstract Machines, Skeletal Semantics, Certified Interpretation},
	location = {Philadelphia, PA, USA},
	series = {CPP 2022}
}

@misc{amd23_vitis_forum,
	author = {AMD},
	title = {Vitis Forums},
	url = {https://bit.ly/vitisifc},
	urldate = {2023-06-02},
	year = {2023},
	note = {Relevant quote from AMD: ``If-Conversion aims to convert a sequence of blocks into a single block for better optimization result.''}
}

@misc{amd23_vitis_high_synth,
	author = {AMD},
	title = {Vitis High-level Synthesis},
	url = {https://bit.ly/41R0204},
	urldate = {2023-05-21},
	year = 2023
}

@InProceedings{anand08_deman_driven_compos_symbol_execut,
	keywords = {value summaries, hyperblocks, symbolic execution},
	author = {Anand, Saswat and Godefroid, Patrice and Tillmann, Nikolai},
	editor = "Ramakrishnan, C. R.
and Rehof, Jakob",
	title = "Demand-Driven Compositional Symbolic Execution",
	booktitle = "Tools and Algorithms for the Construction and Analysis of Systems",
	year = "2008",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "367--381",
	abstract = "We discuss how to perform symbolic execution of large programs in a manner that is both compositional (hence more scalable) and demand-driven. Compositional symbolic execution means finding feasible interprocedural program paths by composing symbolic executions of feasible intraprocedural paths. By demand-driven, we mean that as few intraprocedural paths as possible are symbolically executed in order to form an interprocedural path leading to a specific target branch or statement of interest (like an assertion). A key originality of this work is that our demand-driven compositional interprocedural symbolic execution is performed entirely using first-order logic formulas solved with an off-the-shelf SMT (Satisfiability-Modulo-Theories) solver -- no procedure in-lining or custom algorithm is required for the interprocedural part. This allows a uniform and elegant way of summarizing procedures at various levels of detail and of composing those using logic formulas.",
	isbn = "978-3-540-78800-3"
}

@inproceedings{ancourt91_scann_polyh_do_loops,
	author = {Ancourt, Corinne and Irigoin, François},
	location = {Williamsburg, Virginia, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/109625.109631},
	booktitle = {Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
	doi = {10.1145/109625.109631},
	isbn = {0897913906},
	keywords = {polyhedral model,polyhedral analysis},
	pages = {39--50},
	series = {PPOPP '91},
	title = {Scanning Polyhedra with DO Loops},
	year = {1991}
}

@inproceedings{andler79_predic_path_expres,
	keywords = {predicated execution, path expressions, gated-SSA, if-conversion},
	author = {Andler, Sten},
	title = {Predicate Path Expressions},
	year = {1979},
	isbn = {9781450373579},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/567752.567774},
	doi = {10.1145/567752.567774},
	abstract = {Path expressions are a tool for synchronization of concurrent processes. They are an integral part of the data abstraction mechanism in a programming language, and specify synchronization entirely in terms of the allowable sequences of operations on an object of the abstract data type. This paper describes an attempt to push the path expression synchronization construct along three dimensions - specification, verification, and implementation - into a useful theoretical and practical tool. We define Predicate Path Expressions (PPEs), which allow for a more convenient specification of many synchronization problems. The predicate is a powerful extension to path expressions that increases their expressiveness. We formally define the semantics of PPEs by a transformation to a corresponding nondeterministic program, thus allowing the use of known verification techniques for nondeterministic programs to be used for proving properties of the PPE and the data abstraction of which it is a part. We also describe our existing implementation, in Algol 68, of a data abstraction mechanism that incorporates PPEs.},
	booktitle = {Proceedings of the 6th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
	pages = {226–236},
	numpages = {11},
	location = {San Antonio, Texas},
	series = {POPL '79}
}

@inproceedings{ando95_uncon,
	abstract = {Speculative execution is execution of instructions before it is known whether these instructions should be executed. Compiler-based speculative execution has the potential to achieve both a high instruction per cycle rate and high clock rate. Pure compiler-based approaches, however have greatly limited instruction scheduling due to a limited ability to handle side effects of speculative execution. Significant performance improvement is, thus, difficult in non-numerical applications. This paper proposes a new architectural mechanism, called predicating, which provides unconstrained speculative execution. Predicating removes restrictions which limit the compiler's ability to schedule instructions. Through our hardware support, the compiler is allowed to move instructions past multiple basic block boundaries from any succeeding control path. Predicating buffers the side effects of speculative execution with its predicate, and the buffered predicate efficiently commits or squashes the side effects. The mechanism also provides a speculative exception handling scheme. The scheme, called the future condition properly postpones speculative exceptions and efficiently restarts the process. We show that our mechanism can be implemented through a modest amount of hardware with little complexity. The evaluation results show that our mechanism significantly improves performane, and achieves a 2.45x speedup over scalar machines.},
	author = {Ando, H. and Nakanishi, C. and Hara, T. and Nakaya, M.},
	booktitle = {Proceedings 22nd Annual International Symposium on Computer Architecture},
	doi = {10.1145/223982.224367},
	issn = {1063-6897},
	keywords = {predicated execution,speculative execution},
	month = jun,
	pages = {126--137},
	title = {Unconstrained speculative execution with predicated state buffering},
	year = {1995}
}

@article{andreasen17_survey_dynam_analy_test_gener_javas,
	author = {Andreasen, Esben and Gong, Liang and M\o{}ller, Anders and Pradel, Michael and Selakovic, Marija and Sen, Koushik and Staicu, Cristian-Alexandru},
	title = {A Survey of Dynamic Analysis and Test Generation for JavaScript},
	year = {2017},
	issue_date = {September 2018},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {50},
	number = {5},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3106739},
	doi = {10.1145/3106739},
	abstract = {JavaScript has become one of the most prevalent programming languages. Unfortunately, some of the unique properties that contribute to this popularity also make JavaScript programs prone to errors and difficult for program analyses to reason about. These properties include the highly dynamic nature of the language, a set of unusual language features, a lack of encapsulation mechanisms, and the “no crash” philosophy. This article surveys dynamic program analysis and test generation techniques for JavaScript targeted at improving the correctness, reliability, performance, security, and privacy of JavaScript-based software.},
	journal = {ACM Comput. Surv.},
	month = {sep},
	articleno = {66},
	numpages = {36},
	keywords = {value summaries, symbolic execution, hyperblocks}
}

@MastersThesis{andreazzi23_type_safe_hdl_verif_coq,
	keywords = {hardware description language},
	url = {https://escholarship.mcgill.ca/concern/theses/jm214v69x},
	year = {2023},
	school = {McGill University},
	title = {A Type-Safe HDL Verified in Coq},
	author = {Andreazzi Tavante, Hanneli Carolina}
}

@article{angiuli21_inter_repres_indep_unival,
	abstract = {In their usual form, representation independence metatheorems provide an external guarantee that two implementations of an abstract interface are interchangeable when they are related by an operation-preserving correspondence. If our programming language is dependently-typed, however, we would like to appeal to such invariance results within the language itself, in order to obtain correctness theorems for complex implementations by transferring them from simpler, related implementations. Recent work in proof assistants has shown that Voevodsky's univalence principle allows transferring theorems between isomorphic types, but many instances of representation independence in programming involve non-isomorphic representations. In this paper, we develop techniques for establishing internal relational representation independence results in dependent type theory, by using higher inductive types to simultaneously quotient two related implementation types by a heterogeneous correspondence between them. The correspondence becomes an isomorphism between the quotiented types, thereby allowing us to obtain an equality of implementations by univalence. We illustrate our techniques by considering applications to matrices, queues, and finite multisets. Our results are all formalized in Cubical Agda, a recent extension of Agda which supports univalence and higher inductive types in a computationally well-behaved way.},
	author = {Angiuli, Carlo and Cavallo, Evan and Mörtberg, Anders and Zeuner, Max},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434293},
	doi = {10.1145/3434293},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {Representation Independence,Higher Inductive Types,Proof Assistants,Univalence,Cubical Type Theory},
	month = jan,
	number = {POPL},
	title = {Internalizing Representation Independence with Univalence},
	volume = {5},
	year = {2021}
}

@article{ansótegui16_autom,
	title = {Automated theorem provers for multiple-valued logics with satisfiability modulo theory solvers},
	journal = {Fuzzy Sets and Systems},
	volume = {292},
	pages = {32-48},
	year = {2016},
	note = {Special Issue in Honor of Francesc Esteva on the Occasion of his 70th Birthday},
	issn = {0165-0114},
	doi = {https://doi.org/10.1016/j.fss.2015.04.011},
	url = {https://www.sciencedirect.com/science/article/pii/S0165011415002080},
	author = {Carlos Ansótegui and Miquel Bofill and Felip Manyà and Mateu Villaret},
	keywords = {Multiple-valued logics, Automated theorem provers, SMT, Benchmarks},
	abstract = {There is a relatively large number of papers dealing with complexity and proof theory issues of multiple-valued logics. Nevertheless, little attention has been paid so far to the development of efficient and robust solvers for such logics. In this paper we investigate how the technology of Satisfiability Modulo Theories (SMT) can be effectively used to build efficient automated theorem provers for relevant finitely-valued and infinitely-valued logics, taking the logics of Łukasiewicz, Gödel and Product as case studies. Besides, we report on an experimental investigation that evaluates the performance of SMT technology when solving multiple-valued logic problems, and compares the finitely-valued solvers for Łukasiewicz and Gödel logics with their infinitely-valued solvers from a computational point of view. We also compare the performance of SMT technology and MIP technology when testing the satisfiability on a genuine family of multiple-valued clausal forms.}
}

@article{antonopoulos23_algeb_align_relat_verif,
	author = {Antonopoulos, Timos and Koskinen, Eric and Le, Ton Chanh and Nagasamudram, Ramana and Naumann, David A. and Ngo, Minh},
	title = {An Algebra of Alignment for Relational Verification},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571213},
	doi = {10.1145/3571213},
	abstract = {Relational verification encompasses information flow security, regression verification, translation validation for compilers, and more. Effective alignment of the programs and computations to be related facilitates use of simpler relational invariants and relational procedure specs, which in turn enables automation and modular reasoning. Alignment has been explored in terms of trace pairs, deductive rules of relational Hoare logics (RHL), and several forms of product automata. This article shows how a simple extension of Kleene Algebra with Tests (KAT), called BiKAT, subsumes prior formulations, including alignment witnesses for forall-exists properties, which brings to light new RHL-style rules for such properties. Alignments can be discovered algorithmically or devised manually but, in either case, their adequacy with respect to the original programs must be proved; an explicit algebra enables constructive proof by equational reasoning. Furthermore our approach inherits algorithmic benefits from existing KAT-based techniques and tools, which are applicable to a range of semantic models.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {20},
	numpages = {31},
	keywords = {program algebra, hyperproperties, relational verification, Kleene algebra with tests}
}

@inproceedings{appel11_verif_softw_toolc,
	doi = {10.1007/978-3-642-19718-5_1},
	abstract = {The software toolchain includes static analyzers to check assertions about programs; optimizing compilers to translate programs to machine language; operating systems and libraries to supply context for programs. Our Verified Software Toolchain verifies with machine-checked proofs that the assertions claimed at the top of the toolchain really hold in the machine-language program, running in the operating-system context, on a weakly-consistent-shared-memory machine.},
	author = {Appel, Andrew W.},
	editor = {Barthe, Gilles},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Programming Languages and Systems},
	isbn = {978-3-642-19718-5},
	pages = {1--17},
	title = {Verified Software Toolchain},
	year = {2011}
}

@inproceedings{appel22_coq_vibran_ecosy_verif_engin_invit_talk,
	author = {Appel, Andrew W.},
	title = {Coq’s Vibrant Ecosystem for Verification Engineering (Invited Talk)},
	year = {2022},
	isbn = {9781450391825},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497775.3503951},
	doi = {10.1145/3497775.3503951},
	abstract = {Program verification in the large is not only a matter of mechanizing a program logic to handle the semantics of your programming language. You must reason in the mathematics of your application domain--and there are many application domains, each with their own community of domain experts. So you will need to import mechanized proof theories from many domains, and they must all interoperate. Such an ecosystem is not only a matter of mathematics, it is a matter of software process engineering and social engineering. Coq's ecosystem has been maturing nicely in these senses.},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {2–11},
	numpages = {10},
	keywords = {None, nil},
	location = {Philadelphia, PA, USA},
	series = {CPP 2022}
}

@inproceedings{arasu23_fastv,
	author = {Arasu, Arvind and Ramananandro, Tahina and Rastogi, Aseem and Swamy, Nikhil and Fromherz, Aymeric and Hietala, Kesha and Parno, Bryan and Ramamurthy, Ravi},
	title = {FastVer2: A Provably Correct Monitor for Concurrent, Key-Value Stores},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575687},
	doi = {10.1145/3573105.3575687},
	abstract = {FastVer is a protocol that uses a variety of memory-checking techniques to monitor the integrity of key-value stores with only a modest runtime cost. Arasu et al. formalize the high-level design of FastVer in the F* proof assistant and prove it correct. However, their formalization did not yield a provably correct implementation---FastVer is implemented in unverified C++ code. In this work, we present FastVer2, a low-level, concurrent implementation of FastVer in Steel, an F* DSL based on concurrent separation logic that produces C code, and prove it correct with respect to FastVer's high-level specification. Our proof is the first end-to-end system proven using Steel, and in doing so we contribute new ghost-state constructions for reasoning about monotonic state. Our proof also uncovered a few bugs in the implementation of FastVer. We evaluate FastVer2 by comparing it against FastVer. Although our verified monitor is slower in absolute terms than the unverified code, its performance also scales linearly with the number of cores, yielding a throughput of more that 10M op/sec. We identify several opportunities for performance improvement, and expect to address these in the future.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {30–46},
	numpages = {17},
	keywords = {runtime monitors, separation logic, authenticated data structures, concurrency proofs},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@inproceedings{arenaz08_effic_build_gated_singl_assig,
	abstract = {Understanding program behavior is at the foundation of program optimization. Techniques for automatic recognition of program constructs characterize the behavior of code fragments, providing compilers with valuable information to guide code optimizations. The XARK compiler framework provides a complete, robust and extensible solution to the automatic recognition problem that was shown to be effective to characterize the behavior of Fortran77 applications. Our goal is to migrate XARK to the GNU GCC compiler in order to widen its scope of application to program constructs (e.g., pointers, objects) supported by other programming languages (e.g., Fortran90/95, C/C++, Java). The first step towards this goal is the translation of the GCC intermediate representation into the Gated Single Assignment (GSA) form, an extension of Static Single Assignment (SSA) that captures data/control dependences and reaching definition information for scalar and array variables. This paper presents a simple and fast GSA construction algorithm that takes advantage of the infrastructure for building the SSA form available in modern optimizing compilers. An implementation on top of the GIMPLE-SSA intermediate representation of GCC is described and evaluated in terms of memory consumption and execution time using the UTDSP, Perfect Club and SPEC CPU2000 benchmark suites.},
	author = {Arenaz, Manuel and Amoedo, Pedro and Touriño, Juan},
	editor = {Luque, Emilio and Margalef, Tomàs and Benítez, Domingo},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Euro-Par 2008 -- Parallel Processing},
	isbn = {978-3-540-85451-7},
	keywords = {gated-SSA,SSA},
	pages = {360--369},
	title = {Efficiently Building the Gated Single Assignment Form in Codes with Pointers in Modern Optimizing Compilers},
	year = {2008}
}

@inproceedings{armand11_modul_integ_sat_smt_solver,
	doi = {10.1007/978-3-642-25379-9_12},
	abstract = {We present a way to enjoy the power of SAT and SMT provers in Coq without compromising soundness. This requires these provers to return not only a yes/no answer, but also a proof witness that can be independently rechecked. We present such a checker, written and fully certified in Coq. It is conceived in a modular way, in order to tame the proofs' complexity and to be extendable. It can currently check witnesses from the SAT solver ZChaff and from the SMT solver veriT. Experiments highlight the efficiency of this checker. On top of it, new reflexive Coq tactics have been built that can decide a subset of Coq's logic by calling external provers and carefully checking their answers.},
	author = {Armand, Michael and Faure, Germain and Grégoire, Benjamin and Keller, Chantal and Théry, Laurent and Werner, Benjamin},
	editor = {Jouannaud, Jean-Pierre and Shao, Zhong},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Certified Programs and Proofs},
	isbn = {978-3-642-25379-9},
	keywords = {SAT,verification,coq},
	pages = {135--150},
	title = {A Modular Integration of SAT/SMT Solvers to Coq through Proof Witnesses},
	year = {2011}
}

@article{arora21_provab_space_effic_paral_funct_progr,
	abstract = {Because of its many desirable properties, such as its ability to control effects and thus potentially disastrous race conditions, functional programming offers a viable approach to programming modern multicore computers. Over the past decade several parallel functional languages, typically based on dialects of ML and Haskell, have been developed. These languages, however, have traditionally underperformed procedural languages (such as C and Java). The primary reason for this is their hunger for memory, which only grows with parallelism, causing traditional memory management techniques to buckle under increased demand for memory. Recent work opened a new angle of attack on this problem by identifying a memory property of determinacy-race-free parallel programs, called disentanglement, which limits the knowledge of concurrent computations about each other’s memory allocations. The work has showed some promise in delivering good time scalability. In this paper, we present provably space-efficient automatic memory management techniques for determinacy-race-free functional parallel programs, allowing both pure and imperative programs where memory may be destructively updated. We prove that for a program with sequential live memory of R*, any P-processor garbage-collected parallel run requires at most O(R* · P) memory. We also prove a work bound of O(W+R*P) for P-processor executions, accounting also for the cost of garbage collection. To achieve these results, we integrate thread scheduling with memory management. The idea is to coordinate memory allocation and garbage collection with thread scheduling decisions so that each processor can allocate memory without synchronization and independently collect a portion of memory by consulting a collection policy, which we formulate. The collection policy is fully distributed and does not require communicating with other processors. We show that the approach is practical by implementing it as an extension to the MPL compiler for Parallel ML. Our experimental results confirm our theoretical bounds and show that the techniques perform and scale well.},
	author = {Arora, Jatin and Westrick, Sam and Acar, Umut A.},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434299},
	doi = {10.1145/3434299},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {disentanglement,functional programming,memory management,parallel computing},
	month = jan,
	number = {POPL},
	title = {Provably Space-Efficient Parallel Functional Programming},
	volume = {5},
	year = {2021}
}

@article{arrial23_quant_inhab_differ_lambd_calcul_unify_framew,
	author = {Arrial, Victor and Guerrieri, Giulio and Kesner, Delia},
	title = {Quantitative Inhabitation for Different Lambda Calculi in a Unifying Framework},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571244},
	doi = {10.1145/3571244},
	abstract = {We solve the inhabitation problem for a language called λ!, a subsuming paradigm (inspired by call-by-push-value) being able to encode, among others, call-by-name and call-by-value strategies of functional programming. The type specification uses a non-idempotent intersection type system, which is able to capture quantitative properties about the dynamics of programs. As an application, we show how our general methodology can be used to derive inhabitation algorithms for different lambda-calculi that are encodable into λ!.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {51},
	numpages = {31},
	keywords = {call-by-push-value, inhabitation, lambda-calculus, quantitative types}
}

@inproceedings{asadi21_polyn_reach_witnes_stell,
	abstract = {We consider the fundamental problem of reachability analysis over imperative programs with real variables. Previous works that tackle reachability are either unable to handle programs consisting of general loops (e.g. symbolic execution), or lack completeness guarantees (e.g. abstract interpretation), or are not automated (e.g. incorrectness logic). In contrast, we propose a novel approach for reachability analysis that can handle general and complex loops, is complete, and can be entirely automated for a wide family of programs. Through the notion of Inductive Reachability Witnesses (IRWs), our approach extends ideas from both invariant generation and termination to reachability analysis. We first show that our IRW-based approach is sound and complete for reachability analysis of imperative programs. Then, we focus on linear and polynomial programs and develop automated methods for synthesizing linear and polynomial IRWs. In the linear case, we follow the well-known approaches using Farkas' Lemma. Our main contribution is in the polynomial case, where we present a push-button semi-complete algorithm. We achieve this using a novel combination of classical theorems in real algebraic geometry, such as Putinar's Positivstellensatz and Hilbert's Strong Nullstellensatz. Finally, our experimental results show we can prove complex reachability objectives over various benchmarks that were beyond the reach of previous methods.},
	author = {Asadi, Ali and Chatterjee, Krishnendu and Fu, Hongfei and Goharshady, Amir Kafshdar and Mahdavi, Mohammad},
	location = {Virtual, Canada},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3453483.3454076},
	booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
	doi = {10.1145/3453483.3454076},
	isbn = {9781450383912},
	keywords = {Inductive Reasoning,Stellensätze,Reachability},
	pages = {772--787},
	series = {PLDI 2021},
	title = {Polynomial Reachability Witnesses via StellensäTze},
	year = {2021}
}

@inproceedings{ashar98_verif_rtl,
	author = {Ashar, P. and Bhattacharya, S. and Raghunathan, A. and Mukaiyama, A.},
	booktitle = {1998 IEEE/ACM International Conference on Computer-Aided Design. Digest of Technical Papers (IEEE Cat. No.98CB36287)},
	doi = {10.1145/288548.289080},
	issn = {null},
	keywords = {verification,hardware scheduling,hardware,high-level synthesis},
	month = nov,
	pages = {517--524},
	title = {Verification of {RTL} generated from scheduled behavior in a high-level synthesis flow},
	year = {1998}
}

@INPROCEEDINGS{astolfi07_eaudd,
	keywords = {control-flow, data-flow},
	author = {Astolfi, Vitor Fiorotto and Silva, Jorge Luiz e},
	booktitle = {2007 3rd Southern Conference on Programmable Logic},
	title = {Execution of Algorithms Using a Dynamic Dataflow Model for Reconfigurable Hardware - Commands in Dataflow Graph},
	year = {2007},
	volume = {},
	number = {},
	pages = {225-230},
	doi = {10.1109/SPL.2007.371755}
}

@article{aubury96_handel_c_languag_refer_guide,
	author = {Aubury, Matthew and Page, Ian and Randall, Geoff and Saul, Jonathan and Watts, Robin},
	journaltitle = {Computing Laboratory. Oxford University, UK},
	keywords = {handel-c,synthesis},
	title = {Handel-C Language Reference Guide},
	year = {1996}
}

@thesis{august96_hyper_ilp,
	author = {August, David Isaac},
	institution = {University of Illinois at Urbana-Champaign},
	title = {Hyperblock performance optimizations for ILP processors},
	type = {mathesis},
	year = {1996}
}

@inproceedings{august97_framew_balan_contr_flow_predic,
	abstract = {Predicated execution is a promising architectural feature for exploiting instruction-level parallelism in the presence of control flow. Compiling for predicated execution involves converting program control flow into conditional, or predicated, instructions. This process is known as if-conversion. In order to effectively apply if-conversion, one must address two major issues: what should be if-converted and when the if-conversion should be applied. A compiler's use of predication as a representation is most effective when large amounts of code are if-converted and if-conversion is performed early in the compilation procedure. On the other hand the final code generated for a processor with predicated execution requires a delicate balance between control flow and predication to achieve efficient execution. The appropriate balance is tightly coupled with scheduling decisions and detailed processor characteristics. This paper presents an effective compilation framework that allows the compiler to maximize the benefits of predication as a compiler representation while delaying the final balancing of control flow and predication to schedule time.},
	author = {August, D. I. and Hwu, W. W. and Mahlke, S. A.},
	booktitle = {Proceedings of 30th Annual International Symposium on Microarchitecture},
	doi = {10.1109/MICRO.1997.645801},
	issn = {1072-4451},
	keywords = {predicated execution},
	month = dec,
	pages = {92--103},
	title = {A Framework for Balancing Control Flow and Predication},
	year = {1997}
}

@article{august99_partial_rever_if_conver_framew,
	keywords = {if-conversion, predicated execution, hyperblocks},
	title = {The Partial Reverse If-Conversion Framework for Balancing Control Flow and Predication},
	doi = {10.1023/a:1018787007582},
	url = {https://doi.org/10.1023%2Fa%3A1018787007582},
	year = 1999,
	publisher = {Springer Science and Business Media {LLC}},
	volume = {27},
	number = {5},
	pages = {381--423},
	author = {David I. August and Wen-Mei W. Hwu and Scott A. Mahlke},
	journal = {International Journal of Parallel Programming}
}

@inproceedings{august99_progr_decis_logic_approac_predic_execut,
	author = {August, David I. and Sias, John W. and Puiatti, Jean-Michel and Mahlke, Scott A. and Connors, Daniel A. and Crozier, Kevin M. and Hwu, Wen-mei W.},
	title = {The Program Decision Logic Approach to Predicated Execution},
	year = {1999},
	isbn = {0769501702},
	publisher = {IEEE Computer Society},
	address = {USA},
	url = {https://dl.acm.org/doi/pdf/10.1145/300979.300997},
	doi = {10.1145/300979.300997},
	abstract = {Modern compilers must expose sufficient amounts of Instruction-Level Parallelism (ILP) to achieve the promised performance increases of superscalar and VLIW processors. One of the major impediments to achieving this goal has been inefficient programmatic control flow. Historically, the compiler has translated the programmer's original control structure directly into assembly code with conditional branch instructions. Eliminating inefficiencies in handling branch instructions and exploiting ILP has been the subject of much research. However, traditional branch handling techniques cannot significantly alter the program's inherent control structure. The advent of predication as a program control representation has enabled compilers to manipulate control in a form more closely related to the underlying program logic. This work takes full advantage of the predication paradigm by abstracting the program control flow into a logical form referred to as a program decision logic network. This network is modeled as a Boolean equation and minimized using modified versions of logic synthesis techniques. After minimization, the more efficient version of the program's original control flow is re-expressed in predicated code. Furthermore, this paper proposes extensions to the HPL PlayDoh predication model in support of more effective predicate decision logic network minimization. Finally, this paper shows the ability of the mechanisms presented to overcome limits on ILP previously imposed by rigid program control structure.},
	booktitle = {Proceedings of the 26th Annual International Symposium on Computer Architecture},
	pages = {208–219},
	numpages = {12},
	location = {Atlanta, Georgia, USA},
	series = {ISCA '99}
}

@inproceedings{avigad22_verif_algeb_repres_cairo_progr_execut,
	author = {Avigad, Jeremy and Goldberg, Lior and Levit, David and Seginer, Yoav and Titelman, Alon},
	title = {A Verified Algebraic Representation of Cairo Program Execution},
	year = {2022},
	isbn = {9781450391825},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497775.3503675},
	doi = {10.1145/3497775.3503675},
	abstract = {Cryptographic interactive proof systems provide an efficient and scalable means of verifying the results of computation on blockchain. A prover constructs a proof, off-chain, that the execution of a program on a given input terminates with a certain result. The prover then publishes a certificate that can be verified efficiently and reliably modulo commonly accepted cryptographic assumptions. The method relies on an algebraic encoding of execution traces of programs. Here we report on a verification of the correctness of such an encoding of the Cairo model of computation with respect to the STARK interactive proof system, using the Lean 3 proof assistant.},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {153–165},
	numpages = {13},
	keywords = {formal verification, interactive proof systems, smart contracts},
	location = {Philadelphia, PA, USA},
	series = {CPP 2022}
}

@inproceedings{baanen23_formal_class_group_comput_integ,
	author = {Baanen, Anne and Best, Alex J. and Coppola, Nirvana and Dahmen, Sander R.},
	title = {Formalized Class Group Computations and Integral Points on Mordell Elliptic Curves},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575682},
	doi = {10.1145/3573105.3575682},
	abstract = {Diophantine equations are a popular and active area of research in number theory. In this paper we consider Mordell equations, which are of the form y2=x3+d, where d is a (given) nonzero integer number and all solutions in integers x and y have to be determined. One non-elementary approach for this problem is the resolution via descent and class groups. Along these lines we formalized in Lean 3 the resolution of Mordell equations for several instances of d&lt;0. In order to achieve this, we needed to formalize several other theories from number theory that are interesting on their own as well, such as ideal norms, quadratic fields and rings, and explicit computations of the class number. Moreover, we introduced new computational tactics in order to carry out efficiently computations in quadratic rings and beyond.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {47–62},
	numpages = {16},
	keywords = {tactics, Diophantine equations, formalized mathematics, algebraic number the- ory, Lean, Mathlib},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@article{bach23_hefty_algeb,
	author = {Bach Poulsen, Casper and van der Rest, Cas},
	title = {Hefty Algebras: Modular Elaboration of Higher-Order Algebraic Effects},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571255},
	doi = {10.1145/3571255},
	abstract = {Algebraic effects and handlers is an increasingly popular approach to programming with effects. An attraction of the approach is its modularity: effectful programs are written against an interface of declared operations, which allows the implementation of these operations to be defined and refined without changing or recompiling programs written against the interface. However, higher-order operations (i.e., operations that take computations as arguments) break this modularity. While it is possible to encode higher-order operations by elaborating them into more primitive algebraic effects and handlers, such elaborations are typically not modular. In particular, operations defined by elaboration are typically not a part of any effect interface, so we cannot define and refine their implementation without changing or recompiling programs. To resolve this problem, a recent line of research focuses on developing new and improved effect handlers. In this paper we present a (surprisingly) simple alternative solution to the modularity problem with higher-order operations: we modularize the previously non-modular elaborations commonly used to encode higher-order operations. Our solution is as expressive as the state of the art in effects and handlers.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {62},
	numpages = {31},
	keywords = {Algebraic Effects, Dependent Types, Agda, Reuse, Modularity}
}

@inproceedings{bachrach12_chisel,
	author = {Bachrach, Jonathan and Vo, Huy and Richards, Brian and Lee, Yunsup and Waterman, Andrew and Avižienis, Rimas and Wawrzynek, John and Asanović, Krste},
	organization = {IEEE},
	booktitle = {DAC Design Automation Conference 2012},
	doi = {https://doi.org/10.1145/2228360.2228584},
	pages = {1212--1221},
	title = {{Chisel: Constructing hardware in a Scala embedded language}},
	year = {2012}
}

@article{bahr21_diamon_are_not_forev,
	abstract = {When designing languages for functional reactive programming (FRP) the main challenge is to provide the user with a simple, flexible interface for writing programs on a high level of abstraction while ensuring that all programs can be implemented efficiently in a low-level language. To meet this challenge, a new family of modal FRP languages has been proposed, in which variants of Nakano's guarded fixed point operator are used for writing recursive programs guaranteeing properties such as causality and productivity. As an apparent extension to this it has also been suggested to use Linear Temporal Logic (LTL) as a language for reactive programming through the Curry-Howard isomorphism, allowing properties such as termination, liveness and fairness to be encoded in types. However, these two ideas are in conflict with each other, since the fixed point operator introduces non-termination into the inductive types that are supposed to provide termination guarantees. In this paper we show that by regarding the modal time step operator of LTL a submodality of the one used for guarded recursion (rather than equating them), one can obtain a modal type system capable of expressing liveness properties while retaining the power of the guarded fixed point operator. We introduce the language Lively RaTT, a modal FRP language with a guarded fixed point operator and an `until' type constructor as in LTL, and show how to program with events and fair streams. Using a step-indexed Kripke logical relation we prove operational properties of Lively RaTT including productivity and causality as well as the termination and liveness properties expected of types from LTL. Finally, we prove that the type system of Lively RaTT guarantees the absence of implicit space leaks.},
	author = {Bahr, Patrick and Graulund, Christian Uldal and Møgelberg, Rasmus Ejlers},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434283},
	doi = {10.1145/3434283},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {Type Systems,Modal Types,Synchronous Data Flow Languages,Functional Reactive Programming,Linear Temporal Logic},
	month = jan,
	number = {POPL},
	title = {Diamonds Are Not Forever: Liveness in Reactive Programming with Guarded Recursion},
	volume = {5},
	year = {2021}
}

@article{bahr22_monad_compil_calcul_funct_pearl,
	author = {Bahr, Patrick and Hutton, Graham},
	title = {Monadic Compiler Calculation (Functional Pearl)},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3547624},
	doi = {10.1145/3547624},
	abstract = {Bahr and Hutton recently developed a new approach to calculating correct compilers directly from specifications of their correctness. However, the methodology only considers converging behaviour of the source language, which means that the compiler could potentially produce arbitrary, erroneous code for source programs that diverge. In this article, we show how the methodology can naturally be extended to support the calculation of compilers that address both convergent and divergent behaviour simultaneously, without the need for separate reasoning for each aspect. Our approach is based on the use of the partiality monad to make divergence explicit, together with the use of strong bisimilarity to support equational-style calculations, but also generalises to other forms of effect by changing the underlying monad.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {93},
	numpages = {29},
	keywords = {bisimilarity, non-determinism, program calculation, divergence}
}

@book{baker19_princ,
	abstract = {An updated edition of the text that explores the core topics in scheduling theory The second edition of Principles of Sequencing and Scheduling has been revised and updated to provide comprehensive coverage of sequencing and scheduling topics as well as emerging developments in the field. The text offers balanced coverage of deterministic models and stochastic models and includes new developments in safe scheduling and project scheduling, including coverage of project analytics. These new topics help bridge the gap between classical scheduling and actual practice. The authors—noted experts in the field—present a coherent and detailed introduction to the basic models, problems, and methods of scheduling theory. This book offers an introduction and overview of sequencing and scheduling and covers such topics as single-machine and multi-machine models, deterministic and stochastic problem formulations, optimization and heuristic solution approaches, and generic and specialized software methods. This new edition adds coverage on topics of recent interest in shop scheduling and project scheduling. This important resource: Offers comprehensive coverage of deterministic models as well as recent approaches and developments for stochastic models Emphasizes the application of generic optimization software to basic sequencing problems and the use of spreadsheet-based optimization methods Includes updated coverage on safe scheduling, lognormal modeling, and job selection Provides basic coverage of robust scheduling as contrasted with safe scheduling Adds a new chapter on project analytics, which supports the PERT21 framework for project scheduling in a stochastic environment. Extends the coverage of PERT 21 to include hierarchical scheduling Provides end-of-chapter references and access to advanced Research Notes, to aid readers in the further exploration of advanced topics Written for upper-undergraduate and graduate level courses covering such topics as scheduling theory and applications, project scheduling, and operations scheduling, the second edition of Principles of Sequencing and Scheduling is a resource that covers scheduling techniques and contains the most current research and emerging topics.},
	author = {Baker, Kenneth R.},
	address = {Hoboken, NJ},
	booktitle = {Principles of sequencing and scheduling},
	edition = {Second edition.},
	isbn = {1-119-26259-3},
	keywords = {scheduling, list scheduling},
	language = {eng},
	publisher = {Wiley},
	series = {Wiley series in operations research and management science},
	title = {Principles of sequencing and scheduling },
	year = {2019}
}

@InProceedings{bakst16_predic_abstr_linked_data_struc,
	doi = {10.1007/978-3-662-49122-5_3},
	keywords = {predicate abstraction, abstract interpretation},
	author = {Bakst, Alexander and Jhala, Ranjit},
	editor = {Jobstmann, Barbara and Leino, K. Rustan M.},
	title = "Predicate Abstraction for Linked Data Structures",
	booktitle = "Verification, Model Checking, and Abstract Interpretation",
	year = "2016",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "65--84",
	abstract = "We present Alias Refinement Types (Art), a new approach that uses predicate-abstraction to automate the verification of correctness properties of linked data structures. While there are many techniques for checking that a heap-manipulating program adheres to its specification, they often require that the programmer annotate the behavior of each procedure, for example, in the form of loop invariants and pre- and post-conditions. We introduce a technique that lifts predicate abstraction to the heap by factoring the analysis of data structures into two orthogonal components: (1) Alias Types, which reason about the physical shape of heap structures, and (2) Refinement Types, which use simple predicates from an SMT decidable theory to capture the logical or semantic properties of the structures. We evaluate Art by implementing a tool that performs type inference for an imperative language, and empirically show, using a suite of data-structure benchmarks, that Art requires only 21 {\%} of the annotations needed by other state-of-the-art verification techniques.",
	isbn = "978-3-662-49122-5"
}

@article{baldoni18_survey_symbol_execut_techn,
	abstract = {Many security and software testing applications require checking whether certain properties of a program hold for any possible usage scenario. For instance, a tool for identifying software vulnerabilities may need to rule out the existence of any backdoor to bypass a program’s authentication. One approach would be to test the program using different, possibly random inputs. As the backdoor may only be hit for very specific program workloads, automated exploration of the space of possible inputs is of the essence. Symbolic execution provides an elegant solution to the problem, by systematically exploring many possible execution paths at the same time without necessarily requiring concrete inputs. Rather than taking on fully specified input values, the technique abstractly represents them as symbols, resorting to constraint solvers to construct actual instances that would cause property violations. Symbolic execution has been incubated in dozens of tools developed over the past four decades, leading to major practical breakthroughs in a number of prominent software reliability applications. The goal of this survey is to provide an overview of the main ideas, challenges, and solutions developed in the area, distilling them for a broad audience.},
	author = {Baldoni, Roberto and Coppa, Emilio and D’elia, Daniele Cono and Demetrescu, Camil and Finocchi, Irene},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3182657},
	doi = {10.1145/3182657},
	issn = {0360-0300},
	journaltitle = {ACM Comput. Surv.},
	keywords = {static analysis,symbolic execution, survey},
	month = may,
	number = {3},
	title = {A Survey of Symbolic Execution Techniques},
	volume = {51},
	year = {2018}
}

@inproceedings{ball93_branc_predic_free,
	keywords = {if-conversion},
	author = {Ball, Thomas and Larus, James R.},
	title = {Branch Prediction for Free},
	year = {1993},
	isbn = {0897915984},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/155090.155119},
	abstract = {Many compilers rely on branch prediction to improve program performance by identifying frequently executed regions and by aiding in scheduling instructions.Profile-based predictors require a time-consuming and inconvenient compile-profile-compile cycle in order to make predictions. We present a program-based branch predictor that performs well for a large and diverse set of programs written in C and Fortran. In addition to using natural loop analysis to predict branches that control the iteration of loops, we focus on heuristics for predicting non-loop branches, which dominate the dynamic branch count of many programs. The heuristics are simple and require little program analysis, yet they are effective in terms of coverage and miss rate. Although program-based prediction does not equal the accuracy of profile-based prediction, we believe it reaches a sufficiently high level to be useful. Additional type and semantic information available to a compiler would enhance our heuristics.},
	booktitle = {Proceedings of the ACM SIGPLAN 1993 Conference on Programming Language Design and Implementation},
	pages = {300–313},
	numpages = {14},
	location = {Albuquerque, New Mexico, USA},
	series = {PLDI '93}
}

@book{balyo23_proceed_sat_compet,
	title = "Proceedings of SAT Competition 2023: Solver, Benchmark and Proof Checker Descriptions",
	keywords = "113 Computer and information sciences",
	editor = "Tomas Balyo and Marijn Heule and Markus Iser and Matti J{\"a}rvisalo and Martin Suda",
	year = "2023",
	language = "English",
	series = "Department of Computer Science Series of Publications B",
	publisher = "Department of Computer Science, University of Helsinki",
	address = "Finland"
}

@article{banerjee14_verif_code_motion_techn_using_value_propag,
	author = {Banerjee, K. and Karfa, C. and Sarkar, D. and Mandal, C.},
	url = {https://doi.org/10.1109/TCAD.2014.2314392},
	doi = {10.1109/TCAD.2014.2314392},
	issn = {1937-4151},
	journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {translation validation,compiler optimisation,verification,high-level synthesis},
	month = aug,
	number = {8},
	pages = {1180--1193},
	title = {Verification of Code Motion Techniques Using Value Propagation},
	volume = {33},
	year = {2014}
}

@article{bao22_separ_logic_negat_depen,
	author = {Bao, Jialu and Gaboardi, Marco and Hsu, Justin and Tassarotti, Joseph},
	title = {A Separation Logic for Negative Dependence},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498719},
	doi = {10.1145/3498719},
	abstract = {Formal reasoning about hashing-based probabilistic data structures often requires reasoning about random variables where when one variable gets larger (such as the number of elements hashed into one bucket), the others tend to be smaller (like the number of elements hashed into the other buckets). This is an example of negative dependence, a generalization of probabilistic independence that has recently found interesting applications in algorithm design and machine learning. Despite the usefulness of negative dependence for the analyses of probabilistic data structures, existing verification methods cannot establish this property for randomized programs. To fill this gap, we design LINA, a probabilistic separation logic for reasoning about negative dependence. Following recent works on probabilistic separation logic using separating conjunction to reason about the probabilistic independence of random variables, we use separating conjunction to reason about negative dependence. Our assertion logic features two separating conjunctions, one for independence and one for negative dependence. We generalize the logic of bunched implications (BI) to support multiple separating conjunctions, and provide a sound and complete proof system. Notably, the semantics for separating conjunction relies on a non-deterministic, rather than partial, operation for combining resources. By drawing on closure properties for negative dependence, our program logic supports a Frame-like rule for negative dependence and monotone operations. We demonstrate how LINA can verify probabilistic properties of hash-based data structures and balls-into-bins processes.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {57},
	numpages = {29},
	keywords = {separation logic, negative dependence, Probabilistic programs}
}

@article{barbosa19_scalab_fine_grain_proof_formul_proces,
	keywords = {SAT, proof-carrying},
	doi = {10.1007/s10817-018-09502-y},
	url = {https://doi.org/10.1007%2Fs10817-018-09502-y},
	year = 2019,
	month = {jan},
	publisher = {Springer Science and Business Media {LLC}},
	volume = {64},
	number = {3},
	pages = {485--510},
	author = {Haniel Barbosa and Jasmin Christian Blanchette and Mathias Fleury and Pascal Fontaine},
	title = {Scalable Fine-Grained Proofs for Formula Processing},
	journal = {Journal of Automated Reasoning}
}

@InProceedings{barbosa22_cvc5,
	author = {Barbosa, Haniel and Barrett, Clark and Brain, Martin and Kremer, Gereon and Lachnitt, Hanna and Mann, Makai and Mohamed, Abdalrhman and Mohamed, Mudathir and Niemetz, Aina and N{\"o}tzli, Andres and Ozdemir, Alex and Preiner, Mathias and Reynolds, Andrew and Sheng, Ying and Tinelli, Cesare and Zohar, Yoni},
	editor = "Fisman, Dana
and Rosu, Grigore",
	title = {{cvc5}: A Versatile and Industrial-Strength SMT Solver},
	booktitle = "Tools and Algorithms for the Construction and Analysis of Systems",
	year = "2022",
	publisher = "Springer International Publishing",
	address = "Cham",
	pages = "415--442",
	abstract = "cvc5 is the latest SMT solver in the cooperating validity checker series and builds on the successful code base of CVC4. This paper serves as a comprehensive system description of cvc5 's architectural design and highlights the major features and components introduced since CVC4  1.8. We evaluate cvc5 's performance on all benchmarks in SMT-LIB and provide a comparison against CVC4 and Z3.",
	isbn = "978-3-030-99524-9"
}

@article{barendregt91_introd,
	author = {Barendregt, Henk},
	publisher = {Cambridge University Press},
	doi = {10.1017/S0956796800020025},
	journaltitle = {Journal of Functional Programming},
	number = {2},
	pages = {125--154},
	title = {Introduction to generalized type systems},
	volume = {1},
	year = {1991}
}

@article{barriere21_formal_verif_specul_deopt_jit_compil,
	abstract = {Just-in-time compilers for dynamic languages routinely generate code under assumptions that may be invalidated at run-time, this allows for specialization of program code to the common case in order to avoid unnecessary overheads due to uncommon cases. This form of software speculation requires support for deoptimization when some of the assumptions fail to hold. This paper presents a model just-in-time compiler with an intermediate representation that explicits the synchronization points used for deoptimization and the assumptions made by the compiler's speculation. We also present several common compiler optimizations that can leverage speculation to generate improved code. The optimizations are proved correct with the help of a proof assistant. While our work stops short of proving native code generation, we demonstrate how one could use the verified optimization to obtain significant speed ups in an end-to-end setting.},
	author = {Barrière, Aurèle and Blazy, Sandrine and Flückiger, Olivier and Pichardie, David and Vitek, Jan},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434327},
	doi = {10.1145/3434327},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {just-in-time compilation,verified compilation,CompCert compiler},
	month = jan,
	number = {POPL},
	title = {Formally Verified Speculation and Deoptimization in a JIT Compiler},
	volume = {5},
	year = {2021}
}

@article{barriere23_formal_verif_nativ_code_gener_effec_jit,
	author = {Barri\`{e}re, Aur\`{e}le and Blazy, Sandrine and Pichardie, David},
	title = {Formally Verified Native Code Generation in an Effectful JIT: Turning the CompCert Backend into a Formally Verified JIT Compiler},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571202},
	doi = {10.1145/3571202},
	abstract = {Modern Just-in-Time compilers (or JITs) typically interleave several mechanisms to execute a program. For faster startup times and to observe the initial behavior of an execution, interpretation can be initially used. But after a while, JITs dynamically produce native code for parts of the program they execute often. Although some time is spent compiling dynamically, this mechanism makes for much faster times for the remaining of the program execution. Such compilers are complex pieces of software with various components, and greatly rely on a precise interplay between the different languages being executed, including on-stack-replacement. Traditional static compilers like CompCert have been mechanized in proof assistants, but JITs have been scarcely formalized so far, partly due to their impure nature and their numerous components. This work presents a model JIT with dynamic generation of native code, implemented and formally verified in Coq. Although some parts of a JIT cannot be written in Coq, we propose a proof methodology to delimit, specify and reason on the impure effects of a JIT. We argue that the daunting task of formally verifying a complete JIT should draw on existing proofs of native code generation. To this end, our work successfully reuses CompCert and its correctness proofs during dynamic compilation. Finally, our prototype can be extracted and executed.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {9},
	numpages = {29},
	keywords = {CompCert compiler, verified compilation, just-in-time compilation}
}

@inproceedings{barthe12_formal_verif_ssa_based_middl_end,
	abstract = {CompCert is a formally verified compiler that generates compact and efficient PowerPC, ARM and x86 code for a large and realistic subset of the C language. However, CompCert foregoes using Static Single Assignment (SSA), an intermediate representation that allows for writing simpler and faster optimizers, and is used by many compilers. In fact, it has remained an open problem to verify formally a SSA-based compiler middle-end. We report on a formally verified, SSA-based, middle-end for CompCert. Our middle-end performs conversion from CompCert intermediate form to SSA form, optimization of SSA programs, including Global Value Numbering, and transforming out of SSA to intermediate form. In addition to provide the first formally verified SSA-based middle-end, we address two problems raised by Leroy [13]: giving a simple and intuitive formal semantics to SSA, and leveraging the global properties of SSA to reason locally about program optimizations.},
	author = {Barthe, Gilles and Demange, Delphine and Pichardie, David},
	editor = {Seidl, Helmut},
	location = {Berlin, Heidelberg},
	publisher = {Springer},
	booktitle = {Programming Languages and Systems},
	isbn = {978-3-642-28869-2},
	keywords = {CompCertSSA,CompCert,SSA,coq,verification,compiler optimisation},
	pages = {47--66},
	title = {A Formally Verified SSA-Based Middle-End},
	year = {2012}
}

@article{barthe14_formal_verif_ssa_based_middl_end_compc,
	abstract = {CompCert is a formally verified compiler that generates compact and efficient code for a large subset of the C language. However, CompCert foregoes using SSA, an intermediate representation employed by many compilers that enables writing simpler, faster optimizers. In fact, it has remained an open problem to verify formally an SSA-based compiler. We report on a formally verified, SSA-based middle-end for CompCert. In addition to providing a formally verified SSA-based middle-end, we address two problems raised by Leroy in [2009]: giving an intuitive formal semantics to SSA, and leveraging its global properties to reason locally about program optimizations.},
	author = {Barthe, Gilles and Demange, Delphine and Pichardie, David},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	doi = {10.1145/2579080},
	issn = {0164-0925},
	journaltitle = {ACM Trans. Program. Lang. Syst.},
	keywords = {CompCertSSA,CompCert,SSA,coq,verification,compiler optimisation},
	month = mar,
	number = {1},
	title = {Formal Verification of an SSA-Based Middle-End for CompCert},
	volume = {36},
	year = {2014}
}

@article{barthe19_formal_verif_const_time_preser_c_compil,
	abstract = {Timing side-channels are arguably one of the main sources of vulnerabilities in cryptographic implementations. One effective mitigation against timing side-channels is to write programs that do not perform secret-dependent branches and memory accesses. This mitigation, known as "cryptographic constant-time", is adopted by several popular cryptographic libraries. This paper focuses on compilation of cryptographic constant-time programs, and more specifically on the following question: is the code generated by a realistic compiler for a constant-time source program itself provably constant-time? Surprisingly, we answer the question positively for a mildly modified version of the CompCert compiler, a formally verified and moderately optimizing compiler for C. Concretely, we modify the CompCert compiler to eliminate sources of potential leakage. Then, we instrument the operational semantics of CompCert intermediate languages so as to be able to capture cryptographic constant-time. Finally, we prove that the modified CompCert compiler preserves constant-time. Our mechanization maximizes reuse of the CompCert correctness proof, through the use of new proof techniques for proving preservation of constant-time. These techniques achieve complementary trade-offs between generality and tractability of proof effort, and are of independent interest.},
	author = {Barthe, Gilles and Blazy, Sandrine and Grégoire, Benjamin and Hutin, Rémi and Laporte, Vincent and Pichardie, David and Trieu, Alix},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3371075},
	doi = {10.1145/3371075},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {timing side-channels,verified compilation,CompCert compiler},
	month = dec,
	number = {POPL},
	title = {Formal Verification of a Constant-Time Preserving C Compiler},
	volume = {4},
	year = {2019}
}

@inproceedings{barthe19_verif_relat_proper_trace_logic,
	author = {Barthe, Gilles and Eilers, Renate and Georgiou, Pamina and Gleiss, Bernhard and Kovács, Laura and Maffei, Matteo},
	editor = {Barrett, Clark W. and Yang, Jin},
	publisher = {IEEE},
	url = {https://doi.org/10.23919/FMCAD.2019.8894277},
	booktitle = {2019 Formal Methods in Computer Aided Design, {FMCAD} 2019, San Jose, CA, USA, October 22-25, 2019},
	doi = {10.23919/FMCAD.2019.8894277},
	keywords = {verification,trace logic,relational properties},
	pages = {170--178},
	title = {Verifying Relational Properties using Trace Logic},
	year = {2019}
}

@article{barthe20_formal_c,
	author = {Barthe, Gilles and Blazy, Sandrine and Grégoire, Benjamin and Hutin, Rémi and Laporte, Vincent and Pichardie, David and Trieu, Alix},
	publisher = {Association for Computing Machinery ({ACM})},
	url = {https://doi.org/10.1145/3371075},
	doi = {10.1145/3371075},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	month = jan,
	number = {{POPL}},
	pages = {1--30},
	title = {Formal verification of a constant-time preserving C compiler},
	volume = {4},
	year = {2020}
}

@INPROCEEDINGS{barthe21_high_assur_crypt_spect_era,
	keywords = {predictable execution, cryptography},
	author = {Barthe, Gilles and Cauligi, Sunjay and Grégoire, Benjamin and Koutsos, Adrien and Liao, Kevin and Oliveira, Tiago and Priya, Swarn and Rezk, Tamara and Schwabe, Peter},
	booktitle = {2021 IEEE Symposium on Security and Privacy (SP)},
	title = {High-Assurance Cryptography in the Spectre Era},
	year = {2021},
	volume = {},
	number = {},
	pages = {1884-1901},
	doi = {10.1109/SP40001.2021.00046}
}

@article{barthe22_feller_contin_full_abstr,
	author = {Barthe, Gilles and Crubill\'{e}, Rapha\"{e}lle and Dal Lago, Ugo and Gavazzo, Francesco},
	title = {On Feller Continuity and Full Abstraction},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3547651},
	doi = {10.1145/3547651},
	abstract = {We study the nature of applicative bisimilarity in λ-calculi endowed with operators for sampling from contin- uous distributions. On the one hand, we show that bisimilarity, logical equivalence, and testing equivalence all coincide with contextual equivalence when real numbers can be manipulated through continuous functions only. The key ingredient towards this result is a notion of Feller-continuity for labelled Markov processes, which we believe of independent interest, giving rise a broad class of LMPs for which coinductive and logically inspired equivalences coincide. On the other hand, we show that if no constraint is put on the way real numbers are manipulated, characterizing contextual equivalence turns out to be hard, and most of the aforementioned notions of equivalence are even unsound.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {120},
	numpages = {29},
	keywords = {Event Bisimilarity, Labelled Markov Processes, Lambda Calculus, Applicative Bisimilarity}
}

@ARTICLE{bartlett88_multi,
	keywords = {boolean simplification, multi-valued logic},
	author = {Bartlett, K.A. and Brayton, R.K. and Hachtel, G.D. and Jacoby, R.M. and Morrison, C.R. and Rudell, R.L. and Sangiovanni-Vincentelli, A. and Wang, A.},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	title = {Multi-level logic minimization using implicit don't cares},
	year = {1988},
	volume = {7},
	number = {6},
	pages = {723-740},
	doi = {10.1109/43.3211}
}

@inproceedings{basin15_autom_symbol_proof_obser_equiv,
	author = {Basin, David and Dreier, Jannik and Sasse, Ralf},
	title = {Automated Symbolic Proofs of Observational Equivalence},
	year = {2015},
	isbn = {9781450338325},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2810103.2813662},
	doi = {10.1145/2810103.2813662},
	abstract = {Many cryptographic security definitions can be naturally formulated as observational equivalence properties. However, existing automated tools for verifying the observational equivalence of cryptographic protocols are limited: they do not handle protocols with mutable state and an unbounded number of sessions. We propose a novel definition of observational equivalence for multiset rewriting systems. We then extend the Tamarin prover, based on multiset rewriting, to prove the observational equivalence of protocols with mutable state, an unbounded number of sessions, and equational theories such as Diffie-Hellman exponentiation. We demonstrate its effectiveness on case studies, including a stateful TPM protocol.},
	booktitle = {Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security},
	pages = {1144–1155},
	numpages = {12},
	keywords = {symbolic execution, equivalence checking},
	location = {Denver, Colorado, USA},
	series = {CCS '15}
}

@article{batz21_relat_compl_verif_probab_progr,
	abstract = {We study a syntax for specifying quantitative assertions—functions mapping program states to numbers—for probabilistic program verification. We prove that our syntax is expressive in the following sense: Given any probabilistic program C, if a function f is expressible in our syntax, then the function mapping each initial state σ to the expected value of evaluated in the final states reached after termination of C on σ (also called the weakest preexpectation wp[C](f)) is also expressible in our syntax. As a consequence, we obtain a relatively complete verification system for reasoning about expected values and probabilities in the sense of Cook: Apart from proving a single inequality between two functions given by syntactic expressions in our language, given f, g, and C, we can check whether g ≼ wp[C](f).},
	author = {Batz, Kevin and Kaminski, Benjamin Lucien and Katoen, Joost-Pieter and Matheja, Christoph},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434320},
	doi = {10.1145/3434320},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {weakest precondition,randomized algorithms,quantitative verification,formal verification,probabilistic programs,completeness,weakest preexpectation},
	month = jan,
	number = {POPL},
	title = {Relatively Complete Verification of Probabilistic Programs: An Expressive Language for Expectation-Based Reasoning},
	volume = {5},
	year = {2021}
}

@article{batz23_calcul_amort_expec_runtim,
	author = {Batz, Kevin and Kaminski, Benjamin Lucien and Katoen, Joost-Pieter and Matheja, Christoph and Verscht, Lena},
	title = {A Calculus for Amortized Expected Runtimes},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571260},
	doi = {10.1145/3571260},
	abstract = {We develop a weakest-precondition-style calculus \`{a} la Dijkstra for reasoning about amortized expected runtimes of randomized algorithms with access to dynamic memory — the aert calculus. Our calculus is truly quantitative, i.e. instead of Boolean valued predicates, it manipulates real-valued functions. En route to the aert calculus, we study the ert calculus for reasoning about expected runtimes of Kaminski et al. [2018] extended by capabilities for handling dynamic memory, thus enabling compositional and local reasoning about randomized data structures. This extension employs runtime separation logic, which has been foreshadowed by Matheja [2020] and then implemented in Isabelle/HOL by Haslbeck [2021]. In addition to Haslbeck’s results, we further prove soundness of the so-extended ert calculus with respect to an operational Markov decision process model featuring countably-branching nondeterminism, provide extensive intuitive explanations, and provide proof rules enabling separation logic-style verification for upper bounds on expected runtimes. Finally, we build the so-called potential method for amortized analysis into the ert calculus, thus obtaining the aert calculus. Soundness of the aert calculus is obtained from the soundness of the ert calculus and some probabilistic form of telescoping. Since one needs to be able to handle changes in potential which can in principle be both positive or negative, the aert calculus needs to be — essentially — capable of handling certain signed random variables. A particularly pleasing feature of our solution is that, unlike e.g. Kozen [1985], we obtain a loop rule for our signed random variables, and furthermore, unlike e.g. Kaminski and Katoen [2017], the aert calculus makes do without the need for involved technical machinery keeping track of the integrability of the random variables. Finally, we present case studies, including a formal analysis of a randomized delete-insert-find-any set data structure [Brodal et al. 1996], which yields a constant expected runtime per operation, whereas no deterministic algorithm can achieve this.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {67},
	numpages = {30},
	keywords = {amortized analysis, quantitative verification, randomized data structures}
}

@article{baumann21_contex_bound_verif_liven_proper,
	abstract = {We study context-bounded verification of liveness properties of multi-threaded, shared-memory programs, where each thread can spawn additional threads. Our main result shows that context-bounded fair termination is decidable for the model; context-bounded implies that each spawned thread can be context switched a fixed constant number of times. Our proof is technical, since fair termination requires reasoning about the composition of unboundedly many threads each with unboundedly large stacks. In fact, techniques for related problems, which depend crucially on replacing the pushdown threads with finite-state threads, are not applicable. Instead, we introduce an extension of vector addition systems with states (VASS), called VASS with balloons (VASSB), as an intermediate model; it is an infinite-state model of independent interest. A VASSB allows tokens that are themselves markings (balloons). We show that context bounded fair termination reduces to fair termination for VASSB. We show the latter problem is decidable by showing a series of reductions: from fair termination to configuration reachability for VASSB and thence to the reachability problem for VASS. For a lower bound, fair termination is known to be non-elementary already in the special case where threads run to completion (no context switches). We also show that the simpler problem of context-bounded termination is 2EXPSPACE-complete, matching the complexity bound---and indeed the techniques---for safety verification. Additionally, we show the related problem of fair starvation, which checks if some thread can be starved along a fair run, is also decidable in the context-bounded case. The decidability employs an intricate reduction from fair starvation to fair termination. Like fair termination, this problem is also non-elementary.},
	author = {Baumann, Pascal and Majumdar, Rupak and Thinniyam, Ramanathan S. and Zetzsche, Georg},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434325},
	doi = {10.1145/3434325},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {verification,liveness,decidability,multithreaded programs,computational complexity},
	month = jan,
	number = {POPL},
	title = {Context-Bounded Verification of Liveness Properties for Multithreaded Shared-Memory Programs},
	volume = {5},
	year = {2021}
}

@article{baumann22_contex_bound_verif_thread_pools,
	author = {Baumann, Pascal and Majumdar, Rupak and Thinniyam, Ramanathan S. and Zetzsche, Georg},
	title = {Context-Bounded Verification of Thread Pools},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498678},
	doi = {10.1145/3498678},
	abstract = {Thread pooling is a common programming idiom in which a fixed set of worker threads are maintained to execute tasks concurrently. The workers repeatedly pick tasks and execute them to completion. Each task is sequential, with possibly recursive code, and tasks communicate over shared memory. Executing a task can lead to more new tasks being spawned. We consider the safety verification problem for thread-pooled programs. We parameterize the problem with two parameters: the size of the thread pool as well as the number of context switches for each task. The size of the thread pool determines the number of workers running concurrently. The number of context switches determines how many times a worker can be swapped out while executing a single task---like many verification problems for multithreaded recursive programs, the context bounding is important for decidability. We show that the safety verification problem for thread-pooled, context-bounded, Boolean programs is EXPSPACE-complete, even if the size of the thread pool and the context bound are given in binary. Our main result, the EXPSPACE upper bound, is derived using a sequence of new succinct encoding techniques of independent language-theoretic interest. In particular, we show a polynomial-time construction of downward closures of languages accepted by succinct pushdown automata as doubly succinct nondeterministic finite automata. While there are explicit doubly exponential lower bounds on the size of nondeterministic finite automata accepting the downward closure, our result shows these automata can be compressed. We show that thread pooling significantly reduces computational power: in contrast, if only the context bound is provided in binary, but there is no thread pooling, the safety verification problem becomes 3EXPSPACE-complete. Given the high complexity lower bounds of related problems involving binary parameters, the relatively low complexity of safety verification with thread-pooling comes as a surprise.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {17},
	numpages = {28},
	keywords = {context bounded, multithreaded programs, verification, thread pool, safety, computational complexity}
}

@article{baumann23_contex_bound_verif_contex_free_specif,
	author = {Baumann, Pascal and Ganardi, Moses and Majumdar, Rupak and Thinniyam, Ramanathan S. and Zetzsche, Georg},
	title = {Context-Bounded Verification of Context-Free Specifications},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571266},
	doi = {10.1145/3571266},
	abstract = {A fundamental problem in refinement verification is to check that the language of behaviors of an implementation is included in the language of the specification. We consider the refinement verification problem where the implementation is a multithreaded shared memory system modeled as a multistack pushdown automaton and the specification is an input-deterministic multistack pushdown language. Our main result shows that the context-bounded refinement problem, where we ask that all behaviors generated in runs of bounded number of context switches belong to a specification given by a Dyck language, is decidable and coNP-complete. The more general case of input-deterministic languages follows, with the same complexity. Context-bounding is essential since emptiness for multipushdown automata is already undecidable, and so is the refinement verification problem for the subclass of regular specifications. Input-deterministic languages capture many non-regular specifications of practical interest and our result opens the way for algorithmic analysis of these properties. The context-bounded refinement problem is coNP-hard already with deterministic regular specifications; our result demonstrates that the problem is not harder despite the stronger class of specifications. Our proof introduces several general techniques for formal languages and counter programs and shows that the search for counterexamples can be reduced in non-deterministic polynomial time to the satisfiability problem for existential Presburger arithmetic. These techniques are essential to ensure the coNP upper bound: existing techniques for regular specifications are not powerful enough for decidability, while simple reductions lead to problems that are either undecidable or have high complexities. As a special case, our decidability result gives an algorithmic verification technique to reason about reference counting and re-entrant locking in multithreaded programs.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {73},
	numpages = {30},
	keywords = {multithreaded programs, computational complexity, Dyck language, context bounded, refinement verification, inclusion problem}
}

@article{beck91_fcfd,
	keywords = {control-flow, data-flow},
	title = {From Control Flow to Dataflow},
	journal = {Journal of Parallel and Distributed Computing},
	volume = {12},
	number = {2},
	pages = {118-129},
	year = {1991},
	issn = {0743-7315},
	doi = {10.1016/0743-7315(91)90016-3},
	author = {Micah Beck and Richard Johnson and Keshav Pingali},
	abstract = {Are imperative languages tied inseparably to the von Neumann model or can they be implemented in some natural way on data-flow architectures? In this paper, we show how imperative language programs can be translated into dataflow graphs and executed on a dataflow machine like Monsoon. This translation can exploit both fine-grain and coarse-grain parallelism in imperative language programs. More importantly, we establish a close connection between our work and current research in the imperative languages community on data dependences, control dependences, program dependence graphs, and static single assignment form. These results suggest that dataflow graphs can serve as an executable intermediate representation in parallelizing compilers.}
}

@inproceedings{behroozi22_loner,
	author = {Behroozi, Armand and Park, Sunghyun and Mahlke, Scott},
	title = {Loner: Utilizing the CPU Vector Datapath to Process Scalar Integer Data},
	year = {2022},
	isbn = {9781450391832},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497776.3517767},
	doi = {10.1145/3497776.3517767},
	abstract = {Modern CPUs utilize SIMD vector instructions and hardware extensions to accelerate code with data-level parallelism. This allows for high performance gains in select application domains such as image and signal processing. However, general purpose code often lacks data-level parallelism or has complex control and data dependencies, which prevents vectorization. Thus, CPU vector registers and functional units frequently sit idle while the scalar datapath unilaterally executes code. In this paper, we present Loner, a profile-guided compiler methodology for optimizing scalar integer loops using the otherwise idle vector datapath. Loner expands the traditional definition of vectorization by identifying two situations where it is beneficial to perform vector operations with a single data element ("Loner" data). In the first, the scalar register file and functional units are overburdened, resulting in unnecessary spill/reload operations and stalls due to structural hazards. In the second, we describe a set of "vector-amenable" computation patterns that the vector pipeline naturally executes more efficiently than its scalar counterpart. Loner identifies hot code regions that exhibit either characteristic and offloads a subset of a program's computation graph to the vector datapath for maximum performance. We evaluate Loner on an x86 Whiskey Lake processor using select benchmarks from the SPEC, GAP, and MiBench benchmark suites where it improves performance by 2.64% (geomean) up to 40.28%.},
	booktitle = {Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction},
	pages = {205–217},
	numpages = {13},
	keywords = {vectorization, CPUs, register pressure reduction, profile guided optimization},
	location = {Seoul, South Korea},
	series = {CC 2022}
}

@inproceedings{beidas11_regis,
	abstract = {Variations of list scheduling became the de-facto standard of scheduling straight line code in software compilers, a trend faithfully inherited by high-level synthesis solutions. Due to its nature, list scheduling is oblivious of the tightly coupled register pressure; a dangling fundamental problem that has been attacked by the compiler community for decades, and which results, in case of highlevel synthesis, in excessive instantiations of registers and accompanying steering logic. To alleviate this problem, we propose a synthesis framework called soft scheduling, which acts as a resource unconstrained pre-scheduling stage that restricts subsequent scheduling to minimize register pressure. This optimization objective is formulated as a live range minimization problem, a measure shown to be proportional to register pressure, and optimally solved in polynomial time using minimum cost network flow formulation. Unlike past solutions in the compiler community, which try to reduce register pressure by local serialization of subject instructions, the proposed solution operates on the entire basic block or hyperblock and systematically handles instruction chaining subject to the same objective. The application of the proposed solution to a set of real-life benchmarks results in a register pressure reduction ranging, on average, between 11% and 41% depending on the compilation and synthesis configurations with minor 2% to 4% increase in schedule latency.},
	author = {Beidas, R. and Mong, W. S. and Zhu, J.},
	booktitle = {16th Asia and South Pacific Design Automation Conference (ASP-DAC 2011)},
	doi = {10.1109/ASPDAC.2011.5722234},
	issn = {2153-697X},
	keywords = {register allocation,hardware scheduling,high-level synthesis,static scheduling},
	month = jan,
	pages = {461--466},
	title = {Register pressure aware scheduling for high level synthesis},
	year = {2011}
}

@Inbook{belnap77_useful_four_valued_logic,
	author = "Belnap, Nuel D.",
	editor = "Dunn, J. Michael
and Epstein, George",
	title = "A Useful Four-Valued Logic",
	bookTitle = "Modern Uses of Multiple-Valued Logic",
	year = "1977",
	publisher = "Springer Netherlands",
	address = "Dordrecht",
	pages = "5--37",
	abstract = "It is argued that a sophisticated question-answering machine that has the capability of making inferences from its data base should employ a certain four-valued logic, the motivating consideration being that minor inconsistencies in its data should not be allowed to lead (as in classical logic) to irrelevant conclusions. The actual form of the four-valued logic is `deduced' from an interplay of this motivating consideration with certain ideas of Dana Scott concerning `approximation lattices.'",
	isbn = "978-94-010-1161-7",
	doi = "10.1007/978-94-010-1161-7_2",
	url = "https://doi.org/10.1007/978-94-010-1161-7_2"
}

@article{bembenek23_from_smt_asp,
	author = {Bembenek, Aaron and Greenberg, Michael and Chong, Stephen},
	title = {From SMT to ASP: Solver-Based Approaches to Solving Datalog Synthesis-as-Rule-Selection Problems},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571200},
	doi = {10.1145/3571200},
	abstract = {Given a set of candidate Datalog rules, the Datalog synthesis-as-rule-selection problem chooses a subset of these rules that satisfies a specification (such as an input-output example). Building off prior work using counterexample-guided inductive synthesis, we present a progression of three solver-based approaches for solving Datalog synthesis-as-rule-selection problems. Two of our approaches offer some advantages over existing approaches, and can be used more generally to solve arbitrary SMT formulas containing Datalog predicates; the third—an encoding into standard, off-the-shelf answer set programming (ASP)—leads to significant speedups (∼ 9\texttimes{} geomean) over the state of the art while synthesizing higher quality programs. Our progression of solutions explores the space of interactions between SAT/SMT and Datalog, identifying ASP as a promising tool for working with and reasoning about Datalog. Along the way, we identify Datalog programs as monotonic SMT theories, which enjoy particularly efficient interactions in SMT; our plugins for popular SMT solvers make it easy to load an arbitrary Datalog program into the SMT solver as a custom monotonic theory. Finally, we evaluate our approaches using multiple underlying solvers to provide a more thorough and nuanced comparison against the current state of the art.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {7},
	numpages = {33},
	keywords = {Datalog, inductive logic programming, program synthesis, satisfiability}
}

@inproceedings{benabderrahmane10_polyh_model_is_more_widel,
	abstract = {The polyhedral model is a powerful framework for automatic optimization and parallelization. It is based on an algebraic representation of programs, allowing to construct and search for complex sequences of optimizations. This model is now mature and reaches production compilers. The main limitation of the polyhedral model is known to be its restriction to statically predictable, loop-based program parts. This paper removes this limitation, allowing to operate on general data-dependent control-flow. We embed control and exit predicates as first-class citizens of the algebraic representation, from program analysis to code generation. Complementing previous (partial) attempts in this direction, our work concentrates on extending the code generation step and does not compromise the expressiveness of the model. We present experimental evidence that our extension is relevant for program optimization and parallelization, showing performance improvements on benchmarks that were thought to be out of reach of the polyhedral model.},
	author = {Benabderrahmane, Mohamed-Walid and Pouchet, Louis-Noël and Cohen, Albert and Bastoul, Cédric},
	editor = {Gupta, Rajiv},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Compiler Construction},
	doi = {10.1007/978-3-642-11970-5_16},
	isbn = {978-3-642-11970-5},
	keywords = {polyhedral model,polyhedral analysis,algebraic model},
	pages = {283--303},
	title = {The Polyhedral Model Is More Widely Applicable Than You Think},
	year = {2010}
}

@article{benedikt21_gener_collec_trans_proof,
	abstract = {Nested relations, built up from atomic types via product and set types, form a rich data model. Over the last decades the nested relational calculus, NRC, has emerged as a standard language for defining transformations on nested collections. NRC is a strongly-typed functional language which allows building up transformations using tupling and projections, a singleton-former, and a map operation that lifts transformations on tuples to transformations on sets. In this work we describe an alternative declarative method of describing transformations in logic. A formula with distinguished inputs and outputs gives an implicit definition if one can prove that for each input there is only one output that satisfies it. Our main result shows that one can synthesize transformations from proofs that a formula provides an implicit definition, where the proof is in an intuitionistic calculus that captures a natural style of reasoning about nested collections. Our polynomial time synthesis procedure is based on an analog of Craig's interpolation lemma, starting with a provable containment between terms representing nested collections and generating an NRC expression that interpolates between them. We further show that NRC expressions that implement an implicit definition can be found when there is a classical proof of functionality, not just when there is an intuitionistic one. That is, whenever a formula implicitly defines a transformation, there is an NRC expression that implements it.},
	author = {Benedikt, Michael and Pradic, Pierre},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434295},
	doi = {10.1145/3434295},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {nested collections,proofs,synthesis},
	month = jan,
	number = {POPL},
	title = {Generating Collection Transformations from Proofs},
	volume = {5},
	year = {2021}
}

@software{berkelaar10,
	year = {2010},
	title = {\texttt{lp\_solve} v5.5},
	author = {Berkelaar, Michel},
	url = {https://lpsolve.sourceforge.net/5.5/}
}

@article{bernardi18_correc_high_level_synth_tripl,
	author = {Bernardi, Michael and Cetin, Ediz and Diessel, Oliver},
	keywords = {translation validation,verification,high-level synthesis},
	title = {Correct High Level Synthesis of Triple Modular Redundant User Circuits for Fpgas},
	year = {2018}
}

@article{bernstein21_what_are_seman_hardw,
	keywords = {hardware abstract interpretation},
	title = {What Are Semantics for Hardware?},
	author = {Bernstein, Gilbert and Daly, Ross and Ragan-Kelley, Jonathan and Hanrahan, Pat},
	year = {2021}
}

@book{bertot04_inter_theor_provin_progr_devel,
	author = {Bertot, Yves and Castéran, Pierre},
	publisher = {Springer Berlin Heidelberg},
	url = {https://doi.org/10.1007/978-3-662-07964-5},
	doi = {10.1007/978-3-662-07964-5},
	keywords = {coq,verification},
	title = {Interactive Theorem Proving and Program Development},
	year = {2004}
}

@inproceedings{bertot06_struc_approac_provin_compil_optim,
	abstract = {This paper reports on the correctness proof of compiler optimizations based on data-flow analysis. We formulate the optimizations and analyses as instances of a general framework for data-flow analyses and transformations, and prove that the optimizations preserve the behavior of the compiled programs. This development is a part of a larger effort of certifying an optimizing compiler by proving semantic equivalence between source and compiled code.},
	author = {Bertot, Yves and Grégoire, Benjamin and Leroy, Xavier},
	editor = {Filliâtre, Jean-Christophe and Paulin-Mohring, Christine and Werner, Benjamin},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Types for Proofs and Programs},
	isbn = {978-3-540-31429-5},
	keywords = {compiler optimisation},
	pages = {66--81},
	title = {A Structured Approach to Proving Compiler Optimizations Based on Dataflow Analysis},
	year = {2006}
}

@Inbook{bertot09_struc_abstr_inter,
	keywords = {abstract interpretation, coq},
	author = "Bertot, Yves",
	editor = {Bove, Ana and Barbosa, Lu{\'i}s Soares and Pardo, Alberto and Pinto, Jorge Sousa},
	title = "Structural Abstract Interpretation: A Formal Study Using Coq",
	bookTitle = "Language Engineering and Rigorous Software Development: International LerNet ALFA Summer School 2008, Piriapolis, Uruguay, February 24 - March 1, 2008, Revised Tutorial Lectures",
	year = "2009",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "153--194",
	abstract = "Abstract interpreters are tools to compute approximations for behaviors of a program. These approximations can then be used for optimisation or for error detection. In this paper, we show how to describe an abstract interpreter using the type-theory based theorem prover Coq, using inductive types for syntax and structural recursive programming for the abstract interpreter's kernel. The abstract interpreter can then be proved correct with respect to a Hoare logic for the programming language.",
	isbn = "978-3-642-03153-3",
	doi = "10.1007/978-3-642-03153-3_4",
	url = "https://doi.org/10.1007/978-3-642-03153-3_4"
}

@InProceedings{besson11_modul_smt_proof_fast_reflex,
	doi = {10.1007/978-3-642-25379-9_13},
	author = {Besson, Frédéric and Cornilleau, Pierre-Emmanuel and Pichardie, David},
	editor = "Jouannaud, Jean-Pierre
and Shao, Zhong",
	title = "Modular SMT Proofs for Fast Reflexive Checking Inside Coq",
	booktitle = "Certified Programs and Proofs",
	year = "2011",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "151--166",
	abstract = "We present a new methodology for exchanging unsatisfiability proofs between an untrusted SMT solver and a sceptical proof assistant with computation capabilities like Coq. We advocate modular SMT proofs that separate boolean reasoning and theory reasoning; and structure the communication between theories using Nelson-Oppen combination scheme. We present the design and implementation of a Coq reflexive verifier that is modular and allows for fine-tuned theory-specific verifiers. The current verifier is able to verify proofs for quantifier-free formulae mixing linear arithmetic and uninterpreted functions. Our proof generation scheme benefits from the efficiency of state-of-the-art SMT solvers while being independent from a specific SMT solver proof format. Our only requirement for the SMT solver is the ability to extract unsat cores and generate boolean models. In practice, unsat cores are relatively small and their proof is obtained with a modest overhead by our proof-producing prover. We present experiments assessing the feasibility of the approach for benchmarks obtained from the SMT competition.",
	isbn = "978-3-642-25379-9"
}

@article{besson18_compc,
	author = {Besson, Frédéric and Blazy, Sandrine and Wilke, Pierre},
	publisher = {Springer Science and Business Media {LLC}},
	url = {https://doi.org/10.1007/s10817-018-9496-y},
	doi = {10.1007/s10817-018-9496-y},
	journaltitle = {Journal of Automated Reasoning},
	month = nov,
	number = {2},
	pages = {369--392},
	title = {{CompCertS}: A Memory-Aware Verified C Compiler Using a Pointer as Integer Semantics},
	volume = {63},
	year = {2018}
}

@InProceedings{besson21_itaut,
	author = {Besson, Frédéric},
	title = {{Itauto: An Extensible Intuitionistic SAT Solver}},
	booktitle = {12th International Conference on Interactive Theorem Proving (ITP 2021)},
	pages = {9:1--9:18},
	series = {Leibniz International Proceedings in Informatics (LIPIcs)},
	ISBN = {978-3-95977-188-7},
	ISSN = {1868-8969},
	year = {2021},
	volume = {193},
	editor = {Cohen, Liron and Kaliszyk, Cezary},
	publisher = {Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
	address = {Dagstuhl, Germany},
	URL = {https://drops.dagstuhl.de/opus/volltexte/2021/13904/pdf/LIPIcs-ITP-2021-9.pdf},
	URN = {urn:nbn:de:0030-drops-139043},
	doi = {10.4230/LIPIcs.ITP.2021.9},
	annote = {Keywords: SAT solver, proof by reflection}
}

@Misc{besson23_microm,
	url = {https://coq.inria.fr/refman/addendum/micromega.html#micromega-solvers-for-arithmetic-goals-over-ordered-rings},
	year = {2023},
	title = {Micromega: solvers for arithmetic goals over ordered rings},
	author = {Frédéric Besson and Evgeny Makarov}
}

@inproceedings{bidmeshki15_veric,
	author = {Bidmeshki, M. and Makris, Y.},
	url = {https://doi.org/10.1109/ISCAS.2015.7168562},
	booktitle = {2015 IEEE International Symposium on Circuits and Systems (ISCAS)},
	doi = {10.1109/ISCAS.2015.7168562},
	keywords = {proof-carrying,coq,verilog},
	month = may,
	pages = {29--32},
	title = {VeriCoq: A Verilog-to-Coq converter for proof-carrying hardware automation},
	year = {2015}
}

@article{biernacka22_simpl_effic_implem_stron_call,
	author = {Biernacka, Ma\l{}gorzata and Charatonik, Witold and Drab, Tomasz},
	title = {A Simple and Efficient Implementation of Strong Call by Need by an Abstract Machine},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3549822},
	doi = {10.1145/3549822},
	abstract = {We present an abstract machine for a strong call-by-need strategy in the lambda calculus. The machine has been derived automatically from a higher-order evaluator that uses the technique of memothunks to implement laziness. The derivation has been done with the use of an off-the-shelf transformation tool implementing the "functional correspondence" between higher-order interpreters and abstract machines, and it yields a simple and concise description of the machine. We prove that the resulting machine conservatively extends the lazy version of Krivine machine for the weak call-by-need strategy, and that it simulates the normal-order strategy in bilinear number of steps.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {94},
	numpages = {28},
	keywords = {Normalization by evaluation, Reduction strategies, Abstract machines, λ-calculus, Computational complexity}
}

@inproceedings{bilgili19_system_chip_desig_vivad_high,
	abstract = {In this paper, crypto algorithms such as Advanced Encryption Standard (AES), Rivest Shamir Adleman Algorithm (RSA), Secure Hash Algorithm (SHA-3) are designed and used. Commonly used high level software languages such as C and C++ are used to implement the mentioned algorithms into a hardware. During the design stage of these algorithms with the high-level languages. Vivado High Level Synthesis (Vivado HLS) environment developed by Xilinx was used. The relevant modules designed with high level identification languages and the related modules designed with the hardware description languages were compared with each other. After this comparison, a processor-based system was created by using modules designed with high level languages. AXI4-Stream is selected for the interface of the designed modules. The related modules are then redesigned with hardware description languages in the Xilinx Vivado environment. Direct memory access (DMA) is used in the system for high speed operation of the system, since the size of the data to be secured is likely to be large. Cryptographic communication is simulated between two people with Station-toStation communication protocol in DMA and processor-based system. While this communication protocol is being used, possible attacks carried by third parties and the loss due to noise during the transfer of information are not considered.},
	author = {Bilgili, B. and Yamaneren, C. and Vatansever, K. and {\C{C}}oltu, U. and Örs, B.},
	url = {https://doi.org/10.23919/ELECO47770.2019.8990595},
	booktitle = {2019 11th International Conference on Electrical and Electronics Engineering (ELECO)},
	doi = {10.23919/ELECO47770.2019.8990595},
	keywords = {vivado,high-level synthesis,cryptography},
	month = nov,
	pages = {1047--1050},
	title = {System on Chip Design with Vivado High-Level Synthesis Tool},
	year = {2019}
}

@inproceedings{bird13_under_idiom_traver_backw_forwar,
	author = {Bird, Richard and Gibbons, Jeremy and Mehner, Stefan and Voigtl\"{a}nder, Janis and Schrijvers, Tom},
	title = {Understanding Idiomatic Traversals Backwards and Forwards},
	year = {2013},
	isbn = {9781450323833},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2503778.2503781},
	doi = {10.1145/2503778.2503781},
	abstract = {We present new ways of reasoning about a particular class of effectful Haskell programs, namely those expressed as idiomatic traversals. Starting out with a specific problem about labelling and unlabelling binary trees, we extract a general inversion law, applicable to any monad, relating a traversal over the elements of an arbitrary traversable type to a traversal that goes in the opposite direction. This law can be invoked to show that, in a suitable sense, unlabelling is the inverse of labelling. The inversion law, as well as a number of other properties of idiomatic traversals, is a corollary of a more general theorem characterising traversable functors as finitary containers: an arbitrary traversable object can be decomposed uniquely into shape and contents, and traversal be understood in terms of those. Proof of the theorem involves the properties of traversal in a special idiom related to the free applicative functor.},
	booktitle = {Proceedings of the 2013 ACM SIGPLAN Symposium on Haskell},
	pages = {25–36},
	numpages = {12},
	keywords = {monads, applicative},
	location = {Boston, Massachusetts, USA},
	series = {Haskell '13}
}

@inproceedings{bitat19_formal_verif_crypt_circuit,
	author = {Bitat, Abir and merniz, Salah},
	location = {Rabat, Morocco},
	publisher = {ACM},
	url = {https://doi.org/10.1145/3320326.3320367},
	booktitle = {Proceedings of the 2Nd International Conference on Networking, Information Systems \& Security},
	doi = {10.1145/3320326.3320367},
	isbn = {978-1-4503-6645-8},
	keywords = {cryptography,verification},
	pages = {35:1--35:6},
	series = {NISS19},
	title = {Formal Verification of Cryptographic Circuits: A Semi-automatic Functional Approach},
	year = {2019}
}

@article{blanvillain22_type_level_progr_match_types,
	abstract = {Type-level programming is becoming more and more popular in the realm of functional programming. However, the combination of type-level programming and subtyping remains largely unexplored in practical programming languages. This paper presents match types, a type-level equivalent of pattern matching. Match types integrate seamlessly into programming languages with subtyping and, despite their simplicity, offer significant additional expressiveness. We formalize the feature of match types in a calculus based on System F sub and prove its soundness. We practically evaluate our system by implementing match types in the Scala 3 reference compiler, thus making type-level programming readily available to a broad audience of programmers.},
	author = {Blanvillain, Olivier and Brachthäuser, Jonathan Immanuel and Kjaer, Maxime and Odersky, Martin},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3498698},
	doi = {10.1145/3498698},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {Scala,Match types},
	month = jan,
	number = {POPL},
	title = {Type-Level Programming with Match Types},
	volume = {6},
	year = {2022}
}

@inproceedings{blaumenrohr98_perfor,
	author = {Blaumenrohr, C. and Eisenbiegler, D.},
	url = {https://doi.org/10.1109/EURMIC.1998.711771},
	booktitle = {Proceedings. 24th EUROMICRO Conference (Cat. No.98EX204)},
	doi = {10.1109/EURMIC.1998.711771},
	issn = {1089-6503},
	keywords = {high-level synthesis,theorem prover},
	month = aug,
	pages = {34--37 vol.1},
	title = {Performing high-level synthesis via program transformations within a theorem prover},
	volume = {1},
	year = {1998}
}

@inproceedings{blazy05_formal_verif_memor_model_c,
	abstract = {This paper presents a formal verification with the Coq proof assistant of a memory model for C-like imperative languages. This model defines the memory layout and the operations that manage the memory. The model has been specified at two levels of abstraction and implemented as part of an ongoing certification in Coq of a moderately-optimising C compiler. Many properties of the memory have been verified in the specification. They facilitate the definition of precise formal semantics of C pointers. A certified OCaml code implementing the memory model has been automatically extracted from the specifications.},
	author = {Blazy, Sandrine and Leroy, Xavier},
	editor = {Lau, Kung-Kiu and Banach, Richard},
	location = {Berlin, Heidelberg},
	publisher = {Springer},
	booktitle = {Formal Methods and Software Engineering},
	isbn = {978-3-540-32250-4},
	keywords = {verification,coq,theorem prover,memory model,C,CompCert},
	pages = {280--299},
	title = {Formal Verification of a Memory Model for C-Like Imperative Languages},
	year = {2005}
}

@inproceedings{blazy06_formal_verif_c_compil_front_end,
	abstract = {This paper presents the formal verification of a compiler front-end that translates a subset of the C language into the Cminor intermediate language. The semantics of the source and target languages as well as the translation between them have been written in the specification language of the Coq proof assistant. The proof of observational semantic equivalence between the source and generated code has been machine-checked using Coq. An executable compiler was obtained by automatic extraction of executable Caml code from the Coq specification of the translator, combined with a certified compiler back-end generating PowerPC assembly code from Cminor, described in previous work.},
	author = {Blazy, Sandrine and Dargaye, Zaynah and Leroy, Xavier},
	editor = {Misra, Jayadev and Nipkow, Tobias and Sekerinski, Emil},
	location = {Berlin, Heidelberg},
	publisher = {Springer},
	booktitle = {FM 2006: Formal Methods},
	isbn = {978-3-540-37216-5},
	keywords = {verification,coq,C,CompCert,verification},
	pages = {460--475},
	title = {Formal Verification of a C Compiler Front-End},
	year = {2006}
}

@article{blazy09_mechan_seman_cligh_subset_c_languag,
	abstract = {This article presents the formal semantics of a large subset of the C language called Clight. Clight includes pointer arithmetic, struct and union types, C loops and structured switch statements. Clight is the source language of the CompCert verified compiler. The formal semantics of Clight is a big-step operational semantics that observes both terminating and diverging executions and produces traces of input/output events. The formal semantics of Clight is mechanized using the Coq proof assistant. In addition to the semantics of Clight, this article describes its integration in the CompCert verified compiler and several ways by which the semantics was validated.},
	author = {Blazy, Sandrine and Leroy, Xavier},
	url = {https://doi.org/10.1007/s10817-009-9148-3},
	date = {2009-10-01},
	doi = {10.1007/s10817-009-9148-3},
	issn = {1573-0670},
	journaltitle = {Journal of Automated Reasoning},
	keywords = {C,CompCert,operational semantics,coq,verification},
	number = {3},
	pages = {263--288},
	title = {Mechanized Semantics for the {Clight} Subset of the {C} Language},
	volume = {43}
}

@inproceedings{blazy14_formal_verif_loop_bound_estim_wcet_analy,
	abstract = {Worst-case execution time (WCET) estimation tools are complex pieces of software performing tasks such as computation on control flow graphs (CFGs) and bound calculation. In this paper, we present a formal verification (in Coq) of a loop bound estimation. It relies on program slicing and bound calculation.},
	author = {Blazy, Sandrine and Maroneze, André and Pichardie, David},
	editor = {Cohen, Ernie and Rybalchenko, Andrey},
	location = {Berlin, Heidelberg},
	publisher = {Springer},
	booktitle = {Verified Software: Theories, Tools, Experiments},
	isbn = {978-3-642-54108-7},
	keywords = {loop-bound estimation},
	pages = {281--303},
	title = {Formal Verification of Loop Bound Estimation for WCET Analysis},
	year = {2014}
}

@inproceedings{blazy15_valid_domin_trees_fast_verif_domin_test,
	doi = {10.1007/978-3-319-22102-1_6},
	abstract = {The problem of computing dominators in a control flow graph is central to numerous modern compiler optimizations. Many efficient algorithms have been proposed in the literature, but mechanizing the correctness of the most sophisticated algorithms is still considered as too hard problems, and to this date, verified compilers use less optimized implementations. In contrast, production compilers, like GCC or LLVM, implement the classic, efficient Lengauer-Tarjan algorithm [12], to compute dominator trees. And subsequent optimization phases can then determine whether a CFG node dominates another node in constant time by using their respective depth-first search numbers in the dominator tree. In this work, we aim at integrating such techniques in verified compilers. We present a formally verified validator of untrusted dominator trees, on top of which we implement and prove correct a fast dominance test following these principles. We conduct our formal development in the Coq proof assistant, and integrate it in the middle-end of the CompCertSSA verified compiler. We also provide experimental results showing performance improvement over previous formalizations.},
	author = {Blazy, Sandrine and Demange, Delphine and Pichardie, David},
	editor = {Urban, Christian and Zhang, Xingyuan},
	location = {Cham},
	publisher = {Springer},
	booktitle = {Interactive Theorem Proving},
	isbn = {978-3-319-22102-1},
	keywords = {CompCertSSA,CompCert,SSA,coq,verification,compiler optimisation},
	pages = {84--99},
	title = {Validating Dominator Trees for a Fast, Verified Dominance Test},
	year = {2015}
}

@inproceedings{blech08_certif_coq,
	author = {Blech, Jan Olaf and Grégoire, Benjamin},
	organization = {Citeseer},
	booktitle = {Proceedings of the 7th International Workshop on Compiler Optimization Meets Compiler Verification},
	keywords = {verification},
	title = {Certifying code generation runs with Coq: A tool description},
	year = {2008}
}

@inproceedings{blot23_compos_pre_proces_autom_reason,
	author = {Blot, Valentin and Cousineau, Denis and Crance, Enzo and de Prisque, Louise Dubois and Keller, Chantal and Mahboubi, Assia and Vial, Pierre},
	title = {Compositional Pre-Processing for Automated Reasoning in Dependent Type Theory},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575676},
	doi = {10.1145/3573105.3575676},
	abstract = {In the context of interactive theorem provers based on a dependent type theory, automation tactics (dedicated decision procedures, call of automated solvers, ...) are often limited to goals which are exactly in some expected logical fragment. This very often prevents users from applying these tactics in other contexts, even similar ones. This paper discusses the design and the implementation of pre-processing operations for automating formal proofs in the Coq proof assistant. It presents the implementation of a wide variety of predictible, atomic goal transformations, which can be composed in various ways to target different backends. A gallery of examples illustrates how it helps to expand significantly the power of automation engines.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {63–77},
	numpages = {15},
	keywords = {coq, SMT},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@inbook{bocchino13_alias_contr_deter_paral,
	abstract = {A parallel program is deterministic if it produces the same output on every execution with a given input, regardless of the parallel schedule chosen. Determinism makes parallel programs much easier to write, understand, debug, and maintain. Further, many parallel programs are intended to be deterministic. Therefore a deterministic programming model (i.e., one in which all programs that pass compile-time checking are guaranteed to run deterministically) is attractive. However, aliasing poses difficulties for such a model, because hidden read-write conflicts through shared memory can cause unwanted nondeterminism and even data races. This article surveys the state of the art in program annotations for controlling aliasing in a way that can support a deterministic parallel programming model. It discusses the following techniques: the Deterministic Parallel Java effect system; other effect systems, including systems based on object ownership; permission-based type systems; and annotations based on program logic.},
	author = {Bocchino, Robert L.},
	editor = {Clarke, Dave and Noble, James and Wrigstad, Tobias},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	url = {https://doi.org/10.1007/978-3-642-36946-9_7},
	booktitle = {Aliasing in Object-Oriented Programming. Types, Analysis and Verification},
	doi = {10.1007/978-3-642-36946-9_7},
	isbn = {978-3-642-36946-9},
	keywords = {memory aliasing,ownership types},
	pages = {156--195},
	title = {Alias Control for Deterministic Parallelism},
	year = {2013}
}

@article{boldo15_verif_compil_float_point_comput,
	abstract = {Floating-point arithmetic is known to be tricky: roundings, formats, exceptional values. The IEEE-754 standard was a push towards straightening the field and made formal reasoning about floating-point computations easier and flourishing. Unfortunately, this is not sufficient to guarantee the final result of a program, as several other actors are involved: programming language, compiler, and architecture. The CompCert formally-verified compiler provides a solution to this problem: this compiler comes with a mathematical specification of the semantics of its source language (a large subset of ISO C99) and target platforms (ARM, PowerPC, x86-SSE2), and with a proof that compilation preserves semantics. In this paper, we report on our recent success in formally specifying and proving correct CompCert's compilation of floating-point arithmetic. Since CompCert is verified using the Coq proof assistant, this effort required a suitable Coq formalization of the IEEE-754 standard; we extended the Flocq library for this purpose. As a result, we obtain the first formally verified compiler that provably preserves the semantics of floating-point programs.},
	author = {Boldo, Sylvie and Jourdan, Jacques-Henri and Leroy, Xavier and Melquiond, Guillaume},
	url = {https://doi.org/10.1007/s10817-014-9317-x},
	date = {2015-02-01},
	doi = {10.1007/s10817-014-9317-x},
	issn = {1573-0670},
	journaltitle = {Journal of Automated Reasoning},
	keywords = {verification,floating point,CompCert},
	number = {2},
	pages = {135--163},
	title = {Verified Compilation of Floating-Point Computations},
	volume = {54}
}

@inproceedings{bonaert21_fast_precis_certif_trans,
	abstract = {We present DeepT, a novel method for certifying Transformer networks based on abstract interpretation. The key idea behind DeepT is our new Multi-norm Zonotope abstract domain, an extension of the classical Zonotope designed to handle ℓ1 and ℓ2-norm bound perturbations. We introduce all Multi-norm Zonotope abstract transformers necessary to handle these complex networks, including the challenging softmax function and dot product. Our evaluation shows that DeepT can certify average robustness radii that are 28\texttimes{} larger than the state-of-the-art, while scaling favorably. Further, for the first time, we certify Transformers against synonym attacks on long sequences of words, where each word can be replaced by any synonym. DeepT achieves a high certification success rate on sequences of words where enumeration-based verification would take 2 to 3 orders of magnitude more time.},
	author = {Bonaert, Gregory and Dimitrov, Dimitar I. and Baader, Maximilian and Vechev, Martin},
	location = {Virtual, Canada},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3453483.3454056},
	booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
	doi = {10.1145/3453483.3454056},
	isbn = {9781450383912},
	keywords = {Transformer Networks,Deep Learning,Robustness Certification,Adversarial attacks,Abstract Interpretation},
	pages = {466--481},
	series = {PLDI 2021},
	title = {Fast and Precise Certification of Transformers},
	year = {2021}
}

@article{bonchi23_decon_calcul_relat_tape_diagr,
	author = {Bonchi, Filippo and Di Giorgio, Alessandro and Santamaria, Alessio},
	title = {Deconstructing the Calculus of Relations with Tape Diagrams},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571257},
	doi = {10.1145/3571257},
	abstract = {Rig categories with finite biproducts are categories with two monoidal products, where one is a biproduct and the other distributes over it. In this work we present tape diagrams, a sound and complete diagrammatic language for these categories, that can be intuitively thought as string diagrams of string diagrams. We test the effectiveness of our approach against the positive fragment of Tarski's calculus of relations.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {64},
	numpages = {31},
	keywords = {calculus of relations, rig categories, string diagrams}
}

@inproceedings{bondhugula08_pluto,
	author = {Bondhugula, Uday and Hartono, A and Ramanujam, J and Sadayappan, P},
	organization = {Citeseer},
	booktitle = {Proceedings of the ACM SIGPLAN 2008 Conference on Programming Language Design and Implementation (PLDI 08), Tucson, AZ (June 2008)},
	title = {Pluto: A practical and fully automatic polyhedral program optimization system},
	year = {2008}
}

@inproceedings{bondhugula08_pract_autom_polyh_paral_local_optim,
	abstract = {We present the design and implementation of an automatic polyhedral source-to-source transformation framework that can optimize regular programs (sequences of possibly imperfectly nested loops) for parallelism and locality simultaneously. Through this work, we show the practicality of analytical model-driven automatic transformation in the polyhedral model -- far beyond what is possible by current production compilers. Unlike previous works, our approach is an end-to-end fully automatic one driven by an integer linear optimization framework that takes an explicit view of finding good ways of tiling for parallelism and locality using affine transformations. The framework has been implemented into a tool to automatically generate OpenMP parallel code from C program sections. Experimental results from the tool show very high speedups for local and parallel execution on multi-cores over state-of-the-art compiler frameworks from the research community as well as the best native production compilers. The system also enables the easy use of powerful empirical/iterative optimization for general arbitrarily nested loop sequences.},
	author = {Bondhugula, Uday and Hartono, Albert and Ramanujam, J. and Sadayappan, P.},
	location = {Tucson, AZ, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/1375581.1375595},
	booktitle = {Proceedings of the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation},
	doi = {10.1145/1375581.1375595},
	isbn = {9781595938602},
	keywords = {polyhedral analysis},
	pages = {101--113},
	series = {PLDI '08},
	title = {A Practical Automatic Polyhedral Parallelizer and Locality Optimizer},
	year = {2008}
}

@inproceedings{bordg23_encod_depen_typed_const_simpl_type_theor,
	author = {Bordg, Anthony and Do\~{n}a Mateo, Adri\'{a}n},
	title = {Encoding Dependently-Typed Constructions into Simple Type Theory},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575679},
	doi = {10.1145/3573105.3575679},
	abstract = {In this article, we show how one can formalise in type theory mathematical objects, for which dependent types are usually deemed unavoidable, using only simple types. We outline a method to encode most of the terms of Lean's dependent type theory into the simple type theory of Isabelle/HOL. Taking advantage of Isabelle's automation, we illustrate our method with the formalisation in Isabelle/HOL of a mathematical notion developed in the 1980s: strict omega-categories.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {78–89},
	numpages = {12},
	keywords = {dependent types, formal proofs, Isabelle/HOL, omega-categories, Lean},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@article{bornat06_variab_resour_separ_logic,
	title = {Variables as Resource in Separation Logic},
	journal = {Electronic Notes in Theoretical Computer Science},
	volume = {155},
	pages = {247-276},
	year = {2006},
	note = {Proceedings of the 21st Annual Conference on Mathematical Foundations of Programming Semantics (MFPS XXI)},
	issn = {1571-0661},
	doi = {https://doi.org/10.1016/j.entcs.2005.11.059},
	url = {https://www.sciencedirect.com/science/article/pii/S1571066106001964},
	author = {Richard Bornat and Cristiano Calcagno and Hongseok Yang},
	keywords = {verification, predicated execution, symbolic execution},
	abstract = {Separation logic [Reynolds, J. C., Intuitionistic reasoning about shared mutable data structure, in: J. Davies, B. Roscoe and J. Woodcock, editors, Millennial Perspectives in Computer Science, Palgrave, 2000 pp. 303–321; Reynolds, J. C., Separation logic: A logic for shared mutable data structures, in: LICS '02: Proceedings of the 17th Annual IEEE Symposium on Logic in Computer Science (2002), pp. 55–74; O'Hearn, P., J. Reynolds and H. Yang, Local reasoning about programs that alter data structures, in: L. Fribourg, editor, CSL 2001 (2001), pp. 1–19, LNCS 2142] began life as an extended formalisation of Burstall's treatment of list-mutating programs [Burstall, R., Some techniques for proving correctness of programs which alter data structures, Machine Intelligence 7 (1972), pp. 23–50]. It rapidly became clear that there was more that it could say: O'Hearn's discovery [O'Hearn, P., Notes on separation logic for shared-variable concurrency (2002), unpublished] of ownership transfer of buffers between threads and Boyland's suggestion [Boyland, J., Checking interference with fractional permissions, in: R. Cousot, editor, Static Analysis: 10th International Symposium, Lecture Notes in Computer Science 2694 (2003), pp. 55–72] of permissions to deal with variable and heap sharing pointed the way to a treatment of safe resource management in concurrent programs. That treatment has so far been incomplete because it deals only with heap cells and not with with (stack) variables as resource. Adding variable contexts' — in the simplest case, lists of owned variables — to assertions in Hoare logic allows a resource treatment of variables. It seems that a formal treatment of aliasing is possible too. It gives a complete formal treatment of critical sections (for the first time, so far as I am aware).}
}

@article{bornholt18_findin_code_that_explod_symbol_evaluat,
	author = {Bornholt, James and Torlak, Emina},
	title = {Finding Code That Explodes under Symbolic Evaluation},
	year = {2018},
	issue_date = {November 2018},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {2},
	number = {OOPSLA},
	url = {https://doi.org/10.1145/3276519},
	doi = {10.1145/3276519},
	abstract = {Solver-aided tools rely on symbolic evaluation to reduce programming tasks, such as verification and synthesis, to satisfiability queries. Many reusable symbolic evaluation engines are now available as part of solver-aided languages and frameworks, which have made it possible for a broad population of programmers to create and apply solver-aided tools to new domains. But to achieve results for real-world problems, programmers still need to write code that makes effective use of the underlying engine, and understand where their code needs careful design to elicit the best performance. This task is made difficult by the all-paths execution model of symbolic evaluators, which defies both human intuition and standard profiling techniques.This paper presents symbolic profiling, a new approach to identifying and diagnosing performance bottlenecks in programs under symbolic evaluation. To help with diagnosis, we develop a catalog of common performance anti-patterns in solver-aided code. To locate these bottlenecks, we develop SymPro, a new profiling technique for symbolic evaluation. SymPro identifies bottlenecks by analyzing two implicit resources at the core of every symbolic evaluation engine: the symbolic heap and symbolic evaluation graph. These resources form a novel performance model of symbolic evaluation that is general (encompassing all forms of symbolic evaluation), explainable (providing programmers with a conceptual framework for understanding symbolic evaluation), and actionable (enabling precise localization of bottlenecks). Performant solver-aided code carefully manages the shape of these implicit structures; SymPro makes their evolution explicit to the programmer.To evaluate SymPro, we implement profilers for the Rosette solver-aided language and the Jalangi program analysis framework. Applying SymPro to 15 published solver-aided tools, we discover 8 previously undiagnosed performance issues. Repairing these issues improves performance by orders of magnitude, and our patches were accepted by the tools' developers. We also conduct a small user study with Rosette programmers, finding that SymPro helps them both understand what the symbolic evaluator is doing and identify performance issues they could not otherwise locate.},
	journal = {Proc. ACM Program. Lang.},
	month = {oct},
	articleno = {149},
	numpages = {26},
	keywords = {symbolic execution, value summaries, hyperblocks}
}

@article{borrione00_compos_model_funct_verif_high,
	author = {Borrione, D. and Dushina, J. and Pierre, L.},
	url = {https://doi.org/10.1109/92.894157},
	doi = {10.1109/92.894157},
	journaltitle = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	keywords = {verification,high-level synthesis,model,hardware},
	month = oct,
	number = {5},
	pages = {526--530},
	title = {A Compositional Model for the Functional Verification of High-Level Synthesis Results},
	volume = {8},
	year = {2000}
}

@inproceedings{borrione98_formal,
	publisher = {IEEE},
	author = {Borrione, D. and Dushina, J. and Pierre, L.},
	url = {https://doi.org/10.1109/SBCCI.1998.715419},
	booktitle = {Proceedings. XI Brazilian Symposium on Integrated Circuit Design (Cat. No.98EX216)},
	doi = {10.1109/SBCCI.1998.715419},
	issn = {null},
	keywords = {FSMD,verification,high-level synthesis,hardware},
	month = oct,
	pages = {99--102},
	title = {Formalization of finite state machines with data path for the verification of high-level synthesis},
	year = {1998}
}

@article{borzacchiello19_memor,
	author = {Borzacchiello, Luca and Coppa, Emilio and Cono D'Elia, Daniele and Demetrescu, Camil},
	title = {Memory models in symbolic execution: key ideas and new thoughts},
	journal = {Software Testing, Verification and Reliability},
	volume = {29},
	number = {8},
	pages = {e1722},
	keywords = {symbolic execution, pointer reasoning, value summaries, hyperblocks},
	doi = {https://doi.org/10.1002/stvr.1722},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1722},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/stvr.1722},
	note = {e1722 stvr.1722},
	abstract = {Summary Symbolic execution is a popular program analysis technique that allows seeking for bugs by reasoning over multiple alternative execution states at once. As the number of states to explore may grow exponentially, a symbolic executor may quickly run out of space. For instance, a memory access to a symbolic address may potentially reference the entire address space, leading to a combinatorial explosion of the possible resulting execution states. To cope with this issue, state-of-the-art executors either concretize symbolic addresses that span memory intervals larger than some threshold or rely on advanced capabilities of modern satisfiability modulo theories solvers. Unfortunately, concretization may result in missing interesting execution states, for example, where a bug arises, while offloading the entire problem to constraint solvers can lead to very large query times. In this article, we first contribute to systematizing knowledge about memory models for symbolic execution, discussing how four mainstream symbolic executors deal with symbolic addresses. We then introduce MemSight, a new approach to symbolic memory that reduces the need for concretization: rather than mapping address instances to data as previous approaches do, our technique maps symbolic address expressions to data, maintaining the possible alternative states resulting from the memory referenced by a symbolic address in a compact, implicit form. Experiments on prominent programs show that MemSight, which we implemented in both Angr and Klee, enables the exploration of states that are unreachable for memory models that perform concretization and provides a performance level comparable with memory models relying on advanced solver theories.},
	year = {2019}
}

@inproceedings{bose89_verif,
	author = {Bose, S. and Fisher, A.},
	location = {Los Alamitos, CA, USA},
	publisher = {IEEE Computer Society},
	url = {https://doi.org/10.1109/ICCD.1989.63359},
	booktitle = {Proceedings 1989 IEEE International Conference on Computer Design: VLSI in Computers and Processors},
	doi = {10.1109/ICCD.1989.63359},
	keywords = {translation validation,hardware pipelining,symbolic execution,translation validation,verification,hardware},
	month = oct,
	pages = {217--221},
	title = {Verifying pipelined hardware using symbolic logic simulation},
	year = {1989}
}

@InProceedings{boulmé18_coq_tactic_equal_learn_linear_arith,
	doi = {10.1007/978-3-319-94821-8_7},
	author = {Boulmé, Sylvain and Maréchal, Alexandre},
	editor = "Avigad, Jeremy
and Mahboubi, Assia",
	title = "A Coq Tactic for Equality Learning in Linear Arithmetic",
	booktitle = "Interactive Theorem Proving",
	year = "2018",
	publisher = "Springer International Publishing",
	address = "Cham",
	pages = "108--125",
	abstract = "Coq provides linear arithmetic tactics such as omega or lia. Currently, these tactics either fully prove the current goal in progress, or fail. We propose to improve this behavior: when the goal is not provable in linear arithmetic, we strengthen the hypotheses with new equalities discovered from the linear inequalities. These equalities may help other Coq tactics to discharge the goal. In other words, we apply -- in interactive proofs -- a seminal idea of SMT-solving: combining tactics by exchanging equalities. The paper describes how we have implemented equality learning in a new Coq tactic, dealing with linear arithmetic over rationals. It also illustrates how this tactic interacts with other Coq tactics.",
	isbn = "978-3-319-94821-8"
}

@inproceedings{boulmé18_verif_polyh_librar,
	abstract = {The Verified Polyhedra Library operates upon a constraint-only representation of convex polyhedra and provides all common operations (image, pre-image, projection, convex hull, widening, inclusion and equality tests. . . ). Optionally, the soundness of the results is checked by a layer certified in Coq.},
	author = {Boulmé, Sylvain and Maréchaly, Alexandre and Monniaux, David and Périn, Michaël and Yu, Hang},
	booktitle = {2018 20th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)},
	doi = {10.1109/SYNASC.2018.00014},
	keywords = {,polyhedral analysis,verification,coq},
	month = sep,
	pages = {9--17},
	title = {The Verified Polyhedron Library: an Overview},
	year = {2018}
}

@unpublished{boulmé19_embed_untrus_imper_ml_oracl,
	author = {Boulmé, Sylvain and Vandendorpe, Thomas},
	url = {https://hal.archives-ouvertes.fr/hal-02062288},
	file = {https://hal.archives-ouvertes.fr/hal-02062288v2/file/main_preprint.pdf},
	keywords = {coq,monads,parametricity},
	month = jul,
	note = {working paper or preprint},
	title = {{Embedding Untrusted Imperative ML Oracles into Coq Verified Code}},
	year = {2019}
}

@phdthesis{boulmé21_formal_verif_defen_progr_coq_ml,
	TITLE = {{Formally Verified Defensive Programming (efficient Coq-verified computations from untrusted ML oracles)}},
	AUTHOR = {Boulmé, Sylvain },
	URL = {https://hal.science/tel-03356701},
	NOTE = {See also http://www-verimag.imag.fr/~boulme/hdr.html},
	SCHOOL = {{Universit{\'e} Grenoble-Alpes}},
	YEAR = {2021},
	MONTH = Sep,
	KEYWORDS = {Formal Methods ; SAT-solving ; CompCert verified compiler ; Abstract Interpretation ; M{\'e}thodes formelles},
	TYPE = {Habilitation {\`a} diriger des recherches},
	PDF = {https://hal.science/tel-03356701/file/boulme_hdr.pdf},
	HAL_ID = {tel-03356701},
	HAL_VERSION = {v1}
}

@inproceedings{bourgeat20_essen_blues,
	abstract = {The Bluespec hardware-description language presents a significantly higher-level view than hardware engineers are used to, exposing a simpler concurrency model that promotes formal proof, without compromising on performance of compiled circuits. Unfortunately, the cost model of Bluespec has been unclear, with performance details depending on a mix of user hints and opaque static analysis of potential concurrency conflicts within a design. In this paper we present Koika, a derivative of Bluespec that preserves its desirable properties and yet gives direct control over the scheduling decisions that determine performance. Koika has a novel and deterministic operational semantics that uses dynamic analysis to avoid concurrency anomalies. Our implementation includes Coq definitions of syntax, semantics, key metatheorems, and a verified compiler to circuits. We argue that most of the extra circuitry required for dynamic analysis can be eliminated by compile-time BSV-style static analysis.},
	author = {Bourgeat, Thomas and Pit-Claudel, Clément and Chlipala, Adam and Arvind},
	location = {London, UK},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3385412.3385965},
	booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
	doi = {10.1145/3385412.3385965},
	isbn = {9781450376136},
	keywords = {Bluespec,verification,coq,operational semantics,hardware},
	pages = {243--257},
	series = {PLDI 2020},
	title = {The Essence of Bluespec: A Core Language for Rule-Based Hardware Design},
	year = {2020}
}

@inproceedings{bourke17_fvcl,
	author = {Bourke, Timothy and Brun, L\'{e}lio and Dagand, Pierre-\'{E}variste and Leroy, Xavier and Pouzet, Marc and Rieg, Lionel},
	title = {A Formally Verified Compiler for Lustre},
	year = {2017},
	isbn = {9781450349888},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3062341.3062358},
	doi = {10.1145/3062341.3062358},
	abstract = {The correct compilation of block diagram languages like Lustre, Scade, and a discrete subset of Simulink is important since they are used to program critical embedded control software. We describe the specification and verification in an Interactive Theorem Prover of a compilation chain that treats the key aspects of Lustre: sampling, nodes, and delays. Building on CompCert, we show that repeated execution of the generated assembly code faithfully implements the dataflow semantics of source programs.  We resolve two key technical challenges. The first is the change from a synchronous dataflow semantics, where programs manipulate streams of values, to an imperative one, where computations manipulate memory sequentially. The second is the verified compilation of an imperative language with encapsulated state to C code where the state is realized by nested records. We also treat a standard control optimization that eliminates unnecessary conditional statements.},
	booktitle = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation},
	pages = {586–601},
	numpages = {16},
	keywords = {Synchronous Languages (Lustre), Interactive Theorem Proving (Coq), Verified Compilation},
	location = {Barcelona, Spain},
	series = {PLDI 2017}
}

@inproceedings{bouton09,
	doi = {10.1007/978-3-642-02959-2_12},
	abstract = {This article describes the first public version of the satisfiability modulo theory (SMT) solver veriT. It is open-source, proof-producing, and complete for quantifier-free formulas with uninterpreted functions and difference logic on real numbers and integers.},
	author = {Bouton, Thomas and Caminha B. de Oliveira, Diego and Déharbe, David and Fontaine, Pascal},
	editor = {Schmidt, Renate A.},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Automated Deduction -- CADE-22},
	isbn = {978-3-642-02959-2},
	pages = {151--156},
	title = {veriT: An Open, Trustable and Efficient SMT-Solver},
	year = {2009}
}

@ARTICLE{boutros21_fpga_archit,
	author = {Boutros, Andrew and Betz, Vaughn},
	journal = {IEEE Circuits and Systems Magazine},
	title = {FPGA Architecture: Principles and Progression},
	year = {2021},
	volume = {21},
	number = {2},
	pages = {4-29},
	doi = {10.1109/MCAS.2021.3071607}
}

@article{bowen01_approac_to_specif_verif_hardw_compil_schem,
	abstract = {The use of Field Programmable Gate Arrays (FPGA) to produce custom hardware circuits rapidly using a completely software-based process is becoming increasingly widespread. Specialized Hardware Description Languages (HDL) are used to describe and develop the required circuits. In this paper, we advocate using an even more general purpose programming language, based on Occam, for the automatic compilation of high-level programs to low-level circuits. The parallel constructs of Occam can map directly to hardware as conveniently as to software, with potentially dramatic speed-up of highly parallel algorithms. We demonstrate that the compilation process can be verified using algebraic refinement laws, increasing the confidence in its correctness. Verification is particularly important in high-integrity systems where safety or security is paramount. A prototype compiler has also been produced very directly from the theorems using the logic programming language Prolog.},
	author = {Bowen, Jonathan P. and Jifeng, He},
	url = {https://doi.org/10.1023/A:1011184310224},
	date = {2001-05-01},
	doi = {10.1023/A:1011184310224},
	issn = {1573-0484},
	journaltitle = {The Journal of Supercomputing},
	keywords = {FPGA,handel-c,verification,hardware},
	number = {1},
	pages = {23--39},
	title = {An Approach To the Specification and Verification of a Hardware Compilation Scheme},
	volume = {19}
}

@article{bowen02_towar_verif_system,
	author = {Bowen, Jonathan P. and Jifeng, He and Hale, Roger W. S. and Herbert, John},
	keywords = {verification,hardware},
	month = aug,
	title = {Towards Verified Systems: the Safemos Project},
	year = {2002}
}

@inbook{bowen06_algeb_approac_hardw_compil,
	abstract = {This chapter presents a provably correct compilation scheme that converts a program into a network of abstract components that interact with each other by exchanging request and acknowledgement signals. We provide a systematic and modular technique for correctly realizing the abstract components in hardware device, and use a standard programming language to describe both algorithms and circuits. The resulting circuitry, which behaves according to the program, has the same structure as the program. The circuit logic is asynchronous, with no global clock.},
	author = {Bowen, Jonathan P. and Jifeng, He},
	editor = {Gabbar, Hossam A.},
	location = {Dordrecht},
	publisher = {Springer Netherlands},
	url = {https://doi.org/10.1007/1-4020-4223-X_7},
	booktitle = {Modern Formal Methods and Applications},
	doi = {10.1007/1-4020-4223-X_7},
	isbn = {978-1-4020-4223-2},
	keywords = {algebraic proofs,verification,synthesis,hardware},
	pages = {151--176},
	title = {An Algebraic Approach to Hardware Compilation},
	year = {2006}
}

@inproceedings{bowen20_perfor_cost_softw_based_secur_mitig,
	author = {Bowen, Lucy and Lupo, Chris},
	title = {The Performance Cost of Software-Based Security Mitigations},
	year = {2020},
	isbn = {9781450369916},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3358960.3379139},
	abstract = {Historically, performance has been the most important feature when optimizing computer hardware. Modern processors are so highly optimized that every cycle of computation time matters. However, this practice of optimizing for performance at all costs has been called into question by new microarchitectural attacks, e.g. Meltdown and Spectre. Microarchitectural attacks exploit the effects of microarchitectural components or optimizations in order to leak data to an attacker. These attacks have caused processor manufacturers to introduce performance impacting mitigations in both software and silicon. To investigate the performance impact of the various mitigations, a test suite of forty-seven different tests was created. This suite was run on a series of virtual machines that tested both Ubuntu 16 and Ubuntu 18. These tests investigated the performance change across version updates and the performance impact of CPU core number vs. default microarchitectural mitigations. The testing proved that the performance impact of the microarchitectural mitigations is non-trivial, as the percent difference in performance can be as high as 200\%.},
	booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
	pages = {210–217},
	numpages = {8},
	keywords = {performance evaluation, hardware vulnerabilities, security mitigations},
	location = {Edmonton AB, Canada},
	series = {ICPE '20}
}

@article{bowen93_devel_correc_system,
	author = {Bowen, Jonathan and Fränzle, Martin and Olderog, Ernst-Rüdiger and Ravn, Anders P.},
	journaltitle = {Bulletin of the EATCS},
	keywords = {verification},
	title = {Developing Correct Systems},
	year = {1993}
}

@article{bowers23_top_down_synth_librar_learn,
	author = {Bowers, Matthew and Olausson, Theo X. and Wong, Lionel and Grand, Gabriel and Tenenbaum, Joshua B. and Ellis, Kevin and Solar-Lezama, Armando},
	title = {Top-Down Synthesis for Library Learning},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571234},
	doi = {10.1145/3571234},
	abstract = {This paper introduces corpus-guided top-down synthesis as a mechanism for synthesizing library functions that capture common functionality from a corpus of programs in a domain specific language (DSL). The algorithm builds abstractions directly from initial DSL primitives, using syntactic pattern matching of intermediate abstractions to intelligently prune the search space and guide the algorithm towards abstractions that maximally capture shared structures in the corpus. We present an implementation of the approach in a tool called Stitch and evaluate it against the state-of-the-art deductive library learning algorithm from DreamCoder. Our evaluation shows that Stitch is 3-4 orders of magnitude faster and uses 2 orders of magnitude less memory while maintaining comparable or better library quality (as measured by compressivity). We also demonstrate Stitch’s scalability on corpora containing hundreds of complex programs that are intractable with prior deductive approaches and show empirically that it is robust to terminating the search procedure early—further allowing it to scale to challenging datasets by means of early stopping.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {41},
	numpages = {32},
	keywords = {Abstraction Learning, Library Learning, Program Synthesis}
}

@inproceedings{braibant10_effic_coq_tactic_decid_kleen_algeb,
	doi = {10.1007/978-3-642-14052-5_13},
	abstract = {We present a reflexive tactic for deciding the equational theory of Kleene algebras in the Coq proof assistant. This tactic relies on a careful implementation of efficient finite automata algorithms, so that it solves casual equations almost instantaneously. The corresponding decision procedure was proved correct and complete; correctness is established w.r.t. any model (including binary relations), by formalising Kozen's initiality theorem.},
	author = {Braibant, Thomas and Pous, Damien},
	editor = {Kaufmann, Matt and Paulson, Lawrence C.},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Interactive Theorem Proving},
	isbn = {978-3-642-14052-5},
	keywords = {regex,coq,path expressions,gated-SSA},
	pages = {163--178},
	title = {An Efficient Coq Tactic for Deciding Kleene Algebras},
	year = {2010}
}

@inproceedings{braibant11_coquet,
	abstract = {We propose a new library to model and verify hardware circuits in the Coq proof assistant. This library allows one to easily build circuits by following the usual pen-and-paper diagrams. We define a deep-embedding: we use a (dependently typed) data-type that models the architecture of circuits, and a meaning function. We propose tactics that ease the reasoning about the behavior of the circuits, and we demonstrate that our approach is practicable by proving the correctness of various circuits: a text-book divide and conquer adder of parametric size, some higher-order combinators of circuits, and some sequential circuits: a buffer, and a register.},
	author = {Braibant, Thomas},
	editor = {Jouannaud, Jean-Pierre and Shao, Zhong},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Certified Programs and Proofs},
	isbn = {978-3-642-25379-9},
	keywords = {verification,coq,hardware},
	pages = {330--345},
	title = {Coquet: A Coq Library for Verifying Hardware},
	year = {2011}
}

@InProceedings{braibant11_tactic_reason_modul_ac_coq,
	author = {Braibant, Thomas and Pous, Damien},
	editor = "Jouannaud, Jean-Pierre
and Shao, Zhong",
	title = "Tactics for Reasoning Modulo AC in Coq",
	booktitle = "Certified Programs and Proofs",
	year = "2011",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "167--182",
	abstract = "We present a set of tools for rewriting modulo associativity and commutativity (AC) in Coq, solving a long-standing practical problem. We use two building blocks: first, an extensible reflexive decision procedure for equality modulo AC; second, an OCaml plug-in for pattern matching modulo AC. We handle associative only operations, neutral elements, uninterpreted function symbols, and user-defined equivalence relations. By relying on type-classes for the reification phase, we can infer these properties automatically, so that end-users do not need to specify which operation is A or AC, or which constant is a neutral element.",
	isbn = "978-3-642-25379-9"
}

@inproceedings{braibant13_formal_verif_hardw_synth,
	abstract = {We report on the implementation of a certified compiler for a high-level hardware description language (HDL) called Fe-Si (FEatherweight SynthesIs). Fe-Si is a simplified version of Bluespec, an HDL based on a notion of guarded atomic actions. Fe-Si is defined as a dependently typed deep embedding in Coq. The target language of the compiler corresponds to a synthesisable subset of Verilog or VHDL. A key aspect of our approach is that input programs to the compiler can be defined and proved correct inside Coq. Then, we use extraction and a Verilog back-end (written in OCaml) to get a certified version of a hardware design.},
	author = {Braibant, Thomas and Chlipala, Adam},
	editor = {Sharygina, Natasha and Veith, Helmut},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Computer Aided Verification},
	isbn = {978-3-642-39799-8},
	keywords = {synthesis,hardware,verification},
	pages = {213--228},
	title = {Formal Verification of Hardware Synthesis},
	year = {2013}
}

@incollection{braibant13_implem_hash_consed_struc_coq,
	author = {Braibant, Thomas and Jourdan, Jacques-Henri and Monniaux, David},
	publisher = {Springer Berlin Heidelberg},
	url = {https://doi.org/10.1007/978-3-642-39634-2_36},
	booktitle = {Interactive Theorem Proving},
	doi = {10.1007/978-3-642-39634-2_36},
	keywords = {hash consing,coq,verification},
	pages = {477--483},
	title = {Implementing Hash-Consed Structures in Coq},
	year = {2013}
}

@article{braibant14_implem_reason_about_hash_data_struc_coq,
	author = {Braibant, Thomas and Jourdan, Jacques-Henri and Monniaux, David},
	publisher = {Springer Science and Business Media {LLC}},
	url = {https://doi.org/10.1007/s10817-014-9306-0},
	doi = {10.1007/s10817-014-9306-0},
	journaltitle = {Journal of Automated Reasoning},
	keywords = {hash consing,verification,coq,memoization},
	month = jun,
	number = {3},
	pages = {271--304},
	title = {Implementing and Reasoning About Hash-consed Data Structures in Coq},
	volume = {53},
	year = {2014}
}

@InProceedings{brummayer09_b,
	keywords = {SMT},
	author = {Brummayer, Robert and Biere, Armin},
	editor = "Kowalewski, Stefan
and Philippou, Anna",
	title = "Boolector: An Efficient SMT Solver for Bit-Vectors and Arrays",
	booktitle = "Tools and Algorithms for the Construction and Analysis of Systems",
	year = "2009",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "174--177",
	abstract = "Satisfiability Modulo Theories (SMT) is the problem of deciding satisfiability of a logical formula, expressed in a combination of first-order theories. We present the architecture and selected features of Boolector, which is an efficient SMT solver for the quantifier-free theories of bit-vectors and arrays. It uses term rewriting, bit-blasting to handle bit-vectors, and lemmas on demand for arrays.",
	isbn = "978-3-642-00768-2"
}

@InProceedings{bubel09_abstr_inter_symbol_execut_explic_state_updat,
	keywords = {abstract interpretation, symbolic execution, predicated execution},
	doi = {10.1007/978-3-642-04167-9_13},
	author = "Bubel, Richard and H{\"a}hnle, Reiner and Wei{\ss}, Benjamin",
	editor = "de Boer, Frank S.
and Bonsangue, Marcello M.
and Madelaine, Eric",
	title = "Abstract Interpretation of Symbolic Execution with Explicit State Updates",
	booktitle = "Formal Methods for Components and Objects",
	year = "2009",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "247--277",
	abstract = "Systems for deductive software verification model the semantics of their target programming language with full precision. On the other hand, abstraction based approaches work with approximations of the semantics in order to be fully automatic. In this paper we aim at providing a uniform framework for both fully precise and approximate reasoning about programs. We present a sound dynamic logic calculus that integrates abstraction in the sense of abstract interpretation theory. In the second part of the paper, we apply the approach to the analysis of secure information flow.",
	isbn = "978-3-642-04167-9"
}

@inproceedings{budiu02_compil_applic_specif_hardw,
	author = {Mihai Budiu and Seth Copen Goldstein},
	editor = {Manfred Glesner and
                  Peter Zipf and
                  Michel Renovell},
	title = {Compiling Application-Specific Hardware},
	booktitle = {Field-Programmable Logic and Applications, Reconfigurable Computing
                  Is Going Mainstream, 12th International Conference, {FPL} 2002, Montpellier,
                  France, September 2-4, 2002, Proceedings},
	series = {Lecture Notes in Computer Science},
	volume = {2438},
	pages = {853--863},
	publisher = {Springer},
	year = {2002},
	doi = {10.1007/3-540-46117-5\_88},
	timestamp = {Tue, 14 May 2019 10:00:48 +0200},
	biburl = {https://dblp.org/rec/conf/fpl/BudiuG02.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{burlyaev15_formal,
	author = {Burlyaev, D. and Fradet, P.},
	url = {https://doi.org/10.1109/FMCAD.2015.7542251},
	booktitle = {2015 Formal Methods in Computer-Aided Design (FMCAD)},
	doi = {10.1109/FMCAD.2015.7542251},
	keywords = {verification,hardware},
	month = sep,
	pages = {41--48},
	title = {Formal verification of automatic circuit transformations for fault-tolerance},
	year = {2015}
}

@misc{bury17_ocaml,
	author = {Guillaume Bury},
	howpublished = {\url{https://github.com/Gbury/mSAT/blob/master/articles/icfp_2017.pdf}},
	title = {an OCaml sat solver},
	year = {2017}
}

@misc{cabanac21_tortur,
	author = {Cabanac, Guillaume and Labbé, Cyril and Magazinov, Alexander},
	eprint = {2107.06751},
	eprintclass = {cs.DL},
	eprinttype = {arXiv},
	title = {Tortured phrases: A dubious writing style emerging in science. Evidence of critical issues affecting established journals},
	year = {2021}
}

@inproceedings{cadar15_target_progr_trans_symbol_execut,
	keywords = {three-valued logic, predicated execution, symbolic execution},
	doi = {10.1145/2786805.2803205},
	url = {https://doi.org/10.1145/2786805.2803205},
	year = {2015},
	month = aug,
	publisher = {{ACM}},
	author = {Cristian Cadar},
	title = {Targeted Program Transformations for Symbolic Execution},
	booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering}
}

@Software{cadence23_c,
	urldate = {2023-12-20},
	url = {https://www.cadence.com/en_US/home/tools/digital-design-and-signoff/logic-equivalence-checking/conformal-equivalence-checker.html},
	year = {2023},
	title = {Conformal Equivalence Checker},
	author = {Cadence}
}

@Software{cadence23_j,
	urldate = {2023-12-20},
	url = {https://www.cadence.com/en_US/home/tools/system-design-and-verification/formal-and-static-verification/jasper-c-formal-verification.html},
	year = {2023},
	title = {Jasper {C2RTL}},
	author = {Cadence}
}

@inproceedings{callahan98_instr_level_paral_recon_comput,
	author = {Timothy J. Callahan and John Wawrzynek},
	editor = {Reiner W. Hartenstein and
                  Andres Keevallik},
	title = {Instruction-Level Parallelism for Reconfigurable Computing},
	booktitle = {Field-Programmable Logic and Applications, From FPGAs to Computing
                  Paradigm, 8th International Workshop, FPL'98, Tallinn, Estonia, August
                  31 - September 3, 1998, Proceedings},
	series = {Lecture Notes in Computer Science},
	volume = {1482},
	pages = {248--257},
	publisher = {Springer},
	year = {1998},
	doi = {10.1007/BFb0055252},
	timestamp = {Tue, 14 May 2019 10:00:48 +0200},
	biburl = {https://dblp.org/rec/conf/fpl/CallahanW98.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{cambronero23_flash,
	author = {Cambronero, Jos\'{e} and Gulwani, Sumit and Le, Vu and Perelman, Daniel and Radhakrishna, Arjun and Simon, Clint and Tiwari, Ashish},
	title = {FlashFill++: Scaling Programming by Example by Cutting to the Chase},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571226},
	doi = {10.1145/3571226},
	abstract = {Programming-by-Examples (PBE) involves synthesizing an "intended program" from a small set of user-provided input-output examples. A key PBE strategy has been to restrict the search to a carefully designed small domain-specific language (DSL) with "effectively-invertible" (EI) operators at the top and "effectively-enumerable" (EE) operators at the bottom. This facilitates an effective combination of top-down synthesis strategy (which backpropagates outputs over various paths in the DSL using inverse functions) with a bottom-up synthesis strategy (which propagates inputs over various paths in the DSL). We address the problem of scaling synthesis to large DSLs with several non-EI/EE operators. This is motivated by the need to support a richer class of transformations and the need for readable code generation. We propose a novel solution strategy that relies on propagating fewer values and over fewer paths. Our first key idea is that of "cut functions" that prune the set of values being propagated by using knowledge of the sub-DSL on the other side. Cuts can be designed to preserve completeness of synthesis; however, DSL designers may use incomplete cuts to have finer control over the kind of programs synthesized. In either case, cuts make search feasible for non-EI/EE operators and efficient for deep DSLs. Our second key idea is that of "guarded DSLs" that allow a precedence on DSL operators, which dynamically controls exploration of various paths in the DSL. This makes search efficient over grammars with large fanouts without losing recall. It also makes ranking simpler yet more effective in learning an intended program from very few examples. Both cuts and precedence provide a mechanism to the DSL designer to restrict search to a reasonable, and possibly incomplete, space of programs. Using cuts and gDSLs, we have built FlashFill++, an industrial-strength PBE engine for performing rich string transformations, including datetime and number manipulations. The FlashFill++ gDSL is designed to enable readable code generation in different target languages including Excel's formula language, PowerFx, and Python. We show FlashFill++ is more expressive, more performant, and generates better quality code than comparable existing PBE systems. FlashFill++ is being deployed in several mass-market products ranging from spreadsheet software to notebooks and business intelligence applications, each with millions of users.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {33},
	numpages = {30},
	keywords = {programming by example, domain-specific languages, string transformations}
}

@inbook{cameron13_under_owner_types_depen_types,
	abstract = {In this paper we will explore the relationship between Ownership Types and more fundamental type systems. In particular, we show that ownership types (in both simple and embellished flavours) are dependent types by translating object calculi with object ownership to lambda calculi with dependent types. We discuss which ownership features share features in the underlying dependent type system, and which additional features require additional complexity.},
	author = {Cameron, Nicholas and Drossopoulou, Sophia and Noble, James},
	editor = {Clarke, Dave and Noble, James and Wrigstad, Tobias},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	url = {https://doi.org/10.1007/978-3-642-36946-9_5},
	booktitle = {Aliasing in Object-Oriented Programming. Types, Analysis and Verification},
	doi = {10.1007/978-3-642-36946-9_5},
	isbn = {978-3-642-36946-9},
	keywords = {ownership types,memory aliasing},
	pages = {84--108},
	title = {Understanding Ownership Types with Dependent Types},
	year = {2013}
}

@article{campbell93_refin,
	author = {Campbell, Philip L and Krishna, Ksheerabdhi and Ballance, Robert A},
	publisher = {Citeseer},
	journaltitle = {Cs93-6, University of New Mexico, Albuquerque},
	keywords = {gated-SSA},
	title = {Refining and defining the program dependence web},
	year = {1993}
}

@article{campion22_partial_in_compl_abstr_inter,
	author = {Campion, Marco and Dalla Preda, Mila and Giacobazzi, Roberto},
	title = {Partial (In)Completeness in Abstract Interpretation: Limiting the Imprecision in Program Analysis},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498721},
	doi = {10.1145/3498721},
	abstract = {Imprecision is inherent in any decidable (sound) approximation of undecidable program properties. In abstract interpretation this corresponds to the release of false alarms, e.g., when it is used for program analysis and program verification. As all alarming systems, a program analysis tool is credible when few false alarms are reported. As a consequence, we have to live together with false alarms, but also we need methods to control them. As for all approximation methods, also for abstract interpretation we need to estimate the accumulated imprecision during program analysis. In this paper we introduce a theory for estimating the error propagation in abstract interpretation, and hence in program analysis. We enrich abstract domains with a weakening of a metric distance. This enriched structure keeps coherence between the standard partial order relating approximated objects by their relative precision and the effective error made in this approximation. An abstract interpretation is precise when it is complete. We introduce the notion of partial completeness as a weakening of precision. In partial completeness the abstract interpreter may produce a bounded number of false alarms. We prove the key recursive properties of the class of programs for which an abstract interpreter is partially complete with a given bound of imprecision. Then, we introduce a proof system for estimating an upper bound of the error accumulated by the abstract interpreter during program analysis. Our framework is general enough to be instantiated to most known metrics for abstract domains.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {59},
	numpages = {31},
	keywords = {Abstract Interpretation, Abstract Domain, Program Analysis, Partial Completeness}
}

@inproceedings{canesche22_polyn_time_exact_solut_bit,
	author = {Canesche, Michael and Ferreira, Ricardo and Nacif, Jos\'{e} Augusto and Quint\~{a}o Pereira, Fernando Magno},
	title = {A Polynomial Time Exact Solution to the Bit-Aware Register Binding Problem},
	year = {2022},
	isbn = {9781450391832},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497776.3517773},
	doi = {10.1145/3497776.3517773},
	abstract = {Finding the minimum register bank is an optimization problem related to the synthesis of hardware. Given a program, the problem asks for the minimum number of registers plus their minimum size, in bits, that suffices to compile said program. This problem is NP-complete; hence, usually solved via heuristics. In this paper, we show that this problem has an optimal solution in polynomial time, as long as swaps can be inserted in the program to move variables across registers. This observation sets a lower bound to heuristics that minimize the size of register banks. We have compared the optimal algorithm with two classic heuristics. Our approach uses, on average, 6 to 10% less bits than that previous work.},
	booktitle = {Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction},
	pages = {29–40},
	numpages = {12},
	keywords = {Register allocation, Polynomial, Bit-width, Optimization, High-Level Synthesis},
	location = {Seoul, South Korea},
	series = {CC 2022}
}

@inproceedings{canis11_legup,
	abstract = {In this paper, we introduce a new open source high-level synthesis tool called LegUp that allows software techniques to be used for hardware design. LegUp accepts a standard C program as input and automatically compiles the program to a hybrid architecture containing an FPGA-based MIPS soft processor and custom hardware accelerators that communicate through a standard bus interface. Results show that the tool produces hardware solutions of comparable quality to a commercial high-level synthesis tool.},
	author = {Canis, Andrew and Choi, Jongsok and Aldham, Mark and Zhang, Victor and Kammoona, Ahmed and Anderson, Jason H. and Brown, Stephen and Czajkowski, Tomasz},
	location = {Monterey, CA, USA},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 19th ACM/SIGDA International Symposium on Field Programmable Gate Arrays},
	doi = {10.1145/1950413.1950423},
	isbn = {9781450305549},
	keywords = {fpgas,hardware/software co-design,field-programmable gate arrays,high-level synthesis},
	pages = {33--36},
	series = {FPGA '11},
	title = {LegUp: High-Level Synthesis for FPGA-Based Processor/Accelerator Systems},
	year = {2011}
}

@inproceedings{canis14_modul_sdc,
	abstract = {Loop pipelining is a high-level synthesis scheduling technique that overlaps loop iterations to achieve higher performance. However, industrial designs often have resource constraints and other constraints imposed by cross-iteration dependencies. The interaction between multiple constraints can pose a challenge for HLS modulo scheduling algorithms, which, if not handled properly can lead to a loop pipeline schedule that fails to achieve the minimum possible initiation interval. We propose a novel modulo scheduler based on an SDC formulation that includes a backtracking mechanism to properly handle multiple scheduling constraints and still achieve the minimum possible initiation interval. The SDC formulation has the advantage of being a mathematical framework that supports flexible constraints that are useful for more complex loop pipelines. Furthermore, we describe how to specifically apply associative expression transformations during modulo scheduling to restructure recurrences in complex loops to enable better scheduling. We compared our techniques to existing prior work in modulo scheduling in HLS and also compared against a state-of-art commercial tool. Over a suite of benchmarks, we show that our scheduler and proposed optimizations can result in a geomean wall-clock time reduction of 32 \% versus prior work and 29 \% versus a commercial tool.},
	author = {Canis, A. and Brown, S. D. and Anderson, J. H.},
	booktitle = {2014 24th International Conference on Field Programmable Logic and Applications (FPL)},
	doi = {10.1109/FPL.2014.6927490},
	issn = {1946-1488},
	keywords = {high-level synthesis,modulo scheduling,loop scheduling,static scheduling},
	month = sep,
	pages = {1--8},
	title = {Modulo SDC scheduling with recurrence minimization in high-level synthesis},
	year = {2014}
}

@PhdThesis{canis15_legup,
	author = {Canis, Andrew},
	keywords = {high-level synthesis,hardware/software co-simulation,FPGA},
	title = {Legup: open-source high-level synthesis research framework},
	type = {phdthesis},
	year = {2015}
}

@incollection{canis16_legup_high_level_synth,
	author = {Canis, Andrew and Choi, Jongsok and Fort, Blair and Syrowik, Bain and Lian, Ruo Long and Chen, Yu Ting and Hsiao, Hsuan and Goeders, Jeffrey and Brown, Stephen and Anderson, Jason},
	publisher = {Springer International Publishing},
	url = {https://doi.org/10.1007/978-3-319-26408-0_10},
	booktitle = {{FPGAs} for Software Programmers},
	doi = {10.1007/978-3-319-26408-0_10},
	pages = {175--190},
	title = {{LegUp} High-Level Synthesis},
	year = {2016}
}

@article{cao23_babbl,
	author = {Cao, David and Kunkel, Rose and Nandi, Chandrakana and Willsey, Max and Tatlock, Zachary and Polikarpova, Nadia},
	title = {Babble: Learning Better Abstractions with E-Graphs and Anti-Unification},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571207},
	doi = {10.1145/3571207},
	abstract = {Library learning compresses a given corpus of programs by extracting common structure from the corpus into reusable library functions. Prior work on library learning suffers from two limitations that prevent it from scaling to larger, more complex inputs. First, it explores too many candidate library functions that are not useful for compression. Second, it is not robust to syntactic variation in the input. We propose library learning modulo theory (LLMT), a new library learning algorithm that additionally takes as input an equational theory for a given problem domain. LLMT uses e-graphs and equality saturation to compactly represent the space of programs equivalent modulo the theory, and uses a novel e-graph anti-unification technique to find common patterns in the corpus more directly and efficiently. We implemented LLMT in a tool named babble. Our evaluation shows that babble achieves better compression orders of magnitude faster than the state of the art. We also provide a qualitative evaluation showing that babble learns reusable functions on inputs previously out of reach for library learning.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {14},
	numpages = {29},
	keywords = {e-graphs, library learning, anti-unification}
}

@article{capriotti14_free_applic_funct,
	keywords = {applicative},
	url = {https://doi.org/10.4204%2Feptcs.153.2},
	year = 2014,
	month = {jun},
	publisher = {Open Publishing Association},
	volume = {153},
	pages = {2--30},
	author = {Paolo Capriotti and Ambrus Kaposi},
	title = {Free Applicative Functors},
	journal = {Electronic Proceedings in Theoretical Computer Science}
}

@inproceedings{carbonneaux22_apply_formal_verif_microk_ipc_meta,
	author = {Carbonneaux, Quentin and Zilberstein, Noam and Klee, Christoph and O'Hearn, Peter W. and Zappa Nardelli, Francesco},
	title = {Applying Formal Verification to Microkernel IPC at Meta},
	year = {2022},
	isbn = {9781450391825},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497775.3503681},
	doi = {10.1145/3497775.3503681},
	abstract = {We use Iris, an implementation of concurrent separation logic in the Coq proof assistant, to verify two queue data structures used for inter-process communication in an operating system under development. Our motivations are twofold. First, we wish to leverage formal verification to boost confidence in a delicate piece of industrial code that was subject to numerous revisions. Second, we aim to gain information on the cost-benefit tradeoff of applying a state-of-the-art formal verification tool in our industrial setting. On both fronts, our endeavor has been a success. The verification effort proved that the queue algorithms are correct and uncovered four algorithmic simplifications as well as bugs in client code. The simplifications involve the removal of two memory barriers, one atomic load, and one boolean check, all in a performance-sensitive part of the OS. Removing the redundant boolean check revealed unintended uses of uninitialized memory in multiple device drivers, which were fixed. The proof work was completed in person months, not years, by engineers with no prior familiarity with Iris. These findings are spurring further use of verification at Meta.},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {116–129},
	numpages = {14},
	keywords = {program verification, concurrency, systems, separation logic, circular queue},
	location = {Philadelphia, PA, USA},
	series = {CPP 2022}
}

@article{cardoso10_compil_recon_comput,
	author = {Cardoso, João M. P. and Diniz, Pedro C. and Weinhardt, Markus},
	title = {Compiling for Reconfigurable Computing: A Survey},
	year = {2010},
	issue_date = {June 2010},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {42},
	number = {4},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/1749603.1749604},
	doi = {10.1145/1749603.1749604},
	abstract = {Reconfigurable computing platforms offer the promise of substantially accelerating computations through the concurrent nature of hardware structures and the ability of these architectures for hardware customization. Effectively programming such reconfigurable architectures, however, is an extremely cumbersome and error-prone process, as it requires programmers to assume the role of hardware designers while mastering hardware description languages, thus limiting the acceptance and dissemination of this promising technology. To address this problem, researchers have developed numerous approaches at both the programming languages as well as the compilation levels, to offer high-level programming abstractions that would allow programmers to easily map applications to reconfigurable architectures. This survey describes the major research efforts on compilation techniques for reconfigurable computing architectures. The survey focuses on efforts that map computations written in imperative programming languages to reconfigurable architectures and identifies the main compilation and synthesis techniques used in this mapping.},
	journal = {ACM Comput. Surv.},
	month = {jun},
	articleno = {13},
	numpages = {65},
	keywords = {high-level synthesis, predicated execution, if-conversion, survey}
}

@article{carmona09_elast_circuit,
	author = {Carmona, J. and Cortadella, J. and Kishinevsky, M. and Taubin, A.},
	journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {dynamic scheduling},
	number = {10},
	pages = {1437--1455},
	title = {Elastic Circuits},
	volume = {28},
	year = {2009}
}

@article{carter00_path_analy_renam_predic_instr_sched,
	author = {Carter, Lori and Simon, Beth and Calder, Brad and Carter, Larry and Ferrante, Jeanne},
	publisher = {Springer Science and Business Media {LLC}},
	url = {https://doi.org/10.1023/a:1007512717742},
	doi = {10.1023/a:1007512717742},
	journaltitle = {International Journal of Parallel Programming},
	keywords = {predicated execution,register allocation,register renaming, hyperblocks},
	number = {6},
	pages = {563--588},
	title = {Path Analysis and Renaming for Predicated Instruction Scheduling},
	volume = {28},
	year = {2000}
}

@inproceedings{carter02_using_predic_path_infor_hardw,
	author = {Carter, Lori and Calder, Brad},
	title = {Using Predicate Path Information in Hardware to Determine True Dependences},
	year = {2002},
	isbn = {1581134835},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/514191.514224},
	doi = {10.1145/514191.514224},
	abstract = {Predicated Execution has been put forth as a method for improving processor performance by removing hard-to-predict branches. As part of the process of turning a set of basic blocks into a predicated region, both paths of a branch are combined into a single path. There can be multiple definitions from disjoint paths that reach a use. Waiting to find out the correct definition that actually reaches the use can cause pipeline stalls.In this paper we examine a hardware optimization that dynamically collects and analyzes path information to determine valid dependences for predicated regions of code. We then use this information for an in-order VLIW predicated processor, so that instructions can continue towards execution without having to wait on operands from false dependences. Our results show that using our Disjoint Path Analysis System provides speedups over 6% and elimination of false RAW dependences of up to 14% due to the detection of erroneous dependences in if-converted regions of code.},
	booktitle = {Proceedings of the 16th International Conference on Supercomputing},
	pages = {230–240},
	numpages = {11},
	keywords = {predicated execution, if-conversion, hyperblocks},
	location = {New York, New York, USA},
	series = {ICS '02}
}

@article{castagna22_type_cases_union_elimin_occur_typin,
	author = {Castagna, Giuseppe and Laurent, Micka\"{e}l and Nguyundefinedn, Kim and Lutze, Matthew},
	title = {On Type-Cases, Union Elimination, and Occurrence Typing},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498674},
	doi = {10.1145/3498674},
	abstract = {We extend classic union and intersection type systems with a type-case construction and show that the combination of the union elimination rule of the former and the typing rules for type-cases of our extension encompasses occurrence typing. To apply this system in practice, we define a canonical form for the expressions of our extension, called MSC-form. We show that an expression of the extension is typable if and only if its MSC-form is, and reduce the problem of typing the latter to the one of reconstructing annotations for that term. We provide a sound algorithm that performs this reconstruction and a proof-of-concept implementation.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {13},
	numpages = {31},
	keywords = {union types, type-case, intersection types, type systems, dynamic languages, subtyping}
}

@article{castellan23_geomet_causal,
	author = {Castellan, Simon and Clairambault, Pierre},
	title = {The Geometry of Causality: Multi-Token Geometry of Interaction and Its Causal Unfolding},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571217},
	doi = {10.1145/3571217},
	abstract = {We introduce a multi-token machine for Idealized Parallel Algol (IPA), a higher-order concurrent programming language with shared state and semaphores. Our machine takes the shape of a compositional interpretation of terms as Petri structures, certain coloured Petri nets. For the purely functional fragment of IPA, our machine is conceptually close to Geometry of Interaction token machines, originating from Linear Logic and presenting higher-order computation as the low-level process of a token walking through a graph (a proof net) representing the term. We combine here these ideas with folklore ideas on the representation of first-order imperative concurrent programs as coloured Petri nets. To prove our machine computationally adequate with respect to the reference operational semantics, we follow game semantics and represent types as certain games specifying dependencies and conflict between computational events. Petri strategies are those Petri structures obeying the rules of the game extracted from the type. We show how Petri strategies unfold to concurrent strategies in the sense of concurrent games on event structures. This link with concurrent strategies not only allows us to prove adequacy of our machine, but also lets us generate operationally a causal description of the behaviour of programs at higher-order types, which is shown to coincide with that given denotationally by the interpretation in concurrent games.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {24},
	numpages = {29},
	keywords = {Game Semantics, Shared Memory Concurrency, Geometry of Interaction, Higher-Order, Coloured Petri Nets}
}

@inproceedings{chan22_windm_minds,
	author = {Chan, Hing Lun},
	title = {Windmills of the Minds: An Algorithm for Fermat’s Two Squares Theorem},
	year = {2022},
	isbn = {9781450391825},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497775.3503673},
	doi = {10.1145/3497775.3503673},
	abstract = {The two squares theorem of Fermat is a gem in number theory, with a spectacular one-sentence "proof from the Book". Here is a formalisation of this proof, with an interpretation using windmill patterns. The theory behind involves involutions on a finite set, especially the parity of the number of fixed points in the involutions. Starting as an existence proof that is non-constructive, there is an ingenious way to turn it into a constructive one. This gives an algorithm to compute the two squares by iterating the two involutions alternatively from a known fixed point.},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {251–264},
	numpages = {14},
	keywords = {Algorithm, Iteractive Theorem Proving, Number Theory},
	location = {Philadelphia, PA, USA},
	series = {CPP 2022}
}

@inproceedings{chandy89_paral_progr_desig,
	abstract = {My goal is to propose a set of questions that I think are important. J. Misra and I are working on these questions.},
	author = {Chandy, K. Mani},
	editor = {Sanz, Jorge L. C.},
	location = {New York, NY},
	publisher = {Springer US},
	booktitle = {Opportunities and Constraints of Parallel Computing},
	doi = {10.1007/978-1-4613-9668-0_6},
	isbn = {978-1-4613-9668-0},
	keywords = {unity,semantics},
	pages = {21--24},
	title = {Parallel Program Design},
	year = {1989}
}

@article{chang91_using_profil_infor_assis_class_code_optim,
	author = {Pohua P. Chang and
                  Scott A. Mahlke and
                  Wen{-}mei W. Hwu},
	title = {Using Profile Information to Assist Classic Code Optimizations},
	journal = {Softw. Pract. Exp.},
	volume = {21},
	number = {12},
	pages = {1301--1321},
	year = {1991},
	doi = {10.1002/spe.4380211204},
	timestamp = {Thu, 09 Apr 2020 17:14:34 +0200},
	biburl = {https://dblp.org/rec/journals/spe/ChangMH91.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{chapman92_verif_bedroc,
	abstract = {The authors present the HardwarePal hardware description language and formal operational and denotational semantics for it, briefly discussing their proof of the two semantics' equivalence. They then discuss their intermediate representation, dependence flow graphs, and the operational semantics of DFG. They describe the translation from HardwarePal to dependence flow graphs and outline their proof that this translation preserves the meaning of the initial HardwarePal program. The authors discuss proving the correctness of the translation from behavioral specification to intermediate form, proving the correctness of optimizations, and plans for proving the correctness of scheduling. The authors conclude by discussing their plans for proofs that register-transfer level design produced by BEDROC implements the dependence flow graph.<>},
	author = {Chapman, R. and Brown, G. and Leeser, M.},
	publisher = {IEEE Computer Society},
	url = {https://doi.org/10.1109/EDAC.1992.205894},
	booktitle = {[1992] Proceedings The European Conference on Design Automation},
	doi = {10.1109/EDAC.1992.205894},
	keywords = {verification,synthesis},
	month = mar,
	pages = {59--63},
	title = {Verified high-level synthesis in BEDROC},
	year = {1992}
}

@article{chappe23_choic_trees,
	author = {Chappe, Nicolas and He, Paul and Henrio, Ludovic and Zakowski, Yannick and Zdancewic, Steve},
	title = {Choice Trees: Representing Nondeterministic, Recursive, and Impure Programs in Coq},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571254},
	doi = {10.1145/3571254},
	abstract = {This paper introduces ctrees, a monad for modeling nondeterministic, recursive, and impure programs in Coq. Inspired by Xia et al.'s itrees, this novel data structure embeds computations into coinductive trees with three kind of nodes: external events, and two variants of nondeterministic branching. This apparent redundancy allows us to provide shallow embedding of denotational models with internal choice in the style of CCS, while recovering an inductive LTS view of the computation. ctrees inherit a vast collection of bisimulation and refinement tools, with respect to which we establish a rich equational theory. We connect ctrees to the itree infrastructure by showing how a monad morphism embedding the former into the latter permits to use ctrees to implement nondeterministic effects. We demonstrate the utility of ctrees by using them to model concurrency semantics in two case studies: CCS and cooperative multithreading.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {61},
	numpages = {31},
	keywords = {Concurrency, Interaction Trees, Formal Semantics, Nondeterminism}
}

@article{chatha02_hardw,
	author = {Chatha, K.S. and Vemuri, R.},
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
	url = {https://doi.org/10.1109/tvlsi.2002.1043323},
	doi = {10.1109/tvlsi.2002.1043323},
	journaltitle = {{IEEE} Transactions on Very Large Scale Integration ({VLSI}) Systems},
	month = jun,
	number = {3},
	pages = {193--208},
	title = {Hardware-software partitioning and pipelined scheduling of transformative applications},
	volume = {10},
	year = {2002}
}

@inproceedings{chatterjee21_provin_non_termin_progr_rever,
	abstract = {We present a new approach to proving non-termination of non-deterministic integer programs. Our technique is rather simple but efficient. It relies on a purely syntactic reversal of the program's transition system followed by a constraint-based invariant synthesis with constraints coming from both the original and the reversed transition system. The latter task is performed by a simple call to an off-the-shelf SMT-solver, which allows us to leverage the latest advances in SMT-solving. Moreover, our method offers a combination of features not present (as a whole) in previous approaches: it handles programs with non-determinism, provides relative completeness guarantees and supports programs with polynomial arithmetic. The experiments performed with our prototype tool RevTerm show that our approach, despite its simplicity and stronger theoretical guarantees, is at least on par with the state-of-the-art tools, often achieving a non-trivial improvement under a proper configuration of its parameters.},
	author = {Chatterjee, Krishnendu and Goharshady, Ehsan Kafshdar and Novotný, Petr and Žikelić, undefinedorundefinede},
	location = {Virtual, Canada},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3453483.3454093},
	booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
	doi = {10.1145/3453483.3454093},
	isbn = {9781450383912},
	keywords = {Completeness Guarantees,Invariant Generation,Static Analysis,Program Termination,Backward Analysis},
	pages = {1033--1048},
	series = {PLDI 2021},
	title = {Proving Non-Termination by Program Reversal},
	year = {2021}
}

@inproceedings{chauhan09_non_sequen_equiv_check,
	author = {Chauhan, P. and Goyal, D. and Hasteer, G. and Mathur, A. and Sharma, N.},
	url = {https://doi.org/10.1145/1629911.1630033},
	booktitle = {2009 46th ACM/IEEE Design Automation Conference},
	doi = {10.1145/1629911.1630033},
	keywords = {high-level synthesis,verification,translation validation,equivalence checking},
	month = jul,
	pages = {460--465},
	title = {Non-cycle-accurate Sequential Equivalence Checking},
	year = {2009}
}

@misc{chauhan20_formal_ensur_equiv_c_rtl,
	note = {SLEC},
	author = {Chauhan, Pankaj},
	url = {https://bit.ly/2KbT0ki},
	title = {Formally Ensuring Equivalence between C++ and RTL designs},
	year = {2020}
}

@inproceedings{chen15_reliab,
	author = {Chen, Liang and Ebrahimi, Mojtaba and Tahoori, Mehdi B.},
	booktitle = {2015 20th IEEE European Test Symposium (ETS)},
	doi = {10.1109/ETS.2015.7138739},
	keywords = {operation chaining},
	pages = {1--6},
	title = {Reliability-aware operation chaining in high level synthesis},
	year = {2015}
}

@inproceedings{chen16_empir_compar_compil_testin_techn,
	abstract = {Compilers, as one of the most important infrastructure of today's digital world, are expected to be trustworthy. Different testing techniques are developed for testing compilers automatically. However, it is unknown so far how these testing techniques compared to each other in terms of testing effectiveness: how many bugs a testing technique can find within a time limit.In this paper, we conduct a systematic and comprehensive empirical comparison of three compiler testing techniques, namely, Randomized Differential Testing (RDT), a variant of RDT---Different Optimization Levels (DOL), and Equivalence Modulo Inputs (EMI). Our results show that DOL is more effective at detecting bugs related to optimization, whereas RDT is more effective at detecting other types of bugs, and the three techniques can complement each other to a certain degree.Furthermore, in order to understand why their effectiveness differs, we investigate three factors that influence the effectiveness of compiler testing, namely, efficiency, strength of test oracles, and effectiveness of generated test programs. The results indicate that all the three factors are statistically significant, and efficiency has the most significant impact.},
	author = {Chen, Junjie and Hu, Wenxiang and Hao, Dan and Xiong, Yingfei and Zhang, Hongyu and Zhang, Lu and Xie, Bing},
	location = {Austin, Texas},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/2884781.2884878},
	booktitle = {Proceedings of the 38th International Conference on Software Engineering},
	doi = {10.1145/2884781.2884878},
	isbn = {9781450339001},
	pages = {180--190},
	series = {ICSE '16},
	title = {An Empirical Comparison of Compiler Testing Techniques},
	year = {2016}
}

@inproceedings{chen17_autom,
	abstract = {Some modern high-level synthesis (HLS) tools [1] permit the synthesis of multi-threaded software into parallel hardware, where concurrent software threads are realized as concurrently operating hardware units. A common performance bottleneck in any parallel implementation (whether it be hardware or software) is memory bandwidth - parallel threads demand concurrent access to memory resulting in contention which hurts performance. FPGAs contain an abundance of independently accessible memories offering high internal memory bandwidth. We describe an approach for leveraging such bandwidth in the context of synthesizing parallel software into hardware. Our approach applies trace-based profiling to determine how a program's arrays should be automatically partitioned into sub-arrays, which are then implemented in separate on-chip RAM blocks within the target FPGA. The partitioning is accomplished in a way that requires a single HLS execution and logic simulation for trace extraction. The end result is that each thread, when implemented in hardware, has exclusive access to its own memories to the extent possible, significantly reducing contention and arbitration and thus raising performance.},
	author = {Chen, Y. T. and Anderson, J. H.},
	booktitle = {2017 27th International Conference on Field Programmable Logic and Applications (FPL)},
	doi = {10.23919/FPL.2017.8056841},
	issn = {1946-1488},
	keywords = {predicated execution,memory architecture,legup,high-level synthesis},
	month = sep,
	pages = {1--8},
	title = {Automated generation of banked memory architectures in the high-level synthesis of multi-threaded software},
	year = {2017}
}

@InProceedings{chen18_learn_accel_symbol_execut_code_trans,
	keywords = {symbolic execution, value summaries, hyperblocks},
	author = {Junjie Chen and Wenxiang Hu and Lingming Zhang and Dan Hao and Sarfraz Khurshid and Lu Zhang},
	title = {{Learning to Accelerate Symbolic Execution via Code Transformation}},
	booktitle = {32nd European Conference on Object-Oriented Programming  (ECOOP 2018)},
	pages = {6:1--6:27},
	series = {Leibniz International Proceedings in Informatics (LIPIcs)},
	ISBN = {978-3-95977-079-8},
	ISSN = {1868-8969},
	year = {2018},
	volume = {109},
	editor = {Todd Millstein},
	publisher = {Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik},
	address = {Dagstuhl, Germany},
	URL = {http://drops.dagstuhl.de/opus/volltexte/2018/9211},
	URN = {urn:nbn:de:0030-drops-92115},
	doi = {10.4230/LIPIcs.ECOOP.2018.6},
	annote = {Keywords: Symbolic Execution, Code Transformation, Machine Learning}
}

@INPROCEEDINGS{chen20_savior,
	author = {Chen, Yaohui and Li, Peng and Xu, Jun and Guo, Shengjian and Zhou, Rundong and Zhang, Yulong and Wei, Tao and Lu, Long},
	booktitle = {2020 IEEE Symposium on Security and Privacy (SP)},
	title = {SAVIOR: Towards Bug-Driven Hybrid Testing},
	year = {2020},
	volume = {},
	number = {},
	pages = {1580-1596},
	abstract = {Hybrid testing combines fuzz testing and concolic execution. It leverages fuzz testing to test easy-to-reach code regions and uses concolic execution to explore code blocks guarded by complex branch conditions. As a result, hybrid testing is able to reach deeper into program state space than fuzz testing or concolic execution alone. Recently, hybrid testing has seen significant advancement. However, its code coverage-centric design is inefficient in vulnerability detection. First, it blindly selects seeds for concolic execution and aims to explore new code continuously. However, as statistics show, a large portion of the explored code is often bug-free. Therefore, giving equal attention to every part of the code during hybrid testing is a non-optimal strategy. It slows down the detection of real vulnerabilities by over 43%. Second, classic hybrid testing quickly moves on after reaching a chunk of code, rather than examining the hidden defects inside. It may frequently miss subtle vulnerabilities despite that it has already explored the vulnerable code paths.We propose SAVIOR, a new hybrid testing framework pioneering a bug-driven principle. Unlike the existing hybrid testing tools, SAVIOR prioritizes the concolic execution of the seeds that are likely to uncover more vulnerabilities. Moreover, SAVIOR verifies all vulnerable program locations along the executing program path. By modeling faulty situations using SMT constraints, SAVIOR reasons the feasibility of vulnerabilities and generates concrete test cases as proofs. Our evaluation shows that the bug-driven approach outperforms mainstream automated testing techniques, including state-of-the-art hybrid testing systems driven by code coverage. On average, SAVIOR detects vulnerabilities 43.4% faster than DRILLER and 44.3% faster than QSYM, leading to the discovery of 88 and 76 more unique bugs, respectively. According to the evaluation on 11 well fuzzed benchmark programs, within the first 24 hours, SAVIOR triggers 481 UBSAN violations, among which 243 are real bugs.},
	keywords = {static analysis testing},
	doi = {10.1109/SP40000.2020.00002},
	ISSN = {2375-1207},
	month = {May}
}

@article{chen20_survey_compil_testin,
	author = {Chen, Junjie and Patra, Jibesh and Pradel, Michael and Xiong, Yingfei and Zhang, Hongyu and Hao, Dan and Zhang, Lu},
	title = {A Survey of Compiler Testing},
	year = {2020},
	issue_date = {January 2021},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {53},
	number = {1},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3363562},
	doi = {10.1145/3363562},
	abstract = {Virtually any software running on a computer has been processed by a compiler or a compiler-like tool. Because compilers are such a crucial piece of infrastructure for building software, their correctness is of paramount importance. To validate and increase the correctness of compilers, significant research efforts have been devoted to testing compilers. This survey article provides a comprehensive summary of the current state-of-the-art of research on compiler testing. The survey covers different aspects of the compiler testing problem, including how to construct test programs, what test oracles to use for determining whether a compiler behaves correctly, how to execute compiler tests efficiently, and how to help compiler developers take action on bugs discovered by compiler testing. Moreover, we survey work that empirically studies the strengths and weaknesses of current compiler testing research and practice. Based on the discussion of existing work, we outline several open challenges that remain to be addressed in future work.},
	journal = {ACM Comput. Surv.},
	month = {feb},
	articleno = {4},
	numpages = {36},
	keywords = {static analysis testing, fuzz testing}
}

@article{chen21_comput_inter_compac_closed_categ,
	abstract = {Compact closed categories include objects representing higher-order functions and are well-established as models of linear logic, concurrency, and quantum computing. We show that it is possible to construct such compact closed categories for conventional sum and product types by defining a dual to sum types, a negative type, and a dual to product types, a fractional type. Inspired by the categorical semantics, we define a sound operational semantics for negative and fractional types in which a negative type represents a computational effect that ``reverses execution flow'' and a fractional type represents a computational effect that ``garbage collects'' particular values or throws exceptions. Specifically, we extend a first-order reversible language of type isomorphisms with negative and fractional types, specify an operational semantics for each extension, and prove that each extension forms a compact closed category. We furthermore show that both operational semantics can be merged using the standard combination of backtracking and exceptions resulting in a smooth interoperability of negative and fractional types. We illustrate the expressiveness of this combination by writing a reversible SAT solver that uses backtracking search along freshly allocated and de-allocated locations. The operational semantics, most of its meta-theoretic properties, and all examples are formalized in a supplementary Agda package.},
	author = {Chen, Chao-Hong and Sabry, Amr},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434290},
	doi = {10.1145/3434290},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {Higher-Order Reversible Programming,Duality of Computation,Termination Proofs,Type Isomorphisms,Abstract Machines},
	month = jan,
	number = {POPL},
	title = {A Computational Interpretation of Compact Closed Categories: Reversible Programming with Negative and Fractional Types},
	volume = {5},
	year = {2021}
}

@article{chen22_solvin_strin_const_regex_depen,
	author = {Chen, Taolue and Flores-Lamas, Alejandro and Hague, Matthew and Han, Zhilei and Hu, Denghang and Kan, Shuanglong and Lin, Anthony W. and R\"{u}mmer, Philipp and Wu, Zhilin},
	title = {Solving String Constraints with Regex-Dependent Functions through Transducers with Priorities and Variables},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498707},
	doi = {10.1145/3498707},
	abstract = {Regular expressions are a classical concept in formal language theory. Regular expressions in programming languages (RegEx) such as JavaScript, feature non-standard semantics of operators (e.g. greedy/lazy Kleene star), as well as additional features such as capturing groups and references. While symbolic execution of programs containing RegExes appeals to string solvers natively supporting important features of RegEx, such a string solver is hitherto missing. In this paper, we propose the first string theory and string solver that natively provides such support. The key idea of our string solver is to introduce a new automata model, called prioritized streaming string transducers (PSST), to formalize the semantics of RegEx-dependent string functions. PSSTs combine priorities, which have previously been introduced in prioritized finite-state automata to capture greedy/lazy semantics, with string variables as in streaming string transducers to model capturing groups. We validate the consistency of the formal semantics with the actual JavaScript semantics by extensive experiments. Furthermore, to solve the string constraints, we show that PSSTs enjoy nice closure and algorithmic properties, in particular, the regularity-preserving property (i.e., pre-images of regular constraints under PSSTs are regular), and introduce a sound sequent calculus that exploits these properties and performs propagation of regular constraints by means of taking post-images or pre-images. Although the satisfiability of the string constraint language is generally undecidable, we show that our approach is complete for the so-called straight-line fragment. We evaluate the performance of our string solver on over 195000 string constraints generated from an open-source RegEx library. The experimental results show the efficacy of our approach, drastically improving the existing methods (via symbolic execution) in both precision and efficiency.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {45},
	numpages = {31},
	keywords = {Symbolic Execution, Regular Expressions, Transducers, String Constraint Solving}
}

@article{chen23_dargen,
	author = {Chen, Zilin and Lafont, Ambroise and O'Connor, Liam and Keller, Gabriele and McLaughlin, Craig and Jackson, Vincent and Rizkallah, Christine},
	title = {Dargent: A Silver Bullet for Verified Data Layout Refinement},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571240},
	doi = {10.1145/3571240},
	abstract = {Systems programmers need fine-grained control over the memory layout of data structures, both to produce performant code and to comply with well-defined interfaces imposed by existing code, standardised protocols or hardware. Code that manipulates these low-level representations in memory is hard to get right. Traditionally, this problem is addressed by the implementation of tedious marshalling code to convert between compiler-selected data representations and the desired compact data formats. Such marshalling code is error-prone and can lead to a significant runtime overhead due to excessive copying. While there are many languages and systems that address the correctness issue, by automating the generation and, in some cases, the verification of the marshalling code, the performance overhead introduced by the marshalling code remains. In particular for systems code, this overhead can be prohibitive. In this work, we address both the correctness and the performance problems. We present a data layout description language and data refinement framework, called Dargent, which allows programmers to declaratively specify how algebraic data types are laid out in memory. Our solution is applied to the Cogent language, but the general ideas behind our solution are applicable to other settings. The Dargent framework generates C code that manipulates data directly with the desired memory layout, while retaining the formal proof that this generated C code is correct with respect to the functional semantics. This added expressivity removes the need for implementing and verifying marshalling code, which eliminates copying, smoothens interoperability with surrounding systems, and increases the trustworthiness of the overall system.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {47},
	numpages = {27},
	keywords = {systems programming, data refinement, certifying compiler}
}

@inproceedings{cheng19_easy,
	abstract = {High-Level Synthesis (HLS) tools automatically transform a high-level specification of a circuit into a low-level RTL description. Traditionally, HLS tools have operated on sequential code, however in recent years there has been a drive to synthesize multi-threaded code. A major challenge facing HLS tools in this context is how to automatically partition memory amongst parallel threads to fully exploit the bandwidth available on an FPGA device and avoid memory contention. Current automatic memory partitioning techniques have inefficient arbitration due to conservative assumptions regarding which threads may access a given memory bank. In this paper, we address this problem through formal verification techniques, permitting a less conservative, yet provably correct circuit to be generated. We perform a static analysis on the code to determine which memory banks are shared by which threads. This analysis enables us to optimize the arbitration efficiency of the generated circuit. We apply our approach to the LegUp HLS tool and show that for a set of typical application benchmarks we can achieve up to 87% area savings, and 39% execution time improvement, with little additional compilation time.},
	author = {Cheng, Jianyi and Fleming, Shane T. and Chen, Yu Ting and Anderson, Jason H. and Constantinides, George A.},
	location = {Seaside, CA, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3289602.3293899},
	booktitle = {Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	doi = {10.1145/3289602.3293899},
	isbn = {9781450361378},
	keywords = {memory optimization,formal methods,high-level synthesis},
	pages = {142--151},
	series = {FPGA '19},
	title = {EASY: Efficient Arbiter SYnthesis from Multi-Threaded Code},
	year = {2019}
}

@inproceedings{cheng20_combin_dynam_static_sched_high_level_synth,
	author = {Cheng, Jianyi and Josipovic, Lana and Constantinides, George A. and Ienne, Paolo and Wickerson, John},
	location = {Seaside, CA, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3373087.3375297},
	booktitle = {The 2020 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	doi = {10.1145/3373087.3375297},
	isbn = {9781450370998},
	keywords = {high-level synthesis,dynamic scheduling,static analysis},
	pages = {288--298},
	series = {FPGA '20},
	title = {Combining Dynamic \& Static Scheduling in High-Level Synthesis},
	year = {2020}
}

@inproceedings{cherem07_pract_memor_leak_detec_using,
	author = {Cherem, Sigmund and Princehouse, Lonnie and Rugina, Radu},
	title = {Practical Memory Leak Detection Using Guarded Value-Flow Analysis},
	year = {2007},
	isbn = {9781595936332},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1250734.1250789},
	doi = {10.1145/1250734.1250789},
	abstract = {This paper presents a practical inter-procedural analysis algorithm for detecting memory leaks in C programs. Our algorithm tracks the flow of values from allocation points to deallocation points using a sparse representation of the program consisting of a value flow graph that captures def-use relations and value flows via program assignments. Edges in the graph are annotated with guards that describe branch conditions in the program. The memory leak analysis is reduced to a reachability problem over the guarded value flowgraph. Our implemented tool has been effective at detecting more than 60 memory leaks in the SPEC2000 benchmarks and in two open-source applications, bash and sshd, while keeping the false positive rate below 20\%. The sparse program representation makes the tool efficient in practice, and allows it to report concise error messages.},
	booktitle = {Proceedings of the 28th ACM SIGPLAN Conference on Programming Language Design and Implementation},
	pages = {480–491},
	numpages = {12},
	keywords = {value summaries, symbolic execution, hyperblocks},
	location = {San Diego, California, USA},
	series = {PLDI '07}
}

@inproceedings{cheung22_overc_restr,
	author = {Cheung, Louis and O'Connor, Liam and Rizkallah, Christine},
	title = {Overcoming Restraint: Composing Verification of Foreign Functions with Cogent},
	year = {2022},
	isbn = {9781450391825},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497775.3503686},
	doi = {10.1145/3497775.3503686},
	abstract = {Cogent is a restricted functional language designed to reduce the cost of developing verified systems code. Because of its sometimes-onerous restrictions, such as the lack of support for recursion and its strict uniqueness type system, Cogent provides an escape hatch in the form of a foreign function interface (FFI) to C code. This poses a problem when verifying Cogent programs, as imported C components do not enjoy the same level of static guarantees that Cogent does. Previous verification of file systems implemented in Cogent merely assumed that their C components were correct and that they preserved the invariants of Cogent’s type system. In this paper, we instead prove such obligations. We demonstrate how they smoothly compose with existing Cogent theorems, and result in a correctness theorem of the overall Cogent-C system. The Cogent FFI constraints ensure that key invariants of Cogent’s type system are maintained even when calling C code. We verify reusable higher-order and polymorphic functions including a generic loop combinator and array iterators and demonstrate their application to several examples including binary search and the BilbyFs file system. We demonstrate the feasibility of verification of mixed Cogent-C systems, and provide some insight into verification of software comprised of code in multiple languages with differing levels of static guarantees.},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {13–26},
	numpages = {14},
	keywords = {verification, language interoperability, type-systems, data-structures, compilers},
	location = {Philadelphia, PA, USA},
	series = {CPP 2022}
}

@inproceedings{chipounov09_selec_symbol_execut,
	keywords = {hardware abstract interpretation},
	title = {Selective Symbolic Execution},
	author = {Chipounov, Vitaly and Georgescu, Vlad and Zamfir, Cristian  and Candea, George},
	journal = {Proceedings of the 5th Workshop on Hot Topics in System  Dependability (HotDep)},
	year = {2009},
	abstract = {Symbolic execution is a powerful technique for analyzing  program behavior, finding bugs, and generating tests, but  suffers from severely limited scalability: the largest  programs that can be symbolically executed today are on the  order of thousands of lines of code. To ensure feasibility  of symbolic execution, even small programs must curtail  their interactions with libraries, the operating system,  and hardware devices. This paper introduces selective  symbolic execution, a technique for creating the illusion  of full-system symbolic execution, while symbolically  running only the code that is of interest to the developer.  We describe a prototype that can symbolically execute  arbitrary portions of a full system, including  applications, libraries, operating system, and device  drivers. It seamlessly transitions back and forth between  symbolic and concrete execution, while transparently  converting system state from symbolic to concrete and back.  Our technique makes symbolic execution practical for large  software that runs in real environments, without requiring  explicit modeling of these environments.},
	url = {http://infoscience.epfl.ch/record/139393}
}

@article{chistikov22_subcub_certif_cfl_reach,
	author = {Chistikov, Dmitry and Majumdar, Rupak and Schepper, Philipp},
	title = {Subcubic Certificates for CFL Reachability},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498702},
	doi = {10.1145/3498702},
	abstract = {Many problems in interprocedural program analysis can be modeled as the context-free language (CFL) reachability problem on graphs and can be solved in cubic time. Despite years of efforts, there are no known truly sub-cubic algorithms for this problem. We study the related certification task: given an instance of CFL reachability, are there small and efficiently checkable certificates for the existence and for the non-existence of a path? We show that, in both scenarios, there exist succinct certificates (O(n2) in the size of the problem) and these certificates can be checked in subcubic (matrix multiplication) time. The certificates are based on grammar-based compression of paths (for reachability) and on invariants represented as matrix inequalities (for non-reachability). Thus, CFL reachability lies in nondeterministic and co-nondeterministic subcubic time. A natural question is whether faster algorithms for CFL reachability will lead to faster algorithms for combinatorial problems such as Boolean satisfiability (SAT). As a consequence of our certification results, we show that there cannot be a fine-grained reduction from SAT to CFL reachability for a conditional lower bound stronger than nω, unless the nondeterministic strong exponential time hypothesis (NSETH) fails. In a nutshell, reductions from SAT are unlikely to explain the cubic bottleneck for CFL reachability. Our results extend to related subcubic equivalent problems: pushdown reachability and 2NPDA recognition; as well as to all-pairs CFL reachability. For example, we describe succinct certificates for pushdown non-reachability (inductive invariants) and observe that they can be checked in matrix multiplication time. We also extract a new hardest 2NPDA language, capturing the “hard core” of all these problems.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {41},
	numpages = {29},
	keywords = {CFL reachability, pushdown reachability, subcubic certification}
}

@inproceedings{chlipala08_param_higher_order_abstr_syntax_mechan_seman,
	abstract = {We present parametric higher-order abstract syntax (PHOAS), a new approach to formalizing the syntax of programming languages in computer proof assistants based on type theory. Like higher-order abstract syntax (HOAS), PHOAS uses the meta language's binding constructs to represent the object language's binding constructs. Unlike HOAS, PHOAS types are definable in general-purpose type theories that support traditional functional programming, like Coq's Calculus of Inductive Constructions. We walk through how Coq can be used to develop certified, executable program transformations over several statically-typed functional programming languages formalized with PHOAS; that is, each transformation has a machine-checked proof of type preservation and semantic preservation. Our examples include CPS translation and closure conversion for simply-typed lambda calculus, CPS translation for System F, and translation from a language with ML-style pattern matching to a simpler language with no variable-arity binding constructs. By avoiding the syntactic hassle associated with first-order representation techniques, we achieve a very high degree of proof automation.},
	author = {Chlipala, Adam},
	location = {Victoria, BC, Canada},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/1411204.1411226},
	booktitle = {Proceedings of the 13th ACM SIGPLAN International Conference on Functional Programming},
	doi = {10.1145/1411204.1411226},
	isbn = {9781595939197},
	keywords = {type-theoretic semantics,compiler verification,dependent types,interactive proof assistants},
	pages = {143--156},
	series = {ICFP '08},
	title = {Parametric Higher-Order Abstract Syntax for Mechanized Semantics},
	year = {2008}
}

@book{chlipala13_certif,
	author = {Chlipala, Adam},
	publisher = {MIT Press},
	keywords = {coq,reflection,dependent types},
	title = {Certified programming with dependent types: a pragmatic introduction to the Coq proof assistant},
	year = {2013}
}

@inproceedings{choi01,
	author = {Choi, Youngsoo and Knies, Allan and Gerke, Luke and Ngai, Tin-Fook},
	organization = {Citeseer},
	booktitle = {Proceedings. 34th ACM/IEEE International Symposium on Microarchitecture. MICRO-34},
	pages = {182--182},
	title = {The impact of if-conversion and branch prediction on program execution on the intel itanium processor},
	year = {2001}
}

@thesis{choi16_from_softw_thread_paral_hardw,
	author = {Choi, Jongsok},
	keywords = {high-level synthesis},
	title = {From Software Threads to Parallel Hardware with LegUp High-level Synthesis},
	type = {phdthesis},
	year = {2016}
}

@article{choi17_kami,
	author = {Choi, Joonwon and Vijayaraghavan, Muralidaran and Sherman, Benjamin and Chlipala, Adam and Arvind},
	location = {New York, NY, USA},
	publisher = {ACM},
	doi = {10.1145/3110268},
	issn = {2475-1421},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {hardware,verification,coq},
	month = aug,
	number = {ICFP},
	pages = {24:1--24:30},
	title = {Kami: a Platform for High-Level Parametric Hardware Specification and Its Modular Verification},
	volume = {1},
	year = {2017}
}

@inproceedings{choi18_hbods,
	author = {Choi, Young{-}kyu and Cong, Jason},
	booktitle = {2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
	doi = {10.1145/3240765.3240815},
	pages = {1--8},
	title = {HLS-Based Optimization and Design Space Exploration for Applications with Variable Loop Bounds},
	year = {2018}
}

@article{choudhury21_graded_depen_type_system_usage_aware_seman,
	abstract = {Graded Type Theory provides a mechanism to track and reason about resource usage in type systems. In this paper, we develop GraD, a novel version of such a graded dependent type system that includes functions, tensor products, additive sums, and a unit type. Since standard operational semantics is resource-agnostic, we develop a heap-based operational semantics and prove a soundness theorem that shows correct accounting of resource usage. Several useful properties, including the standard type soundness theorem, non-interference of irrelevant resources in computation and single pointer property for linear resources, can be derived from this theorem. We hope that our work will provide a base for integrating linearity, irrelevance and dependent types in practical programming languages like Haskell.},
	author = {Choudhury, Pritam and Eades III, Harley and Eisenberg, Richard A. and Weirich, Stephanie},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	doi = {10.1145/3434331},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {linearity,Irrelevance,quantitative reasoning,heap semantics},
	month = jan,
	number = {POPL},
	title = {A Graded Dependent Type System with a Usage-Aware Semantics},
	volume = {5},
	year = {2021}
}

@article{choudhury22_symmet_rever_progr,
	author = {Choudhury, Vikraman and Karwowski, Jacek and Sabry, Amr},
	title = {Symmetries in Reversible Programming: From Symmetric Rig Groupoids to Reversible Programming Languages},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	doi = {10.1145/3498667},
	abstract = {The Pi family of reversible programming languages for boolean circuits is presented as a syntax of combinators witnessing type isomorphisms of algebraic data types. In this paper, we give a denotational semantics for this language, using weak groupoids \`{a} la Homotopy Type Theory, and show how to derive an equational theory for it, presented by 2-combinators witnessing equivalences of type isomorphisms. We establish a correspondence between the syntactic groupoid of the language and a formally presented univalent subuniverse of finite types. The correspondence relates 1-combinators to 1-paths, and 2-combinators to 2-paths in the universe, which is shown to be sound and complete for both levels, forming an equivalence of groupoids. We use this to establish a Curry-Howard-Lambek correspondence between Reversible Logic, Reversible Programming Languages, and Symmetric Rig Groupoids, by showing that the syntax of Pi is presented by the free symmetric rig groupoid, given by finite sets and bijections. Using the formalisation of our results, we perform normalisation-by-evaluation, verification and synthesis of reversible logic gates, motivated by examples from quantum computing. We also show how to reason about and transfer theorems between different representations of reversible circuits.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {6},
	numpages = {32},
	keywords = {groupoids, univalent foundations, rewriting, permutations, groups, reversible computing, type isomorphisms, homotopy type theory, reversible programming languages}
}

@article{chouksey19_trans_valid_code_motion_trans_invol_loops,
	author = {Chouksey, R. and Karfa, C. and Bhaduri, P.},
	doi = {10.1109/TCAD.2018.2846654},
	issn = {1937-4151},
	journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {translation validation,verification,compiler optimisation,high-level synthesis},
	month = jul,
	number = {7},
	pages = {1378--1382},
	title = {Translation Validation of Code Motion Transformations Involving Loops},
	volume = {38},
	year = {2019}
}

@article{chouksey20_verif_sched_condit_behav_high_level_synth,
	abstract = {High-level synthesis (HLS) technique translates the behaviors written in high-level languages like C/C++ into register transfer level (RTL) design. Due to its complexity, proving the correctness of an HLS tool is prohibitively expensive. Translation validation is the process of proving that the target code is a correct translation of the source program being compiled. The path-based equivalence checking (PBEC) method is a widely used translation validation method for verification of the scheduling phase of HLS. The existing PBEC methods cannot handle significant control structure modification that occurs in the efficient scheduling of conditional behaviors. Hence, they produce a false-negative result. In this article, we identify some scenarios involving path merge/split where the state-of-the-art PBEC approaches fail to show the equivalence even though behaviors are equivalent. We propose a value propagation-based PBEC method along with a new cutpoint selection scheme to overcome this limitation. Our method can also handle the scenario where adjacent conditional blocks (CBs) having an equivalent conditional expression are combined into one CB. Experimental results demonstrate the usefulness of our method over the existing methods.},
	author = {Chouksey, R. and Karfa, C.},
	doi = {10.1109/TVLSI.2020.2978242},
	issn = {1557-9999},
	journaltitle = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	keywords = {translation validation,high-level synthesis,verification,compiler optimisation},
	pages = {1--14},
	title = {Verification of Scheduling of Conditional Behaviors in High-Level Synthesis},
	year = {2020}
}

@inproceedings{chuang03_phi,
	abstract = {Predicated execution can eliminate hard to predict branches and help to enable instruction level parallelism. Many current predication variants exist where the result update is conditional based upon the outcome of the guarding predicate. However conditional writing of a register creates a naming problem for an out-of-order processor and can stall the issuing of instructions. This problem arises from potential multiple predicated definitions reaching a use, which is unresolved until the prior predicate values are computed. We focus on a light-weight form of predication, phi-predication, where all predicated instructions write a result value to their register regardless of the predicate value (i.e. even if it is false). Therefore, the predicate does not guard the writing of the result register; it instead acts as a form of selection between two input registers. This eliminates the naming problem for an out-of-order processor. Our phi-predicated ISA is derived from the predicated features of the Multiflow ISA, with extensions to efficiently predicate complex control flow. Our compiler modifications also expand upon prior techniques to provide efficient code generation. We examine the use of phi-predication for an in-order and out-of-order architecture and compare its performance to using select-op and IA64 ISA predication.},
	author = {Chuang, Weihaw and Calder, B. and Ferrante, J.},
	booktitle = {International Symposium on Code Generation and Optimization, 2003. CGO 2003.},
	doi = {10.1109/CGO.2003.1191544},
	keywords = {if-conversion,predicated execution,software pipelining},
	month = mar,
	pages = {179--190},
	title = {Phi-predication for light-weight if-conversion},
	year = {2003}
}

@article{ciccone22_fair_termin_binar_session,
	author = {Ciccone, Luca and Padovani, Luca},
	title = {Fair Termination of Binary Sessions},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	doi = {10.1145/3498666},
	abstract = {A binary session is a private communication channel that connects two processes, each adhering to a protocol description called session type. In this work, we study the first type system that ensures the fair termination of binary sessions. A session fairly terminates if all of the infinite executions admitted by its protocol are deemed unrealistic because they violate certain fairness assumptions. Fair termination entails the eventual completion of all pending input/output actions, including those that depend on the completion of an unbounded number of other actions in possibly different sessions. This form of lock freedom allows us to address a large family of natural communication patterns that fall outside the scope of existing type systems. Our type system is also the first to adopt fair subtyping, a liveness-preserving refinement of the standard subtyping relation for session types that so far has only been studied theoretically. Fair subtyping is surprisingly subtle not only to characterize concisely but also to use appropriately, to the point that the type system must carefully account for all usages of fair subtyping to avoid compromising its liveness-preserving properties.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {5},
	numpages = {30},
	keywords = {fair termination, deadlock freedom, fair subtyping, session types}
}

@inproceedings{claessen02,
	author = {Claessen, Koen Lindström and Pace, Gordon J.},
	keywords = {hardware,hardware/software co-simulation},
	title = {An embedded language framework for hardware compilation},
	year = {2002}
}

@inproceedings{clarke03_behav_c_veril,
	abstract = {We present an algorithm that checks behavioral consistency between an ANSI-C program and a circuit given in Verilog using Bounded Model Checking. Both the circuit and the program are unwound and translated into a formula that represents behavioral consistency. The formula is then checked using a SAT solver. We are able to translate C programs that include side effects, pointers, dynamic memory allocation, and loops with conditions that cannot be evaluated statically. We describe experimental results on various reactive circuits and programs, including a small processor given in Verilog and its Instruction Set Architecture given in ANSI-C.},
	author = {Clarke, E. and Kroening, D. and Yorav, K.},
	booktitle = {Proceedings 2003. Design Automation Conference (IEEE Cat. No.03CH37451)},
	doi = {10.1145/775832.775928},
	keywords = {bounded model checking,high-level synthesis,translation validation,verilog,verification},
	month = jun,
	pages = {368--371},
	title = {Behavioral consistency of C and Verilog programs using bounded model checking},
	year = {2003}
}

@article{clarke04_predic_abstr_ansi_c_progr_using_sat,
	keywords = {predicated execution, abstract interpretation, predicate abstraction},
	doi = {10.1023/b:form.0000040025.89719.f3},
	year = 2004,
	month = {sep},
	publisher = {Springer Science and Business Media {LLC}},
	volume = {25},
	number = {2/3},
	pages = {105--127},
	author = {Edmund Clarke and Daniel Kroening and Natasha Sharygina and Karen Yorav},
	title = {Predicate Abstraction of {ANSI}-C Programs Using {SAT}},
	journal = {Formal Methods in System Design}
}

@inbook{clarke13_owner_types,
	abstract = {Ownership types were devised nearly 15 years ago to provide a stronger notion of protection to object-oriented programming languages. Rather than simply protecting the fields of an object from external access, ownership types protect also the objects stored in the fields, thereby enabling an object to claim (exclusive) ownership of and access to other objects. Furthermore, this notion is statically enforced by now-standard type-checking techniques.},
	author = {Clarke, Dave and Östlund, Johan and Sergey, Ilya and Wrigstad, Tobias},
	editor = {Clarke, Dave and Noble, James and Wrigstad, Tobias},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Aliasing in Object-Oriented Programming. Types, Analysis and Verification},
	doi = {10.1007/978-3-642-36946-9_3},
	isbn = {978-3-642-36946-9},
	keywords = {memory aliasing,ownership types},
	pages = {15--58},
	title = {Ownership Types: A Survey},
	year = {2013}
}

@article{clarkson10_hyper,
	author = {Clarkson, Michael R. and Schneider, Fred B.},
	publisher = {IOS Press},
	doi = {10.3233/JCS-2009-0393},
	issn = {18758924, 0926227X},
	journaltitle = {Journal of Computer Security},
	month = sep,
	number = {6},
	pages = {1157--1210},
	title = {Hyperproperties},
	volume = {18},
	year = {2010}
}

@article{click95_simpl_graph_based_inter_repres,
	keywords = {gated-SSA},
	author = {Click, Cliff and Paleczny, Michael},
	title = {A Simple Graph-Based Intermediate Representation},
	year = {1995},
	issue_date = {March 1993},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {30},
	number = {3},
	issn = {0362-1340},
	doi = {10.1145/202530.202534},
	abstract = {We present a graph-based intermediate representation (IR) with simple semantics and a low-memory-cost C++ implementation. The IR uses a directed graph with labeled vertices and ordered inputs but unordered outputs. Vertices are labeled with opcodes, edges are unlabeled. We represent the CFG and basic blocks with the same vertex and edge structures. Each opcode is defined by a C++ class that encapsulates opcode-specific data and behavior. We use inheritance to abstract common opcode behavior, allowing new opcodes to be easily defined from old ones. The resulting IR is simple, fast and easy to use.},
	journal = {SIGPLAN Not.},
	month = {mar},
	pages = {35–49},
	numpages = {15}
}

@inproceedings{clune23_formal_reduc_keller_conjec,
	author = {Clune, Joshua},
	title = {A Formalized Reduction of Keller’s Conjecture},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3573105.3575669},
	abstract = {Keller’s conjecture in d dimensions states that there are no faceshare-free tilings of d-dimensional space by translates of a d-dimensional cube. In 2020, Brakensiek et al. resolved this 90-year-old conjecture by proving that the largest number of dimensions for which no faceshare-free tilings exist is 7. This result, as well as many others pertaining to Keller’s conjecture, critically relies on a reduction from Keller’s original conjecture to a statement about cliques in generalized Keller graphs. In this paper, we present a formalization of this reduction in the Lean 3 theorem prover. Additionally, we combine this formalized reduction with the verification of a large clique in the Keller graph G8 to obtain the first verified end-to-end proof that Keller’s conjecture is false in 8 dimensions.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {90–101},
	numpages = {12},
	keywords = {Formal Verification, Interactive Theorem Proving, Lean, Keller's Conjecture},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@article{cockx21_tamin_rew,
	abstract = {Dependently typed programming languages and proof assistants such as Agda and Coq rely on computation to automatically simplify expressions during type checking. To overcome the lack of certain programming primitives or logical principles in those systems, it is common to appeal to axioms to postulate their existence. However, one can only postulate the bare existence of an axiom, not its computational behaviour. Instead, users are forced to postulate equality proofs and appeal to them explicitly to simplify expressions, making axioms dramatically more complicated to work with than built-in primitives. On the other hand, the equality reflection rule from extensional type theory solves these problems by collapsing computation and equality, at the cost of having no practical type checking algorithm. This paper introduces Rewriting Type Theory (RTT), a type theory where it is possible to add computational assumptions in the form of rewrite rules. Rewrite rules go beyond the computational capabilities of intensional type theory, but in contrast to extensional type theory, they are applied automatically so type checking does not require input from the user. To ensure type soundness of RTT—as well as effective type checking—we provide a framework where confluence of user-defined rewrite rules can be checked modularly and automatically, and where adding new rewrite rules is guaranteed to preserve subject reduction. The properties of RTT have been formally verified using the MetaCoq framework and an implementation of rewrite rules is already available in the Agda proof assistant.},
	author = {Cockx, Jesper and Tabareau, Nicolas and Winterhalter, Théo},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	doi = {10.1145/3434341},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {confluence,termination,rewriting theory,dependent types,type theory},
	month = jan,
	number = {POPL},
	title = {The Taming of the Rew: A Type Theory with Computational Assumptions},
	volume = {5},
	year = {2021}
}

@inproceedings{collingbourne11_symbol_simd,
	keywords = {symbolic execution, three-valued logic, predicated execution},
	doi = {10.1145/1966445.1966475},
	year = {2011},
	month = apr,
	publisher = {{ACM}},
	author = {Peter Collingbourne and Cristian Cadar and Paul H.J. Kelly},
	title = {Symbolic crosschecking of floating-point and {SIMD} code},
	booktitle = {Proceedings of the sixth conference on Computer systems}
}

@inproceedings{collins10_taylor_funct_calcul_hybrid_system_analy,
	TITLE = {{A Taylor Function Calculus for Hybrid System Analysis: Validation in Coq}},
	AUTHOR = {Collins, Pieter and Niqui, Milad and Revol, Nathalie},
	URL = {https://hal.inria.fr/inria-00473270/file/Collins-Niqui-Revol.pdf},
	TYPE = {Research Report},
	BOOKTITLE = {{NSV-3: Third International Workshop on Numerical Software Verification.}},
	ADDRESS = {Edinburgh, United Kingdom},
	ORGANIZATION = {{Fainekos, Georgios and Goubault, Eric and Putot, Sylvie}},
	YEAR = {2010},
	MONTH = Jul,
	KEYWORDS = {coq, verification, abstract interpretation, predicate abstraction},
	HAL_ID = {inria-00473270},
	HAL_VERSION = {v1}
}

@article{colwell88_vliw,
	author = {Colwell, R. P. and Nix, R. P. and O'Donnell, J. J. and Papworth, D. B. and Rodman, P. K.},
	doi = {10.1109/12.2247},
	journaltitle = {IEEE Transactions on Computers},
	keywords = {static scheduling,trace scheduling},
	number = {8},
	pages = {967--979},
	title = {A VLIW architecture for a trace scheduling compiler},
	volume = {37},
	year = {1988}
}

@inproceedings{cong06_sdc,
	abstract = {Scheduling plays a central role in the behavioral synthesis process, which automatically compiles high-level specifications into optimized hardware implementations. However, most of the existing behavior-level scheduling heuristics either have a limited efficiency in a specific class of applications or lack general support of various design constraints. In this paper we describe a new scheduler that converts a rich set of scheduling constraints into a system of difference constraints (SDC) and performs a variety of powerful optimizations under a unified mathematical programming framework. In particular, we show that our SDC-based scheduling algorithm can efficiently support resource constraints, frequency constraints, latency constraints, and relative timing constraints, and effectively optimize longest path latency, expected overall latency, and the slack distribution. Experiments demonstrate that our proposed technique provides efficient solutions for a broader range of applications with higher quality of results (in terms of system performance) when compared to the state-of-the-art scheduling heuristics},
	author = {Cong, Jason and Zhang, Zhiru},
	booktitle = {2006 43rd ACM/IEEE Design Automation Conference},
	doi = {10.1145/1146909.1147025},
	issn = {0738-100X},
	keywords = {high-level synthesis,static scheduling},
	month = jul,
	pages = {433--438},
	title = {An efficient and versatile scheduling algorithm based on SDC formulation},
	year = {2006}
}

@article{cong11_high_level_synth_fpgas,
	abstract = {Escalating system-on-chip design complexity is pushing the design community to raise the level of abstraction beyond register transfer level. Despite the unsuccessful adoptions of early generations of commercial high-level synthesis (HLS) systems, we believe that the tipping point for transitioning to HLS msystem-on-chip design complexityethodology is happening now, especially for field-programmable gate array (FPGA) designs. The latest generation of HLS tools has made significant progress in providing wide language coverage and robust compilation technology, platform-based modeling, advancement in core HLS algorithms, and a domain-specific approach. In this paper, we use AutoESL's AutoPilot HLS tool coupled with domain-specific system-level implementation platforms developed by Xilinx as an example to demonstrate the effectiveness of state-of-art C-to-FPGA synthesis solutions targeting multiple application domains. Complex industrial designs targeting Xilinx FPGAs are also presented as case studies, including comparison of HLS solutions versus optimized manual designs. In particular, the experiment on a sphere decoder shows that the HLS solution can achieve an 11-31 \% reduction in FPGA resource usage with improved design productivity compared to hand-coded design.},
	author = {Cong, J. and Liu, B. and Neuendorffer, S. and Noguera, J. and Vissers, K. and Zhang, Z.},
	doi = {10.1109/TCAD.2011.2110592},
	issn = {1937-4151},
	journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {high-level synthesis},
	month = apr,
	number = {4},
	pages = {473--491},
	title = {High-Level Synthesis for Fpgas: From Prototyping To Deployment},
	volume = {30},
	year = {2011}
}

@inproceedings{cong14_combin_comput_commun_optim_system,
	abstract = {Data streaming is a widely-used technique to exploit task-level parallelism in many application domains such as video processing, signal processing and wireless communication. In this paper we propose an efficient system-level synthesis flow to map streaming applications onto FPGAs with consideration of simultaneous computation and communication optimizations. The throughput of a streaming system is significantly impacted by not only the performance and number of replicas of the computation kernels, but also the buffer size allocated for the communications between kernels. In general, module selection/replication and buffer size optimization were addressed separately in previous work. Our approach combines these optimizations together in system scheduling which minimizes the area cost for both logic and memory under the required throughput constraint. We first propose an integer linear program (ILP) based solution to the combined problem which has the optimal quality of results. Then we propose an iterative algorithm which can achieve the near-optimal quality of results but has a significant improvement on the algorithm scalability for large and complex designs. The key contribution is that we have a polynomial-time algorithm for an exact schedulability checking problem and a polynomial-time algorithm to improve the system performance with better module implementation and buffer size optimization. Experimental results show that compared to the separate scheme of module select/replication and buffer size optimization, the combined optimization scheme can gain 62% area saving on average under the same performance requirements. Moreover, our heuristic can achieve 2 to 3 orders of magnitude of speed-up in runtime, with less than 10% area overhead compared to the optimal solution by ILP.},
	author = {Cong, Jason and Huang, Muhuan and Zhang, Peng},
	location = {Monterey, California, USA},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 2014 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	doi = {10.1145/2554688.2554771},
	isbn = {9781450326711},
	keywords = {module duplication,module selection,buffer size optimization,streaming applications,fpga,system-level synthesis},
	pages = {213--222},
	series = {FPGA '14},
	title = {Combining Computation and Communication Optimizations in System Synthesis for Streaming Applications},
	year = {2014}
}

@article{cong22_fpga_hls_today,
	author = {Cong, Jason and Lau, Jason and Liu, Gai and Neuendorffer, Stephen and Pan, Peichen and Vissers, Kees and Zhang, Zhiru},
	title = {FPGA HLS Today: Successes, Challenges, and Opportunities},
	year = {2022},
	issue_date = {December 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {15},
	number = {4},
	issn = {1936-7406},
	doi = {10.1145/3530775},
	abstract = {The year 2011 marked an important transition for FPGA high-level synthesis (HLS), as it went from prototyping to deployment. A decade later, in this article, we assess the progress of the deployment of HLS technology and highlight the successes in several application domains, including deep learning, video transcoding, graph processing, and genome sequencing. We also discuss the challenges faced by today’s HLS technology and the opportunities for further research and development, especially in the areas of achieving high clock frequency, coping with complex pragmas and system integration, legacy code transformation, building on open source HLS infrastructures, supporting domain-specific languages, and standardization. It is our hope that this article will inspire more research on FPGA HLS and bring it to a new height.},
	journal = {ACM Trans. Reconfigurable Technol. Syst.},
	month = {aug},
	articleno = {51},
	numpages = {42},
	keywords = {high-level synthesis, survey}
}

@inproceedings{conrad22_compos_proof_framew_fretis_requir,
	author = {Conrad, Esther and Titolo, Laura and Giannakopoulou, Dimitra and Pressburger, Thomas and Dutle, Aaron},
	title = {A Compositional Proof Framework for FRETish Requirements},
	year = {2022},
	isbn = {9781450391825},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3497775.3503685},
	abstract = {Structured natural languages provide a trade space between ambiguous natural languages that make up most written requirements, and mathematical formal specifications such as Linear Temporal Logic. FRETish is a structured natural language for the elicitation of system requirements developed at NASA. The related open-source tool Fret provides support for translating FRETish requirements into temporal logic formulas that can be input to several verification and analysis tools. In the context of safety-critical systems, it is crucial to ensure that a generated formula captures the semantics of the corresponding FRETish requirement precisely. This paper presents a rigorous formalization of the FRETish language including a new denotational semantics and a proof of semantic equivalence between FRETish specifications and their temporal logic counterparts computed by Fret. The complete formalization and the proof have been developed in the Prototype Verification System (PVS) theorem prover.},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {68–81},
	numpages = {14},
	keywords = {Formal Proofs, Requirements, Structured Natural Language, PVS, Metric Temporal Logic},
	location = {Philadelphia, PA, USA},
	series = {CPP 2022}
}

@thesis{coquand86,
	author = {Coquand, Thierry and Huet, Gérard},
	institution = {INRIA},
	title = {The calculus of constructions},
	type = {phdthesis},
	year = {1986}
}

@inproceedings{coupet-grimal99_hardw_verif_using_co_coq,
	doi = {10.1007/3-540-48256-3_7},
	abstract = {This paper presents a toolbox implemented in Coq and dedicated to the specification and verification of synchronous sequential devices. The use of Coq co-inductive types underpins our methodology and leads to elegant and uniform descriptions of the circuits and their behaviours as well as clear and short proofs. An application to a non trivial circuit is given as an illustration.},
	author = {Coupet-Grimal, Solange and Jakubiec, Line},
	editor = {Bertot, Yves and Dowek, Gilles and Théry, Laurent and Hirschowitz, André and Paulin, Christine},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Theorem Proving in Higher Order Logics},
	isbn = {978-3-540-48256-7},
	pages = {91--108},
	title = {Hardware Verification Using Co-induction in COQ},
	year = {1999}
}

@article{courant21_verif_code_gener_polyh_model,
	abstract = {The polyhedral model is a high-level intermediate representation for loop nests that supports elegantly a great many loop optimizations. In a compiler, after polyhedral loop optimizations have been performed, it is necessary and difficult to regenerate sequential or parallel loop nests before continuing compilation. This paper reports on the formalization and proof of semantic preservation of such a code generator that produces sequential code from a polyhedral representation. The formalization and proofs are mechanized using the Coq proof assistant.},
	author = {Courant, Nathanaël and Leroy, Xavier},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	doi = {10.1145/3434321},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {polyhedral analysis,coq,verification},
	month = jan,
	number = {POPL},
	title = {Verified Code Generation for the Polyhedral Model},
	volume = {5},
	year = {2021}
}

@book{cousot21_princ,
	author = {Cousot, Patrick},
	address = {Cambridge, Massachusetts},
	isbn = {9780262361521},
	keywords = {abstract interpretation, verification},
	language = {eng},
	publisher = {The MIT Press},
	title = {Principles of abstract interpretation},
	year = {2021}
}

@Article{cousot77_static_deter_dynam_proper_progr,
	keywords = {abstract interpretation},
	author = {Cousot, P and Cousot, R},
	title = {Static Determination of Dynamic Properties of Programs},
	journal = {In: Programmation. Int. Symp. Program. 2. Proc.; Paris; 1976; Paris; Dunod; Da. 1977; Pp. 106-130; Bibl. 1 P. 1/2},
	year = {1977},
	affiliation = {UNIV. SCI. MED. GRENOBLE},
	descriptors = {COMPILATION; ALGORITHME; LANGAGE EVOLUE; VERIFICATION PROGRAMME; PROGRAMMATION; TYPE DONNEE; VERIFICATION DYNAMIQUE; INFORMATIQUE; MATHEMATIQUES APPLIQUEES},
	subject = {Mathematiques[001A02]},
	language = {English},
	document_type = {Conference Paper},
	inist_number = {PASCAL7830090397}
}

@inproceedings{cousot79_system_desig_progr_analy_framew,
	author = {Cousot, Patrick and Cousot, Radhia},
	title = {Systematic Design of Program Analysis Frameworks},
	year = {1979},
	isbn = {9781450373579},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/567752.567778},
	abstract = {Semantic analysis of programs is essential in optimizing compilers and program verification systems. It encompasses data flow analysis, data type determination, generation of approximate invariant assertions, etc.Several recent papers (among others Cousot \& Cousot[77a], Graham \& Wegman[76], Kam \& Ullman[76], Kildall[73], Rosen[78], Tarjan[76], Wegbreit[75]) have introduced abstract approaches to program analysis which are tantamount to the use of a program analysis framework (A,t,\~{a}) where A is a lattice of (approximate) assertions, t is an (approximate) predicate transformer and \~{a} is an often implicit function specifying the meaning of the elements of A. This paper is devoted to the systematic and correct design of program analysis frameworks with respect to a formal semantics.Preliminary definitions are given in Section 2 concerning the merge over all paths and (least) fixpoint program-wide analysis methods. In Section 3 we briefly define the (forward and backward) deductive semantics of programs which is later used as a formal basis in order to prove the correctness of the approximate program analysis frameworks. Section 4 very shortly recall the main elements of the lattice theoretic approach to approximate semantic analysis of programs.The design of a space of approximate assertions A is studied in Section 5. We first justify the very reasonable assumption that A must be chosen such that the exact invariant assertions of any program must have an upper approximation in A and that the approximate analysis of any program must be performed using a deterministic process. These assumptions are shown to imply that A is a Moore family, that the approximation operator (wich defines the least upper approximation of any assertion) is an upper closure operator and that A is necessarily a complete lattice. We next show that the connection between a space of approximate assertions and a computer representation is naturally made using a pair of isotone adjoined functions. This type of connection between two complete lattices is related to Galois connections thus making available classical mathematical results. Additional results are proved, they hold when no two approximate assertions have the same meaning.In Section 6 we study and examplify various methods which can be used in order to define a space of approximate assertions or equivalently an approximation function. They include the characterization of the least Moore family containing an arbitrary set of assertions, the construction of the least closure operator greater than or equal to an arbitrary approximation function, the definition of closure operators by composition, the definition of a space of approximate assertions by means of a complete join congruence relation or by means of a family of principal ideals.Section 7 is dedicated to the design of the approximate predicate transformer induced by a space of approximate assertions. First we look for a reasonable definition of the correctness of approximate predicate transformers and show that a local correctness condition can be given which has to be verified for every type of elementary statement. This local correctness condition ensures that the (merge over all paths or fixpoint) global analysis of any program is correct. Since isotony is not required for approximate predicate transformers to be correct it is shown that non-isotone program analysis frameworks are manageable although it is later argued that the isotony hypothesis is natural. We next show that among all possible approximate predicate transformers which can be used with a given space of approximate assertions there exists a best one which provides the maximum information relative to a program-wide analysis method. The best approximate predicate transformer induced by a space of approximate assertions turns out to be isotone. Some interesting consequences of the existence of a best predicate transformer are examined. One is that we have in hand a formal specification of the programs which have to be written in order to implement a program analysis framework once a representation of the space of approximate assertions has been chosen. Examples are given, including ones where the semantics of programs is formalized using Hoare[78]'s sets of traces.In Section 8 we show that a hierarchy of approximate analyses can be defined according to the fineness of the approximations specified by a program analysis framework. Some elements of the hierarchy are shortly exhibited and related to the relevant literature.In Section 9 we consider global program analysis methods. The distinction between "distributive" and "non-distributive" program analysis frameworks is studied. It is shown that when the best approximate predicate transformer is considered the coincidence or not of the merge over all paths and least fixpoint global analyses of programs is a consequence of the choice of the space of approximate assertions. It is shown that the space of approximate assertions can always be refined so that the merge over all paths analysis of a program can be defined by means of a least fixpoint of isotone equations.Section 10 is devoted to the combination of program analysis frameworks. We study and examplify how to perform the "sum", "product" and "power" of program analysis frameworks. It is shown that combined analyses lead to more accurate information than the conjunction of the corresponding separate analyses but this can only be achieved by a new design of the approximate predicate transformer induced by the combined program analysis frameworks.},
	booktitle = {Proceedings of the 6th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
	pages = {269–282},
	numpages = {14},
	location = {San Antonio, Texas},
	series = {POPL '79}
}

@article{cousot92_abstr_inter_framew,
	keywords = {abstract interpretation},
	author = {Cousot, Patrick and Cousot, Radhia},
	title = "{Abstract Interpretation Frameworks}",
	journal = {Journal of Logic and Computation},
	volume = {2},
	number = {4},
	pages = {511-547},
	year = {1992},
	month = {08},
	abstract = "{We introduce abstract interpretation frameworks which are variations on the archetypal framework using Galois connections between concrete and abstract semantics, widenings and narrowings and are obtained by relaxation of the original hypotheses. We consider various ways of establishing the correctness of an abstract interpretation depending on how the relation between the concrete and abstract semantics is denned. We insist upon those correspondences allowing for the inducing of the approximate abstract semantics from the concrete one. Furthermore we study various notions of widening and narrowing as a means of obtaining convergence in the iterations used in abstract interpretation.}",
	issn = {0955-792X},
	doi = {10.1093/logcom/2.4.511},
	url = {https://doi.org/10.1093/logcom/2.4.511},
	eprint = {https://academic.oup.com/logcom/article-pdf/2/4/511/2740133/2-4-511.pdf}
}

@inbook{coussy08_gaut,
	abstract = {This chapter presents GAUT, an academic and open-source high-level synthesis tool dedicated to digital signal processing applications. Starting from an algorithmic bit-accurate specification written in C/C++, GAUT extracts the potential parallelism before processing the allocation, the scheduling and the binding tasks. Mandatory synthesis constraints are the throughput and the clock period while the memory mapping and the I/O timing diagram are optional. GAUT next generates a potentially pipelined architecture composed of a processing unit, a memory unit and a communication with a GALS/LIS interface.},
	author = {Coussy, Philippe and Chavet, Cyrille and Bomel, Pierre and Heller, Dominique and Senn, Eric and Martin, Eric},
	editor = {Coussy, Philippe and Morawiec, Adam},
	location = {Dordrecht},
	publisher = {Springer Netherlands},
	booktitle = {High-Level Synthesis: From Algorithm to Digital Circuit},
	doi = {10.1007/978-1-4020-8588-8_9},
	isbn = {978-1-4020-8588-8},
	pages = {147--169},
	title = {GAUT: A High-Level Synthesis Tool for DSP Applications},
	year = {2008}
}

@book{coussy08_high,
	author = {Coussy, Philippe and Morawiec, Adam},
	publisher = {Springer Science \& Business Media},
	keywords = {high-level synthesis},
	title = {High-level synthesis: from algorithm to digital circuit},
	year = {2008}
}

@article{coussy09_introd_to_high_level_synth,
	author = {Coussy, P. and Gajski, D. D. and Meredith, M. and Takach, A.},
	url = {https://doi.org/10.1109/MDT.2009.69},
	doi = {10.1109/MDT.2009.69},
	journaltitle = {IEEE Design Test of Computers},
	keywords = {high-level synthesis,introduction,survey},
	month = jul,
	number = {4},
	pages = {8--17},
	title = {An Introduction To High-Level Synthesis},
	volume = {26},
	year = {2009}
}

@inproceedings{cowan21_porcup,
	abstract = {Homomorphic encryption (HE) is a privacy-preserving technique that enables computation directly on encrypted data. Despite its promise, HE has seen limited use due to performance overheads and compilation challenges. Recent work has made significant advances to address the performance overheads but automatic compilation of efficient HE kernels remains relatively unexplored. This paper presents Porcupine, an optimizing compiler that generates vectorized HE code using program synthesis. HE poses three major compilation challenges: it only supports a limited set of SIMD-like operators, it uses long-vector operands, and decryption can fail if ciphertext noise growth is not managed properly. Porcupine captures the underlying HE operator behavior so that it can automatically reason about the complex trade-offs imposed by these challenges to generate optimized, verified HE kernels. To improve synthesis time, we propose a series of optimizations including a sketch design tailored to HE to narrow the program search space. We evaluate Porcupine using a set of kernels and show speedups of up to 52% (25% geometric mean) compared to heuristic-driven hand-optimized kernels. Analysis of Porcupine’s synthesized code reveals that optimal solutions are not always intuitive, underscoring the utility of automated reasoning in this domain.},
	author = {Cowan, Meghan and Dangwal, Deeksha and Alaghi, Armin and Trippel, Caroline and Lee, Vincent T. and Reagen, Brandon},
	location = {Virtual, Canada},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3453483.3454050},
	booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
	doi = {10.1145/3453483.3454050},
	isbn = {9781450383912},
	keywords = {program synthesis,homomorphic encryption,vectorization},
	pages = {375--389},
	series = {PLDI 2021},
	title = {Porcupine: A Synthesizing Compiler for Vectorized Homomorphic Encryption},
	year = {2021}
}

@InProceedings{cruz-filipe17_effic_certif_rat_verif,
	doi = {10.1007/978-3-319-63046-5_14},
	author = "Cruz-Filipe, Lu{\'i}s
and Heule, Marijn J. H.
and Hunt, Warren A.
and Kaufmann, Matt
and Schneider-Kamp, Peter",
	editor = "de Moura, Leonardo",
	title = "Efficient Certified RAT Verification",
	booktitle = "Automated Deduction -- CADE 26",
	year = "2017",
	publisher = "Springer International Publishing",
	address = "Cham",
	pages = "220--236",
	abstract = "Clausal proofs have become a popular approach to validate the results of SAT solvers. However, validating clausal proofs in the most widely supported format (DRAT) is expensive even in highly optimized implementations. We present a new format, called LRAT, which extends the DRAT format with hints that facilitate a simple and fast validation algorithm. Checking validity of LRAT proofs can be implemented using trusted systems such as the languages supported by theorem provers. We demonstrate this by implementing two certified LRAT checkers, one in Coq and one in ACL2.",
	isbn = "978-3-319-63046-5"
}

@article{cummings00_nonbl_assig_veril_synth_codin,
	author = {Cummings, Clifford E and others},
	journaltitle = {SNUG (Synopsys Users Group) 2000 User Papers},
	keywords = {verilog,synthesis,simulation},
	title = {Nonblocking Assignments in Verilog Synthesis, Coding Styles That Kill!},
	year = {2000}
}

@article{cyphert19_refin_path_expres_static_analy,
	abstract = {Algebraic program analyses compute information about a program’s behavior by first (a) computing a valid path expression—i.e., a regular expression that recognizes all feasible execution paths (and usually more)—and then (b) interpreting the path expression in a semantic algebra that defines the analysis. There are an infinite number of different regular expressions that qualify as valid path expressions, which raises the question “Which one should we choose?” While any choice yields a sound result, for many analyses the choice can have a drastic effect on the precision of the results obtained. This paper investigates the following two questions: (1) What does it mean for one valid path expression to be “better” than another? (2) Can we compute a valid path expression that is “better,” and if so, how? We show that it is not satisfactory to compare two path expressions E1 and E2 solely by means of the languages that they generate. Counter to one’s intuition, it is possible for L(E2) ⊊ L(E1), yet for E2 to produce a less-precise analysis result than E1—and thus we would not want to perform the transformation E1 → E2. However, the exclusion of paths so as to analyze a smaller language of paths is exactly the refinement criterion used by some prior methods. In this paper, we develop an algorithm that takes as input a valid path expression E, and returns a valid path expression E′ that is guaranteed to yield analysis results that are at least as good as those obtained using E. While the algorithm sometimes returns E itself, it typically does not: (i) we prove a no-degradation result for the algorithm’s base case—for transforming a leaf loop (i.e., a most-deeply-nested loop); (ii) at a non-leaf loop L, the algorithm treats each loop L′ in the body of L as an indivisible atom, and applies the leaf-loop algorithm to L; the no-degradation result carries over to (ii), as well. Our experiments show that the technique has a substantial impact: the loop-refinement algorithm allows the implementation of Compositional Recurrence Analysis to prove over 25% more assertions for a collection of challenging loop micro-benchmarks.},
	author = {Cyphert, John and Breck, Jason and Kincaid, Zachary and Reps, Thomas},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3290358},
	doi = {10.1145/3290358},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {path expressions,gated-SSA},
	month = jan,
	number = {POPL},
	title = {Refinement of Path Expressions for Static Analysis},
	volume = {3},
	year = {2019}
}

@InProceedings{cyrluk95_effec,
	doi = {10.1007/3-540-59047-1_50},
	author = {Cyrluk, D. and Rajan, S. and Shankar, N. and Srivas, M. K.},
	editor = "Kumar, Ramayya
and Kropf, Thomas",
	title = "Effective theorem proving for hardware verification",
	booktitle = "Theorem Provers in Circuit Design",
	year = "1995",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "203--222",
	abstract = "The attractiveness of using theorem provers for system design verification lies in their generality. The major practical challenge confronting theorem proving technology is in combining this generality with an acceptable degree of automation. We describe an approach for enhancing the effectiveness of theorem provers for hardware verification through the use of efficient automatic procedures for rewriting, arithmetic and equality reasoning, and an off-the-shelf BDD-based propo-sitional simplifier. These automatic procedures can be combined into general-purpose proof strategies that can efficiently automate a number of proofs including those of hardware correctness. The inference procedures and proof strategies have been implemented in the PVS verification system. They are applied to several examples including an N-bit adder, the Saxe pipelined processor, and the benchmark Tamarack microprocessor design. These examples illustrate the basic design philosophy underlying PVS where powerful and efficient low-level inferences are employed within high-level user-defined proof strategies. This approach is contrasted with approaches based on tactics or batch-oriented theorem proving.",
	isbn = "978-3-540-49177-4"
}

@inproceedings{daggitt23_compil_higher_order_specif_smt_solver,
	author = {Daggitt, Matthew L. and Atkey, Robert and Kokke, Wen and Komendantskaya, Ekaterina and Arnaboldi, Luca},
	title = {Compiling Higher-Order Specifications to SMT Solvers: How to Deal with Rejection Constructively},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575674},
	doi = {10.1145/3573105.3575674},
	abstract = {Modern verification tools frequently rely on compiling high-level specifications to SMT queries. However, the high-level specification language is usually more expressive than the available solvers and therefore some syntactically valid specifications must be rejected by the tool. In such cases, the challenge is to provide a comprehensible error message to the user that relates the original syntactic form of the specification to the semantic reason it has been rejected. In this paper we demonstrate how this analysis may be performed by combining a standard unification-based type-checker with type classes and automatic generalisation. Concretely, type-checking is used as a constructive procedure for under-approximating whether a given specification lies in the subset of problems supported by the solver. Any resulting proof of rejection can be transformed into a detailed explanation to the user. The approach is compositional and does not require the user to add extra typing annotations to their program. We subsequently describe how the type system may be leveraged to provide a sound and complete compilation procedure from suitably typed expressions to SMT queries, which we have verified in Agda.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {102–120},
	numpages = {19},
	keywords = {compilers, domain specific languages, type-checking, Agda, SMT solvers, verification},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@article{dal21_inter_types_posit_almos_sure_termin,
	abstract = {Randomized higher-order computation can be seen as being captured by a λ-calculus endowed with a single algebraic operation, namely a construct for binary probabilistic choice. What matters about such computations is the probability of obtaining any given result, rather than the possibility or the necessity of obtaining it, like in (non)deterministic computation. Termination, arguably the simplest kind of reachability problem, can be spelled out in at least two ways, depending on whether it talks about the probability of convergence or about the expected evaluation time, the second one providing a stronger guarantee. In this paper, we show that intersection types are capable of precisely characterizing both notions of termination inside a single system of types: the probability of convergence of any λ-term can be underapproximated by its type, while the underlying derivation’s weight gives a lower bound to the term’s expected number of steps to normal form. Noticeably, both approximations are tight—not only soundness but also completeness holds. The crucial ingredient is non-idempotency, without which it would be impossible to reason on the expected number of reduction steps which are necessary to completely evaluate any term. Besides, the kind of approximation we obtain is proved to be optimal recursion theoretically: no recursively enumerable formal system can do better than that.},
	author = {Dal Lago, Ugo and Faggian, Claudia and Rocca, Simona Ronchi Della},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434313},
	doi = {10.1145/3434313},
	journaltitle = {Proc. ACM Program. Lang. (POPL)},
	keywords = {intersection types,type systems,almost-sure termination,expected time},
	month = jan,
	number = {POPL},
	title = {Intersection Types and (Positive) Almost-Sure Termination},
	volume = {5},
	year = {2021}
}

@article{dal22_effec_progr_distan,
	author = {Dal Lago, Ugo and Gavazzo, Francesco},
	title = {Effectful Program Distancing},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498680},
	doi = {10.1145/3498680},
	abstract = {Semantics is traditionally concerned with program equivalence, in which all pairs of programs which are not equivalent are treated the same, and simply dubbed as incomparable. In recent years, various forms of program metrics have been introduced such that the distance between non-equivalent programs is measured as an element of an appropriate quantale. By letting the underlying quantale vary as the type of the compared programs become more complex, the recently introduced framework of differential logical relations allows for a new contextual form of reasoning. In this paper, we show that all this can be generalised to effectful higher-order programs, in which not only the values, but also the effects computations produce can be appropriately distanced in a principled way. We show that the resulting framework is flexible, allowing various forms of effects to be handled, and that it provides compact and informative judgments about program differences.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {19},
	numpages = {30},
	keywords = {Lambda Calculus, Program Distances, Monads, Metrics, Logical Relations}
}

@article{dal22_relat_theor_effec_coeff,
	author = {Dal Lago, Ugo and Gavazzo, Francesco},
	title = {A Relational Theory of Effects and Coeffects},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498692},
	doi = {10.1145/3498692},
	abstract = {Graded modal types systems and coeffects are becoming a standard formalism to deal with context-dependent, usage-sensitive computations, especially when combined with computational effects. From a semantic perspective, effectful and coeffectful languages have been studied mostly by means of denotational semantics and almost nothing has been done from the point of view of relational reasoning. This gap in the literature is quite surprising, since many cornerstone results — such as non-interference, metric preservation, and proof irrelevance — on concrete coeffects are inherently relational. In this paper, we fill this gap by developing a general theory and calculus of program relations for higher-order languages with combined effects and coeffects. The relational calculus builds upon the novel notion of a corelator (or comonadic lax extension) to handle coeffects relationally. Inside such a calculus, we define three notions of effectful and coeffectful program refinements: contextual approximation, logical preorder, and applicative similarity. These are the first operationally-based notions of program refinement (and, consequently, equivalence) for languages with combined effects and coeffects appearing in the literature. We show that the axiomatics of a corelator (together with the one of a relator) is precisely what is needed to prove all the aforementioned program refinements to be precongruences, this way obtaining compositional relational techniques for reasoning about combined effects and coeffects.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {31},
	numpages = {28},
	keywords = {(Co)Relator, Applicative Bisimulation, Lax Extension, (Algebraic) Effects, Logical Relations, Program Equivalence and Refinement, Relational Reasoning, Coeffects, Graded Modal Types}
}

@InProceedings{dalla06_opaque_predic_detec_abstr_inter,
	doi = {10.1007/11784180_9},
	author = {Dalla Preda, Mila and Madou, Matias and De Bosschere, Koen and Giacobazzi, Roberto},
	editor = "Johnson, Michael
and Vene, Varmo",
	title = "Opaque Predicates Detection by Abstract Interpretation",
	booktitle = "Algebraic Methodology and Software Technology",
	year = "2006",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "81--95",
	abstract = "Code obfuscation and software watermarking are well known techniques designed to prevent the illegal reuse of software. Code obfuscation prevents malicious reverse engineering, while software watermarking protects code from piracy. An interesting class of algorithms for code obfuscation and software watermarking relies on the insertion of opaque predicates. It turns out that attackers based on a dynamic or an hybrid static-dynamic approach are either not precise or time consuming in eliminating opaque predicates. We present an abstract interpretation-based methodology for removing opaque predicates from programs. Abstract interpretation provides the right framework for proving the correctness of our approach, together with a general methodology for designing efficient attackers for a relevant class of opaque predicates. Experimental evaluations show that abstract interpretation based attacks significantly reduce the time needed to eliminate opaque predicates.",
	isbn = "978-3-540-35636-3"
}

@inproceedings{damani22_memor_acces_sched_reduc_thread_migrat,
	author = {Damani, Sana and Barua, Prithayan and Sarkar, Vivek},
	title = {Memory Access Scheduling to Reduce Thread Migrations},
	year = {2022},
	isbn = {9781450391832},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497776.3517768},
	doi = {10.1145/3497776.3517768},
	abstract = {It has been widely observed that data movement is emerging as the primary bottleneck to scalability and energy efficiency in future hardware, especially for applications and algorithms that are not cache-friendly and achieve below 1% of peak performance on today’s systems. The idea of “moving compute to data” has been suggested as one approach to address this challenge. While there are approaches that can achieve this migration in software, hardware support is a promising direction from the perspectives of lower overheads and programmer productivity. Migratory thread architectures migrate lightweight hardware thread contexts to the location of the data instead of transferring data to the requesting processor. However, while transporting thread contexts is cheaper than moving data, thread migrations still incur energy and bandwidth overheads and can be particularly expensive if threads frequently migrate in a ping-pong manner between processors due to poor locality of access. In this paper, we propose Memory Access Scheduling, a new compiler optimization that aims to reduce the number of overall thread migrations when executing a program on migratory thread architectures. Our experiments show performance improvements with a geometric mean speedup of 1.23\texttimes{} for a set of 7 explicitly-parallelized kernels, and of 1.10\texttimes{} for a set of 15 automatically-parallelized kernels. We believe that memory access scheduling will also be an important optimization for other locality-centric architectures that benefit from software thread migrations, such as multi-threaded NUMA architectures.},
	booktitle = {Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction},
	pages = {144–155},
	numpages = {12},
	keywords = {Instruction Scheduling, Compilers, Dataflow Analysis, Integer Linear Programming (ILP), Thread Migration, Emu Architecture, Sequential Ordering Problem},
	location = {Seoul, South Korea},
	series = {CC 2022}
}

@inproceedings{daoud14_survey_high_level_synth_languag,
	abstract = {High Level Languages (HLLs) make programming easier and more efficient; therefore, powerful applications can be written, modified, and debugged easily. Nowadays, applications can be divided into parallel tasks and run on different processing elements, such as CPUs, GPUs, or FPGAs; for achieving higher performance. However, in the case of FPGAs, generating hardware modules automatically from high level representation is one of the major research activities in the last few years. Current research focuses on designing programming platforms that allow parallel applications to be run on different platforms, including FPGA. In this paper, a survey of HLLs, tools, and compilers used for translating high level representation to hardware description language is presented. Technical analysis of such tools and compilers is discussed as well.},
	author = {Daoud, Luka and Zydek, Dawid and Selvaraj, Henry},
	editor = {Swiątek, Jerzy and Grzech, Adam and Swiątek, Pawe{ł} and Tomczak, Jakub M.},
	location = {Cham},
	publisher = {Springer International Publishing},
	booktitle = {Advances in Systems Science},
	isbn = {978-3-319-01857-7},
	keywords = {high-level synthesis,survey},
	pages = {483--492},
	title = {A Survey of High Level Synthesis Languages, Tools, and Compilers for Reconfigurable High Performance Computing},
	year = {2014}
}

@article{das23_combin_funct_autom_synth_discov,
	author = {Das, Ria and Tenenbaum, Joshua B. and Solar-Lezama, Armando and Tavares, Zenna},
	title = {Combining Functional and Automata Synthesis to Discover Causal Reactive Programs},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571249},
	doi = {10.1145/3571249},
	abstract = {We present a new algorithm that synthesizes functional reactive programs from observation data. The key novelty is to iterate between a functional synthesis step, which attempts to generate a transition function over observed states, and an automata synthesis step, which adds any additional latent state necessary to fully account for the observations. We develop a functional reactive DSL called Autumn that can express a rich variety of causal dynamics in time-varying, Atari-style grid worlds, and apply our method to synthesize Autumn programs from data. We evaluate our algorithm on a benchmark suite of 30 Autumn programs as well as a third-party corpus of grid-world-style video games. We find that our algorithm synthesizes 27 out of 30 programs in our benchmark suite and 21 out of 27 programs from the third-party corpus, including several programs describing complex latent state transformations, and from input traces containing hundreds of observations. We expect that our approach will provide a template for how to integrate functional and automata synthesis in other induction domains.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {56},
	numpages = {31},
	keywords = {causal, synthesis, automata, reactive}
}

@article{das23_probab_resour_aware_session_types,
	author = {Das, Ankush and Wang, Di and Hoffmann, Jan},
	title = {Probabilistic Resource-Aware Session Types},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571259},
	doi = {10.1145/3571259},
	abstract = {Session types guarantee that message-passing processes adhere to predefined communication protocols. Prior work on session types has focused on deterministic languages but many message-passing systems, such as Markov chains and randomized distributed algorithms, are probabilistic. To implement and analyze such systems, this article develops the meta theory of probabilistic session types with an application focus on automatic expected resource analysis. Probabilistic session types describe probability distributions over messages and are a conservative extension of intuitionistic (binary) session types. To send on a probabilistic channel, processes have to utilize internal randomness from a probabilistic branching or external randomness from receiving on a probabilistic channel. The analysis for expected resource bounds is smoothly integrated with the type system and is a variant of automatic amortized resource analysis. Type inference relies on linear constraint solving to automatically derive symbolic bounds for various cost metrics. The technical contributions include the meta theory that is based on a novel nested multiverse semantics and a type-reconstruction algorithm that allows flexible mixing of different sources of randomness without burdening the programmer with complex type annotations. The type system has been implemented in the language NomosPro with linear-time type checking. Experiments demonstrate that NomosPro is applicable in different domains such as cost analysis of randomized distributed algorithms, analysis of Markov chains, probabilistic analysis of amortized data structures and digital contracts. NomosPro is also shown to be scalable by (i) implementing two broadcast and a bounded retransmission protocol where messages are dropped with a fixed probability, and (ii) verifying the limiting distribution of a Markov chain with 64 states and 420 transitions.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {66},
	numpages = {32},
	keywords = {Nested Multiverse Semantics, Probabilistic Concurrency, Resource Analysis, Session Types}
}

@article{dash23_affin_monad_lazy_struc_bayes_progr,
	author = {Dash, Swaraj and Kaddar, Younesse and Paquet, Hugo and Staton, Sam},
	title = {Affine Monads and Lazy Structures for Bayesian Programming},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571239},
	doi = {10.1145/3571239},
	abstract = {We show that streams and lazy data structures are a natural idiom for programming with infinite-dimensional Bayesian methods such as Poisson processes, Gaussian processes, jump processes, Dirichlet processes, and Beta processes. The crucial semantic idea, inspired by developments in synthetic probability theory, is to work with two separate monads: an affine monad of probability, which supports laziness, and a commutative, non-affine monad of measures, which does not. (Affine means that T(1)≅ 1.) We show that the separation is important from a decidability perspective, and that the recent model of quasi-Borel spaces supports these two monads. To perform Bayesian inference with these examples, we introduce new inference methods that are specially adapted to laziness; they are proven correct by reference to the Metropolis-Hastings-Green method. Our theoretical development is implemented as a Haskell library, LazyPPL.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {46},
	numpages = {31},
	keywords = {functional programming, quasi-Borel spaces, categorical semantics, commutative monads, probabilistic programming, Haskell, synthetic measure theory, nonparametric statistics, laziness, Bayesian inference}
}

@article{daumas10_certif_bound_expres_invol_round_operat,
	abstract = {Gappa is a tool designed to formally verify the correctness of numerical software and hardware. It uses interval arithmetic and forward error analysis to bound mathematical expressions that involve rounded as well as exact operators. It then generates a theorem and its proof for each verified enclosure. This proof can be automatically checked with a proof assistant, such as Coq or HOL Light. It relies on a large companion library of facts that we have developed. This Coq library provides theorems dealing with addition, multiplication, division, and square root, for both fixed- and floating-point arithmetics. Gappa uses multiple-precision dyadic fractions for the endpoints of intervals and performs forward error analysis on rounded operators when necessary. When asked, Gappa reports the best bounds it is able to reach for a given expression in a given context. This feature can be used to identify where the set of facts and automatic techniques implemented in Gappa becomes insufficient. Gappa handles seamlessly additional properties expressed as interval properties or rewriting rules in order to establish more intricate bounds. Recent work showed that Gappa is suited to discharge proof obligations generated for small pieces of software. They may be produced by third-party tools and the first applications of Gappa use proof obligations written by designers or obtained from traces of execution.},
	author = {Daumas, Marc and Melquiond, Guillaume},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/1644001.1644003},
	doi = {10.1145/1644001.1644003},
	issn = {0098-3500},
	journaltitle = {ACM Trans. Math. Softw.},
	keywords = {dyadic fraction,PVS,proof obligation,interval arithmetic,Forward error analysis,proof system,Coq,HOL Light,floating point},
	month = jan,
	number = {1},
	title = {Certification of Bounds on Expressions Involving Rounded Operators},
	volume = {37},
	year = {2010}
}

@book{davis07,
	abstract = {Finite State Machine Datapath Design, Optimization, and Implementation explores the design space of combined FSM/Datapath implementations. The lecture starts by examining performance issues in digital systems such as clock skew and its effect on setup and hold time constraints, and the use of pipelining for increasing system clock frequency. This is followed by definitions for latency and throughput, with associated resource tradeoffs explored in detail through the use of dataflow graphs and scheduling tables applied to examples taken from digital signal processing applications. Also, design issues relating to functionality, interfacing, and performance for different types of memories commonly found in ASICs and FPGAs such as FIFOs, single-ports, and dual-ports are examined. Selected design examples are presented in implementation-neutral Verilog code and block diagrams, with associated design files available as downloads for both Altera Quartus and Xilinx Virtex FPGA platforms. A working knowledge of Verilog, logic synthesis, and basic digital design techniques is required. This lecture is suitable as a companion to the synthesis lecture titled Introduction to Logic Synthesis using Verilog HDL. Table of Contents: Calculating Maximum Clock Frequency / Improving Design Performance / Finite State Machine with Datapath (FSMD) Design / Embedded Memory Usage in Finite State Machine with Datapath (FSMD) Designs},
	author = {Davis, J. and Reese, R.},
	publisher = {Morgan & Claypool},
	url = {https://ieeexplore.ieee.org/document/6812910},
	booktitle = {Finite State Machine Datapath Design, Optimization, and Implementation},
	doi = {10.2200/S00087ED1V01Y200702DCS014},
	isbn = {9781598295306},
	keywords = {Verilog;datapath;scheduling;latency;throughput;timing;pipelining;memories;FPGA;flowgraph},
	year = {2007}
}

@article{day23_expres_power_strin_const,
	author = {Day, Joel D. and Ganesh, Vijay and Grewal, Nathan and Manea, Florin},
	title = {On the Expressive Power of String Constraints},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571203},
	doi = {10.1145/3571203},
	abstract = {We investigate properties of strings which are expressible by canonical types of string constraints. Specifically, we consider a landscape of 20 logical theories, whose syntax is built around combinations of four common elements of string constraints: language membership (e.g. for regular languages), concatenation, equality between string terms, and equality between string-lengths. For a variable x and formula f from a given theory, we consider the set of values for which x may be substituted as part of a satisfying assignment, or in other words, the property f expresses through x. Since we consider string-based logics, this set is a formal language. We firstly consider the relative expressive power of different combinations of string constraints by comparing the classes of languages expressible in the corresponding theories, and are able to establish a mostly complete picture in this regard. Secondly, we consider the question of deciding whether the language or property expressed by a variable/formula in one theory can be expressed in another theory. We establish several negative results which are relevant to preprocessing and normalisation of string constraints in practice. Some of our results have strong connections to important open problems regarding word equations and the theory of string solving.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {10},
	numpages = {31},
	keywords = {String solving, Word equations, String constraints}
}

@book{de94_synth_optim_digit_circuit,
	abstract = {From the Publisher:Synthesis and Optimization of Digital Circuits offers a modern, up-to-date look at computer-aided design (CAD) of very large-scale integration (VLSI) circuits. In particular, this book covers techniques for synthesis and optimization of digital circuits at the architectural and logic levels, i.e., the generation of performance-and/or area-optimal circuits representations from models in hardware description languages. The book provides a thorough explanation of synthesis and optimization algorithms accompanied by a sound mathematical formulation and a unified notation. The text covers the following topics: modern hardware description languages (e.g., VHDL, Verilog); architectural-level synthesis of data flow and control units, including algorithms for scheduling and resource binding; combinational logic optimization algorithms for two-level and multiple-level circuits; sequential logic optimization methods; and library binding techniques, including those applicable to FPGAs.},
	author = {De Micheli, Giovanni},
	publisher = {McGraw-Hill Higher Education},
	edition = {1st},
	isbn = {0070163332},
	title = {Synthesis and Optimization of Digital Circuits},
	year = {1994}
}

@inproceedings{demange15_verif_fast_spars_ssa_based_optim_coq,
	abstract = {The Static Single Assignment (SSA) form is a predominant technology in modern compilers, enabling powerful and fast program optimizations. Despite its great success in the implementation of production compilers, it is only very recently that this technique has been introduced in verified compilers. As of today, few evidence exist on that, in this context, it also allows faster and simpler optimizations. This work builds on the CompCertSSA verified compiler (an SSA branch of the verified CompCert C compiler). We implement and verify two prevailing SSA optimizations: Sparse Conditional Constant Propagation and Global Value Numbering. For both transformations, we mechanically prove their soundness in the Coq proof assistant. Both optimization proofs are embedded in a single sparse optimization framework, factoring out many of the dominance-based reasoning steps required in proofs of SSA-based optimizations. Our experimental evaluations indicate both a better precision, and a significant compilation time speedup.},
	author = {Demange, Delphine and Pichardie, David and Stefanesco, Léo},
	editor = {Franke, Björn},
	location = {Berlin, Heidelberg},
	publisher = {Springer},
	booktitle = {Compiler Construction},
	isbn = {978-3-662-46663-6},
	keywords = {CompCertSSA,CompCert,SSA,coq,verification,compiler optimisation},
	pages = {233--252},
	title = {Verifying Fast and Sparse {SSA}-Based Optimizations in Coq},
	year = {2015}
}

@inproceedings{demange16_mechan_conven_ssa_verif_destr_coales,
	abstract = {Modern optimizing compilers rely on the Static Single Assignment (SSA) form to make optimizations fast and simpler to implement. From a semantic perspective, the SSA form is nowadays fairly well understood, as witnessed by recent advances in the field of formally verified compilers. The destruction of the SSA form, however, remains a difficult problem, even in a non-verified environment. In fact, the out-of-SSA transformation has been revisited, for correctness and performance issues, up until recently. Unsurprisingly, state-of-the-art compiler formalizations thus either completely ignore, only partially handle, or implement naively the SSA destruction. This paper reports on the implementation of such a destruction within a verified compiler. We formally define and prove the properties of the generation of Conventional SSA (CSSA) which make its destruction simple to implement and prove. Second, we implement and prove correct a coalescing destruction of CSSA, a la Boissinot et al., where variables can be coalesced according to a refined notion of interference. This formalization work extends the CompCertSSA compiler, whose correctness proof is mechanized in the Coq proof assistant. Our CSSA-based, coalescing destruction removes, on average, more than 99% of introduced copies, and leads to encouraging results concerning spilling during post-SSA register allocation.},
	author = {Demange, Delphine and Fernandez de Retana, Yon},
	location = {Barcelona, Spain},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/2892208.2892222},
	booktitle = {Proceedings of the 25th International Conference on Compiler Construction},
	doi = {10.1145/2892208.2892222},
	isbn = {9781450342414},
	keywords = {CompCertSSA,CompCert,SSA,coq,verification,compiler optimisation},
	pages = {77--87},
	series = {CC 2016},
	title = {Mechanizing Conventional {SSA} for a Verified Destruction with Coalescing},
	year = {2016}
}

@article{derrien20_towar_specul_loop_pipel_high_level_synth,
	author = {Derrien, Steven and Marty, Thibaut and Rokicki, Simon and Yuki, Tomofumi},
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
	url = {https://doi.org/10.1109/tcad.2020.3012866},
	doi = {10.1109/tcad.2020.3012866},
	journaltitle = {{IEEE} Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {high-level synthesis,hardware pipelining,speculative execution,gated-SSA,SSA,application},
	month = nov,
	number = {11},
	pages = {4229--4239},
	title = {Toward Speculative Loop Pipelining for High-Level Synthesis},
	volume = {39},
	year = {2020}
}

@InProceedings{despeyroux95_higher_order_abstr_syntax_coq,
	keywords = {grammar, coq},
	doi = {10.1007/BFb0014049},
	author = {Despeyroux, Joëlle and Felty, Amy and Hirschowitz, André},
	editor = "Dezani-Ciancaglini, Mariangiola
and Plotkin, Gordon",
	title = {Higher-Order Abstract Syntax in Coq},
	booktitle = "Typed Lambda Calculi and Applications",
	year = "1995",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "124--138",
	abstract = "The terms of the simply-typed $\lambda$-calculus can be used to express the higher-order abstract syntax of objects such as logical formulas, proofs, and programs. Support for the manipulation of such objects is provided in several programming languages (e.g. $\lambda$Prolog, Elf). Such languages also provide embedded implication, a tool which is widely used for expressing hypothetical judgments in natural deduction. In this paper, we show how a restricted form of second-order syntax and embedded implication can be used together with induction in the Coq Proof Development system. We specify typing rules and evaluation for a simple functional language containing only function abstraction and application, and we fully formalize a proof of type soundness in the system. One difficulty we encountered is that expressing the higher-order syntax of an object-language as an inductive type in Coq generates a class of terms that contains more than just those that directly represent objects in the language. We overcome this difficulty by defining a predicate in Coq that holds only for those terms that correspond to programs. We use this predicate to express and prove the adequacy for our syntax.",
	isbn = "978-3-540-49178-1"
}

@inproceedings{dessouky19_hardf,
	author = {Dessouky, Ghada and Gens, David and Haney, Patrick and Persyn, Garrett and Kanuparthi, Arun and Khattri, Hareesh and Fung, Jason M. and Sadeghi, Ahmad-Reza and Rajendran, Jeyavijayan},
	title = {{HardFails}: Insights into {Software-Exploitable} Hardware Bugs},
	booktitle = {28th USENIX Security Symposium (USENIX Security 19)},
	year = {2019},
	isbn = {978-1-939133-06-9},
	address = {Santa Clara, CA},
	pages = {213--230},
	url = {https://www.usenix.org/conference/usenixsecurity19/presentation/dessouky},
	publisher = {USENIX Association},
	month = aug
}

@article{di21_funct_seman_partial_theor,
	abstract = {We provide a Lawvere-style definition for partial theories, extending the classical notion of equational theory by allowing partially defined operations. As in the classical case, our definition is syntactic: we use an appropriate class of string diagrams as terms. This allows for equational reasoning about the class of models defined by a partial theory. We demonstrate the expressivity of such equational theories by considering a number of examples, including partial combinatory algebras and cartesian closed categories. Moreover, despite the increase in expressivity of the syntax we retain a well-behaved notion of semantics: we show that our categories of models are precisely locally finitely presentable categories, and that free models exist.},
	author = {Di Liberti, Ivan and Loregian, Fosco and Nester, Chad and Sobociński, Paweł},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434338},
	doi = {10.1145/3434338},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {categories of partial maps,Lawvere theory,variety theorem,semantics,syntax},
	month = jan,
	number = {POPL},
	title = {Functorial Semantics for Partial Theories},
	volume = {5},
	year = {2021}
}

@article{di23_partial_order_view_messag_passin_commun_model,
	author = {Di Giusto, Cinzia and Ferr\'{e}, Davide and Laversa, Laetitia and Lozes, Etienne},
	title = {A Partial Order View of Message-Passing Communication Models},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571248},
	doi = {10.1145/3571248},
	abstract = {There is a wide variety of message-passing communication models, ranging from synchronous "rendez-vous" communications to fully asynchronous/out-of-order communications. For large-scale distributed systems, the communication model is determined by the transport layer of the network, and a few classes of orders of message delivery (FIFO, causally ordered) have been identified in the early days of distributed computing. For local-scale message-passing applications, e.g., running on a single machine, the communication model may be determined by the actual implementation of message buffers and by how FIFO queues are used. While large-scale communication models, such as causal ordering, are defined by logical axioms, local-scale models are often defined by an operational semantics. In this work, we connect these two approaches, and we present a unified hierarchy of communication models encompassing both large-scale and local-scale models, based on their concurrent behaviors. We also show that all the communication models we consider can be axiomatized in the monadic second order logic, and may therefore benefit from several bounded verification techniques based on bounded special treewidth.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {55},
	numpages = {27},
	keywords = {Bounded Treewidth, Bounded Model Checking, Monadic Second order Logic, Asynchronous Communication, Message-passing}
}

@thesis{dimitrov02_devel_veril_hdl,
	author = {Dimitrov, Jordan},
	institution = {PhD thesis, Software Technology Research Laboratory, De Montfort University},
	keywords = {verilog,operational semantics,hardware/software co-simulation},
	title = {Developing semantics of Verilog HDL in formal compositional design of mixed hardware/software system},
	type = {phdthesis},
	year = {2002}
}

@article{dinechin11_desig_custom_arith_data_paths_flopoc,
	abstract = {Efficient implementation of basic, data-path circuit elements is of fundamental importance to achieving high performance in FPGA-based acceleration of scientific computing. This work presents a leading effort to automate the production of pipelined data-path circuits for implementing numerical functions.},
	author = {de Dinechin, F. and Pasca, B.},
	doi = {10.1109/MDT.2011.44},
	issn = {1558-1918},
	journaltitle = {IEEE Design Test of Computers},
	keywords = {field programmable gate arrays;floating point arithmetic;logic design;pipeline arithmetic;custom arithmetic data path design;FloPoCo;data-path circuit elements;FPGA-based acceleration;scientific computing;pipelined data-path circuits;numerical functions;Pipeline processing;Field programmable gate arrays;Digital signal processing;Synchronization;Generators;design and test;FloPoCo;core generator;arithmetic circuit;floating-point;pipelining;data path;FPGAs;reconfigurable computing;VHDL;C+;+;framework},
	month = jul,
	number = {4},
	pages = {18--27},
	title = {Designing Custom Arithmetic Data Paths with FloPoCo},
	volume = {28},
	year = {2011}
}

@article{dinechin12_multip_ration_const,
	abstract = {Multiplications by simple rational constants often appear in fixed- or floating-point application code, for instance, in the form of division by an integer constant. The hardware implementation of such operations is of practical interest to reconfigurable computing. It is well known that the binary representation of rational constants is eventually periodic. This brief shows how this feature can be exploited to implement multiplication by a rational constant in a number of additions that is logarithmic in the precision. An open-source implementation of these techniques is provided and is shown to be practically relevant for constants with small numerators and denominators, where it provides improvements of 20% to 40% in area with respect to the state of the art. It is also shown that, for such constants, the additional cost for a correctly rounded result is very small and that correct rounding very often comes for free in practice.},
	author = {de Dinechin, F.},
	doi = {10.1109/TCSII.2011.2177706},
	issn = {1558-3791},
	journaltitle = {IEEE Transactions on Circuits and Systems II: Express Briefs},
	keywords = {field programmable gate arrays;fixed point arithmetic;floating point arithmetic;rational constant binary representation;floating-point application code;fixed-point application code;integer constant;reconfigurable computing;field- programmable gate arrays;FPGA;Adders;Table lookup;Field programmable gate arrays;Hardware;Generators;Optimization;Floating-point;multiplication by a constant;rational number;reconfigurable computing},
	month = feb,
	number = {2},
	pages = {98--102},
	title = {Multiplication by Rational Constants},
	volume = {59},
	year = {2012}
}

@inproceedings{ding14_singl_assig_compil_singl_assig_archit,
	abstract = {We present a new static single assignment form which can be used by an optimizing compiler as its internal representation and the micro-architecture as its instruction set. This representation, Future Gated Single Assignment Form (FGSA), directly represents the use-def relationship of variables by employing the concept of congruence classes and the concept of future dependencies. We show that FGSA is efficiently computable by using a series of T1/T2 transformations, yielding an expected linear time algorithm for the construction of single assignment form. Our interval analysis method includes a novel transformation TR which eliminates irreducible loops without node splitting and combines computation of single-assignment form with irreducible loop elimination. The algorithm produces pruned single assignment form, rendering a separate pruning step unnecessary. In practice, the FGSA approach results in an average reduction of 7.7%, with a maximum of 67% in the number of gating functions compared to the pruned SSA form on the SPEC2000 benchmark suite, owing to its ability to represent dataflow within a congruence class by using a single gating function. We illustrate that FGSA is convenient to use as an internal representation in an optimizing compiler by presenting two case studies of optimization algorithms on FGSA.},
	author = {Ding, Shuhan and Earnest, John and Önder, Soner},
	location = {Orlando, FL, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/2544137.2544158},
	booktitle = {Proceedings of Annual IEEE/ACM International Symposium on Code Generation and Optimization},
	doi = {10.1145/2544137.2544158},
	isbn = {9781450326704},
	keywords = {gated-SSA},
	pages = {196--207},
	series = {CGO '14},
	title = {Single Assignment Compiler, Single Assignment Architecture: Future Gated Single Assignment Form*; Static Single Assignment with Congruence Classes},
	year = {2014}
}

@article{ding23_witnes_undec_probl,
	author = {Ding, Shuo and Zhang, Qirun},
	title = {Witnessability of Undecidable Problems},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571227},
	doi = {10.1145/3571227},
	abstract = {Many problems in programming language theory and formal methods are undecidable, so they cannot be solved precisely. Practical techniques for dealing with undecidable problems are often based on decidable approximations. Undecidability implies that those approximations are always imprecise. Typically, practitioners use heuristics and ad hoc reasoning to identify imprecision issues and improve approximations, but there is a lack of computability-theoretic foundations about whether those efforts can succeed. This paper shows a surprising interplay between undecidability and decidable approximations: there exists a class of undecidable problems, such that it is computable to transform any decidable approximation to a witness input demonstrating its imprecision. We call those undecidable problems witnessable problems. For example, if a program property P is witnessable, then there exists a computable function fP, such that fP takes as input the code of any program analyzer targeting P and produces an input program w on which the program analyzer is imprecise. An even more surprising fact is that the class of witnessable problems includes almost all undecidable problems in programming language theory and formal methods. Specifically, we prove the diagonal halting problem K is witnessable, and the class of witnessable problems is closed under complements and many-one reductions. In particular, all “non-trivial semantic properties of programs” mentioned in Rice’s theorem are witnessable. We also explicitly construct a problem in the non-witnessable (and undecidable) class and show that both classes have cardinality 2ℵ0. Our results offer a new perspective on the understanding of undecidability: for witnessable problems, although it is impossible to solve them precisely, it is always possible to improve any decidable approximation to make it closer to the precise solution. This fact formally demonstrates that research efforts on such approximations are promising and shows there exist universal ways to identify precision issues of program analyzers, program verifiers, SMT solvers, etc., because their essences are decidable approximations of witnessable problems.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {34},
	numpages = {21},
	keywords = {Automated Theorem Proving, Mathematical Logic, Program Semantics}
}

@inbook{ditu13_improv_instr_sched,
	abstract = {Instruction Scheduling as a compiler optimization is a very powerful technique to enable instruction level parallelism for many types of modern architectures. Instruction Scheduling can be used for making best fill of the micro-architecture pipeline (by minimizing the number of pipeline stalls) and is also of critical importance for keeping as busy as possible the multiple execution units for the architectures with parallel execution sets (like VLIW, EPIC or VLES architectures). This paper will present a set of improvements that can be brought to an instruction scheduling technique implemented in a real compiler for a VLIW architecture, where both pipeline aspects and multiple execution units are exploited. The improvements are based on practical and theoretical observations. They include a possible false-WAW dependence improvement, another improvement by considering inter-block latencies and also some improvements about hyper-block scheduling and IF-conversion integration with instruction scheduling.},
	author = {Ditu, Bogdan and Tapus, Nicolae},
	editor = {Dumitrache, Loan},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	url = {https://doi.org/10.1007/978-3-642-32548-9_29},
	booktitle = {Advances in Intelligent Control Systems and Computer Science},
	doi = {10.1007/978-3-642-32548-9_29},
	isbn = {978-3-642-32548-9},
	keywords = {trace scheduling,static scheduling},
	pages = {407--421},
	title = {Improvements of Instruction Scheduling},
	year = {2013}
}

@InProceedings{doherty09_nonbl_algor_backw_simul,
	doi = {10.1007/978-3-642-04355-0_28},
	keywords = {simulation proof},
	author = {Doherty, Simon and Moir, Mark},
	editor = "Keidar, Idit",
	title = "Nonblocking Algorithms and Backward Simulation",
	booktitle = "Distributed Computing",
	year = "2009",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "274--288",
	abstract = "Optimistic and nonblocking concurrent algorithms are increasingly finding their way into practical use; an important example is software transactional memory implementations. Such algorithms are notoriously difficult to design and verify as correct, and we believe complete, formal, and machine-checked correctness proofs for such algorithms are critical. We have been studying the use of automated tools such as the PVS theorem proving system to model algorithms and their specifications using formalisms such as I/O automata, and using simulation proof techniques to show the algorithms implement their specifications. While it has been relatively rare in the past, optimistic and nonblocking algorithms often require a special flavour of simulation proof, known as backward simulation. In this paper, we present what we believe is by far the most challenging backward simulation proof achieved to date; this proof was developed and completely checked using PVS.",
	isbn = "978-3-642-04355-0"
}

@InProceedings{donaldson11_symmet_aware_predic_abstr_shared,
	keywords = {predicated execution},
	doi = {10.1007/978-3-642-22110-1_28},
	author = {Donaldson, Alastair and Kaiser, Alexander and Kroening, Daniel and Wahl, Thomas},
	editor = "Gopalakrishnan, Ganesh
and Qadeer, Shaz",
	title = "Symmetry-Aware Predicate Abstraction for Shared-Variable Concurrent Programs",
	booktitle = "Computer Aided Verification",
	year = "2011",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "356--371",
	abstract = "Predicate abstraction is a key enabling technology for applying finite-state model checkers to programs written in mainstream languages. It has been used very successfully for debugging sequential system-level C code. Although model checking was originally designed for analyzing concurrent systems, there is little evidence of fruitful applications of predicate abstraction to shared-variable concurrent software. The goal of this paper is to close this gap. We have developed a symmetry-aware predicate abstraction strategy: it takes into account the replicated structure of C programs that consist of many threads executing the same procedure, and generates a Boolean program template whose multi-threaded execution soundly overapproximates the concurrent C program. State explosion during model checking parallel instantiations of this template can now be absorbed by exploiting symmetry. We have implemented our method in the satabs predicate abstraction framework, and demonstrate its superior performance over alternative approaches on a large range of synchronization programs.",
	isbn = "978-3-642-22110-1"
}

@inproceedings{donaldson21_test_case_reduc_dedup_almos,
	abstract = {Recent transformation-based approaches to compiler testing look for mismatches between the results of pairs of equivalent programs, where one program is derived from the other by randomly applying semantics-preserving transformations. We present a formulation of transformation-based compiler testing that provides effective test-case reduction almost for free: if transformations are designed to be as small and independent as possible, standard delta debugging can be used to shrink a bug-inducing transformation sequence to a smaller subsequence that still triggers the bug. The bug can then be reported as a delta between an original and minimally-transformed program. Minimized transformation sequences can also be used to heuristically deduplicate a set of bug-inducing tests, recommending manual investigation of those that involve disparate types of transformations and thus may have different root causes. We demonstrate the effectiveness of our approach via a new tool, spirv-fuzz, the first compiler-testing tool for the SPIR-V intermediate representation that underpins the Vulkan GPU programming model.},
	author = {Donaldson, Alastair F. and Thomson, Paul and Teliman, Vasyl and Milizia, Stefano and Maselco, André Perez and Karpiński, Antoni},
	location = {Virtual, Canada},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3453483.3454092},
	booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
	doi = {10.1145/3453483.3454092},
	isbn = {9781450383912},
	keywords = {metamorphic testing,Compilers,SPIR-V},
	pages = {1017--1032},
	series = {PLDI 2021},
	title = {Test-Case Reduction and Deduplication Almost for Free with Transformation-Based Compiler Testing},
	year = {2021}
}

@inproceedings{donato22_drag_drop_proof_tactic,
	author = {Donato, Pablo and Strub, Pierre-Yves and Werner, Benjamin},
	title = {A Drag-and-Drop Proof Tactic},
	year = {2022},
	isbn = {9781450391825},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497775.3503692},
	doi = {10.1145/3497775.3503692},
	abstract = {We explore the features of a user interface where formal proofs can be built through gestural actions. In particular, we show how proof construction steps can be associated to drag-and-drop actions. We argue that this can provide quick and intuitive proof construction steps. This work builds on theoretical tools coming from deep inference. It also resumes and integrates some ideas of the former proof-by-pointing project.},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {197–209},
	numpages = {13},
	keywords = {formal proofs, deep inference, logic, user interfaces},
	location = {Philadelphia, PA, USA},
	series = {CPP 2022}
}

@inproceedings{dong15_study,
	author = {Dong, S. and Olivo, O. and Zhang, L. and Khurshid, S.},
	booktitle = {2015 IEEE 26th International Symposium on Software Reliability Engineering (ISSRE)},
	keywords = {symbolic execution,compiler optimisation,survey},
	pages = {205--215},
	title = {Studying the influence of standard compiler optimizations on symbolic execution},
	year = {2015}
}

@inproceedings{doorn23_formal_h_princ_spher_evers,
	author = {van Doorn, Floris and Massot, Patrick and Nash, Oliver},
	title = {Formalising the H-Principle and Sphere Eversion},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575688},
	doi = {10.1145/3573105.3575688},
	abstract = {In differential topology and geometry, the h-principle is a property enjoyed by certain construction problems. Roughly speaking, it states that the only obstructions to the existence of a solution come from algebraic topology. We describe a formalisation in Lean of the local h-principle for first-order open and ample partial differential relations. This is a significant result in differential topology, originally proven by Gromov in 1973 as part of his sweeping effort which greatly generalised many previous flexibility results in geometry. In particular it reproves Smale’s celebrated sphere eversion theorem, a visually striking and counter-intuitive construction. Our formalisation uses Theilli\`{e}re’s implementation of convex integration from 2018. This paper reports on the first part of the sphere eversion project formalising the global version of the h-principle for open and ample first order differential relations, for maps between smooth manifolds. The local version for vector spaces described in this paper is the main ingredient of this proof, and is sufficient to prove the titular corollary of the project. From a broader perspective, the goal of this project is to show that one can formalise advanced mathematics with a strongly geometric flavour and not only algebraically-flavoured mathematics.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {121–134},
	numpages = {14},
	keywords = {differential topology, homotopy principle, convex integration, formalized mathematics, sphere eversion, Lean},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@inproceedings{doshi01_optim,
	author = {Doshi, G. and Krishnaiyer, R. and Muthukumar, K.},
	booktitle = {Proceedings 2001 International Conference on Parallel Architectures and Compilation Techniques},
	doi = {10.1109/PACT.2001.953306},
	keywords = {loop scheduling,software pipelining,rotating registers},
	pages = {257--267},
	title = {Optimizing software data prefetches with rotating registers},
	year = {2001}
}

@article{dosualdo23_path_durab_linear,
	author = {D'Osualdo, Emanuele and Raad, Azalea and Vafeiadis, Viktor},
	title = {The Path to Durable Linearizability},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571219},
	doi = {10.1145/3571219},
	abstract = {There is an increasing body of literature proposing new and efficient persistent versions of concurrent data structures ensuring that a consistent state can be recovered after a power failure or a crash. Their correctness is typically stated in terms of durable linearizability (DL), which requires that individual library operations appear to be executed atomically in a sequence consistent with the real-time order and, moreover, that recovering from a crash return a state corresponding to a prefix of that sequence. Sadly, however, there are hardly any formal DL proofs, and those that do exist cover the correctness of rather simple persistent algorithms on specific (simplified) persistency models. In response, we propose a general, powerful, modular, and incremental proof technique that can be used to guide the development and establish DL. Our technique is (1) general, in that it is not tied to a specific persistency and/or consistency model, (2) powerful, in that it can handle the most advanced persistent algorithms in the literature, (3) modular, in that it allows the reuse of an existing linearizability argument, and (4) incremental, in that the additional requirements for establishing DL depend on the complexity of the algorithm to be verified. We illustrate this technique on various versions of a persistent set, leading to the link-free set of Zuriel et al.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {26},
	numpages = {27},
	keywords = {Weak Memory Models, Px86, Persistency, Non-Volatile Memory, Linearizability, Concurrency}
}

@inproceedings{egolf22_verbat,
	author = {Egolf, Derek and Lasser, Sam and Fisher, Kathleen},
	title = {Verbatim++: Verified, Optimized, and Semantically Rich Lexing with Derivatives},
	year = {2022},
	isbn = {9781450391825},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497775.3503694},
	doi = {10.1145/3497775.3503694},
	abstract = {Lexers and parsers are attractive targets for attackers because they often sit at the boundary between a software system's internals and the outside world. Formally verified lexers can reduce the attack surface of these systems, thus making them more secure. One recent step in this direction is the development of Verbatim, a verified lexer based on the concept of Brzozowski derivatives. Two limitations restrict the tool's usefulness. First, its running time is quadratic in the length of its input string. Second, the lexer produces tokens with a simple "tag and string" representation, which limits the tool's ability to integrate with parsers that operate on more expressive token representations. In this work, we present a suite of extensions to Verbatim that overcomes these limitations while preserving the tool's original correctness guarantees. The lexer achieves effectively linear performance on a JSON benchmark through a combination of optimizations that, to our knowledge, has not been previously verified. The enhanced version of Verbatim also enables users to augment their lexical specifications with custom semantic actions, and it uses these actions to produce semantically rich tokens---i.e., tokens that carry values with arbitrary, user-defined types. All extensions were implemented and verified with the Coq Proof Assistant.},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {27–39},
	numpages = {13},
	keywords = {Brzozowski derivatives, formal verification, semantic actions, lexical analysis},
	location = {Philadelphia, PA, USA},
	series = {CPP 2022}
}

@inproceedings{eichenberger00,
	author = {Eichenberger, Alexandre and Meleis, Waleed and Maradani, Suman},
	publisher = {{ACM} Press},
	url = {https://doi.org/10.1145%2F360128.360140},
	booktitle = {Proceedings of the 33rd annual {ACM}/{IEEE} international symposium on Microarchitecture - {MICRO} 33},
	doi = {10.1145/360128.360140},
	keywords = {predicated execution,hyperblocks},
	title = {An integrated approach to accelerate data and predicate computations in hyperblocks},
	year = {2000}
}

@inproceedings{eichenberger00_integ_approac_accel_data_predic_comput_hyper,
	keywords = {hyperblocks, symbolic execution, abstract interpretation},
	author = {Eichenberger, Alexandre E. and Meleis, Waleed and Maradani, Suman},
	title = {An Integrated Approach to Accelerate Data and Predicate Computations in Hyperblocks},
	year = {2000},
	isbn = {1581131968},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/360128.360140},
	doi = {10.1145/360128.360140},
	booktitle = {Proceedings of the 33rd Annual ACM/IEEE International Symposium on Microarchitecture},
	pages = {101–111},
	numpages = {11},
	location = {Monterey, California, USA},
	series = {MICRO 33}
}

@article{eichholz22_depen_typed_data_plane_progr,
	author = {Eichholz, Matthias and Campbell, Eric Hayden and Krebs, Matthias and Foster, Nate and Mezini, Mira},
	title = {Dependently-Typed Data Plane Programming},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498701},
	doi = {10.1145/3498701},
	abstract = {Programming languages like P4 enable specifying the behavior of network data planes in software. However, with increasingly powerful and complex applications running in the network, the risk of faults also increases. Hence, there is growing recognition of the need for methods and tools to statically verify the correctness of P4 code, especially as the language lacks basic safety guarantees. Type systems are a lightweight and compositional way to establish program properties, but there is a significant gap between the kinds of properties that can be proved using simple type systems (e.g., SafeP4) and those that can be obtained using full-blown verification tools (e.g., p4v). In this paper, we close this gap by developing Π4, a dependently-typed version of P4 based on decidable refinements. We motivate the design of Π4, prove the soundness of its type system, develop an SMT-based implementation, and present case studies that illustrate its applicability to a variety of data plane programs.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {40},
	numpages = {28},
	keywords = {Software-Defined Networking, P4, Dependent Types}
}

@InProceedings{ekici17_smtcoq,
	keywords = {SAT, coq},
	doi = {10.1007/978-3-319-63390-9_7},
	author = {Ekici, Burak and Mebsout, Alain and Tinelli, Cesare and Keller, Chantal and Katz, Guy and Reynolds, Andrew and Barrett, Clark},
	editor = "Majumdar, Rupak
and Kun{\v{c}}ak, Viktor",
	title = "SMTCoq: A Plug-In for Integrating SMT Solvers into Coq",
	booktitle = "Computer Aided Verification",
	year = "2017",
	publisher = "Springer International Publishing",
	address = "Cham",
	pages = "126--133",
	abstract = "This paper describes SMTCoq, a plug-in for the integration of external solvers into the Coq proof assistant. Based on a checker for generic first-order proof certificates fully implemented and proved correct in Coq, SMTCoq offers facilities to check answers from external SAT and SMT solvers and to increase Coq's automation using such solvers, all in a safe way. The current version supports proof certificates produced by the SAT solver ZChaff, for propositional logic, and the SMT solvers veriT and CVC4, for the quantifier-free fragment of the combined theory of fixed-size bit vectors, functional arrays with extensionality, linear integer arithmetic, and uninterpreted function symbols.",
	isbn = "978-3-319-63390-9"
}

@inproceedings{elkhouly16_optim_analy_if_conver_trans,
	author = {Elkhouly, Reem and El-Mahdy, Ahmed and Elmasry, Amr},
	title = {Optimality Analysis of If-Conversion Transformation},
	year = {2016},
	isbn = {9781510823181},
	publisher = {Society for Computer Simulation International},
	address = {San Diego, CA, USA},
	url = {https://doi.org/10.22360/SpringSim.2016.HPC.041},
	doi = {10.22360/SpringSim.2016.HPC.041},
	abstract = {Control-flow dependence has always been posited as a substantial dilemma against program acceleration. With the availability of instruction-level parallel architectures, if-conversion optimization has become pivotal for extracting parallelism from serial programs. While many if-conversion optimization heuristics have been proposed in the literature, few of them investigated the proximity to optimality paying more concern to the input program subjectively. In this paper, we study the effectiveness of the LLVM if-conversion transformation using a broad range optimality analysis technique. The whole optimization space of branch instructions within an arbitrarily selected hotspot function of a program is analyzed, tackling the best-performing combination sequence of converted or non-converted branches. In order to precisely observe the influence of selectively converted branches we magnify individual function call and study the behavior with different call circumstances and input data detecting the impact of data size and distribution. We implemented our technique on LLVM 3.6.1 compilation infrastructure and experimented on several representative kernels of the SPEC-CPU2006 v1.1 benchmarks suite running on a multicore system of Intel(R) Xeon(R) 3.50GHz processors. Our findings show a performance gain up to 22.7% at the best optimization over the standard optimized code, indicating the need for more aggressive compilation optimization that is able to capture the individual characteristics of every branch.},
	booktitle = {Proceedings of the 24th High Performance Computing Symposium},
	articleno = {23},
	numpages = {8},
	keywords = {if-conversion, predicated execution},
	location = {Pasadena, California},
	series = {HPC '16}
}

@thesis{ellis08_csicgfu,
	author = {Ellis, Martin},
	institution = {Newcastle University},
	url = {https://theses.ncl.ac.uk/jspui/handle/10443/828},
	title = {Correct Synthesis and Integration of Compiler-Generated Function Units},
	type = {phdthesis},
	year = {2008}
}

@phdthesis{ellis85_bulld,
	title = {Bulldog: A compiler for {VLIW} architectures},
	author = {Ellis, John R},
	year = {1985},
	school = {Yale University}
}

@article{erné93_primer_galois_connec,
	author = {Erné, M. and Koslowski, J. and Melton, A. and Strecker, G. E.},
	title = {A Primer on Galois Connections},
	journal = {Annals of the New York Academy of Sciences},
	volume = {704},
	number = {1},
	pages = {103-125},
	keywords = {Galois connection, closure operation, interior operation, polarity, axiality},
	doi = {https://doi.org/10.1111/j.1749-6632.1993.tb52513.x},
	url = {https://nyaspubs.onlinelibrary.wiley.com/doi/abs/10.1111/j.1749-6632.1993.tb52513.x},
	eprint = {https://nyaspubs.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1749-6632.1993.tb52513.x},
	abstract = {ABSTRACT. The rudiments of the theory of Galois connections (or residuation theory, as it is sometimes called) are provided, together with many examples and applications. Galois connections occur in profusion and are well known to most mathematicians who deal with order theory; they seem to be less known to topologists. However, because of their ubiquity and simplicity, they (like equivalence relations) can be used as an effective research tool throughout mathematics and related areas. If one recognizes that a Galois connection is involved in a phenomenon that may be relatively complex, then many aspects of that phenomenon immediately become clear, and thus, the whole situation typically becomes much easier to understand.},
	year = {1993}
}

@inproceedings{eveking99_autom,
	author = {Eveking, Hans and Hinrichsen, Holger and Ritter, Gerd},
	organization = {IEEE},
	booktitle = {Design, Automation and Test in Europe Conference and Exhibition, 1999. Proceedings (Cat. No. PR00078)},
	pages = {59--64},
	title = {Automatic verification of scheduling results in high-level synthesis},
	year = {1999}
}

@inproceedings{faissole19_formal_loop_carried_depen_coq,
	author = {Faissole, F. and Constantinides, G. A. and Thomas, D.},
	url = {https://doi.org/10.1109/FCCM.2019.00056},
	booktitle = {2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
	doi = {10.1109/FCCM.2019.00056},
	issn = {2576-2613},
	keywords = {C++ language;formal specification;formal verification;high level synthesis;optimisation;program compilers;theorem proving;loop-carried dependencies;high-level synthesis;proprietary optimization directives;loop pipelining;HLS compilers;nontrivial loop-carried data dependencies;Coq proof assistant;VivadoHLS dependence pragma;register-transfer level design;C/C++ code;Tools;Pipeline processing;Hardware;Optimization;Program processors;Semantics;Field programmable gate arrays;High level synthesis;Formal proofs;Loop dependencies},
	month = apr,
	pages = {315--315},
	title = {Formalizing Loop-Carried Dependencies in Coq for High-Level Synthesis},
	year = {2019}
}

@article{farka21_algeb_abstr_concur_separ_logic,
	abstract = {Concurrent separation logic is distinguished by transfer of state ownership upon parallel composition and framing. The algebraic structure that underpins ownership transfer is that of partial commutative monoids (PCMs). Extant research considers ownership transfer primarily from the logical perspective while comparatively less attention is drawn to the algebraic considerations. This paper provides an algebraic formalization of ownership transfer in concurrent separation logic by means of structure-preserving partial functions (i.e., morphisms) between PCMs, and an associated notion of separating relations. Morphisms of structures are a standard concept in algebra and category theory, but haven't seen ubiquitous use in separation logic before. Separating relations. are binary relations that generalize disjointness and characterize the inputs on which morphisms preserve structure. The two abstractions facilitate verification by enabling concise ways of writing specs, by providing abstract views of threads' states that are preserved under ownership transfer, and by enabling user-level construction of new PCMs out of existing ones.},
	author = {Farka, František and Nanevski, Aleksandar and Banerjee, Anindya and Delbianco, Germán Andrés and Fábregas, Ignacio},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434286},
	doi = {10.1145/3434286},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {Hoare/Separation Logics,Program Logics for Concurrency,Coq},
	month = jan,
	number = {POPL},
	title = {On Algebraic Abstractions for Concurrent Separation Logics},
	volume = {5},
	year = {2021}
}

@article{farzan23_strat_commut_verif_algor_concur_progr,
	author = {Farzan, Azadeh and Klumpp, Dominik and Podelski, Andreas},
	title = {Stratified Commutativity in Verification Algorithms for Concurrent Programs},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571242},
	doi = {10.1145/3571242},
	abstract = {The importance of exploiting commutativity relations in verification algorithms for concurrent programs is well-known. They can help simplify the proof and improve the time and space efficiency. This paper studies commutativity relations as a first-class object in the setting of verification algorithms for concurrent programs. A first contribution is a general framework for abstract commutativity relations. We introduce a general soundness condition for commutativity relations, and present a method to automatically derive sound abstract commutativity relations from a given proof. The method can be used in a verification algorithm based on abstraction refinement to compute a new commutativity relation in each iteration of the abstraction refinement loop. A second result is a general proof rule that allows one to combine multiple commutativity relations, with incomparable power, in a stratified way that preserves soundness and allows one to profit from the full power of the combined relations. We present an algorithm for the stratified proof rule that performs an optimal combination (in a sense made formal), enabling usage of stratified commutativity in algorithmic verification. We empirically evaluate the impact of abstract commutativity and stratified combination of commutativity relations on verification algorithms for concurrent programs.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {49},
	numpages = {28},
	keywords = {Partial Order Reduction, Commutativity}
}

@article{feldman21_learn_bound_induc_invar,
	abstract = {We study the complexity of invariant inference and its connections to exact concept learning. We define a condition on invariants and their geometry, called the fence condition, which permits applying theoretical results from exact concept learning to answer open problems in invariant inference theory. The condition requires the invariant's boundary---the states whose Hamming distance from the invariant is one---to be backwards reachable from the bad states in a small number of steps. Using this condition, we obtain the first polynomial complexity result for an interpolation-based invariant inference algorithm, efficiently inferring monotone DNF invariants with access to a SAT solver as an oracle. We further harness Bshouty's seminal result in concept learning to efficiently infer invariants of a larger syntactic class of invariants beyond monotone DNF. Lastly, we consider the robustness of inference under program transformations. We show that some simple transformations preserve the fence condition, and that it is sensitive to more complex transformations.},
	author = {Feldman, Yotam M. Y. and Sagiv, Mooly and Shoham, Sharon and Wilcox, James R.},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434296},
	doi = {10.1145/3434296},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {complexity,exact learning,interpolation,invariant inference,Hamming geometry},
	month = jan,
	number = {POPL},
	title = {Learning the Boundary of Inductive Invariants},
	volume = {5},
	year = {2021}
}

@article{feldman22_proper_direc_reach_abstr_inter_monot_theor,
	author = {Feldman, Yotam M. Y. and Sagiv, Mooly and Shoham, Sharon and Wilcox, James R.},
	title = {Property-Directed Reachability as Abstract Interpretation in the Monotone Theory},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498676},
	doi = {10.1145/3498676},
	abstract = {Inferring inductive invariants is one of the main challenges of formal verification. The theory of abstract interpretation provides a rich framework to devise invariant inference algorithms. One of the latest breakthroughs in invariant inference is property-directed reachability (PDR), but the research community views PDR and abstract interpretation as mostly unrelated techniques. This paper shows that, surprisingly, propositional PDR can be formulated as an abstract interpretation algorithm in a logical domain. More precisely, we define a version of PDR, called Λ-PDR, in which all generalizations of counterexamples are used to strengthen a frame. In this way, there is no need to refine frames after their creation, because all the possible supporting facts are included in advance. We analyze this algorithm using notions from Bshouty’s monotone theory, originally developed in the context of exact learning. We show that there is an inherent overapproximation between the algorithm’s frames that is related to the monotone theory. We then define a new abstract domain in which the best abstract transformer performs this overapproximation, and show that it captures the invariant inference process, i.e., Λ-PDR corresponds to Kleene iterations with the best transformer in this abstract domain. We provide some sufficient conditions for when this process converges in a small number of iterations, with sometimes an exponential gap from the number of iterations required for naive exact forward reachability. These results provide a firm theoretical foundation for the benefits of how PDR tackles forward reachability.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {15},
	numpages = {31},
	keywords = {reachability diameter, monotone theory, property-directed reachability, invariant inference, abstract interpretation}
}

@inproceedings{feree23_formal_comput_propos_quant,
	author = {F\'{e}r\'{e}e, Hugo and van Gool, Sam},
	title = {Formalizing and Computing Propositional Quantifiers},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575668},
	doi = {10.1145/3573105.3575668},
	abstract = {A surprising result of Pitts (1992) says that propositional quantifiers are definable internally in intuitionistic propositional logic (IPC). The main contribution of this paper is to provide a formalization of Pitts’ result in the Coq proof assistant, and thus a verified implementation of Pitts’ construction. We in addition provide an OCaml program, extracted from the Coq formalization, which computes propositional formulas that realize intuitionistic versions of ∃ p φ and ∀ p φ from p and φ.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {148–158},
	numpages = {11},
	keywords = {automated theorem proving, extraction, sequent calculus, intuitionistic logic, propositional quantifiers},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@misc{ferrandi14_panda_bambu,
	author = {Ferrandi, Fabrizio},
	title = {PandA-Bambu release notes},
	url = {https://github.com/ferrandi/PandA-bambu/blob/c443bf14c33a9a74008ada12f56e7a62e30e5efe/NEWS#L304},
	urldate = {2023-11-16},
	year = {2014}
}

@INPROCEEDINGS{ferrandi21_bambu,
	author = {Ferrandi, Fabrizio and Castellana, Vito Giovanni 
          and Curzel, Serena and Fezzardi, Pietro and Fiorito, Michele 
          and Lattuada, Marco and Minutoli, Marco and Pilato, Christian 
          and Tumeo, Antonino},
	booktitle = {2021 58th ACM/IEEE Design Automation Conference (DAC)},
	title = {Bambu: an Open-Source Research Framework for the 
         High-Level Synthesis of Complex Applications},
	year = {2021},
	pages = {1327-1330},
	abstract = {This paper presents the open-source high-level synthesis (HLS) research 
              framework Bambu. Bambu provides a research environment to experiment with 
              new ideas across HLS, high-level verification and debugging, FPGA/ASIC design,
              design flow space exploration, and parallel hardware accelerator design. The 
              tool accepts as input standard C/C++ specifications and compiler intermediate 
              representations (IRs) coming from the well-known Clang/LLVM and GCC compilers. 
              The broad spectrum and flexibility of input formats allow the electronic 
              design automation (EDA) research community to explore and integrate new 
              transformations and optimizations. The easily extendable modular framework 
              already includes many optimizations and HLS benchmarks used to evaluate 
              the QoR of the tool against existing approaches [1]. The integration with 
              synthesis and verification backends (commercial and open-source) allows 
              researchers to quickly test any new finding and easily obtain performance 
              and resource usage metrics for a given application. Different FPGA devices 
              are supported from several different vendors: AMD/Xilinx, Intel/Altera, 
              Lattice Semiconductor, and NanoXplore. Finally, integration with the OpenRoad 
              open-source end-to-end silicon compiler perfectly fits with the recent push 
              towards open-source EDA.},
	publisher = {{IEEE}},
	doi = {10.1109/DAC18074.2021.9586110},
	ISSN = {0738-100X},
	month = {12},
	pdf = {https://re.public.polimi.it/retrieve/668507/dac21_bambu.pdf}
}

@article{ferrante87_progr_depen_graph_its_use_optim,
	abstract = {In this paper we present an intermediate program representation, called the program dependence graph (PDG), that makes explicit both the data and control dependences for each operation in a program. Data dependences have been used to represent only the relevant data flow relationships of a program. Control dependences are introduced to analogously represent only the essential control flow relationships of a program. Control dependences are derived from the usual control flow graph. Many traditional optimizations operate more efficiently on the PDG. Since dependences in the PDG connect computationally related parts of the program, a single walk of these dependences is sufficient to perform many optimizations. The PDG allows transformations such as vectorization, that previously required special treatment of control dependence, to be performed in a manner that is uniform for both control and data dependences. Program transformations that require interaction of the two dependence types can also be easily handled with our representation. As an example, an incremental approach to modifying data dependences resulting from branch deletion or loop unrolling is introduced. The PDG supports incremental optimization, permitting transformations to be triggered by one another and applied only to affected dependences.},
	author = {Ferrante, Jeanne and Ottenstein, Karl J. and Warren, Joe D.},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	doi = {10.1145/24039.24041},
	issn = {0164-0925},
	journaltitle = {ACM Trans. Program. Lang. Syst.},
	keywords = {program dependence graph,gated-SSA,SSA},
	month = jul,
	number = {3},
	pages = {319--349},
	title = {The Program Dependence Graph and Its Use in Optimization},
	volume = {9},
	year = {1987}
}

@ARTICLE{ferry23_increas_fpga_accel_memor_bandw,
	author = {Ferry, Corentin and Yuki, Tomofumi and Derrien, Steven and Rajopadhye, Sanjay},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	title = {Increasing FPGA Accelerators Memory Bandwidth With a Burst-Friendly Memory Layout},
	year = {2023},
	volume = {42},
	number = {5},
	pages = {1546-1559},
	abstract = {Offloading compute-intensive kernels to hardware accelerators relies on the large degree of parallelism offered by these platforms. However, the effective bandwidth of the memory interface often causes a bottleneck, hindering the accelerator’s effective performance. Techniques enabling data reuse, such as tiling, lower the pressure on memory traffic but do not fully exploit the bandwidth. A further increase in bandwidth utilization is possible by using burst rather than element-wise accesses, provided the data is contiguous in memory. In this article, we propose a memory allocation technique, and provide a proof-of-concept source-to-source compiler pass, that enables such burst transfers by modifying the data layout in external memory. Our experiments show the new memory allocation yields close to 100% bandwidth utilization while the memory engines occupy less than 5% of the field-programmable gate array (FPGA) logic area.},
	keywords = {high-level synthesis},
	doi = {10.1109/TCAD.2022.3201494},
	ISSN = {1937-4151},
	month = {May}
}

@inproceedings{fey06_effic_multi_valued_encod_sat_atpg,
	author = {Fey, G. and Shi, Junhao and Drechsler, R.},
	booktitle = {36th International Symposium on Multiple-Valued Logic (ISMVL'06)},
	doi = {10.1109/ISMVL.2006.19},
	keywords = {multi-valued logic},
	pages = {25--25},
	title = {Efficiency of Multi-Valued Encoding in SAT-based ATPG},
	year = {2006}
}

@article{fiore22_formal_metat_secon_order_abstr_syntax,
	author = {Fiore, Marcelo and Szamozvancev, Dmitrij},
	title = {Formal Metatheory of Second-Order Abstract Syntax},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498715},
	doi = {10.1145/3498715},
	abstract = {Despite extensive research both on the theoretical and practical fronts, formalising, reasoning about, and implementing languages with variable binding is still a daunting endeavour – repetitive boilerplate and the overly complicated metatheory of capture-avoiding substitution often get in the way of progressing on to the actually interesting properties of a language. Existing developments offer some relief, however at the expense of inconvenient and error-prone term encodings and lack of formal foundations. We present a mathematically-inspired language-formalisation framework implemented in Agda. The system translates the description of a syntax signature with variable-binding operators into an intrinsically-encoded, inductive data type equipped with syntactic operations such as weakening and substitution, along with their correctness properties. The generated metatheory further incorporates metavariables and their associated operation of metasubstitution, which enables second-order equational/rewriting reasoning. The underlying mathematical foundation of the framework – initial algebra semantics – derives compositional interpretations of languages into their models satisfying the semantic substitution lemma by construction.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {53},
	numpages = {29},
	keywords = {Agda, abstract syntax, category theory, language formalisation}
}

@inproceedings{firsov22_reflec_rewin_coin_toss_easyc,
	author = {Firsov, Denis and Unruh, Dominique},
	title = {Reflection, Rewinding, and Coin-Toss in EasyCrypt},
	year = {2022},
	isbn = {9781450391825},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497775.3503693},
	doi = {10.1145/3497775.3503693},
	abstract = {In this paper we derive a suite of lemmas which allows users to internally reflect EasyCrypt programs into distributions which correspond to their denotational semantics (probabilistic reflection). Based on this we develop techniques for reasoning about rewinding of adversaries in EasyCrypt. (A widely used technique in cryptology.) We use our reflection and rewindability results to prove the security of a coin-toss protocol.},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {166–179},
	numpages = {14},
	keywords = {cryptography, EasyCrypt, formal methods, reflection, binding, coin-toss, rewindability, commitments},
	location = {Philadelphia, PA, USA},
	series = {CPP 2022}
}

@article{fisher81_trace_sched,
	author = {Fisher, Joseph A.},
	doi = {10.1109/TC.1981.1675827},
	journaltitle = {IEEE Transactions on Computers},
	keywords = {static scheduling,trace scheduling},
	number = {7},
	pages = {478--490},
	title = {Trace Scheduling: A Technique for Global Microcode Compaction},
	volume = {C-30},
	year = {1981}
}

@inproceedings{flanagan02_predic_abstr_softw_verif,
	keywords = {predicate abstraction, abstract interpretation, predicated execution},
	author = {Flanagan, Cormac and Qadeer, Shaz},
	title = {Predicate Abstraction for Software Verification},
	year = {2002},
	isbn = {1581134509},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/503272.503291},
	doi = {10.1145/503272.503291},
	abstract = {Software verification is an important and difficult problem. Many static checking techniques for software require annotations from the programmer in the form of method specifications and loop invariants. This annotation overhead, particularly of loop invariants, is a significant hurdle in the acceptance of static checking. We reduce the annotation burden by inferring loop invariants automatically.Our method is based on predicate abstraction, an abstract interpretation technique in which the abstract domain is constructed from a given set of predicates over program variables. A novel feature of our approach is that it infers universally-quantified loop invariants, which are crucial for verifying programs that manipulate unbounded data such as arrays. We present heuristics for generating appropriate predicates for each loop automatically; the programmer can specify additional predicates as well. We also present an efficient algorithm for computing the abstraction of a set of states in terms of a collection of predicates.Experiments on a 44KLOC program show that our approach can automatically infer the necessary predicates and invariants for all but 31 of the 396 routines that contain loops.},
	booktitle = {Proceedings of the 29th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
	pages = {191–202},
	numpages = {12},
	location = {Portland, Oregon},
	series = {POPL '02}
}

@inproceedings{flor18_pi_ware,
	author = {Flor, João Paulo Pizani and Swierstra, Wouter and Sijsling, Yorick},
	editor = {Uustalu, Tarmo},
	location = {Dagstuhl, Germany},
	publisher = {Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik},
	url = {https://doi.org/10.4230/LIPIcs.TYPES.2015.9},
	annotation = {Keywords: dependently typed programming, Agda, EDSL, hardware description languages, functional programming},
	booktitle = {21st International Conference on Types for Proofs and Programs (TYPES 2015)},
	doi = {10.4230/LIPIcs.TYPES.2015.9},
	isbn = {978-3-95977-030-9},
	issn = {1868-8969},
	pages = {9:1--9:27},
	series = {Leibniz International Proceedings in Informatics (LIPIcs)},
	title = {{Pi-Ware: Hardware Description and Verification in Agda}},
	volume = {69},
	year = {2018}
}

@article{fluri07_chang_distil,
	author = {Fluri, Beat and Würsch, Michael and Pinzger, Martin and Gall, Harald},
	journaltitle = {IEEE Transactions on Software Engineering},
	number = {11},
	pages = {725--743},
	title = {Change Distilling: Tree Differencing for Fine-Grained Source Code Change Extraction},
	volume = {33},
	year = {2007}
}

@inproceedings{forster23_comput_cantor_berns_myhil_isomor,
	author = {Forster, Yannick and Jahn, Felix and Smolka, Gert},
	title = {A Computational Cantor-Bernstein and Myhill’s Isomorphism Theorem in Constructive Type Theory (Proof Pearl)},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575690},
	doi = {10.1145/3573105.3575690},
	abstract = {The Cantor-Bernstein theorem (CB) from set theory, stating that two sets which can be injectively embedded into each other are in bijection, is inherently classical in its full generality, i.e. implies the law of excluded middle, a result due to Pradic and Brown. Recently, Escard\'{o} has provided a proof of CB in univalent type theory, assuming the law of excluded middle. It is a natural question to ask which restrictions of CB can be proved without axiomatic assumptions. We give a partial answer to this question contributing an assumption-free proof of CB restricted to enumerable discrete types, i.e. types which can be computationally treated. In fact, we construct several bijections from injections: The first is by translating a proof of the Myhill isomorphism theorem from computability theory – stating that 1-equivalent predicates are recursively isomorphic – to constructive type theory, where the bijection is constructed in stages and an algorithm with an intricate termination argument is used to extend the bijection in every step. The second is also constructed in stages, but with a simpler extension algorithm sufficient for CB. The third is constructed directly in such a way that it only relies on the given enumerations of the types, not on the given injections. We aim at keeping the explanations simple, accessible, and concise in the style of a “proof pearl”. All proofs are machine-checked in Coq but should transport to other foundations – they do not rely on impredicativity, on choice principles, or on large eliminations.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {159–166},
	numpages = {8},
	keywords = {Coq, type theory, computability theory, constructive mathematics, constructive logic},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@inproceedings{franceschino21_verif_funct_progr_abstr_inter,
	doi = {10.1007/978-3-030-88806-0_6},
	abstract = {Abstract interpreters are complex pieces of software: even if the abstract interpretation theory and companion algorithms are well understood, their implementations are subject to bugs, that might question the soundness of their computations.},
	author = {Franceschino, Lucas and Pichardie, David and Talpin, Jean-Pierre},
	editor = {Drăgoi, Cezara and Mukherjee, Suvam and Namjoshi, Kedar},
	location = {Cham},
	publisher = {Springer International Publishing},
	booktitle = {Static Analysis},
	isbn = {978-3-030-88806-0},
	pages = {124--143},
	title = {Verified Functional Programming of an Abstract Interpreter},
	year = {2021}
}

@inproceedings{frumin22_seman_cut_elimin_logic_bunch,
	author = {Frumin, Dan},
	title = {Semantic Cut Elimination for the Logic of Bunched Implications, Formalized in Coq},
	year = {2022},
	isbn = {9781450391825},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497775.3503690},
	doi = {10.1145/3497775.3503690},
	abstract = {The logic of bunched implications (BI) is a substructural logic that forms the backbone of separation logic, the much studied logic for reasoning about heap-manipulating programs. Although the proof theory and metatheory of BI are mathematically involved, the formalization of important metatheoretical results is still incipient. In this paper we present a self-contained formalized, in the Coq proof assistant, proof of a central metatheoretical property of BI: cut elimination for its sequent calculus. The presented proof is semantic, in the sense that is obtained by interpreting sequents in a particular “universal” model. This results in a more modular and elegant proof than a standard Gentzen-style cut elimination argument, which can be subtle and error-prone in manual proofs for BI. In particular, our semantic approach avoids unnecessary inversions on proof derivations, or the uses of cut reductions and the multi-cut rule. Besides modular, our approach is also robust: we demonstrate how our method scales, with minor modifications, to (i) an extension of BI with an arbitrary set of simple structural rules, and (ii) an extension with an S4-like □ modality.},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {291–306},
	numpages = {16},
	keywords = {bunched implications, substructural logics, Coq, interactive theorem proving, cut elimination},
	location = {Philadelphia, PA, USA},
	series = {CPP 2022}
}

@article{fu23_proto_quipp_dynam_liftin,
	author = {Fu, Peng and Kishida, Kohei and Ross, Neil J. and Selinger, Peter},
	title = {Proto-Quipper with Dynamic Lifting},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571204},
	doi = {10.1145/3571204},
	abstract = {Quipper is a functional programming language for quantum computing. Proto-Quipper is a family of languages aiming to provide a formal foundation for Quipper. In this paper, we extend Proto-Quipper-M with a construct called dynamic lifting, which is present in Quipper. By virtue of being a circuit description language, Proto-Quipper has two separate runtimes: circuit generation time and circuit execution time. Values that are known at circuit generation time are called parameters, and values that are known at circuit execution time are called states. Dynamic lifting is an operation that enables a state, such as the result of a measurement, to be lifted to a parameter, where it can influence the generation of the next portion of the circuit. As a result, dynamic lifting enables Proto-Quipper programs to interleave classical and quantum computation. We describe the syntax of a language we call Proto-Quipper-Dyn. Its type system uses a system of modalities to keep track of the use of dynamic lifting. We also provide an operational semantics, as well as an abstract categorical semantics for dynamic lifting based on enriched category theory. We prove that both the type system and the operational semantics are sound with respect to our categorical semantics. Finally, we give some examples of Proto-Quipper-Dyn programs that make essential use of dynamic lifting.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {11},
	numpages = {26},
	keywords = {Proto-Quipper, quantum programming languages, categorical semantics, dynamic lifting, Quipper}
}

@inproceedings{färber22_safe_fast_concur_proof_check,
	author = {Färber, Michael},
	title = {Safe, Fast, Concurrent Proof Checking for the Lambda-Pi Calculus modulo Rewriting},
	year = {2022},
	isbn = {9781450391825},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497775.3503683},
	doi = {10.1145/3497775.3503683},
	abstract = {Several proof assistants, such as Isabelle or Coq, can concurrently check multiple proofs. In contrast, the vast majority of today's small proof checkers either does not support concurrency at all or only limited forms thereof, restricting the efficiency of proof checking on multi-core processors. This work shows the design of a small, memory- and thread-safe kernel that efficiently checks proofs both concurrently and sequentially. This design is implemented in a new proof checker called Kontroli for the lambda-Pi calculus modulo rewriting, which is an established framework to uniformly express a multitude of logical systems. Kontroli is faster than the reference proof checker for this calculus, Dedukti, on all of five evaluated datasets obtained from proof assistants and interactive theorem provers. Furthermore, Kontroli reduces the time of the most time-consuming part of proof checking using eight threads by up to 6.6x.},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {225–238},
	numpages = {14},
	keywords = {verification, rewriting, Dedukti, concurrency, type checking, reduction, Rust, sharing, performance},
	location = {Philadelphia, PA, USA},
	series = {CPP 2022}
}

@inproceedings{färber23_terms_effic_proof_check_parsin,
	author = {Färber, Michael},
	title = {Terms for Efficient Proof Checking and Parsing},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575686},
	doi = {10.1145/3573105.3575686},
	abstract = {Proofs automatically generated by interactive or automated theorem provers are often several orders of magnitude larger than proofs written by hand. This implies significant challenges for processing such proofs efficiently. It turns out that the data structures used to encode terms have a high impact on performance. This article proposes several term data structures; in particular, heterogeneous terms for proof checking that distinguish long- and short-lived terms, and abstract terms for proof parsing. Both term data structures are implemented in the proof checker Kontroli, enabling it to parse and check proofs both sequentially and concurrently without overhead. The evaluation on three datasets exported from interactive theorem provers shows that the new term data structures significantly improve proof checking performance.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {135–147},
	numpages = {13},
	keywords = {type checking, concurrency, sharing, Rust, Dedukti, verification, performance},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@inproceedings{férée23_formal_comput_propos_quant,
	author = {Férée, Hugo and van Gool, Sam},
	title = {Formalizing and Computing Propositional Quantifiers},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575668},
	doi = {10.1145/3573105.3575668},
	abstract = {A surprising result of Pitts (1992) says that propositional quantifiers are definable internally in intuitionistic propositional logic (IPC). The main contribution of this paper is to provide a formalization of Pitts’ result in the Coq proof assistant, and thus a verified implementation of Pitts’ construction. We in addition provide an OCaml program, extracted from the Coq formalization, which computes propositional formulas that realize intuitionistic versions of ∃ p φ and ∀ p φ from p and φ.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {148–158},
	numpages = {11},
	keywords = {propositional quantifiers, automated theorem proving, extraction, intuitionistic logic, sequent calculus},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@inproceedings{gajski10_what_hls,
	author = {Gajski, Dan and Austin, Todd and Svoboda, Steve},
	booktitle = {Design Automation Conference},
	doi = {10.1145/1837274.1837489},
	pages = {857--858},
	title = {What input-language is the best choice for high level synthesis (HLS)?},
	year = {2010}
}

@book{gajski92_high_level_synth,
	author = {Gajski, Daniel D. and Dutt, Nikil D. and Wu, Allen C-H and Lin, Steve Y-L},
	publisher = {Springer US},
	url = {http://dx.doi.org/10.1007/978-1-4615-3636-9},
	doi = {10.1007/978-1-4615-3636-9},
	title = {High — Level Synthesis},
	year = {1992}
}

@article{gancher23_core_calcul_equat_proof_crypt_protoc,
	author = {Gancher, Joshua and Sojakova, Kristina and Fan, Xiong and Shi, Elaine and Morrisett, Greg},
	title = {A Core Calculus for Equational Proofs of Cryptographic Protocols},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571223},
	doi = {10.1145/3571223},
	abstract = {Many proofs of interactive cryptographic protocols (e.g., as in Universal Composability) operate by proving the protocol at hand to be observationally equivalent to an idealized specification. While pervasive, formal tool support for observational equivalence of cryptographic protocols is still a nascent area of research. Current mechanization efforts tend to either focus on diff-equivalence, which establishes observational equivalence between protocols with identical control structures, or require an explicit witness for the observational equivalence in the form of a bisimulation relation. Our goal is to simplify proofs for cryptographic protocols by introducing a core calculus, IPDL, for cryptographic observational equivalences. Via IPDL, we aim to address a number of theoretical issues for cryptographic proofs in a simple manner, including probabilistic behaviors, distributed message-passing, and resource-bounded adversaries and simulators. We demonstrate IPDL on a number of case studies, including a distributed coin toss protocol, Oblivious Transfer, and the GMW multi-party computation protocol. All proofs of case studies are mechanized via an embedding of IPDL into the Coq proof assistant.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {30},
	numpages = {27},
	keywords = {cryptographic protocols, observational equivalence, equational reasoning}
}

@article{ganser17_iterat_sched_optim_paral_polyh_model,
	abstract = {The polyhedron model is a powerful model to identify and apply systematically loop transformations that improve data locality (e.g., via tiling) and enable parallelization. In the polyhedron model, a loop transformation is, essentially, represented as an affine function. Well-established algorithms for the discovery of promising transformations are based on performance models. These algorithms have the drawback of not being easily adaptable to the characteristics of a specific program or target hardware. An iterative search for promising loop transformations is more easily adaptable and can help to learn better models. We present an iterative optimization method in the polyhedron model that targets tiling and parallelization. The method enables either a sampling of the search space of legal loop transformations at random or a more directed search via a genetic algorithm. For the latter, we propose a set of novel, tailored reproduction operators. We evaluate our approach against existing iterative and model-driven optimization strategies. We compare the convergence rate of our genetic algorithm to that of random exploration. Our approach of iterative optimization outperforms existing optimization techniques in that it finds loop transformations that yield significantly higher performance. If well configured, then random exploration turns out to be very effective and reduces the need for a genetic algorithm.},
	author = {Ganser, Stefan and Grösslinger, Armin and Siegmund, Norbert and Apel, Sven and Lengauer, Christian},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3109482},
	doi = {10.1145/3109482},
	issn = {1544-3566},
	journaltitle = {ACM Trans. Archit. Code Optim.},
	keywords = {parallelization,polyhedron model,OpenMP,genetic algorithm,tiling,Automatic loop optimization,polyhedral model,polyhedral analysis},
	month = aug,
	number = {3},
	title = {Iterative Schedule Optimization for Parallelization in the Polyhedron Model},
	volume = {14},
	year = {2017}
}

@misc{gauthier20_high_level_synth,
	author = {Gauthier, Stephane and Wadood, Zubair},
	url = {https://info.silexica.com/high-level-synthesis/1},
	note = {White paper},
	title = {High-Level Synthesis: Can it outperform hand-coded {HDL}?},
	year = {2020}
}

@article{gavazzo23_elemen_quant_rewrit,
	author = {Gavazzo, Francesco and Di Florio, Cecilia},
	title = {Elements of Quantitative Rewriting},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571256},
	doi = {10.1145/3571256},
	abstract = {We introduce a general theory of quantitative and metric rewriting systems, namely systems with a rewriting relation enriched over quantales modelling abstract quantities. We develop theories of abstract and term-based systems, refining cornerstone results of rewriting theory (such as Newman’s Lemma, Church-Rosser Theorem, and critical pair-like lemmas) to a metric and quantitative setting. To avoid distance trivialisation and lack of confluence issues, we introduce non-expansive, linear term rewriting systems, and then generalise the latter to the novel class of graded term rewriting systems. These systems make quantitative rewriting modal and context-sensitive, this way endowing rewriting with coeffectful behaviours.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {63},
	numpages = {32},
	keywords = {quantalic relations, modal graded rewriting, metric rewriting, quantitative equational theory, quantitative calculus of relations, quantitative rewriting}
}

@inproceedings{georgelin06_towar_c,
	author = {Georgelin, P. and Krishnaswamy, V.},
	url = {https://doi.org/10.1145/1146909.1146938},
	booktitle = {2006 43rd ACM/IEEE Design Automation Conference},
	doi = {10.1145/1146909.1146938},
	keywords = {C++ language;logic design;sequential circuits;system-on-chip;sequential equivalence checking;systems-on-chip;C++-based design methodology;formal verification;RTL verification;Design methodology;Hardware;Software prototyping;Formal verification;Writing;Signal processing algorithms;Electronic design automation and methodology;High level synthesis;Image coding;Computational modeling;Verification;Languages;Modeling Methodology;Sequential Equivalence Checking},
	month = jul,
	pages = {93--96},
	title = {Towards a C++-based design methodology facilitating sequential equivalence checking},
	year = {2006}
}

@inproceedings{georgiadis14_loop_nestin_fores_domin_applic,
	abstract = {Loop nesting forests and dominator trees are important tools in program optimization and code generation, and they have applications in other diverse areas. In this work we first present carefully engineered implementations of efficient algorithms for computing a loop nesting forest of a given directed graph, including a very efficient algorithm that computes the forest in a single depth-first search. Then we revisit the problem of computing dominators and present efficient implementations of the algorithms recently proposed by Fraczak et al. [12], which include an algorithm for acyclic graphs and an algorithm that computes both the dominator tree and a loop nesting forest. We also propose a new algorithm than combines the algorithm of Fraczak et al. for acyclic graphs with the algorithm of Lengauer and Tarjan. Finally, we provide fast algorithms for the following related problems: computing bridges and testing 2-edge connectivity, verifying dominators and testing 2-vertex connectivity, and computing a low-high order and two independent spanning trees. We exhibit the efficiency of our algorithms experimentally on large graphs taken from a variety of application areas.},
	author = {Georgiadis, Loukas and Laura, Luigi and Parotsidis, Nikos and Tarjan, Robert E.},
	editor = {Gudmundsson, Joachim and Katajainen, Jyrki},
	location = {Cham},
	publisher = {Springer International Publishing},
	booktitle = {Experimental Algorithms},
	isbn = {978-3-319-07959-2},
	keywords = {loop identification},
	pages = {174--186},
	title = {Loop Nesting Forests, Dominators, and Applications},
	year = {2014}
}

@inproceedings{gerard22_qrane,
	author = {Gerard, Blake and Grosser, Tobias and Kong, Martin},
	title = {QRANE: Lifting QASM Programs to an Affine IR},
	year = {2022},
	isbn = {9781450391832},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497776.3517775},
	doi = {10.1145/3497776.3517775},
	abstract = {This paper introduces QRANE, a tool that produces the affine intermediate representation (IR) from a quantum program expressed in Quantum Assembly language such as OpenQASM. QRANE finds subsets of quantum gates prescribed by the same operation type and linear relationships, and constructs a structured program representation expressed with polyhedral iteration domains and access relations, all while preserving the original semantics of the quantum program. We explore various policies for deciding amongst different delinearization strategies and discuss their effect on the quality of the reconstruction. Our evaluation demonstrates the high coverage and efficiency obtained with QRANE while enabling research on the benefits of affine transformations for large quantum circuits. Specifically, QRANE reconstructs affine iteration domains of up to 6 dimensions and up to 184 points per domain.},
	booktitle = {Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction},
	pages = {15–28},
	numpages = {14},
	keywords = {quantum assembly, delinearization, polyhedral model, quantum affine computing},
	location = {Seoul, South Korea},
	series = {CC 2022}
}

@book{gerard87_proof_theor_logic_compl,
	author = {Gerard, Jean-Yves},
	editor = {Cellucci, C. and Prawitz, D. and Girard, J.-Y. and Schwichtenberg, H.},
	location = {Napoli, via Arangio Ruiz 83},
	publisher = {Bibliopolis},
	isbn = {88-7088-123-7},
	keywords = {proof theory,logic,intuitionistic logic,gentzen,sequent calculus},
	title = {Proof Theory and Logical Complexity},
	year = {1987}
}

@article{goncharov23_towar_higher_order_mathem_operat_seman,
	author = {Goncharov, Sergey and Milius, Stefan and Schr\"{o}der, Lutz and Tsampas, Stelios and Urbat, Henning},
	title = {Towards a Higher-Order Mathematical Operational Semantics},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571215},
	doi = {10.1145/3571215},
	abstract = {Compositionality proofs in higher-order languages are notoriously involved, and general semantic frameworks guaranteeing compositionality are hard to come by. In particular, Turi and Plotkin’s bialgebraic abstract GSOS framework, which has been successfully applied to obtain off-the-shelf compositionality results for first-order languages, so far does not apply to higher-order languages. In the present work, we develop a theory of abstract GSOS specifications for higher-order languages, in effect transferring the core principles of Turi and Plotkin’s framework to a higher-order setting. In our theory, the operational semantics of higher-order languages is represented by certain dinatural transformations that we term pointed higher-order GSOS laws. We give a general compositionality result that applies to all systems specified in this way and discuss how compositionality of the SKI calculus and the λ-calculus w.r.t. a strong variant of Abramsky’s applicative bisimilarity are obtained as instances.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {22},
	numpages = {27},
	keywords = {Higher-order reasoning, Abstract GSOS, Categorical semantics}
}

@article{gondelman21_distr_causal_memor,
	abstract = {We present the first specification and verification of an implementation of a causally-consistent distributed database that supports modular verification of full functional correctness properties of clients and servers. We specify and reason about the causally-consistent distributed database in Aneris, a higher-order distributed separation logic for an ML-like programming language with network primitives for programming distributed systems. We demonstrate that our specifications are useful, by proving the correctness of small, but tricky, synthetic examples involving causal dependency and by verifying a session manager library implemented on top of the distributed database. We use Aneris's facilities for modular specification and verification to obtain a highly modular development, where each component is verified in isolation, relying only on the specifications (not the implementations) of other components. We have used the Coq formalization of the Aneris logic to formalize all the results presented in the paper in the Coq proof assistant.},
	author = {Gondelman, Léon and Gregersen, Simon Oddershede and Nieto, Abel and Timany, Amin and Birkedal, Lars},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434323},
	doi = {10.1145/3434323},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {higher-order logic,causal consistency,Distributed systems,formal verification,separation logic,concurrency},
	month = jan,
	number = {POPL},
	title = {Distributed Causal Memory: Modular Specification and Verification in Higher-Order Distributed Separation Logic},
	volume = {5},
	year = {2021}
}

@article{gonthier08_fp,
	title = {Formal Proof--the Four-Color Theorem},
	author = {Gonthier, Georges},
	journal = {Notices of the AMS},
	volume = {55},
	number = {11},
	pages = {1382--1393},
	year = {2008}
}

@misc{google23_xls,
	author = {Google},
	title = {{XLS: Accelerated HW Synthesis}},
	url = {https://github.com/google/xls/blob/dde7095ff1050b09c37cb44d1977bff1af8de050/xls/scheduling/mutual_exclusion_pass.h#L112},
	urldate = {2023-11-14},
	year = {2023},
	note = {The XLS scheduler refers to using an SMT solver to merge mutually exclusive nodes}
}

@ARTICLE{gorius22_spech,
	author = {Gorius, Jean-Michel and Rokicki, Simon and Derrien, Steven},
	journal = {IEEE Micro},
	title = {SpecHLS: Speculative Accelerator Design Using High-Level Synthesis},
	year = {2022},
	volume = {42},
	number = {5},
	pages = {99-107},
	abstract = {Custom hardware accelerators usage is shifting toward new application domains such as graph analytics and unstructured text analysis. These applications expose complex control-flow which is challenging to map to hardware, especially when operating from a C/C++ description using high-level synthesis toolchains. Several approaches relying on speculative execution have been proposed to overcome those limitations, but they often fail to handle the multiple interacting speculations required for realistic use-cases. This article proposes a fully automated hardware synthesis flow based on a source-to-source compiler that identifies and explores intricate speculation configurations to generate speculative hardware accelerators.},
	keywords = {loop scheduling, high-level synthesis},
	doi = {10.1109/MM.2022.3188136},
	ISSN = {1937-4143},
	month = {Sep.}
}

@inproceedings{grass94_high,
	author = {Grass, W. and Mutz, M. and Tiedemann, W. -.},
	url = {https://doi.org/10.1109/EURMIC.1994.390403},
	booktitle = {Proceedings of Twentieth Euromicro Conference. System Architecture and Integration},
	doi = {10.1109/EURMIC.1994.390403},
	keywords = {high-level synthesis, formal methods},
	month = sep,
	pages = {83--91},
	title = {High level synthesis based on formal methods},
	year = {1994}
}

@misc{gratzer20_syntac,
	author = {Gratzer, Daniel and Sterling, Jonathan},
	eprint = {2012.10783},
	eprintclass = {cs.LO},
	eprinttype = {arXiv},
	title = {Syntactic categories for dependent type theory: sketching and adequacy},
	year = {2020}
}

@inproceedings{greaves08_kiwi,
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4724884},
	author = {Greaves, David J. and Singh, Satnam},
	publisher = {{IEEE} Computer Society},
	booktitle = {{FCCM}},
	doi = {10.1109/FCCM.2008.46},
	pages = {3--12},
	title = {Kiwi: Synthesis of {FPGA} Circuits from Parallel Programs},
	year = {2008}
}

@misc{greaves19_resear_note,
	author = {Greaves, David J.},
	eprint = {1905.03746},
	eprintclass = {cs.PL},
	eprinttype = {arXiv},
	title = {Research Note: An Open Source Bluespec Compiler},
	year = {2019}
}

@inproceedings{greco98_greed_algor_datal_choic_negat,
	author = {Greco, Sergio and Zaniolo, Carlo},
	location = {Manchester, United Kingdom},
	publisher = {MIT Press},
	booktitle = {Proceedings of the 1998 Joint International Conference and Symposium on Logic Programming},
	isbn = {0262600315},
	pages = {294--309},
	series = {JICSLP'98},
	title = {Greedy Algorithms in Datalog with Choice and Negation},
	year = {1998}
}

@inproceedings{gregoire23_pract_sound_equal_tests_autom,
	author = {Grégoire, Benjamin and Léchenet, Jean-Christophe and Tassi, Enrico},
	title = {Practical and Sound Equality Tests, Automatically: Deriving EqType Instances for Jasmin’s Data Types with Coq-Elpi},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575683},
	doi = {10.1145/3573105.3575683},
	abstract = {In this paper we describe the design and implementation of feqb, a tool that synthesizes sound equality tests for inductive data types in the dependent type theory of the Coq system. Our procedure scales to large inductive data types, as in hundreds of constructors, since the terms and proofs it synthesizes are linear in the size of the inductive type. Moreover it supports some forms of dependently typed arguments and sigma types pairing data with proofs of decidable properties. Finally feqb handles deeply nested containers without requiring any human intervention.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {167–181},
	numpages = {15},
	keywords = {Elpi, decision procedure, Coq, computational complexity, program synthesis},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@Article{grimm18_sfvts,
	keywords = {motivation},
	AUTHOR = {Grimm, Tomás and Lettnin, Djones and Hübner, Michael},
	TITLE = {A Survey on Formal Verification Techniques for Safety-Critical Systems-on-Chip},
	JOURNAL = {Electronics},
	VOLUME = {7},
	YEAR = {2018},
	NUMBER = {6},
	ARTICLE-NUMBER = {81},
	URL = {https://www.mdpi.com/2079-9292/7/6/81},
	ISSN = {2079-9292},
	ABSTRACT = {The high degree of miniaturization in the electronics industry has been, for several years, a driver to push embedded systems to different fields and applications. One example is safety-critical systems, where the compactness in the form factor helps to reduce the costs and allows for the implementation of new techniques. The automotive industry is a great example of a safety-critical area with a great rise in the adoption of microelectronics. With it came the creation of the ISO 26262 standard with the goal of guaranteeing a high level of dependability in the designs. Other areas in the safety-critical applications domain have similar standards. However, these standards are mostly guidelines to make sure that designs reach the desired dependability level without explicit instructions. In the end, the success of the design to fulfill the standard is the result of a thorough verification process. Naturally, the goal of any verification team dealing with such important designs is complete coverage as well as standards conformity, but as these are complex hardware, complete functional verification is a difficult task. From the several techniques that exist to verify hardware, where each has its pros and cons, we studied six well-established in academia and in industry. We can divide them into two categories: simulation, which needs extremely large amounts of time, and formal verification, which needs unrealistic amounts of resources. Therefore, we conclude that a hybrid approach offers the best balance between simulation (time) and formal verification (resources).},
	DOI = {10.3390/electronics7060081}
}

@inproceedings{groce22_makin_no_fuss_compil_fuzzin_effec,
	author = {Groce, Alex and van Tonder, Rijnard and Kalburgi, Goutamkumar Tulajappa and Le Goues, Claire},
	title = {Making No-Fuss Compiler Fuzzing Effective},
	year = {2022},
	isbn = {9781450391832},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497776.3517765},
	doi = {10.1145/3497776.3517765},
	abstract = {Developing a bug-free compiler is difficult; modern optimizing compilers are among the most complex software systems humans build. Fuzzing is one way to identify subtle compiler bugs that are hard to find with human-constructed tests. Grammar-based fuzzing, however, requires a grammar for a compiler’s input language, and can miss bugs induced by code that does not actually satisfy the grammar the compiler should accept. Grammar-based fuzzing also seldom uses advanced modern fuzzing techniques based on coverage feedback. However, modern mutation-based fuzzers are often ineffective for testing compilers because most inputs they generate do not even come close to getting past the parsing stage of compilation. This paper introduces a technique for taking a modern mutation-based fuzzer (AFL in our case, but the method is general) and augmenting it with operators taken from mutation testing, and program splicing. We conduct a controlled study to show that our hybrid approaches significantly improve fuzzing effectiveness qualitatively (consistently finding unique bugs that baseline approaches do not) and quantitatively (typically finding more unique bugs in the same time span, despite fewer program executions). Our easy-to-apply approach has allowed us to report more than 100 confirmed and fixed bugs in production compilers, and found a bug in the Solidity compiler that earned a security bounty.},
	booktitle = {Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction},
	pages = {194–204},
	numpages = {11},
	keywords = {fuzzing, mutation testing, compiler development},
	location = {Seoul, South Korea},
	series = {CC 2022}
}

@article{grosser11_polly_polyh_llvm,
	abstract = {Various powerful polyhedral techniques exist to optimize computation intensive programs effectively. Applying these techniques on any non-trivial program is still surprisingly difficult and often not as effective as expected. Most polyhedral tools are limited to a specific programming language. Even for this language, relevant code needs to match specific syntax that rarely appears in existing code. It is therefore hard or even impossible to process existing programs automatically. In addition, most tools target C or OpenCL code, which prevents effective communication with compiler internal optimizers. As a result target architecture specific optimizations are either little effective or not approached at all. In this paper we present Polly, a project to enable polyhedral optimizations in LLVM. Polly automatically detects and transforms relevant programparts in a language-independent and syntactically transparent way. Therefore, it supports programs written in most common programming languages and constructs like C++ iterators, goto based loops and pointer arithmetic. Internally it provides a state-of-the-art polyhedral library with full support for Z-polyhedra, ad- vanced data dependency analysis and support for external optimizers. Polly includes integrated SIMD and OpenMP code generation. Through LLVM, machine code for CPUs and GPU accelerators, C source code and even hardware descriptions can be targeted.},
	author = {Grosser, Tobias and Zheng, Hongbin and Aloor, Raghesh and Simbürger, Andreas and Grö{ß}linger, Armin and Pouchet, Louis-Noël},
	url = {http://perso.ens-lyon.fr/christophe.alias/impact2011/impact-07.pdf},
	journaltitle = {Proceedings of the First International Workshop on Polyhedral Compilation Techniques (IMPACT '11)},
	pages = {None},
	title = {{Polly - Polyhedral optimization in LLVM}},
	year = {2011}
}

@thesis{grosser14,
	author = {Grosser, Tobias},
	institution = {Université Pierre et Marie Curie - Paris VI},
	url = {https://tel.archives-ouvertes.fr/tel-01144563/file/pdf2star-1417100348-these_archivage_3160267.pdf},
	keywords = {polyhedral model,polyhedral analysis},
	month = oct,
	number = {2014PA066270},
	title = {{A decoupled approach to high-level loop optimization : tile shapes, polyhedral building blocks and low-level compilers}},
	type = {Theses},
	year = {2014}
}

@inproceedings{grosser15,
	author = {Grosser, Tobias and Pop, Sebastian and Ramanujam, J and Sadayappan, P},
	organization = {Citeseer},
	booktitle = {5th International Workshop on Polyhedral Compilation Techniques.},
	keywords = {polyhedral model,polyhedral analysis},
	pages = {52},
	series = {IMPACT},
	title = {On recovering multi-dimensional arrays in polly},
	year = {2015}
}

@article{grosser15_optim,
	abstract = {A number of legacy codes make use of linearized array references (i.e., references to one-dimensional arrays) to encode accesses to multi-dimensional arrays. This is also true of a number of optimized libraries and the well-known LLVM intermediate representation, which linearize array accesses. In many cases, the only information available is an array base pointer and a single dimensional offset. For problems with parametric array extents, this offset is usually a multivariate polynomial. Compiler analyses such as data dependence analysis are impeded because the standard formulations with integer linear programming (ILP) solvers cannot be used. In this paper, we present an approach to delinearization, i.e., recovering the multi-dimensional nature of accesses to arrays of parametric size. In case of insufficient static information, the developed algorithm produces run-time conditions to validate the recovered multi-dimensional form. The obtained access description enhances the precision of data dependence analysis. Experimental evaluation in the context of the LLVM/Polly system using a number of benchmarks reveals significant performance benefits due to increased precision of dependence analysis and enhanced optimization opportunities that are exploited by the compiler after delinearization.},
	author = {Grosser, Tobias and Ramanujam, J. and Pouchet, Louis Noël and Sadayappan, P. and Pop, Sebastian},
	doi = {10.1145/2751205.2751248},
	file = {:home/aditya/Downloads/2751205.2751248.pdf:pdf},
	isbn = {9781450335591},
	journaltitle = {Proceedings of the International Conference on Supercomputing},
	keywords = {Linear memory layout,Multi-dimensional arrays,Polyhedral analysis},
	pages = {351--360},
	title = {{Optimistic delinearization of parametrically sized arrays}},
	volume = {2015-June},
	year = {2015}
}

@article{gu23_optim_chc_solvin_termin_proof,
	author = {Gu, Yu and Tsukada, Takeshi and Unno, Hiroshi},
	title = {Optimal CHC Solving via Termination Proofs},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571214},
	doi = {10.1145/3571214},
	abstract = {Motivated by applications to open program reasoning such as maximal specification inference, this paper studies optimal CHC solving, a problem to compute maximal and/or minimal solutions of constrained Horn clauses (CHCs). This problem and its subproblems have been studied in the literature, and a major approach is to iteratively improve a solution of CHCs until it becomes optimal. So a key ingredient of optimization methods is the optimality checking of a given solution. We propose a novel optimality checking method, as well as an optimization method using the proposed optimality checker, based on a computational theoretical analysis of the optimality checking problem. The key observation is that the optimality checking problem is closely related to the termination analysis of programs, and this observation is useful both theoretically and practically. From a theoretical perspective, it clarifies a limitation of an existing method and incorrectness of another method in the literature. From a practical perspective, it allows us to apply techniques of termination analysis to the optimality checking of a solution of CHCs. We present an optimality checking method based on constraint-based synthesis of termination arguments, implemented our method, evaluated it on CHCs that encode maximal specification synthesis problems, and obtained promising results.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {21},
	numpages = {28},
	keywords = {termination analysis, specification synthesis, constrained Horn clause}
}

@inproceedings{gupta03_spark,
	abstract = {This paper presents a modular and extensible high-level synthesis research system, called SPARK, that takes a behavioral description in ANSI-C as input and produces synthesizable register-transfer level VHDL. SPARK uses parallelizing compiler technology, developed previously, to enhance instruction-level parallelism and re-instruments it for high-level synthesis by incorporating ideas of mutual exclusivity of operations, resource sharing and hardware cost models. In this paper, we present the design flow through the SPARK system, a set of transformations that include speculative code motions and dynamic transformations and show how these transformations and other optimizing synthesis and compiler techniques are employed by a scheduling heuristic. Experiments are performed on two moderately complex industrial applications, namely MPEG-1 and the GIMP image processing tool. The results show that the various code transformations lead to up to 70 \% improvements in performance without any increase in the overall area and critical path of the final synthesized design.},
	author = {Gupta, S. and Dutt, N. and Gupta, R. and Nicolau, A.},
	url = {https://doi.org/10.1109/ICVD.2003.1183177},
	booktitle = {16th International Conference on VLSI Design, 2003. Proceedings.},
	doi = {10.1109/ICVD.2003.1183177},
	issn = {1063-9667},
	keywords = {high level synthesis;hardware description languages;circuit optimisation;image processing;parallelising compilers;processor scheduling;heuristic programming;code optimization;SPARK modular system;high-level synthesis framework;parallelizing compiler transformations;ANSI-C behavioral description;register-transfer level VHDL;instruction-level parallelism;operation mutual exclusivity;resource sharing;hardware cost models;dynamic transformations;speculative code motions;optimizing synthesis;scheduling heuristic;MPEG-1;GIMP image processing;Sparks;High level synthesis;Resource management;Hardware;Costs;Design optimization;Optimizing compilers;Job shop scheduling;Dynamic scheduling;Image processing},
	month = jan,
	pages = {461--466},
	title = {SPARK: a high-level synthesis framework for applying parallelizing compiler transformations},
	year = {2003}
}

@inproceedings{guzel16_using,
	abstract = {In this work, we share our experience in using High-Level Synthesis (HLS) for rapid development of an optical flow design on FPGA. We have performed HLS using Vivado HLS as well as a HLS tool we have developed for the optical flow design at hand and similar video processing problems. The paper first describes the design problem we have and then discusses our own HLS tool. The tool we developed has turned out to be pretty general-purpose except for the ability to handle cyclic inter-iteration dependencies. It also introduces some novel concepts to HLS, such as "pipelined multiplexers". The synthesis results show that we can achieve better timing or better area results compared to Vivado HLS. Furthermore, the Verilog RTL our HLS tool outputs is much more readable than the one from Vivado HLS. This makes it much easier for the designer to debug and modify the RTL.},
	author = {Guzel, A. E. and Levent, V. E. and Tosun, M. and Özkan, M. A. and Akgun, T. and Büyükaydin, D. and Erbas, C. and Ugurdag, H. F.},
	url = {https://doi.org/10.1109/EWDTS.2016.7807644},
	booktitle = {2016 IEEE East-West Design Test Symposium (EWDTS)},
	doi = {10.1109/EWDTS.2016.7807644},
	issn = {2472-761X},
	keywords = {field programmable gate arrays;iterative methods;video signal processing;high-level synthesis;video processing pipes;optical flow design;FPGA;Vivado HLS;cyclic inter iteration dependencies;Verilog RTL;HLS tool outputs;Field programmable gate arrays;Pipeline processing;Algorithm design and analysis;Timing;Hardware;Optical design;Software;High-Level Synthesis;Vivado HLS;Video Processing Pipelines;Optical Flow},
	month = oct,
	pages = {1--4},
	title = {Using high-level synthesis for rapid design of video processing pipes},
	year = {2016}
}

@article{gäher22_simul,
	author = {Gäher, Lennard and Sammler, Michael and Spies, Simon and Jung, Ralf and Dang, Hoang-Hai and Krebbers, Robbert and Kang, Jeehoon and Dreyer, Derek},
	title = {Simuliris: A Separation Logic Framework for Verifying Concurrent Program Optimizations},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498689},
	doi = {10.1145/3498689},
	abstract = {Today’s compilers employ a variety of non-trivial optimizations to achieve good performance. One key trick compilers use to justify transformations of concurrent programs is to assume that the source program has no data races: if it does, they cause the program to have undefined behavior (UB) and give the compiler free rein. However, verifying correctness of optimizations that exploit this assumption is a non-trivial problem. In particular, prior work either has not proven that such optimizations preserve program termination (particularly non-obvious when considering optimizations that move instructions out of loop bodies), or has treated all synchronization operations as external functions (losing the ability to reorder instructions around them). In this work we present Simuliris, the first simulation technique to establish termination preservation (under a fair scheduler) for a range of concurrent program transformations that exploit UB in the source language. Simuliris is based on the idea of using ownership to reason modularly about the assumptions the compiler makes about programs with well-defined behavior. This brings the benefits of concurrent separation logics to the space of verifying program transformations: we can combine powerful reasoning techniques such as framing and coinduction to perform thread-local proofs of non-trivial concurrent program optimizations. Simuliris is built on a (non-step-indexed) variant of the Coq-based Iris framework, and is thus not tied to a particular language. In addition to demonstrating the effectiveness of Simuliris on standard compiler optimizations involving data race UB, we also instantiate it with Jung et al.’s Stacked Borrows semantics for Rust and generalize their proofs of interesting type-based aliasing optimizations to account for concurrency.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {28},
	numpages = {31},
	keywords = {separation logic, data races, program optimizations, Iris}
}

@article{habibi06_desig_system,
	author = {Habibi, A. and Tahar, S.},
	doi = {10.1109/TVLSI.2005.863187},
	journaltitle = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	keywords = {formal verification;hardware description languages;high level synthesis;just-in-time;zero cycle detection;CHStone benchmarks;RTL implementation;register transfer level;large-scale software systems;software development;hardware design;HLS tools;high-level synthesis tools;JIT trace-based verification;just-in-time traces;Instruments;Debugging;Registers;Optimization;Hardware;Computer bugs;Radiation detectors;JIT;Trace-based Verification;High-Level Synthesis},
	number = {1},
	pages = {57--68},
	title = {Design and verification of SystemC transaction-level models},
	volume = {14},
	year = {2006}
}

@article{hainry23_gener_nonin_polic_polyn_time,
	author = {Hainry, Emmanuel and P\'{e}choux, Romain},
	title = {A General Noninterference Policy for Polynomial Time},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571221},
	doi = {10.1145/3571221},
	abstract = {We introduce a new noninterference policy to capture the class of functions computable in polynomial time on an object-oriented programming language. This policy makes a clear separation between the standard noninterference techniques for the control flow and the layering properties required to ensure that each “security” level preserves polynomial time soundness, and is thus very powerful as for the class of programs it can capture. This new characterization is a proper extension of existing tractable characterizations of polynomial time based on safe recursion. Despite the fact that this noninterference policy is Π10-complete, we show that it can be instantiated to some decidable and conservative instance using shape analysis techniques.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {28},
	numpages = {27},
	keywords = {Shape Analysis, Computational Complexity, Noninterference, Polynomial time}
}

@ARTICLE{halbwachs91_sdfpll,
	author = {Halbwachs, N. and Caspi, P. and Raymond, P. and Pilaud, D.},
	journal = {Proceedings of the IEEE},
	title = {The Synchronous Data Flow Programming Language {LUSTRE}},
	year = {1991},
	volume = {79},
	number = {9},
	pages = {1305-1320},
	doi = {10.1109/5.97300}
}

@inproceedings{hao10_optim,
	author = {Hao, K. and Xie, F. and Ray, S. and Yang, J.},
	url = {https://doi.org/10.1109/DATE.2010.5457049},
	booktitle = {2010 Design, Automation Test in Europe Conference Exhibition (DATE 2010)},
	doi = {10.1109/DATE.2010.5457049},
	issn = {1530-1591},
	keywords = {electronic design automation;optimisation;equivalence checking;behavioral synthesis;electronic system-level design;RTL implementation;verification complexity;Circuit synthesis;Design optimization;Job shop scheduling;Clocks;Pipeline processing;Flow graphs;Computer science;Design automation;Cryptography;Resource management},
	month = mar,
	pages = {1500--1505},
	title = {Optimizing equivalence checking for behavioral synthesis},
	year = {2010}
}

@InProceedings{harrison23_fhlsach,
	doi = {10.1007/978-3-031-33170-1_20},
	author = {Harrison, William and Blumenfeld, Ian and Bond, Eric and Hathhorn, Chris and Li, Paul and Torrence, May and Ziegler, Jared},
	editor = {Rozier, Kristin Yvonne and Chaudhuri, Swarat},
	title = "Formalized High Level Synthesis with Applications to Cryptographic Hardware",
	booktitle = "NASA Formal Methods",
	year = "2023",
	publisher = "Springer Nature Switzerland",
	address = "Cham",
	pages = "332--352",
	abstract = "Verification of hardware-based cryptographic accelerators connects a low-level RTL implementation to the abstract algorithm itself; generally, the more optimized for performance an accelerator is, the more challenging its verification. This paper introduces a verification methodology, model validation, that uses a formalized high-level synthesis language (FHLS) as an intermediary between algorithm specification and hardware implementation. The foundation of our approach to model validation is a mechanized denotational semantics for the ReWire HLS language. Model validation proves the faithfulness of FHLS models to the RTL implementation and we summarize a model validation case study for a suite of pipelined Barrett multipliers.",
	isbn = "978-3-031-33170-1"
}

@InProceedings{hasuo06_gener_forwar_backw_simul,
	doi = {10.1007/11817949_27},
	keywords = {simulation proof},
	author = "Hasuo, Ichiro",
	editor = "Baier, Christel
and Hermanns, Holger",
	title = "Generic Forward and Backward Simulations",
	booktitle = "CONCUR 2006 -- Concurrency Theory",
	year = "2006",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "406--420",
	abstract = "The technique of forward/backward simulations has been applied successfully in many distributed and concurrent applications. In this paper, however, we claim that the technique can actually have more genericity and mathematical clarity. We do so by identifying forward/backward simulations as lax/oplax morphisms of coalgebras. Starting from this observation, we present a systematic study of this generic notion of simulations. It is meant to be a generic version of the study by Lynch and Vaandrager, covering both non-deterministic and probabilistic systems. In particular we prove soundness and completeness results with respect to trace inclusion: the proof is by coinduction using the generic theory of traces developed by Jacobs, Sokolova and the author. By suitably instantiating our generic framework, one obtains the appropriate definition of forward/backward simulations for various kinds of systems, for which soundness and completeness come for free.",
	isbn = "978-3-540-37377-3"
}

@book{hatcher01_algeb,
	note = {Central Library, Level 2, 514.2 HAT},
	publisher = {Cambridge University Press},
	title = {Algebraic topology },
	year = {2001},
	author = {Hatcher, Allen.},
	address = {Cambridge},
	booktitle = {Algebraic topology},
	isbn = {9780521795401},
	keywords = {topology, book},
	language = {eng},
	lccn = {00065166}
}

@book{hauck10_recon,
	author = {Hauck, Scott and DeHon, Andre},
	publisher = {Elsevier},
	title = {Reconfigurable computing: the theory and practice of {FPGA}-based computation},
	year = {2010}
}

@inproceedings{havanki98_treeg,
	abstract = {Instruction scheduling is one of the most important phases of compilation for high-performance processors. A compiler typically divides a program into multiple regions of code and then schedules each region. Many past efforts have focused on linear regions such as traces and superblocks. The linearity of these regions can limit speculation, leading to under-utilization of processor resources, especially on wide-issue machines. A type of non-linear region called a treegion is presented in this paper. The formation and scheduling of treegions takes into account multiple execution paths, and the larger scope of treegions allows more speculation, leading to higher utilization and better performance. Multiple scheduling heuristics for treegions are compared against scheduling for several types of linear regions. Empirical results illustrate that instruction scheduling using treegions treegion scheduling-holds promise. Treegion scheduling using the global weight heuristic outperforms the next highest performing region-superblocks by up to 20%.},
	author = {Havanki, W. A. and Banerjia, S. and Conte, T. M.},
	booktitle = {Proceedings 1998 Fourth International Symposium on High-Performance Computer Architecture},
	doi = {10.1109/HPCA.1998.650566},
	keywords = {processor scheduling;instruction scheduling;compilation;wide issue processors;high-performance processors;linear regions;traces;superblocks;treegion;multiple execution paths;Processor scheduling;Tree graphs;Program processors;Microprocessors;Parallel processing;Hardware;Linearity;Tree data structures;Tail;Topology},
	month = feb,
	pages = {266--276},
	title = {Treegion scheduling for wide issue processors},
	year = {1998}
}

@inproceedings{havlak94_const,
	abstract = {Analysis of symbolic expressions benefits from a suitable program representation. We show how to build thinned gated single-assignment (TGSA) form, a value-oriented program representation which is more complete than standard SSA form, defined on all reducible programs, and better for representing symbolic expressions than program dependence graphs or original GSA form. We present practical algorithms for constructing thinned GSA form from the control dependence graph and SSA form. Extensive experiments on large Fortran programs show these methods to take linear time and space in practice. Our implementation of value numbering on TGSA form drives scalar symbolic analysis in the ParaScope programming environment.},
	author = {Havlak, Paul},
	editor = {Banerjee, Utpal and Gelernter, David and Nicolau, Alex and Padua, David},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Languages and Compilers for Parallel Computing},
	isbn = {978-3-540-48308-3},
	keywords = {gated-SSA,SSA},
	pages = {477--499},
	title = {Construction of thinned gated single-assignment form},
	year = {1994}
}

@article{havlak97_nestin_reduc_irred_loops,
	abstract = {Recognizing and transforming loops are essential steps in any attempt to improve the running time of a program. Aggressive restructuring techniques have been developed for single-entry (reducible) loops, but restructurers and the dataflow and dependence analysis they rely on often give up in the presence of multientry (irreducible) loops. Thus one irreducible loop can prevent the improvement of all loops in a procedure. This article give an algorithm to build a loop nesting tree for a procedure with arbitrary control flow. The algorithm uses definitions of reducible and irreducible loops which allow either kind of loop to be nested in the other. The tree construction algorithm, an extension of Tarjan's algorithm for testing reducibility, runs in almost linear time. In the presence of irreducible loops, the loop nesting tree can depend on the depth-first spanning tree used to build it. In particular, the header node representing a reducible loop in one version of the loop nesting tree can be the representative of an irreducible loop in another. We give a normalization method that maximizes the set of reducible loops discovered, independent of the depth-first spanning tree used. The normalization require the insertion of at most one node and one edge per reducible loop.},
	author = {Havlak, Paul},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/262004.262005},
	doi = {10.1145/262004.262005},
	issn = {0164-0925},
	journaltitle = {ACM Trans. Program. Lang. Syst.},
	keywords = {reducible loops,strongly-connected regions,loop identification},
	month = jul,
	number = {4},
	pages = {557--567},
	title = {Nesting of Reducible and Irreducible Loops},
	volume = {19},
	year = {1997}
}

@inproceedings{he94_provab_correc_system,
	abstract = {The goal of the Provably Correct Systems project (ProCoS) is to develop a mathematical basis for development of embedded, real-time, computer systems. This survey paper introduces the specification languages and verification techniques for four levels of development: Requirements definition and control design; Transformation to a systems architecture with program designs and their transformation to programs; Compilation of real-time programs to conventional processors, and Compilation of programs to hardware.},
	author = {He, Jifeng and Hoare, C. A. R. and FrÄnzle, Martin and Müller-Olm, Markus and Olderog, Ernst-Rüdiger and Schenke, Michael and Hansen, Michael R. and Ravn, Anders P. and Rischel, Hans},
	editor = {Langmaack, Hans and de Roever, Willem-Paul and Vytopil, Jan},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Formal Techniques in Real-Time and Fault-Tolerant Systems},
	isbn = {978-3-540-48984-9},
	pages = {288--335},
	title = {Provably Correct Systems},
	year = {1994}
}

@book{hennessy11_comput_archit_fifth_edition,
	abstract = {The computing world today is in the middle of a revolution: mobile clients and cloud computing have emerged as the dominant paradigms driving programming and hardware innovation today. The Fifth Edition of Computer Architecture focuses on this dramatic shift, exploring the ways in which software and technology in the "cloud" are accessed by cell phones, tablets, laptops, and other mobile computing devices. Each chapter includes two real-world examples, one mobile and one datacenter, to illustrate this revolutionary change. Updated to cover the mobile computing revolutionEmphasizes the two most important topics in architecture today: memory hierarchy and parallelism in all its forms.Develops common themes throughout each chapter: power, performance, cost, dependability, protection, programming models, and emerging trends ("What's Next")Includes three review appendices in the printed text. Additional reference appendices are available online.Includes updated Case Studies and completely new exercises.},
	author = {Hennessy, John L. and Patterson, David A.},
	location = {San Francisco, CA, USA},
	publisher = {Morgan Kaufmann Publishers Inc.},
	edition = {5th},
	isbn = {012383872X},
	title = {Computer Architecture, Fifth Edition: A Quantitative Approach},
	year = {2011}
}

@InProceedings{henzinger13_aspec_orien_linear_proof,
	doi = {10.1007/978-3-642-40184-8_18},
	keywords = {simulation proof},
	author = {Henzinger, Thomas A. and Sezgin, Ali and Vafeiadis, Viktor},
	editor = "D'Argenio, Pedro R.
and Melgratti, Hern{\'a}n",
	title = "Aspect-Oriented Linearizability Proofs",
	booktitle = "CONCUR 2013 -- Concurrency Theory",
	year = "2013",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "242--256",
	abstract = "Linearizability of concurrent data structures is usually proved by monolithic simulation arguments relying on identifying the so-called linearization points. Regrettably, such proofs, whether manual or automatic, are often complicated and scale poorly to advanced non-blocking concurrency patterns, such as helping and optimistic updates.",
	isbn = "978-3-642-40184-8"
}

@inproceedings{herklotz20_findin_under_bugs_fpga_synth_tools,
	abstract = {All software ultimately relies on hardware functioning correctly. Hardware correctness is becoming increasingly important due to the growing use of custom accelerators using FPGAs to speed up applications on servers. Furthermore, the increasing complexity of hardware also leads to ever more reliance on automation, meaning that the correctness of synthesis tools is vital for the reliability of the hardware. This paper aims to improve the quality of FPGA synthesis tools by introducing a method to test them automatically using randomly generated, correct Verilog, and checking that the synthesised netlist is always equivalent to the original design. The main contributions of this work are twofold: firstly a method for generating random behavioural Verilog free of undefined values, and secondly a Verilog test case reducer used to locate the cause of the bug that was found. These are implemented in a tool called Verismith. This paper also provides a qualitative and quantitative analysis of the bugs found in Yosys, Vivado, XST and Quartus Prime. Every synthesis tool except Quartus Prime was found to introduce discrepancies between the netlist and the design. In addition to that, Vivado and a development version of Yosys were found to crash when given valid input. Using Verismith, eleven bugs were reported to tool vendors, of which six have already been fixed.},
	author = {Herklotz, Yann and Wickerson, John},
	location = {Seaside, CA, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3373087.3375310},
	booktitle = {Proceedings of the 2020 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	doi = {10.1145/3373087.3375310},
	isbn = {9781450370998},
	keywords = {test case reduction,logic synthesis,fuzzing,verilog},
	pages = {277--287},
	series = {FPGA '20},
	title = {Finding and Understanding Bugs in FPGA Synthesis Tools},
	year = {2020}
}

@inproceedings{herklotz21_empir_study_reliab_high_level_synth_tools,
	author = {Herklotz, Yann and Du, Zewei and Ramanathan, Nadesh and Wickerson, John},
	booktitle = {2021 IEEE 29th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
	doi = {10.1109/FCCM51124.2021.00034},
	pages = {219--223},
	title = {An Empirical Study of the Reliability of High-Level Synthesis Tools},
	year = {2021}
}

@article{herklotz21_formal_verif_high_level_synth,
	abstract = {High-level synthesis (HLS), which refers to the automatic compilation of software into hardware, is rapidly gaining popularity. In a world increasingly reliant on application-specific hardware accelerators, HLS promises hardware designs of comparable performance and energy efficiency to those coded by hand in a hardware description language such as Verilog, while maintaining the convenience and the rich ecosystem of software development. However, current HLS tools cannot always guarantee that the hardware designs they produce are equivalent to the software they were given, thus undermining any reasoning conducted at the software level. Furthermore, there is mounting evidence that existing HLS tools are quite unreliable, sometimes generating wrong hardware or crashing when given valid inputs. To address this problem, we present the first HLS tool that is mechanically verified to preserve the behaviour of its input software. Our tool, called Vericert, extends the CompCert verified C compiler with a new hardware-oriented intermediate language and a Verilog back end, and has been proven correct in Coq. Vericert supports most C constructs, including all integer operations, function calls, local arrays, structs, unions, and general control-flow statements. An evaluation on the PolyBench/C benchmark suite indicates that Vericert generates hardware that is around an order of magnitude slower (only around 2\texttimes{} slower in the absence of division) and about the same size as hardware generated by an existing, optimising (but unverified) HLS tool.},
	author = {Herklotz, Yann and Pollard, James D. and Ramanathan, Nadesh and Wickerson, John},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	doi = {10.1145/3485494},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {high-level synthesis,Coq,Verilog,CompCert,C},
	month = oct,
	number = {OOPSLA},
	title = {Formal Verification of High-Level Synthesis},
	volume = {5},
	year = {2021}
}

@misc{herklotz21_veric,
	author = {Herklotz, Yann and Pollard, James D. and Ramanathan, Nadesh and Wickerson, John},
	publisher = {Zenodo},
	url = {https://doi.org/10.5281/zenodo.5093839},
	doi = {10.5281/zenodo.5093839},
	month = jul,
	title = {Vericert v1.2.1},
	version = {v1.2.1},
	year = {2021}
}

@inproceedings{herklotz23_mechan_seman_gated_static_singl_assig,
	author = {Herklotz, Yann and Demange, Delphine and Blazy, Sandrine},
	title = {Mechanised Semantics for Gated Static Single Assignment},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575681},
	doi = {10.1145/3573105.3575681},
	abstract = {The Gated Static Single Assignment (GSA) form was proposed by Ottenstein et al. in 1990, as an intermediate representation for implementing advanced static analyses and optimisation passes in compilers. Compared to SSA, GSA records additional data dependencies and provides more context, making optimisations more effective and allowing one to reason about programs as data-flow graphs. Many practical implementations have been proposed that follow, more or less faithfully, Ottenstein et al.'s seminal paper. But many discrepancies remain between these, depending on the kind of dependencies they are supposed to track and to leverage in analyses and code optimisations. In this paper, we present a formal semantics for GSA, mechanised in Coq. In particular, we clarify the nature and the purpose of gates in GSA, and define control-flow insensitive semantics for them. We provide a specification that can be used as a reference description for GSA. We also specify a translation from SSA to GSA and prove that this specification is semantics-preserving. We demonstrate that the approach is practical by implementing the specification as a validated translation within the CompCertSSA verified compiler.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {182–196},
	numpages = {15},
	keywords = {SSA, Verified Compilation, Gated SSA},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@inproceedings{herzog21_price_meltd_spect,
	author = {Herzog, Benedict and Reif, Stefan and Preis, Julian and Schr\"{o}der-Preikschat, Wolfgang and H\"{o}nig, Timo},
	title = {The Price of Meltdown and Spectre: Energy Overhead of Mitigations at Operating System Level},
	year = {2021},
	isbn = {9781450383370},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3447852.3458721},
	abstract = {The Meltdown and Spectre hardware vulnerabilities shocked hardware manufacturers and system users upon discovery. Numerous attack vectors and mitigations have been developed and deployed. However, due to the deep entanglement in core CPU components they will be an important topic for years. Although the performance overhead of software mitigations has been examined closely, the energy overhead has experienced little attention---even though the energy demand is a critical cost factor in data centres.This work contributes a fine-grained energy-overhead analysis of Meltdown/Spectre software mitigations, which reveals application-specific energy overheads of up to 72 \%. We further compare energy overheads to execution time overheads. Additionally, we examine subsystem-specific effects (i.e., CPU, memory, I/O, network/interprocess communication) and develop a model that predicts energy overheads for applications.},
	booktitle = {Proceedings of the 14th European Workshop on Systems Security},
	pages = {8–14},
	numpages = {7},
	location = {Online, United Kingdom},
	series = {EuroSec '21}
}

@article{heunen22_quant_infor_effec,
	author = {Heunen, Chris and Kaarsgaard, Robin},
	title = {Quantum Information Effects},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498663},
	doi = {10.1145/3498663},
	abstract = {We study the two dual quantum information effects to manipulate the amount of information in quantum computation: hiding and allocation. The resulting type-and-effect system is fully expressive for irreversible quantum computing, including measurement. We provide universal categorical constructions that semantically interpret this arrow metalanguage with choice, starting with any rig groupoid interpreting the reversible base language. Several properties of quantum measurement follow in general, and we translate (noniterative) quantum flow charts into our language. The semantic constructions turn the category of unitaries between Hilbert spaces into the category of completely positive trace-preserving maps, and they turn the category of bijections between finite sets into the category of functions with chosen garbage. Thus they capture the fundamental theorems of classical and quantum reversible computing of Toffoli and Stinespring.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {2},
	numpages = {27},
	keywords = {arrows, reversible computation, categorical semantics, measurement, quantum computation, effects, information effects}
}

@article{hietala21_verif_optim_quant_circuit,
	abstract = {We present VOQC, the first fully verified optimizer for quantum circuits, written using the Coq proof assistant. Quantum circuits are expressed as programs in a simple, low-level language called SQIR, a simple quantum intermediate representation, which is deeply embedded in Coq. Optimizations and other transformations are expressed as Coq functions, which are proved correct with respect to a semantics of SQIR programs. SQIR uses a semantics of matrices of complex numbers, which is the standard for quantum computation, but treats matrices symbolically in order to reason about programs that use an arbitrary number of quantum bits. SQIR's careful design and our provided automation make it possible to write and verify a broad range of optimizations in VOQC, including full-circuit transformations from cutting-edge optimizers.},
	author = {Hietala, Kesha and Rand, Robert and Hung, Shih-Han and Wu, Xiaodi and Hicks, Michael},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434318},
	doi = {10.1145/3434318},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {Quantum Computing,Formal Verification,Programming Languages,Certified Compilation,Circuit Optimization},
	month = jan,
	number = {POPL},
	title = {A Verified Optimizer for Quantum Circuits},
	volume = {5},
	year = {2021}
}

@article{ho22_aeneas,
	author = {Ho, Son and Protzenko, Jonathan},
	title = {Aeneas: Rust Verification by Functional Translation},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3547647},
	doi = {10.1145/3547647},
	abstract = {We present Aeneas, a new verification toolchain for Rust programs based on a lightweight functional translation. We leverage Rust’s rich region-based type system to eliminate memory reasoning for a large class of Rust programs, as long as they do not rely on interior mutability or unsafe code. Doing so, we relieve the proof engineer of the burden of memory-based reasoning, allowing them to instead focus on functional properties of their code. The first contribution of Aeneas is a new approach to borrows and controlled aliasing. We propose a pure, functional semantics for LLBC, a Low-Level Borrow Calculus that captures a large subset of Rust programs. Our semantics is value-based, meaning there is no notion of memory, addresses or pointer arithmetic. Our semantics is also ownership-centric, meaning that we enforce soundness of borrows via a semantic criterion based on loans rather than through a syntactic type-based lifetime discipline. We claim that our semantics captures the essence of the borrow mechanism rather than its current implementation in the Rust compiler. The second contribution of Aeneas is a translation from LLBC to a pure lambda-calculus. This allows the user to reason about the original Rust program through the theorem prover of their choice, and fulfills our promise of enabling lightweight verification of Rust programs. To deal with the well-known technical difficulty of terminating a borrow, we rely on a novel approach, in which we approximate the borrow graph in the presence of function calls. This in turn allows us to perform the translation using a new technical device called backward functions. We implement our toolchain in a mixture of Rust and OCaml; our chief case study is a low-level, resizing hash table, for which we prove functional correctness, the first such result in Rust. Our evaluation shows significant gains of verification productivity for the programmer. This paper therefore establishes a new point in the design space of Rust verification toolchains, one that aims to verify Rust programs simply, and at scale. Rust goes to great lengths to enforce static control of aliasing; the proof engineer should not waste any time on memory reasoning when so much already comes “for free”!},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {116},
	numpages = {31},
	keywords = {functional translation, verification, Rust}
}

@article{hoare78_commun_sequen_proces,
	author = {Hoare, C. A. R.},
	title = {Communicating Sequential Processes},
	year = {1978},
	issue_date = {Aug. 1978},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {21},
	number = {8},
	issn = {0001-0782},
	doi = {10.1145/359576.359585},
	abstract = {This paper suggests that input and output are basic primitives of programming and that parallel composition of communicating sequential processes is a fundamental program structuring method. When combined with a development of Dijkstra's guarded command, these concepts are surprisingly versatile. Their use is illustrated by sample solutions of a variety of a familiar programming exercises.},
	journal = {Commun. ACM},
	month = {8},
	pages = {666–677},
	numpages = {12},
	keywords = {programming, guarded commands, multiple entries, nondeterminacy, data representations, monitors, multiple exits, iterative arrays, coroutines, conditional critical regions, program structures, concurrency, recursion, procedures, classes, programming primitives, parallel programming, programming languages, input, output}
}

@inbook{hoare78_proof_correc_data_repres,
	abstract = {A powerful method of simplifying the proofs of program correctness is suggested; and some new light is shed on the problem of functions with side-effects.},
	author = {Hoare, C. A. R.},
	editor = {Gries, David},
	location = {New York, NY},
	publisher = {Springer New York},
	url = {https://doi.org/10.1007/978-1-4612-6315-9_20},
	booktitle = {Programming Methodology: A Collection of Articles by Members of IFIP WG2.3},
	doi = {10.1007/978-1-4612-6315-9_20},
	isbn = {978-1-4612-6315-9},
	pages = {269--281},
	title = {Proof of Correctness of Data Representations},
	year = {1978}
}

@inproceedings{homsirikamol14_can,
	author = {Homsirikamol, Ekawat and Gaj, Kris},
	publisher = {IEEE},
	booktitle = {ReConFig},
	doi = {10.1109/ReConFig.2014.7032504},
	pages = {1--8},
	title = {Can high-level synthesis compete against a hand-written code in the cryptographic domain? {A} case study},
	year = {2014}
}

@inproceedings{homsirikamol17_towar_hls_fpga,
	abstract = {The increasing number of candidates competing in cryptographic contests has made hardware benchmarking using the traditional Register-Transfer Level (RTL) methodology too inefficient and potentially unfair, especially at the early stages of the competitions. In this paper, we propose supplementing, and eventually replacing, this traditional RTL methodology with the use of High-Level Synthesis (HLS) tools. We apply our proposed HLS-based approach to FPGA benchmarking in the ongoing CAESAR contest, by comparing and ranking 16 authenticated ciphers, including the current standard, AES-GCM, and the primary variants of 13 Round 3 CAESAR candidates. After a careful survey of available HLS tools, we chose Xilinx Vivado HLS as our primary benchmarking tool. Our study has demonstrated high correlation between the rankings of the evaluated algorithms, obtained using both investigated methodologies. In particular, after applying HLS, the algorithm rankings in terms of two major performance metrics - throughput and throughput to area ratio - have either remained unchanged or have been affected only for algorithms with very similar RTL performance.},
	author = {{Homsirikamol}, E. and {George}, K. G.},
	url = {https://doi.org/10.1109/FPT.2017.8280129},
	booktitle = {2017 International Conference on Field Programmable Technology (ICFPT)},
	doi = {10.1109/FPT.2017.8280129},
	keywords = {benchmark testing;cryptography;field programmable gate arrays;high level synthesis;performance evaluation;cryptographic competitions;CAESAR contest case study;cryptographic contests;hardware benchmarking;FPGA benchmarking;Xilinx Vivado HLS;primary benchmarking tool;algorithm rankings;high-level synthesis;HLS tools;Register-Transfer Level methodology;RTL methodology;Tools;Benchmark testing;Hardware;Field programmable gate arrays;Ciphers;Hardware design languages;authenticated ciphers;CAESAR;benchmarking;hardware;FPGA;HLS},
	month = dec,
	pages = {120--127},
	title = {Toward a new HLS-based methodology for FPGA benchmarking of candidates in cryptographic competitions: The CAESAR contest case study},
	year = {2017}
}

@phdthesis{hopwood78_decom,
	author = {Hopwood, Gregory Littell},
	title = {Decompilation},
	year = {1978},
	publisher = {University of California, Irvine},
	note = {AAI7811860}
}

@INPROCEEDINGS{hormigo09_effic_implem_carry_save_adder_fpgas,
	author = {Hormigo, Javier and Ortiz, Manuel and Quiles, Francisco and Jaime, Francisco J. and Villalba, Julio and Zapata, Emilio L.},
	booktitle = {2009 20th IEEE International Conference on Application-specific Systems, Architectures and Processors},
	title = {Efficient Implementation of Carry-Save Adders in FPGAs},
	year = {2009},
	volume = {},
	number = {},
	pages = {207-210},
	abstract = {Most field programmable gate array (FPGA) devices have a special fast carry propagation logic intended to optimize addition operations. The redundant adders do not easily fit into this specialized carry-logic and, consequently, they require double hardware resources than carry propagate adders, while showing a similar delay for small size operands. Therefore, carry-save adders are not usually implemented on FPGA devices, although they are very useful in ASIC implementations. In this paper we study efficient implementations of carry-save adders on FPGA devices, taking advantage of the specialized carry-logic. We show that it is possible to implement redundant adders with a hardware cost close to that of a carry propagate adder. Specifically, for 16 bits and bigger wordlengths, redundant adders are clearly faster and have an area requirement similar to carry propagate adders. Among all the redundant adders studied, the 4:2 compressor is the fastest one, presents the best exploitation of the logic resources within FPGA slices and the easiest way to adapt classical algorithms to efficiently fit FPGA resources.},
	keywords = {arithmetic, carry-save adder},
	doi = {10.1109/ASAP.2009.22},
	ISSN = {1063-6862},
	month = {July}
}

@article{hou22_logar_progr_testin,
	author = {Hou (Favonia), Kuen-Bang and Wang, Zhuyang},
	title = {Logarithm and Program Testing},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498726},
	doi = {10.1145/3498726},
	abstract = {Randomized property-based testing has gained much attention recently, but most frameworks stop short at polymorphic properties. Although Bernardy&nbsp;et&nbsp;al. have developed a theory to reduce a wide range of polymorphic properties to monomorphic ones, it relies upon ad-hoc embedding-projection pairs to massage the types into a particular form. This paper skips the embedding-projection pairs and presents a mechanical monomorphization for a general class of polymorphic functions, a step towards automatic testing for polymorphic properties. The calculation of suitable types for monomorphization turns out to be logarithm.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {64},
	numpages = {26},
	keywords = {parametricity, logarithm, polymorphism}
}

@article{hou23_order_theor_analy_univer_polym,
	author = {Hou (Favonia), Kuen-Bang and Angiuli, Carlo and Mullanix, Reed},
	title = {An Order-Theoretic Analysis of Universe Polymorphism},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571250},
	doi = {10.1145/3571250},
	abstract = {We present a novel formulation of universe polymorphism in dependent type theory in terms of monads on the category of strict partial orders, and a novel algebraic structure, displacement algebras, on top of which one can implement a generalized form of McBride’s “crude but effective stratification” scheme for lightweight universe polymorphism. We give some examples of exotic but consistent universe hierarchies, and prove that every universe hierarchy in our sense can be embedded in a displacement algebra and hence implemented via our generalization of McBride’s scheme. Many of our technical results are mechanized in Agda, and we have an OCaml library for universe levels based on displacement algebras, for use in proof assistant implementations.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {57},
	numpages = {27},
	keywords = {universes, type theory, universe polymorphism}
}

@article{hu19_equiv_check_sched_high_level,
	author = {{Hu}, J. and {Wang}, G. and {Chen}, G. and {Wei}, X.},
	url = {https://doi.org/10.1109/ACCESS.2019.2960511},
	doi = {10.1109/ACCESS.2019.2960511},
	issn = {2169-3536},
	journaltitle = {IEEE Access},
	keywords = {automatic test pattern generation;finite state machines;high level synthesis;source code (software);deep state sequences;FSMD;VP based equivalence checking method;high-level synthesis tools;electronic system level design;growing design productivity gap;high quality hardware systems;target code;source code;value propagation based equivalence checking method;finite state machine with datapath;automatic test vector generation;ATVG;Equivalence checking;high-level synthesis;deep state sequence;FSMD},
	pages = {183435--183443},
	title = {Equivalence Checking of Scheduling in High-Level Synthesis Using Deep State Sequences},
	volume = {7},
	year = {2019}
}

@inproceedings{huang13_elast_cgras,
	abstract = {Vital technology trends such as voltage scaling and homogeneous multicore scaling have reached their limits and architects turn to alternate computing paradigms, such as heterogeneous and domain-specialized solutions. Coarse-Grain Reconfigurable Arrays (CGRAs) promise the performance of massively spatial computing while offering interesting trade-offs of flexibility versus energy efficiency. Yet, configuring and scheduling execution for CGRAs generally runs into the classic difficulties that have hampered Very-Long Instruction Word (VLIW) architectures: efficient schedules are difficult to generate, especially for applications with complex control flow and data structures, and they are inherently static - thus, in adapted to variable-latency components (such as the read ports of caches). Over the years, VLIWs have been relegated to important but specific application domains where such issues are more under the control of the designers; similarly, statically-scheduled CGRAs may prove inadequate for future general-purpose computing systems. In this paper, we introduce Elastic CGRAs, the superscalar processors of computing fabrics: no complex schedule needs to be computed at configuration time, and the operations execute dynamically in the CGRA when data are ready, thus exploiting the data parallelism that an application offers. We designed, down to a manufacturable layout, a simple CGRA where we demonstrated and optimized our elastic control circuitry. We also built a complete compilation toolchain that transforms arbitrary C code in a configuration for the array. The area overhead (26.2%), critical path overhead (8.2%) and energy overhead (53.6%) of Elastic CGRAs over non-elastic CGRAs are significantly lower than the overhead of superscalar processors over VLIWs, while providing the same benefits. At such moderate costs, elasticity may prove to be one of the key enablers to make the adoption of CGRAs widespread.},
	author = {Huang, Yuanjie and Ienne, Paolo and Temam, Olivier and Chen, Yunji and Wu, Chengyong},
	location = {Monterey, California, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/2435264.2435296},
	booktitle = {Proceedings of the ACM/SIGDA International Symposium on Field Programmable Gate Arrays},
	doi = {10.1145/2435264.2435296},
	isbn = {9781450318877},
	keywords = {elastic circuit,dataflow,CGRA,reconfigurable computing},
	pages = {171--180},
	series = {FPGA '13},
	title = {Elastic CGRAs},
	year = {2013}
}

@article{hunt23_recon_shann_scott_lattic_comput_infor,
	author = {Hunt, Sebastian and Sands, David and Stucki, Sandro},
	title = {Reconciling Shannon and Scott with a Lattice of Computable Information},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571740},
	doi = {10.1145/3571740},
	abstract = {This paper proposes a reconciliation of two different theories of information. The first, originally proposed in a lesser-known work by Claude Shannon (some five years after the publication of his celebrated quantitative theory of communication), describes how the information content of channels can be described qualitatively, but still abstractly, in terms of information elements, where information elements can be viewed as equivalence relations over the data source domain. Shannon showed that these elements have a partial ordering, expressing when one information element is more informative than another, and that these partially ordered information elements form a complete lattice. In the context of security and information flow this structure has been independently rediscovered several times, and used as a foundation for understanding and reasoning about information flow. The second theory of information is Dana Scott’s domain theory, a mathematical framework for giving meaning to programs as continuous functions over a particular topology. Scott’s partial ordering also represents when one element is more informative than another, but in the sense of computational progress – i.e. when one element is a more defined or evolved version of another. To give a satisfactory account of information flow in computer programs it is necessary to consider both theories together, in order to understand not only what information is conveyed by a program (viewed as a channel, \`{a} la Shannon) but also how the precision with which that information can be observed is determined by the definedness of its encoding (\`{a} la Scott). To this end we show how these theories can be fruitfully combined, by defining the Lattice of Computable Information (LoCI), a lattice of preorders rather than equivalence relations. LoCI retains the rich lattice structure of Shannon’s theory, filters out elements that do not make computational sense, and refines the remaining information elements to reflect how Scott’s ordering captures possible varieties in the way that information is presented. We show how the new theory facilitates the first general definition of termination-insensitive information flow properties, a weakened form of information flow property commonly targeted by static program analyses.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {68},
	numpages = {30},
	keywords = {Information Flow, Semantics}
}

@article{hwang91_formal_approac_to_sched_probl,
	author = {Hwang, C. -. and Lee, J. -. and Hsu, Y. -.},
	url = {https://doi.org/10.1109/43.75629},
	doi = {10.1109/43.75629},
	journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {circuit CAD;integer programming;linear programming;scheduling;integer LP model;CAD;scheduling problem;high level synthesis;integer linear programming;time-constrained scheduling;resource-constrained scheduling;feasible scheduling;chaining;multicycle operations;nonpipelined function units;pipelined function units;functional pipelining;loop folding;mutually exclusive operations;bus constraint;ASAP;ALAP;list scheduling;High level synthesis;Digital systems;Space exploration;Scheduling algorithm;Timing;Integer linear programming;Pipeline processing;Filters;Hardware design languages;Flow graphs},
	month = apr,
	number = {4},
	pages = {464--475},
	title = {A Formal Approach To the Scheduling Problem in High Level Synthesis},
	volume = {10},
	year = {1991}
}

@inbook{hwu93_super,
	abstract = {A compiler for VLIW and superscalar processors must expose sufficient instruction-level parallelism (ILP) to effectively utilize the parallel hardware. However, ILP within basic blocks is extremely limited for control-intensive programs. We have developed a set of techniques for exploiting ILP across basic block boundaries. These techniques are based on a novel structure called the superblock. The superblock enables the optimizer and scheduler to extract more ILP along the important execution paths by systematically removing constraints due to the unimportant paths. Superblock optimization and scheduling have been implemented in the IMPACT-I compiler. This implementation gives us a unique opportunity to fully understand the issues involved in incorporating these techniques into a real compiler. Superblock optimizations and scheduling are shown to be useful while taking into account a variety of architectural features.},
	author = {Hwu, Wen-Mei W. and Mahlke, Scott A. and Chen, William Y. and Chang, Pohua P. and Warter, Nancy J. and Bringmann, Roger A. and Ouellette, Roland G. and Hank, Richard E. and Kiyohara, Tokuzo and Haab, Grant E. and Holm, John G. and Lavery, Daniel M.},
	editor = {Rau, B. R. and Fisher, J. A.},
	location = {Boston, MA},
	publisher = {Springer US},
	booktitle = {Instruction-Level Parallelism: A Special Issue of The Journal of Supercomputing},
	doi = {10.1007/978-1-4615-3200-2_7},
	isbn = {978-1-4615-3200-2},
	keywords = {superblock scheduling,trace scheduling,static scheduling},
	pages = {229--248},
	title = {The Superblock: An Effective Technique for VLIW and Superscalar Compilation},
	year = {1993}
}

@thesis{iampietro20_towar_formal_verif_hilec,
	author = {Iampietro, Vincent and Andreu, David and Delahaye, David},
	institution = {LIRMM, Université de Montpellier},
	keywords = {petri nets,verification,operational semantics},
	title = {Toward the Formal Verification of HILECOP: Formalization and Implementation of Synchronously Executed Petri Nets},
	type = {phdthesis},
	year = {2020}
}

@article{ikebuchi22_certif_deriv_state_machin_corout,
	abstract = {One of the biggest implementation challenges in security-critical network protocols is nested state machines. In practice today, state machines are either implemented manually at a low level, risking bugs easily missed in audits; or are written using higher-level abstractions like threads, depending on runtime systems that may sacrifice performance or compatibility with the ABIs of important platforms (e.g., resource-constrained IoT systems). We present a compiler-based technique allowing the best of both worlds, coding protocols in a natural high-level form, using freer monads to represent nested coroutines, which are then compiled automatically to lower-level code with explicit state. In fact, our compiler is implemented as a tactic in the Coq proof assistant, structuring compilation as search for an equivalence proof for source and target programs. As such, it is straightforwardly (and soundly) extensible with new hints, for instance regarding new data structures that may be used for efficient lookup of coroutines. As a case study, we implemented a core of TLS sufficient for use with popular Web browsers, and our experiments show that the extracted Haskell code achieves reasonable performance.},
	author = {Ikebuchi, Mirai and Erbsen, Andres and Chlipala, Adam},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3498685},
	doi = {10.1145/3498685},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {interaction trees,cryptographic protocols,coroutines,proof assistants,nested state machines,program derivation},
	month = jan,
	number = {POPL},
	title = {Certifying Derivation of State Machines from Coroutines},
	volume = {6},
	year = {2022}
}

@misc{inc23_smart_tool_suite,
	author = {{Microchip Technology Inc}},
	title = {SmartHLS Tool Suite},
	url = {https://bit.ly/smarthls},
	urldate = {2023-06-28},
	year = {2023}
}

@misc{intel19_intel_quart,
	author = {Intel},
	url = {https://intel.ly/2m7wbCs},
	title = {{Intel Quartus}},
	urldate = {2019-01-14},
	year = {2019}
}

@misc{intel20_high_synth_compil,
	author = {Intel},
	url = {https://intel.ly/2UDiWr5},
	title = {High-level Synthesis Compiler},
	urldate = {2023-06-23},
	year = {2023}
}

@misc{intel20_sdk_openc_applic,
	author = {Intel},
	url = {https://intel.ly/30sYHz0},
	title = {{SDK} for {OpenCL} Applications},
	urldate = {2020-07-20},
	year = {2020}
}

@inproceedings{itzhaky21_cyclic_progr_synth,
	abstract = {We describe the first approach to automatically synthesizing heap-manipulating programs with auxiliary recursive procedures. Such procedures occur routinely in data structure transformations (e.g., flattening a tree into a list) or traversals of composite structures (e.g., n-ary trees). Our approach, dubbed cyclic program synthesis, enhances deductive program synthesis with a novel application of cyclic proofs. Specifically, we observe that the machinery used to form cycles in cyclic proofs can be reused to systematically and efficiently abduce recursive auxiliary procedures. We develop the theory of cyclic program synthesis by extending Synthetic Separation Logic (SSL), a logical framework for deductive synthesis of heap-manipulating programs from Separation Logic specifications. We implement our approach as a tool called Cypress, and showcase it by automatically synthesizing a number of programs manipulating linked data structures using recursive auxiliary procedures and mutual recursion, many of which were beyond the reach of existing program synthesis tools.},
	author = {Itzhaky, Shachar and Peleg, Hila and Polikarpova, Nadia and Rowe, Reuben N. S. and Sergey, Ilya},
	location = {Virtual, Canada},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3453483.3454087},
	booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
	doi = {10.1145/3453483.3454087},
	isbn = {9781450383912},
	keywords = {Cyclic Proofs,Program Synthesis,Separation Logic},
	pages = {944--959},
	series = {PLDI 2021},
	title = {Cyclic Program Synthesis},
	year = {2021}
}

@report{iyoda07_trans_hol,
	author = {Iyoda, Juliano},
	institution = {University of Cambridge, Computer Laboratory},
	url = {https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-682.pdf},
	month = apr,
	number = {UCAM-CL-TR-682},
	title = {{Translating HOL functions to hardware}},
	type = {techreport},
	year = {2007}
}

@article{jacobs21_fully_abstr_static_gradual,
	abstract = {What is a good gradual language? Siek et al. have previously proposed the refined criteria, a set of formal ideas that characterize a range of guarantees typically expected from a gradual language. While these go a long way, they are mostly focused on syntactic and type safety properties and fail to characterize how richer semantic properties and reasoning principles that hold in the static language, like non-interference or parametricity for instance, should be upheld in the gradualization. In this paper, we investigate and argue for a new criterion previously hinted at by Devriese et al.: the embedding from the static to the gradual language should be fully abstract. Rather than preserving an arbitrarily chosen interpretation of source language types, this criterion requires that all source language equivalences are preserved. We demonstrate that the criterion weeds out erroneous gradualizations that nevertheless satisfy the refined criteria. At the same time, we demonstrate that the criterion is realistic by reporting on a mechanized proof that the property holds for a standard example: GTLCµ, the natural gradualization of STLCµ, the simply typed lambda-calculus with equirecursive types. We argue thus that the criterion is useful for understanding, evaluating, and guiding the design of gradual languages, particularly those which are intended to preserve source language guarantees in a rich way.},
	author = {Jacobs, Koen and Timany, Amin and Devriese, Dominique},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434288},
	doi = {10.1145/3434288},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {fully abstract embedding,gradual typing,fully abstract compilation},
	month = jan,
	number = {POPL},
	title = {Fully Abstract from Static to Gradual},
	volume = {5},
	year = {2021}
}

@article{jacobs21_parad_probab_progr,
	abstract = {Abstract Probabilistic programming languages allow programmers to write down conditional probability distributions that represent statistical and machine learning models as programs that use observe statements. These programs are run by accumulating likelihood at each observe statement, and using the likelihood to steer random choices and weigh results with inference algorithms such as importance sampling or MCMC. We argue that naive likelihood accumulation does not give desirable semantics and leads to paradoxes when an observe statement is used to condition on a measure-zero event, particularly when the observe statement is executed conditionally on random data. We show that the paradoxes disappear if we explicitly model measure-zero events as a limit of positive measure events, and that we can execute these type of probabilistic programs by accumulating infinitesimal probabilities rather than probability densities. Our extension improves probabilistic programming languages as an executable notation for probability distributions by making it more well-behaved and more expressive, by allowing the programmer to be explicit about which limit is intended when conditioning on an event of measure zero.},
	author = {Jacobs, Jules},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434339},
	doi = {10.1145/3434339},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {probabilistic programming},
	month = jan,
	number = {POPL},
	title = {Paradoxes of Probabilistic Programming: And How to Condition on Events of Measure Zero with Infinitesimal Probabilities},
	volume = {5},
	year = {2021}
}

@article{jacobs22_connec_graph,
	author = {Jacobs, Jules and Balzer, Stephanie and Krebbers, Robbert},
	title = {Connectivity Graphs: A Method for Proving Deadlock Freedom Based on Separation Logic},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498662},
	doi = {10.1145/3498662},
	abstract = {We introduce the notion of a connectivity graph—an abstract representation of the topology of concurrently interacting entities, which allows us to encapsulate generic principles of reasoning about deadlock freedom. Connectivity graphs are parametric in their vertices (representing entities like threads and channels) and their edges (representing references between entities) with labels (representing interaction protocols). We prove deadlock and memory leak freedom in the style of progress and preservation and use separation logic as a meta theoretic tool to treat connectivity graph edges and labels substructurally. To prove preservation locally, we distill generic separation logic rules for local graph transformations that preserve acyclicity of the connectivity graph. To prove global progress locally, we introduce a waiting induction principle for acyclic connectivity graphs. We mechanize our results in Coq, and instantiate our method with a higher-order binary session-typed language to obtain the first mechanized proof of deadlock and leak freedom.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {1},
	numpages = {33},
	keywords = {deadlock freedom, session types, Message passing, concurrency, separation logic, Coq, graphs}
}

@article{jacobs22_multip_gv,
	author = {Jacobs, Jules and Balzer, Stephanie and Krebbers, Robbert},
	title = {Multiparty GV: Functional Multiparty Session Types with Certified Deadlock Freedom},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3547638},
	doi = {10.1145/3547638},
	abstract = {Session types have recently been integrated with functional languages, bringing message-passing concurrency to functional programming. Channel endpoints then become first-class and can be stored in data structures, captured in closures, and sent along channels. Representatives of the GV (Wadler's "Good Variation") session type family are of particular appeal because they not only assert session fidelity but also deadlock freedom, inspired by a Curry-Howard correspondence to linear logic. A restriction of current versions of GV, however, is the focus on binary sessions, limiting concurrent interactions within a session to two participants. This paper introduces Multiparty GV (MPGV), a functional language with multiparty session types, allowing concurrent interactions among several participants. MPGV upholds the strong guarantees of its ancestor GV, including deadlock freedom, despite session interleaving and delegation. MPGV has a novel redirecting construct for modular programming with first-class endpoints, thanks to which we give a type-preserving translation from binary session types to MPGV to show that MPGV is strictly more general than binary GV. All results in this paper have been mechanized using the Coq proof assistant.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {107},
	numpages = {30},
	keywords = {message-passing concurrency, deadlock freedom, Session types}
}

@article{jacobs23_fast_coalg_bisim_minim,
	author = {Jacobs, Jules and Wi\ss{}mann, Thorsten},
	title = {Fast Coalgebraic Bisimilarity Minimization},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571245},
	doi = {10.1145/3571245},
	abstract = {Coalgebraic bisimilarity minimization generalizes classical automaton minimization to a large class of automata whose transition structure is specified by a functor, subsuming strong, weighted, and probabilistic bisimilarity. This offers the enticing possibility of turning bisimilarity minimization into an off-the-shelf technology, without having to develop a new algorithm for each new type of automaton. Unfortunately, there is no existing algorithm that is fully general, efficient, and able to handle large systems. We present a generic algorithm that minimizes coalgebras over an arbitrary functor in the category of sets as long as the action on morphisms is sufficiently computable. The functor makes at most O(m logn) calls to the functor-specific action, where n is the number of states and m is the number of transitions in the coalgebra. While more specialized algorithms can be asymptotically faster than our algorithm (usually by a factor of (m/n)), our algorithm is especially well suited to efficient implementation, and our tool often uses much less time and memory on existing benchmarks, and can handle larger automata, despite being more generic.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {52},
	numpages = {28},
	keywords = {Coalgebra, Monotone Neighbourhoods, Partition Refinement}
}

@article{jacobs23_higher_order_leak_deadl_free_locks,
	author = {Jacobs, Jules and Balzer, Stephanie},
	title = {Higher-Order Leak and Deadlock Free Locks},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571229},
	doi = {10.1145/3571229},
	abstract = {Reasoning about concurrent programs is challenging, especially if data is shared among threads. Program correctness can be violated by the presence of data races—whose prevention has been a topic of concern both in research and in practice. The Rust programming language is a prime example, putting the slogan fearless concurrency in practice by not only employing an ownership-based type system for memory management, but also using its type system to enforce mutual exclusion on shared data. Locking, unfortunately, not only comes at the price of deadlocks but shared access to data may also cause memory leaks. This paper develops a theory of deadlock and leak freedom for higher-order locks in a shared memory concurrent setting. Higher-order locks allow sharing not only of basic values but also of other locks and channels, and are themselves first-class citizens. The theory is based on the notion of a sharing topology, administrating who is permitted to access shared data at what point in the program. The paper first develops higher-order locks for acyclic sharing topologies, instantiated in a λ-calculus with higher-order locks and message-passing concurrency. The paper then extends the calculus to support circular dependencies with dynamic lock orders, which we illustrate with a dynamic version of Dijkstra’s dining philosophers problem. Well-typed programs in the resulting calculi are shown to be free of deadlocks and memory leaks, with proofs mechanized in the Coq proof assistant.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {36},
	numpages = {31},
	keywords = {Concurrency, Memory Leak, Higher-Order Lock, Deadlock}
}

@misc{jadhav12_modif_quine_mcclus_method,
	doi = {10.48550/ARXIV.1203.2289},
	url = {https://arxiv.org/abs/1203.2289},
	author = {Jadhav, Vitthal and Buchade, Amar},
	keywords = {boolean simplification, boolean logic},
	title = {Modified Quine-McCluskey Method},
	publisher = {arXiv},
	year = {2012},
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{jaffar06_clp_method_compos_inter_predic_abstr,
	doi = {10.1007/11609773_2},
	keywords = {abstract interpretation, predicate abstraction},
	author = {Jaffar, Joxan and Santosa, Andrew E. and Voicu, R{\u{a}}zvan},
	editor = {Emerson, E. Allen and Namjoshi, Kedar S.},
	title = "A CLP Method for Compositional and Intermittent Predicate Abstraction",
	booktitle = "Verification, Model Checking, and Abstract Interpretation",
	year = "2006",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "17--32",
	abstract = "We present an implementation of symbolic reachability analysis with the features of compositionality, and intermittent abstraction, in the sense of pefrorming approximation only at selected program points, if at all. The key advantages of compositionality are well known, while those of intermittent abstraction are that the abstract domain required to ensure convergence of the algorithm can be minimized, and that the cost of performing abstractions, now being intermittent, is reduced.",
	isbn = "978-3-540-31622-0"
}

@INPROCEEDINGS{jain08_optim_quine_mcclus_method_minim_boolean_expres,
	keywords = {boolean simplification, boolean logic},
	author = {Jain, Tarun Kumar and Kushwaha, D. S. and Misra, A. K.},
	booktitle = {Fourth International Conference on Autonomic and Autonomous Systems (ICAS'08)},
	title = {Optimization of the Quine-McCluskey Method for the Minimization of the Boolean Expressions},
	year = {2008},
	volume = {},
	number = {},
	pages = {165-168},
	doi = {10.1109/ICAS.2008.11}
}

@article{jang22_mundef,
	author = {Jang, Junyoung and G\'{e}lineau, Samuel and Monnier, Stefan and Pientka, Brigitte},
	title = {Mundefinedbius: Metaprogramming Using Contextual Types: The Stage Where System f Can Pattern Match on Itself},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498700},
	doi = {10.1145/3498700},
	abstract = {We describe the foundation of the metaprogramming language, Mundefinedbius, which supports the generation of polymorphic code and, more importantly, the analysis of polymorphic code via pattern matching. Mundefinedbius has two main ingredients: 1) we exploit contextual modal types to describe open code together with the context in which it is meaningful. In Mundefinedbius, open code can depend on type and term variables (level 0) whose values are supplied at a later stage, as well as code variables (level 1) that stand for code templates supplied at a later stage. This leads to a multi-level modal lambda-calculus that supports System-F style polymorphism and forms the basis for polymorphic code generation. 2) we extend the multi-level modal lambda-calculus to support pattern matching on code. As pattern matching on polymorphic code may refine polymorphic type variables, we extend our type-theoretic foundation to generate and track typing constraints that arise. We also give an operational semantics and prove type preservation. Our multi-level modal foundation for Mundefinedbius provides the appropriate abstractions for both generating and pattern matching on open code without committing to a concrete representation of variable binding and contexts. Hence, our work is a step towards building a general type-theoretic foundation for multi-staged metaprogramming that, on the one hand, enforces strong type guarantees and, on the other hand, makes it easy to generate and manipulate code. This will allow us to exploit the full potential of metaprogramming without sacrificing the reliability of and trust in the code we are producing and running.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {39},
	numpages = {27},
	keywords = {Metaprogramming, Polymorphism, Type Systems, Contextual Types}
}

@InProceedings{jansen14_gener_induc_predic_symbol_execut,
	doi = {10.1007/978-3-319-09108-2_5},
	keywords = {symbolic execution, predicated execution},
	author = {Jansen, Christina and Göbe, Florian and Noll, Thomas},
	editor = {Giese, Holger and König, Barbara},
	title = "Generating Inductive Predicates for Symbolic Execution of Pointer-Manipulating Programs",
	booktitle = "Graph Transformation",
	year = "2014",
	publisher = "Springer International Publishing",
	address = "Cham",
	pages = "65--80",
	abstract = "We study the relationship between two abstraction approaches for pointer programs, Separation Logic and hyperedge replacement grammars. Both employ inductively defined predicates and replacement rules, respectively, for representing (dynamic) data structures, involving abstraction and concretisation operations for symbolic execution. In the Separation Logic case, automatically generating a complete set of such operations requires certain properties of predicates, which are currently implicitly described and manually established. In contrast, the structural properties that guarantee correctness of grammar abstraction are decidable and automatable. Using a property-preserving translation we argue that it is exactly the logic counterparts of those properties that ensure the direct applicability of predicate definitions for symbolic execution.",
	isbn = "978-3-319-09108-2"
}

@article{jaskelioff15_repres_theor_secon_order_funct,
	title = {A Representation Theorem for Second-Order Functionals},
	volume = {25},
	DOI = {10.1017/S0956796815000088},
	journal = {Journal of Functional Programming},
	publisher = {Cambridge University Press},
	author = {Jaskelioff, Mauro and O'Connor, Russell},
	year = {2015},
	pages = {e13}
}

@article{jeffrey22_leaky_semic,
	author = {Jeffrey, Alan and Riely, James and Batty, Mark and Cooksey, Simon and Kaysin, Ilya and Podkopaev, Anton},
	title = {The Leaky Semicolon: Compositional Semantic Dependencies for Relaxed-Memory Concurrency},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498716},
	doi = {10.1145/3498716},
	abstract = {Program logics and semantics tell a pleasant story about sequential composition: when executing (S1;S2), we first execute S1 then S2. To improve performance, however, processors execute instructions out of order, and compilers reorder programs even more dramatically. By design, single-threaded systems cannot observe these reorderings; however, multiple-threaded systems can, making the story considerably less pleasant. A formal attempt to understand the resulting mess is known as a “relaxed memory model.” Prior models either fail to address sequential composition directly, or overly restrict processors and compilers, or permit nonsense thin-air behaviors which are unobservable in practice. To support sequential composition while targeting modern hardware, we enrich the standard event-based approach with preconditions and families of predicate transformers. When calculating the meaning of (S1; S2), the predicate transformer applied to the precondition of an event e from S2 is chosen based on the set of events in S1 upon which e depends. We apply this approach to two existing memory models.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {54},
	numpages = {30},
	keywords = {Arm8, Preconditions, C11, Relaxed Memory Models, Predicate Transformers, Multi-Copy Atomicity, Thin-Air Reads, Pomsets, Compiler Optimizations, Concurrency}
}

@article{jia22_seman_variat_quant_progr,
	author = {Jia, Xiaodong and Kornell, Andre and Lindenhovius, Bert and Mislove, Michael and Zamdzhiev, Vladimir},
	title = {Semantics for Variational Quantum Programming},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498687},
	doi = {10.1145/3498687},
	abstract = {We consider a programming language that can manipulate both classical and quantum information. Our language is type-safe and designed for variational quantum programming, which is a hybrid classical-quantum computational paradigm. The classical subsystem of the language is the Probabilistic FixPoint Calculus (PFPC), which is a lambda calculus with mixed-variance recursive types, term recursion and probabilistic choice. The quantum subsystem is a first-order linear type system that can manipulate quantum information. The two subsystems are related by mixed classical/quantum terms that specify how classical probabilistic effects are induced by quantum measurements, and conversely, how classical (probabilistic) programs can influence the quantum dynamics. We also describe a sound and computationally adequate denotational semantics for the language. Classical probabilistic effects are interpreted using a recently-described commutative probabilistic monad on DCPO. Quantum effects and resources are interpreted in a category of von Neumann algebras that we show is enriched over (continuous) domains. This strong sense of enrichment allows us to develop novel semantic methods that we use to interpret the relationship between the quantum and classical probabilistic effects. By doing so we provide a very detailed denotational analysis that relates domain-theoretic models of classical probabilistic programming to models of quantum programming.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {26},
	numpages = {31},
	keywords = {Variational Quantum Programming, Probabilistic Programming, Semantics}
}

@article{jiang15_polyh_patter,
	abstract = {We study the design and optimization of polyhedral patterns, which are patterns of planar polygonal faces on freeform surfaces. Working with polyhedral patterns is desirable in architectural geometry and industrial design. However, the classical tiling patterns on the plane must take on various shapes in order to faithfully and feasibly approximate curved surfaces. We define and analyze the deformations these tiles must undertake to account for curvature, and discover the symmetries that remain invariant under such deformations. We propose a novel method to regularize polyhedral patterns while maintaining these symmetries into a plethora of aesthetic and feasible patterns.},
	author = {Jiang, Caigui and Tang, Chengcheng and Vaxman, Amir and Wonka, Peter and Pottmann, Helmut},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/2816795.2818077},
	doi = {10.1145/2816795.2818077},
	issn = {0730-0301},
	journaltitle = {ACM Trans. Graph.},
	month = oct,
	number = {6},
	title = {Polyhedral Patterns},
	volume = {34},
	year = {2015}
}

@inproceedings{jiang21_fluid,
	abstract = {In this work, we introduce the Fluid framework, a set of language, compiler and runtime extensions that allow for the expression of regions within which dataflow dependencies can be approximated in a disciplined manner. Our framework allows the eager execution of dependent tasks before their inputs have finalized in order to capitalize on situations where an eagerly-consumed input has a high probability of sufficiently resembling the value or structure of the final value that would have been produced in a conservative/precise execution schedule. We introduce controlled access to the early consumption of intermediate values and provide hooks for user-specified quality assurance mechanisms that can automatically enforce re-execution of eagerly-executed tasks if their output values do not meet heuristic expectations. Our experimental analysis indicates that the fluidized versions of the applications bring 22.2% average execution time improvements, over their original counterparts, under the default values of our fluidization parameters. The Fluid approach is largely orthogonal to approaches that aim to reduce the task effort itself and we show that utilizing the Fluid framework can yield benefits for both originally precise and originally approximate versions of computation.},
	author = {Jiang, Huaipan and Zhang, Haibo and Tang, Xulong and Govindaraj, Vineetha and Sampson, Jack and Kandemir, Mahmut Taylan and Zhang, Danfeng},
	location = {Virtual, Canada},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3453483.3454042},
	booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
	doi = {10.1145/3453483.3454042},
	isbn = {9781450383912},
	keywords = {Approximate Computing,Eager Execution},
	pages = {252--267},
	series = {PLDI 2021},
	title = {Fluid: A Framework for Approximate Concurrency via Controlled Dependency Relaxation},
	year = {2021}
}

@inbook{jifeng03_algeb_approac_veril_progr,
	author = {Jifeng, He},
	editor = {Aichernig, Bernhard K. and Maibaum, Tom},
	location = {Berlin, Heidelberg},
	publisher = {Springer},
	url = {https://doi.org/10.1007/978-3-540-40007-3_5},
	booktitle = {Formal Methods at the Crossroads. From Panacea to Foundational Support: 10th Anniversary Colloquium of UNU/IIST, the International Institute for Software Technology of The United Nations University, Lisbon, Portugal, March 18-20, 2002. Revised Papers},
	doi = {10.1007/978-3-540-40007-3_5},
	isbn = {978-3-540-40007-3},
	pages = {65--80},
	title = {An Algebraic Approach to the {Verilog} Programming},
	year = {2003}
}

@inproceedings{jifeng93_towar,
	abstract = {This paper shows how to compile a program written in a subset of occam into a normal form suitable for further processing into a netlist of components which may be loaded into a Field-Programmable Gate Array (FPGA). A simple state-machine model is adopted for specifying the behaviour of a synchronous circuit where the observable includes the state of the control path and the data path of the circuit. We identify the behaviour of a circuit with a program consisting of a very restricted subset of occam. Algebraic laws are used to facilitate the transformation from a program into a normal form. The compiling specification is presented as a set of theorems that must be proved correct with respect to these laws. A rapid prototype compiler in the form of a logic program may be implemented from these theorems.},
	author = {Jifeng, He and Page, Ian and Bowen, Jonathan},
	editor = {Milne, George J. and Pierre, Laurence},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Correct Hardware Design and Verification Methods},
	isbn = {978-3-540-70655-7},
	pages = {214--225},
	title = {Towards a provably correct hardware implementation of occam},
	year = {1993}
}

@inproceedings{jifeng94_simul,
	author = {Jifeng, He and Jianping, Zheng},
	organization = {Springer},
	booktitle = {Formal Techniques in Real-Time and Fault-Tolerant Systems},
	pages = {336--350},
	title = {Simulation approach to provably correct hardware compilation},
	year = {1994}
}

@article{jochems23_higher_order_msl_horn_const,
	author = {Jochems, Jerome and Jones, Eddie and Ramsay, Steven},
	title = {Higher-Order MSL Horn Constraints},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571262},
	doi = {10.1145/3571262},
	abstract = {The monadic shallow linear (MSL) class is a decidable fragment of first-order Horn clauses that was discovered and rediscovered around the turn of the century, with applications in static analysis and verification. We propose a new class of higher-order Horn constraints which extend MSL to higher-order logic and develop a resolution-based decision procedure. Higher-order MSL Horn constraints can quite naturally capture the complex patterns of call and return that are possible in higher-order programs, which make them well suited to higher-order program verification. In fact, we show that the higher-order MSL satisfiability problem and the HORS model checking problem are interreducible, so that higher-order MSL can be seen as a constraint-based approach to higher-order model checking. Finally, we describe an implementation of our decision procedure and its application to verified socket programming.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {69},
	numpages = {31},
	keywords = {higher-order program verification, constraint-based program analysis}
}

@inproceedings{johann22_deep_induc_rules_gadts,
	author = {Johann, Patricia and Ghiorzi, Enrico},
	title = {(Deep) Induction Rules for GADTs},
	year = {2022},
	isbn = {9781450391825},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497775.3503680},
	doi = {10.1145/3497775.3503680},
	abstract = {Deep data types are those that are constructed from other data types, including, possibly, themselves. In this case, they are said to be truly nested. Deep induction is an extension of structural induction that traverses all of the structure in a deep data type, propagating predicates on its primitive data throughout the entire structure. Deep induction can be used to prove properties of nested types, including truly nested types, that cannot be proved via structural induction. In this paper we show how to extend deep induction to GADTs that are not truly nested GADTs. This opens the way to incorporating automatic generation of (deep) induction rules for them into proof assistants. We also show that the techniques developed in this paper do not suffice for extending deep induction to truly nested GADTs, so more sophisticated techniques are needed to derive deep induction rules for them.},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {324–337},
	numpages = {14},
	keywords = {GADTs, proof assistants, induction},
	location = {Philadelphia, PA, USA},
	series = {CPP 2022}
}

@incollection{johansson17_autom_theor_explor_inter_theor_provin,
	author = {Johansson, Moa},
	publisher = {Springer International Publishing},
	url = {https://doi.org/10.1007%2F978-3-319-66107-0_1},
	booktitle = {Interactive Theorem Proving},
	doi = {10.1007/978-3-319-66107-0_1},
	pages = {1--11},
	title = {Automated Theory Exploration for Interactive Theorem Proving:},
	year = {2017}
}

@article{jones21_inten_datat_refin,
	abstract = {The pattern-match safety problem is to verify that a given functional program will never crash due to non-exhaustive patterns in its function definitions. We present a refinement type system that can be used to solve this problem. The system extends ML-style type systems with algebraic datatypes by a limited form of structural subtyping and environment-level intersection. We describe a fully automatic, sound and complete type inference procedure for this system which, under reasonable assumptions, is worst-case linear-time in the program size. Compositionality is essential to obtaining this complexity guarantee. A prototype implementation for Haskell is able to analyse a selection of packages from the Hackage database in a few hundred milliseconds.},
	author = {Jones, Eddie and Ramsay, Steven},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434336},
	doi = {10.1145/3434336},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {higher-order program verification,refinement types},
	month = jan,
	number = {POPL},
	title = {Intensional Datatype Refinement: With Application to Scalable Verification of Pattern-Match Safety},
	volume = {5},
	year = {2021}
}

@inproceedings{jones22_cycleq,
	author = {Jones, Eddie and Ong, C.-H. Luke and Ramsay, Steven},
	title = {CycleQ: An Efficient Basis for Cyclic Equational Reasoning},
	year = {2022},
	isbn = {9781450392655},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3519939.3523731},
	doi = {10.1145/3519939.3523731},
	abstract = {We propose a new cyclic proof system for automated, equational reasoning about the behaviour of pure functional programs. The key to the system is the way in which cyclic proofs and equational reasoning are mediated by the use of contextual substitution as a cut rule. We show that our system, although simple, already subsumes several of the approaches to implicit induction variously known as “inductionless induction”, “rewriting induction”, and “proof by consistency”. By restricting the form of the traces, we show that global correctness in our system can be verified incrementally, taking advantage of the well-known size-change principle, which leads to an efficient implementation of proof search. Our CycleQ tool, implemented as a GHC plugin, shows promising results on a number of standard benchmarks.},
	booktitle = {Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
	pages = {395–409},
	numpages = {15},
	keywords = {cyclic proofs, proof theory},
	location = {San Diego, CA, USA},
	series = {PLDI 2022}
}

@book{jones86_system_vdm,
	author = {Jones, Cliff B.},
	title = {Systematic software development using {VDM}},
	series = {Prentice Hall International Series in Computer Science},
	publisher = {Prentice Hall},
	year = {1986},
	isbn = {978-0-13-880725-2},
	timestamp = {Thu, 14 Apr 2011 14:43:25 +0200},
	biburl = {https://dblp.org/rec/books/daglib/0068091.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{josipovic21_buffer_placem_sizin_high_perfor_dataf_circuit,
	author = {Josipović, Lana and Sheikhha, Shabnam and Guerrieri, Andrea and Ienne, Paolo and Cortadella, Jordi},
	title = {Buffer Placement and Sizing for High-Performance Dataflow Circuits},
	year = {2021},
	issue_date = {March 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {15},
	number = {1},
	issn = {1936-7406},
	url = {https://doi.org/10.1145/3477053},
	doi = {10.1145/3477053},
	abstract = {Commercial high-level synthesis tools typically produce statically scheduled circuits. Yet, effective C-to-circuit conversion of arbitrary software applications calls for dataflow circuits, as they can handle efficiently variable latencies (e.g., caches), unpredictable memory dependencies, and irregular control flow. Dataflow circuits exhibit an unconventional property: registers (usually referred to as “buffers”) can be placed anywhere in the circuit without changing its semantics, in strong contrast to what happens in traditional datapaths. Yet, although functionally irrelevant, this placement has a significant impact on the circuit’s timing and throughput. In this work, we show how to strategically place buffers into a dataflow circuit to optimize its performance. Our approach extracts a set of choice-free critical loops from arbitrary dataflow circuits and relies on the theory of marked graphs to optimize the buffer placement and sizing. Our performance optimization model supports important high-level synthesis features such as pipelined computational units, units with variable latency and throughput, and if-conversion. We demonstrate the performance benefits of our approach on a set of dataflow circuits obtained from imperative code.},
	journal = {ACM Trans. Reconfigurable Technol. Syst.},
	month = {nov},
	articleno = {4},
	numpages = {32},
	keywords = {Petri nets, Dataflow circuits, performance optimization, high-level synthesis}
}

@article{josipović17_out_order_load_store_queue_spatial_comput,
	author = {Josipović, Lana and Brisk, Philip and Ienne, Paolo},
	title = {An Out-of-Order Load-Store Queue for Spatial Computing},
	year = {2017},
	issue_date = {October 2017},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {16},
	number = {5s},
	issn = {1539-9087},
	url = {https://doi.org/10.1145/3126525},
	doi = {10.1145/3126525},
	abstract = {The efficiency of spatial computing depends on the ability to achieve maximal parallelism. This necessitates memory interfaces that can correctly handle memory accesses that arrive in arbitrary order while still respecting data dependencies and ensuring appropriate ordering for semantic correctness. However, a typical memory interface for out-of-order processors (i.e., a load-store queue) cannot immediately meet these requirements: a different allocation policy is needed to achieve out-of-order execution in spatial systems that naturally omit the notion of sequential program order, a fundamental piece of information for correct execution. We show a novel and practical way to organize the allocation for an out-of-order load-store queue for spatial computing. The main idea is to dynamically allocate groups of memory accesses (depending on the dynamic behavior of the application), where the access order within the group is statically predetermined (for instance by a high-level synthesis tool). We detail the construction of our load-store queue and demonstrate on a few practical cases its advantages over standard accelerator-memory interfaces.},
	journal = {ACM Trans. Embed. Comput. Syst.},
	month = {sep},
	articleno = {125},
	numpages = {19},
	keywords = {allocation, spatial computing, Load-store queue, dynamic scheduling}
}

@inproceedings{josipović18_dynam_sched_high_synth,
	author = {Josipović, Lana and Ghosal, Radhika and Ienne, Paolo},
	location = {Monterey, CALIFORNIA, USA},
	publisher = {ACM},
	booktitle = {Proceedings of the 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	doi = {10.1145/3174243.3174264},
	isbn = {978-1-4503-5614-5},
	keywords = {compiler,dynamically scheduled circuits,high-level synthesis,pipelining},
	pages = {127--136},
	series = {FPGA '18},
	title = {Dynamically Scheduled High-level Synthesis},
	year = {2018}
}

@inproceedings{josipović20_buffer_placem_sizin_high_perfor_dataf_circuit,
	author = {Josipović, Lana and Sheikhha, Shabnam and Guerrieri, Andrea and Ienne, Paolo and Cortadella, Jordi},
	location = {Seaside, CA, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3373087.3375314},
	booktitle = {The 2020 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	doi = {10.1145/3373087.3375314},
	isbn = {9781450370998},
	keywords = {buffers,high-level synthesis,dataflow circuits,timing optimization},
	pages = {186--196},
	series = {FPGA '20},
	title = {Buffer Placement and Sizing for High-Performance Dataflow Circuits},
	year = {2020}
}

@ARTICLE{josipović21_synth_gener_purpos_code_into,
	author = {Josipović, Lana and Guerrieri, Andrea and Ienne, Paolo},
	journal = {IEEE Circuits and Systems Magazine},
	title = {Synthesizing General-Purpose Code Into Dynamically Scheduled Circuits},
	year = {2021},
	volume = {21},
	number = {2},
	pages = {97-118},
	doi = {10.1109/MCAS.2021.3071631}
}

@ARTICLE{josipović22_from_c_c_code_high,
	author = {Josipović, Lana and Guerrieri, Andrea and Ienne, Paolo},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	title = {From C/C++ Code to High-Performance Dataflow Circuits},
	year = {2022},
	volume = {41},
	number = {7},
	pages = {2142-2155},
	doi = {10.1109/TCAD.2021.3105574}
}

@INPROCEEDINGS{josipović22_resour_sharin_dataf_circuit,
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9786084},
	author = {Josipović, Lana and Marmet, Axel and Guerrieri, Andrea and Ienne, Paolo},
	booktitle = {2022 IEEE 30th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
	title = {Resource Sharing in Dataflow Circuits},
	year = {2022},
	volume = {},
	number = {},
	pages = {1-9},
	doi = {10.1109/FCCM53951.2022.9786084}
}

@inproceedings{jourdan12_valid_lr_parser,
	abstract = {An LR(1) parser is a finite-state automaton, equipped with a stack, which uses a combination of its current state and one lookahead symbol in order to determine which action to perform next. We present a validator which, when applied to a context-free grammar {\$}{\backslash}mathcal G{\$}and an automaton {\$}{\backslash}mathcal A{\$}, checks that {\$}{\backslash}mathcal A{\$}and {\$}{\backslash}mathcal G{\$}agree. Validating the parser provides the correctness guarantees required by verified compilers and other high-assurance software that involves parsing. The validation process is independent of which technique was used to construct {\$}{\backslash}mathcal A{\$}. The validator is implemented and proved correct using the Coq proof assistant. As an application, we build a formally-verified parser for the C99 language.},
	author = {Jourdan, Jacques-Henri and Pottier, François and Leroy, Xavier},
	editor = {Seidl, Helmut},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Programming Languages and Systems},
	isbn = {978-3-642-28869-2},
	pages = {397--416},
	title = {Validating LR(1) Parsers},
	year = {2012}
}

@inproceedings{jourdan15_formal_verif_c_static_analy,
	author = {Jourdan, Jacques-Henri and Laporte, Vincent and Blazy, Sandrine and Leroy, Xavier and Pichardie, David},
	location = {Mumbai, India},
	publisher = {ACM},
	url = {https://doi.org/10.1145/2676726.2676966},
	booktitle = {Proceedings of the 42Nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
	doi = {10.1145/2676726.2676966},
	isbn = {978-1-4503-3300-9},
	keywords = {abstract interpretation,proof assistants,soundness proofs,static analysis},
	pages = {247--259},
	series = {POPL '15},
	title = {A Formally-Verified C Static Analyzer},
	year = {2015}
}

@inproceedings{journault20_combin_reusab_abstr_domain_multil_static_analy,
	keywords = {abstract interpretation testing},
	abstract = {We discuss the design of Mopsa, an ongoing effort to design a novel semantic static analyzer by abstract interpretation. Mopsa strives to achieve a high degree of modularity and extensibility by considering value abstractions for numeric, pointer, objects, arrays, etc. as well as syntax-driven iterators and control-flow abstractions uniformly as domain modules, which offer a unified signature and loose coupling, so that they can be combined and reused at will. Moreover, domains can dynamically rewrite expressions, which simplifies the design of relational abstractions, encourages a design based on layered semantics, and enables domain reuse across different analyses and different languages. We present preliminary applications of Mopsa analyzing simple programs in subsets of the C and Python programming languages, checking them for run-time errors and uncaught exceptions.},
	address = {Cham},
	author = {Journault, Matthieu and Min{\'e}, Antoine and Monat, Rapha{\"e}l and Ouadjaout, Abdelraouf},
	booktitle = {Verified Software. Theories, Tools, and Experiments},
	editor = {Chakraborty, Supratik and Navas, Jorge A.},
	isbn = {978-3-030-41600-3},
	pages = {1--18},
	publisher = {Springer International Publishing},
	title = {Combinations of Reusable Abstract Domains for a Multilingual Static Analyzer},
	year = {2020}
}

@inproceedings{jung04_hscgd,
	author = {Jung, Hyunuk and Ha, Soonhoi},
	title = {Hardware Synthesis from Coarse-Grained Dataflow Specification for Fast HW/SW Cosynthesis},
	year = {2004},
	isbn = {158113 9373},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/1016720.1016730},
	abstract = {This paper concerns automatic hardware synthesis from data flow graph (DFG) specification for fast HW/SW cosynthesis. A node in DFG represents a coarse grain block such as FIR and DCT and a port in a block may consume multiple data samples per invocation, which distinguishes our approach from behavioral synthesis and complicates the problem. In the presented design methodology, a dataflow graph with specified algorithm can be mapped to various hardware structures according to the resource allocation and schedule information. This simplifies the management of the area/performance tradeoff in hardware design and widens the design space of hardware implementation of a dataflow graph compared with the previous approaches. Through experiments with some examples, the usefulness of the proposed technique is demonstrated.},
	booktitle = {Proceedings of the 2nd IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis},
	pages = {24–29},
	numpages = {6},
	keywords = {data-flow, control-flow},
	location = {Stockholm, Sweden},
	series = {CODES+ISSS '04}
}

@article{k22_solvin_const_horn_claus_algeb,
	author = {K, Hari Govind V and Shoham, Sharon and Gurfinkel, Arie},
	title = {Solving Constrained Horn Clauses modulo Algebraic Data Types and Recursive Functions},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498722},
	doi = {10.1145/3498722},
	abstract = {This work addresses the problem of verifying imperative programs that manipulate data structures, e.g., Rust programs. Data structures are usually modeled by Algebraic Data Types (ADTs) in verification conditions. Inductive invariants of such programs often require recursively defined functions (RDFs) to represent abstractions of data structures. From the logic perspective, this reduces to solving Constrained Horn Clauses (CHCs) modulo both ADT and RDF. The underlying logic with RDFs is undecidable. Thus, even verifying a candidate inductive invariant is undecidable. Similarly, IC3-based algorithms for solving CHCs lose their progress guarantee: they may not find counterexamples when the program is unsafe. We propose a novel IC3-inspired algorithm Racer for solving CHCs modulo ADT and RDF (i.e., automatically synthesizing inductive invariants, as opposed to only verifying them as is done in deductive verification). Racer ensures progress despite the undecidability of the underlying theory, and is guaranteed to terminate with a counterexample for unsafe programs. It works with a general class of RDFs over ADTs called catamorphisms. The key idea is to represent catamorphisms as both CHCs, via relationification, and RDFs, using novel abstractions. Encoding catamorphisms as CHCs allows learning inductive properties of catamorphisms, as well as preserving unsatisfiabilty of the original CHCs despite the use of RDF abstractions, whereas encoding catamorphisms as RDFs allows unfolding the recursive definition, and relying on it in solutions. Abstractions ensure that the underlying theory remains decidable. We implement our approach in Z3 and show that it works well in practice.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {60},
	numpages = {29},
	keywords = {Formal verification, Algebraic Data Types, Recursive Functions, Model Checking}
}

@article{kallas23_execut_micros_applic_server_correc,
	author = {Kallas, Konstantinos and Zhang, Haoran and Alur, Rajeev and Angel, Sebastian and Liu, Vincent},
	title = {Executing Microservice Applications on Serverless, Correctly},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571206},
	doi = {10.1145/3571206},
	abstract = {While serverless platforms substantially simplify the provisioning, configuration, and management of cloud applications, implementing correct services on top of these platforms can present significant challenges to programmers. For example, serverless infrastructures introduce a host of failure modes that are not present in traditional deployments. Individual serverless instances can fail while others continue to make progress, correct but slow instances can be killed by the cloud provider as part of resource management, and providers will often respond to such failures by re-executing requests. For functions with side-effects, these scenarios can create behaviors that are not observable in serverful deployments. In this paper, we propose mu2sls, a framework for implementing microservice applications on serverless using standard Python code with two extra primitives: transactions and asynchronous calls. Our framework orchestrates user-written services to address several challenges, such as failures and re-executions, and provides formal guarantees that the generated serverless implementations are correct. To that end, we present a novel service specification abstraction and formalization of serverless implementations that facilitate reasoning about the correctness of a given application’s serverless implementation. This formalization forms the basis of the mu2sls prototype, which we then use to develop a few real-world microservice applications and show that the performance of the generated serverless implementations achieves significant scalability (3-5\texttimes{} the throughput of a sequential implementation) while providing correctness guarantees in the context of faults, re-execution, and concurrency.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {13},
	numpages = {29},
	keywords = {microservices, transactions, stateful serverless}
}

@article{kam76_gdfaia,
	keywords = {data-flow},
	author = {Kam, John B. and Ullman, Jeffrey D.},
	title = {Global Data Flow Analysis and Iterative Algorithms},
	year = {1976},
	issue_date = {Jan. 1976},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {23},
	number = {1},
	issn = {0004-5411},
	doi = {10.1145/321921.321938},
	abstract = {Kildall has developed data propagation algorithms for code optimization in a general lattice theoretic framework. In another direction, Hecht and Ullman gave a strong upper bound on the number of iterations required for propagation algorithms when the data is represented by bit vectors and depth-first ordering of the flow graph is used. The present paper combines the ideas of these two papers by considering conditions under which the bound of Hecht and Ullman applies to the depth-first version of Kildall's general data propagation algorithm. It is shown that the following condition is necessary and sufficient: Let undefined and g be any two functions which could be associated with blocks of a flow graph, let x be an arbitrary lattice element, and let 0 be the lattice zero. Then (*) (∀undefined,g,x) [undefinedg(0) ≥ g(0) ∧ undefined(x) ∧ x]. Then it is shown that several of the particular instances of the techniques Kildall found useful do not meet condition (*).},
	journal = {J. ACM},
	month = {jan},
	pages = {158–171},
	numpages = {14}
}

@article{kammar22_fully_abstr_model_effec_calcul,
	abstract = {We present a construction which, under suitable assumptions, takes a model of Moggi’s computational λ-calculus with sum types, effect operations and primitives, and yields a model that is adequate and fully abstract. The construction, which uses the theory of fibrations, categorical glueing, ⊤⊤-lifting, and ⊤⊤-closure, takes inspiration from O’Hearn &amp; Riecke’s fully abstract model for PCF. Our construction can be applied in the category of sets and functions, as well as the category of diffeological spaces and smooth maps and the category of quasi-Borel spaces, which have been studied as semantics for differentiable and probabilistic programming.},
	author = {Kammar, Ohad and Katsumata, Shin-ya and Saville, Philip},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3498705},
	doi = {10.1145/3498705},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {call-by-value,monad,O'Hearn &amp; Riecke,full abstraction,fibration},
	month = jan,
	number = {POPL},
	title = {Fully Abstract Models for Effectful λ-Calculi via Category-Theoretic Logical Relations},
	volume = {6},
	year = {2022}
}

@inproceedings{kan22_certis,
	author = {Kan, Shuanglong and Lin, Anthony Widjaja and R\"{u}mmer, Philipp and Schrader, Micha},
	title = {CertiStr: A Certified String Solver},
	year = {2022},
	isbn = {9781450391825},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497775.3503691},
	doi = {10.1145/3497775.3503691},
	abstract = {Theories over strings are among the most heavily researched logical theories in the SMT community in the past decade, owing to the error-prone nature of string manipulations, which often leads to security vulnerabilities (e.g. cross-site scripting and code injection). The majority of the existing decision procedures and solvers for these theories are themselves intricate; they are complicated algorithmically, and also have to deal with a very rich vocabulary of operations. This has led to a plethora of bugs in implementation, which have for instance been discovered through fuzzing. In this paper, we present CertiStr, a certified implementation of a string constraint solver for the theory of strings with concatenation and regular constraints. CertiStr aims to solve string constraints using a forward-propagation algorithm based on symbolic representations of regular constraints as symbolic automata, which returns three results: sat, unsat, and unknown, and is guaranteed to terminate for the string constraints whose concatenation dependencies are acyclic. The implementation has been developed and proven correct in Isabelle/HOL, through which an effective solver in OCaml was generated. We demonstrate the effectiveness and efficiency of CertiStr against the standard Kaluza benchmark, in which 80.4% tests are in the string constraint fragment of CertiStr. Of these 80.4% tests, CertiStr can solve 83.5% (i.e. CertiStr returns sat or unsat) within 60s.},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {210–224},
	numpages = {15},
	keywords = {SMT solvers, symbolic automata, string theory, Isabelle},
	location = {Philadelphia, PA, USA},
	series = {CPP 2022}
}

@article{kang18_crell,
	author = {Kang, Jeehoon and Kim, Yoonseung and Song, Youngju and Lee, Juneyoung and Park, Sanghoon and Shin, Mark Dongyeon and Kim, Yonghyun and Cho, Sungkeun and Choi, Joonwon and Hur, Chung-Kil and et al.},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3296979.3192377},
	doi = {10.1145/3296979.3192377},
	issn = {0362-1340},
	journaltitle = {SIGPLAN Not.},
	keywords = {verification,translation validation,coq},
	month = jun,
	number = {4},
	pages = {631--645},
	title = {Crellvm: Verified Credible Compilation for Llvm},
	volume = {53},
	year = {2018}
}

@inproceedings{kapp04_autom_correc_sched_contr_flow,
	author = {Kapp, Kai and Sabelfeld, Viktor},
	location = {San Diego, CA, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/996566.996584},
	booktitle = {Proceedings of the 41st Annual Design Automation Conference},
	doi = {10.1145/996566.996584},
	isbn = {1581138288},
	keywords = {static scheduling,synthesis},
	pages = {61--66},
	series = {DAC '04},
	title = {Automatic Correct Scheduling of Control Flow Intensive Behavioral Descriptions in Formal Synthesis},
	year = {2004}
}

@inproceedings{karfa06_formal_verif_method_sched_high_synth,
	author = {Karfa, C and Mandal, C and Sarkar, D and Pentakota, S R. and Reade, Chris},
	location = {Washington, DC, USA},
	publisher = {IEEE Computer Society},
	booktitle = {Proceedings of the 7th International Symposium on Quality Electronic Design},
	doi = {10.1109/ISQED.2006.10},
	isbn = {0-7695-2523-7},
	pages = {71--78},
	series = {ISQED '06},
	title = {A Formal Verification Method of Scheduling in High-level Synthesis},
	year = {2006}
}

@inproceedings{karfa07_hand_verif_high_synth,
	author = {Karfa, Chandan and Sarkar, Dipankar and Mandal, Chittaranjan and Reade, Chris},
	location = {Stresa-Lago Maggiore, Italy},
	publisher = {ACM},
	url = {https://doi.org/10.1145/1228784.1228885},
	booktitle = {Proceedings of the 17th ACM Great Lakes Symposium on VLSI},
	doi = {10.1145/1228784.1228885},
	isbn = {978-1-59593-605-9},
	keywords = {FSMD,verification,high-level synthesis,equivalence checking,translation validation},
	pages = {429--434},
	series = {GLSVLSI '07},
	title = {Hand-in-hand Verification of High-level Synthesis},
	year = {2007}
}

@article{karfa08_equiv_check_method_sched_verif,
	author = {{Karfa}, C. and {Sarkar}, D. and {Mandal}, C. and {Kumar}, P.},
	url = {https://doi.org/10.1109/TCAD.2007.913390},
	doi = {10.1109/TCAD.2007.913390},
	issn = {1937-4151},
	journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {data visualisation;finite state machines;formal specification;formal verification;high level synthesis;scheduling;equivalence-checking method;scheduling verification;high-level synthesis;formal method;behavioral specification;finite state machine-data path model;FSMD;data visualization;code-motion techniques;arithmetic transformation;High level synthesis;Processor scheduling;Automata;Data visualization;Arithmetic;Formal verification;Very large scale integration;Government;Computer science;Calculus;Equivalence checking;finite state machine with data path (FSMD) models;formal verification;high-level synthesis (HLS);scheduling;Formal Verification;Equivalence Checking;FSMD models;High-level Synthesis;Scheduling},
	month = mar,
	number = {3},
	pages = {556--569},
	title = {An Equivalence-Checking Method for Scheduling Verification in High-Level Synthesis},
	volume = {27},
	year = {2008}
}

@article{karfa10_verif_datap_contr_gener_phase,
	author = {{Karfa}, C. and {Sarkar}, D. and {Mandal}, C.},
	url = {https://doi.org/10.1109/TCAD.2009.2035542},
	doi = {10.1109/TCAD.2009.2035542},
	issn = {1937-4151},
	journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {finite state machines;high level synthesis;integrated circuit design;datapath verification;controller generation phase;digital circuits high-level synthesis;formal verification method;datapath interconnection;controller finite state machine description;control assertion pattern;equivalence checking method;pipelined operations;multicycle operations;structured architecture synthesis tool;HLS benchmarks tool;High level synthesis;Digital circuits;Computer bugs;Formal verification;Automata;Scheduling;Control system synthesis;Signal synthesis;Integrated circuit interconnections;Pattern analysis;Controller;datapath;equivalence checking;formal verification;FSM;FSMD models;high-level synthesis;register transfer level},
	month = mar,
	number = {3},
	pages = {479--492},
	title = {Verification of Datapath and Controller Generation Phase in High-Level Synthesis of Digital Circuits},
	volume = {29},
	year = {2010}
}

@article{karfa12_formal_verif_code_motion_techn,
	author = {Karfa, Chandan and Mandal, Chittaranjan and Sarkar, Dipankar},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/2209291.2209303},
	doi = {10.1145/2209291.2209303},
	issn = {1084-4309},
	journaltitle = {ACM Trans. Des. Autom. Electron. Syst.},
	keywords = {high-level synthesis,Formal verification,equivalence checking,model checking,FSMD models,code motion},
	month = jul,
	number = {3},
	title = {Formal Verification of Code Motion Techniques Using Data-Flow-Driven Equivalence Checking},
	volume = {17},
	year = {2012}
}

@article{karimov22_what_decid_linear_loops,
	abstract = {We consider the MSO model-checking problem for simple linear loops, or equivalently discrete-time linear dynamical systems, with semialgebraic predicates (i.e., Boolean combinations of polynomial inequalities on the variables). We place no restrictions on the number of program variables, or equivalently the ambient dimension. We establish decidability of the model-checking problem provided that each semialgebraic predicate either has intrinsic dimension at most 1, or is contained within some three-dimensional subspace. We also note that lifting either of these restrictions and retaining decidability would necessarily require major breakthroughs in number theory.},
	author = {Karimov, Toghrul and Lefaucheux, Engel and Ouaknine, Joël and Purser, David and Varonka, Anton and Whiteland, Markus A. and Worrell, James},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3498727},
	doi = {10.1145/3498727},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {verification,MSO model checking,linear dynamical systems},
	month = jan,
	number = {POPL},
	title = {What’s Decidable about Linear Loops?},
	volume = {6},
	year = {2022}
}

@article{karpenko16_bochv_three_valued_logic_liter_paral,
	keywords = {three-valued logic},
	doi = {10.12775/llp.2016.029},
	url = {https://doi.org/10.12775/llp.2016.029},
	year = {2016},
	month = oct,
	publisher = {Uniwersytet Mikolaja Kopernika/Nicolaus Copernicus University},
	author = {Alexander Karpenko and Natalya Tomova},
	title = {Bochvar's Three-Valued Logic and Literal Paralogics: Their Lattice and Functional Equivalence},
	journal = {Logic and Logical Philosophy}
}

@inproceedings{kasampalis21_languag_param_compil_valid_applic_llvm,
	abstract = {We propose a new design for a Translation Validation (TV) system geared towards practical use with modern optimizing compilers, such as LLVM. Unlike existing TV systems, which are custom-tailored for a particular sequence of transformations and a specific, common language for input and output programs, our design clearly separates the transformation-specific components from the rest of the system, and generalizes the transformation-independent components. Specifically, we present Keq, the first program equivalence checker that is parametric to the input and output language semantics and has no dependence on the transformation between the input and output programs. The Keq algorithm is based on a rigorous formalization, namely cut-bisimulation, and is proven correct. We have prototyped a TV system for the Instruction Selection pass of LLVM, being able to automatically prove equivalence for translations from LLVM IR to the MachineIR used in compiling to x86-64. This transformation uses different input and output languages, and as such has not been previously addressed by the state of the art. An experimental evaluation shows that Keq successfully proves correct the translation of over 90% of 4732 supported functions in GCC from SPEC 2006.},
	author = {Kasampalis, Theodoros and Park, Daejun and Lin, Zhengyao and Adve, Vikram S. and Roşu, Grigore},
	location = {Virtual, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3445814.3446751},
	booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
	doi = {10.1145/3445814.3446751},
	isbn = {9781450383172},
	keywords = {Compilers,Translation Validation,Program Equivalence,Simulation},
	pages = {1004--1019},
	series = {ASPLOS 2021},
	title = {Language-Parametric Compiler Validation with Application to LLVM},
	year = {2021}
}

@inproceedings{katel22_mlir_based_code_gener_gpu_tensor_cores,
	author = {Katel, Navdeep and Khandelwal, Vivek and Bondhugula, Uday},
	title = {MLIR-Based Code Generation for GPU Tensor Cores},
	year = {2022},
	isbn = {9781450391832},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497776.3517770},
	doi = {10.1145/3497776.3517770},
	abstract = {The state-of-the-art in high-performance deep learning today is primarily driven by manually developed libraries optimized and highly tuned by expert programmers using low-level abstractions with significant effort. This effort is often repeated for similar hardware and future ones. In this work, we pursue and evaluate the more modular and reusable approach of using compiler IR infrastructure to generate libraries by encoding all the required optimizations as a sequence of transformations and customized passes on an IR. We believe that until the recent introduction of MLIR (Multi-level intermediate representation), it had been hard to represent and transform computation at various levels of abstraction within a single IR. Using the MLIR infrastructure, we build a transformation and lowering pipeline to automatically generate near-peak performance code for matrix-matrix multiplication (matmul) as well as matmul fused with simple pointwise operators targeting tensor cores on NVIDIA GPUs. On a set of problem sizes ranging from 256 to 16384, our performance evaluation shows that we can obtain performance that is 0.95\texttimes{} to 1.19\texttimes{} and 0.80\texttimes{} to 1.60\texttimes{} of cuBLAS for FP32 and FP16 accumulate respectively on NVIDIA’s Ampere based Geforce 3090 RTX. Furthermore, by allowing the fusion of common pointwise operations with matrix-matrix multiplication, we obtain performance ranging from 0.95\texttimes{} to 1.67\texttimes{} of a cuBLAS-based implementation. Additionally, we present matmul-like examples such as 3-d contraction and batched matmul, which the pipeline can efficiently handle while providing competitive performance. We believe that these results motivate further research and engineering on automatic domain-specific library generation using compiler IR infrastructure for similar specialized accelerators.},
	booktitle = {Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction},
	pages = {117–128},
	numpages = {12},
	keywords = {tensor cores, matrix-matrix multiplication, GPU, MLIR},
	location = {Seoul, South Korea},
	series = {CC 2022}
}

@article{kathail02_pico,
	abstract = {The paper discusses the PICO (program in, chip out) project, a long-range HP Labs research effort that aims to automate the design of optimized, application-specific computing systems - thus enabling the rapid and cost-effective design of custom chips when no adequately specialized, off-the-shelf design is available. PICO research takes a systematic approach to the hierarchical design of complex systems and advances technologies for automatically designing custom nonprogrammable accelerators and VLIW processors. While skeptics often assume that automated design must emulate human designers who invent new solutions to problems, PICO's approach is to automatically pick the most suitable designs from a well-engineered space of designs. Such automation of embedded computer design promises an era of yet more growth in the number and variety of innovative smart products by lowering the barriers of design time, designer availability, and design cost.},
	author = {Kathail, V. and Aditya, S. and Schreiber, R. and Ramakrishna Rau, B. and Cronquist, D. C. and Sivaraman, M.},
	doi = {10.1109/MC.2002.1033026},
	issn = {1558-0814},
	journaltitle = {Computer},
	keywords = {special purpose computers;embedded systems;application specific integrated circuits;parallel machines;parallel architectures;logic CAD;microprocessor chips;PICO project;program in chip out project;HP Labs research;application-specific computing systems;cost-effective design;custom chips;off-the-shelf design;custom nonprogrammable accelerators;VLIW processors;embedded computer design;design cost;automatic custom computer design;VLIW;Costs;Embedded computing;Design automation;Space exploration;Hardware;Design optimization;Computer applications;Design methodology;High performance computing},
	month = sep,
	number = {9},
	pages = {39--47},
	title = {PICO: automatically designing custom computers},
	volume = {35},
	year = {2002}
}

@inproceedings{katsumata20_inter_laws_monad_comon,
	author = {Katsumata, Shin-ya and Rivas, Exequiel and Uustalu, Tarmo},
	title = {Interaction Laws of Monads and Comonads},
	year = {2020},
	isbn = {9781450371049},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3373718.3394808},
	doi = {10.1145/3373718.3394808},
	abstract = {We introduce and study functor-functor and monad-comonad interaction laws as mathematical objects to describe interaction of effectful computations with behaviors of effect-performing machines. Monad-comonad interaction laws are monoid objects of the monoidal category of functor-functor interaction laws. We show that, for suitable generalizations of the concepts of dual and Sweedler dual, the greatest functor resp. monad interacting with a given functor or comonad is its dual while the greatest comonad interacting with a given monad is its Sweedler dual. We relate monad-comonad interaction laws to stateful runners. We show that functor-functor interaction laws are Chu spaces over the category of endofunctors taken with the Day convolution monoidal structure. Hasegawa's glueing endows the category of these Chu spaces with a monoidal structure whose monoid objects are monad-comonad interaction laws.},
	booktitle = {Proceedings of the 35th Annual ACM/IEEE Symposium on Logic in Computer Science},
	pages = {604–618},
	numpages = {15},
	keywords = {monads, applicative},
	location = {Saarbr\"{u}cken, Germany},
	series = {LICS '20}
}

@article{katsumata22_flexib_presen_graded_monad,
	author = {Katsumata, Shin-ya and McDermott, Dylan and Uustalu, Tarmo and Wu, Nicolas},
	title = {Flexible Presentations of Graded Monads},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3547654},
	doi = {10.1145/3547654},
	abstract = {A large class of monads used to model computational effects have natural presentations by operations and equations, for example, the list monad can be presented by a constant and a binary operation subject to unitality and associativity. Graded monads are a generalization of monads that enable us to track quantitative information about the effects being modelled. Correspondingly, a large class of graded monads can be presented using an existing notion of graded presentation. However, the existing notion has some deficiencies, in particular many effects do not have natural graded presentations. We introduce a notion of flexibly graded presentation that does not suffer from these issues, and develop the associated theory. We show that every flexibly graded presentation induces a graded monad equipped with interpretations of the operations of the presentation, and that all graded monads satisfying a particular condition on colimits have a flexibly graded presentation. As part of this, we show that the usual algebra-preserving correspondence between presentations and a class of monads transfers to an algebra-preserving correspondence between flexibly graded presentations and a class of flexibly graded monads.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {123},
	numpages = {29},
	keywords = {presentation, flexible grading, algebraic theory, graded monad, computational effect, monad}
}

@inbook{keller19_smtcoq,
	keywords = {SAT, coq},
	abstract = {SMTCoq is a plugin for the Coq interactive theorem prover to work in conjunction with automated theorem provers based on Boolean Satisfiability (SAT) and Satisfiability Modulo Theories (SMT), in an efficient and expressive way. As such, it allows one to formally establish, in a proof assistant, mathematical results relying on large combinatorial properties that require automatic Boolean reasoning. To this end, the huge SAT proofs coming from such problems can be safely checked in Coq and combined with standard mathematical Coq developments in a generic and modular way, for instance to obtain a formal proof of the Erdős Discrepancy Conjecture. To achieve this objective with the same degree of safety as Coq itself, SMTCoq communicates with SAT and SMT solvers that, in addition to a yes/no answer, can output traces of their internal proof search. The heart of SMTCoq is thus a certified, efficient and modular checker for such traces expressed in a format that can encompass most aspects of SMT reasoning. Preprocessors---that need not be certified---for proof traces coming from the state-of-the-art SMT solvers CVC4 and veriT and SAT solvers zChaff and Glucose are implemented. Coq can thus work in conjunction with widely used provers. From a proof assistant perspective, SMTCoq also provides a mechanism to let Coq users enjoy automation provided by external provers.},
	author = {Keller, Chantal},
	editor = {Hanna, Gila and Reid, David A. and de Villiers, Michael},
	location = {Cham},
	publisher = {Springer International Publishing},
	url = {https://doi.org/10.1007/978-3-030-28483-1_4},
	booktitle = {Proof Technology in Mathematics Research and Teaching},
	doi = {10.1007/978-3-030-28483-1_4},
	isbn = {978-3-030-28483-1},
	pages = {73--90},
	title = {SMTCoq: Mixing Automatic and Interactive Proof Technologies},
	year = {2019}
}

@Proceedings{keller21_proof_theor_provin,
	url = {https://cgi.cse.unsw.edu.au/~eptcs/content.cgi?PxTP2021},
	booktitle = {Proceedings Seventh Workshop on Proof eXchange for Theorem Proving },
	editor = {Keller, Chantal and Fleury, Mathias},
	year = {2021},
	address = {Pittsburg, USA},
	series = {EPTCS 336},
	number = {7},
	title = {Proof eXchange for Theorem Proving}
}

@inproceedings{kellison22_machin_check_direc_proof_stein_lehmus_theor,
	author = {Kellison, Ariel},
	title = {A Machine-Checked Direct Proof of the Steiner-Lehmus Theorem},
	year = {2022},
	isbn = {9781450391825},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497775.3503682},
	doi = {10.1145/3497775.3503682},
	abstract = {A direct proof of the Steiner-Lehmus theorem has eluded geometers for over 170 years. The challenge has been that a proof is only considered direct if it does not rely on reductio ad absurdum. Thus, any proof that claims to be direct must show, going back to the axioms, that all of the auxiliary theorems used are also proved directly. In this paper, we give a proof of the Steiner-Lehmus theorem that is guaranteed to be direct. The evidence for this claim is derived from our methodology: we have formalized a constructive axiom set for Euclidean plane geometry in a proof assistant that implements a constructive logic and have built the proof of the Steiner-Lehmus theorem on this constructive foundation.},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {265–273},
	numpages = {9},
	keywords = {foundations of mathematics, constructive logic, proof assistants, constructive geometry},
	location = {Philadelphia, PA, USA},
	series = {CPP 2022}
}

@article{kerinec23_why_are_proof_relev_proof_relev_model,
	author = {Kerinec, Axel and Manzonetto, Giulio and Olimpieri, Federico},
	title = {Why Are Proofs Relevant in Proof-Relevant Models?},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571201},
	doi = {10.1145/3571201},
	abstract = {Relational models of λ-calculus can be presented as type systems, the relational interpretation of a λ-term being given by the set of its typings. Within a distributors-induced bicategorical semantics generalizing the relational one, we identify the class of ‘categorified’ graph models and show that they can be presented as type systems as well. We prove that all the models living in this class satisfy an Approximation Theorem stating that the interpretation of a program corresponds to the filtered colimit of the denotations of its approximants. As in the relational case, the quantitative nature of our models allows to prove this property via a simple induction, rather than using impredicative techniques. Unlike relational models, our 2-dimensional graph models are also proof-relevant in the sense that the interpretation of a λ-term does not contain only its typings, but the whole type derivations. The additional information carried by a type derivation permits to reconstruct an approximant having the same type in the same environment. From this, we obtain the characterization of the theory induced by the categorified graph models as a simple corollary of the Approximation Theorem: two λ-terms have isomorphic interpretations exactly when their B'ohm trees coincide.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {8},
	numpages = {31},
	keywords = {Lambda calculus, Approximation Theorem, Intersection Types, Distributors}
}

@article{kern99_formal_verif_hardw_desig,
	author = {Kern, Christoph and Greenstreet, Mark R.},
	title = {Formal Verification in Hardware Design: A Survey},
	year = {1999},
	issue_date = {April 1999},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {4},
	number = {2},
	issn = {1084-4309},
	doi = {10.1145/307988.307989},
	abstract = {In recent years, formal methods have emerged as an alternative approach to ensuring the quality and correctness of hardware designs, overcoming some of the limitations of traditional validation techniques such as simulation and testing.There are two main aspects to the application of formal methods in a design process: the formal framework used to specify desired properties of a design and the verification techniques and tools used to reason about the relationship between a specification and a corresponding implementation. We survey a variety of frameworks and techniques proposed in the literature and applied to actual designs. The specification frameworks we describe include temporal logics, predicate logic, abstraction and refinement, as well as containment between  ω-regular languages. The verification techniques presented include model checking, automata-theoretic techniques, automated theorem proving, and approaches that integrate the above methods.In order to provide insight into the scope and limitations of currently available techniques, we present a selection of case studies where formal methods were applied to industrial-scale designs, such as microprocessors, floating-point hardware, protocols, memory subsystems, and communications hardware.},
	journal = {ACM Trans. Des. Autom. Electron. Syst.},
	month = {apr},
	pages = {123–193},
	numpages = {71},
	keywords = {survey, formal verification, model checking, formal methods, theorem proving, hardware verification, language containment, case studies}
}

@article{kesner22_fine_grain_comput_inter_girar,
	abstract = {This paper introduces a functional term calculus, called pn, that captures the essence of the operational semantics of Intuitionistic Linear Logic Proof-Nets with a faithful degree of granularity, both statically and dynamically. On the static side, we identify an equivalence relation on pn-terms which is sound and complete with respect to the classical notion of structural equivalence for proof-nets. On the dynamic side, we show that every single (exponential) step in the term calculus translates to a different single (exponential) step in the graphical formalism, thus capturing the original Girard’s granularity of proof-nets but on the level of terms. We also show some fundamental properties of the calculus such as confluence, strong normalization, preservation of β-strong normalization and the existence of a strong bisimulation that captures pairs of pn-terms having the same graph reduction.},
	author = {Kesner, Delia},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3498669},
	doi = {10.1145/3498669},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {explicit substitutions,proof-nets,lambda-calculus,linear logic},
	month = jan,
	number = {POPL},
	title = {A Fine-Grained Computational Interpretation of Girard’s Intuitionistic Proof-Nets},
	volume = {6},
	year = {2022}
}

@article{keuchel22_verif_symbol_execut_kripk_specif,
	author = {Keuchel, Steven and Huyghebaert, Sander and Lukyanov, Georgy and Devriese, Dominique},
	title = {Verified Symbolic Execution with Kripke Specification Monads (and No Meta-Programming)},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3547628},
	doi = {10.1145/3547628},
	abstract = {Verifying soundness of symbolic execution-based program verifiers is a significant challenge. This is especially true if the resulting tool needs to be usable outside of the proof assistant, in which case we cannot rely on shallowly embedded assertion logics and meta-programming. The tool needs to manipulate deeply embedded assertions, and it is crucial for efficiency to eagerly prune unreachable paths and simplify intermediate assertions in a way that can be justified towards the soundness proof. Only a few such tools exist in the literature, and their soundness proofs are intricate and hard to generalize or reuse. We contribute a novel, systematic approach for the construction and soundness proof of such a symbolic execution-based verifier. We first implement a shallow verification condition generator as an object language interpreter in a specification monad, using an abstract interface featuring angelic and demonic nondeterminism. Next, we build a symbolic executor by implementing a similar interpreter, in a symbolic specification monad. This symbolic monad lives in a universe that is Kripke-indexed by variables in scope and a path condition. Finally, we reduce the soundness of the symbolic execution to the soundness of the shallow execution by relating both executors using a Kripke logical relation. We report on the practical application of these techniques in Katamaran, a tool for verifying security guarantees offered by instruction set architectures (ISAs). The tool is fully verified by combining our symbolic execution machinery with a soundness proof of the shallow verification conditions against an axiomatized separation logic, and an Iris-based implementation of the axioms, proven sound against the operational semantics. Based on our experience with Katamaran, we can report good results on practicality and efficiency of the tool, demonstrating practical viability of our symbolic execution approach.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {97},
	numpages = {31},
	keywords = {predicate transformers, program verification, refinement, separation logic, symbolic execution, logical relations}
}

@inproceedings{khan17_verif,
	author = {Khan, Wilayat and Tiu, Alwen and Sanán, David},
	month = feb,
	title = {{VeriFormal}: An Executable Formal Model of a Hardware},
	year = {2017}
}

@inproceedings{kildall73_unified_approac_global_progr_optim,
	author = {Kildall, Gary A.},
	location = {Boston, Massachusetts},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 1st Annual ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
	doi = {10.1145/512927.512945},
	isbn = {9781450373494},
	pages = {194--206},
	series = {POPL '73},
	title = {A Unified Approach to Global Program Optimization},
	year = {1973}
}

@inproceedings{kim04_autom_fsmd,
	author = {{Youngsik Kim} and {Kopuri}, S. and {Mansouri}, N.},
	booktitle = {International Symposium on Signals, Circuits and Systems. Proceedings, SCS 2003. (Cat. No.03EX720)},
	doi = {10.1109/ISQED.2004.1283659},
	issn = {null},
	keywords = {formal verification;high level synthesis;data flow graphs;finite state machines;theorem proving;automated formal verification;scheduling process;finite state machines with datapath;high-level synthesis;behavioral specification;control-data flow graph;equivalence conditions;higher-order specification language;theorem proving environment;PVS proof checker;Formal verification;Automata;High level synthesis;Processor scheduling;Computer science;Flow graphs;Mathematical model;Specification languages;Libraries;Computer applications},
	month = mar,
	pages = {110--115},
	title = {Automated formal verification of scheduling process using finite state machines with datapath (FSMD)},
	year = {2004}
}

@inproceedings{kim05_wish,
	abstract = {Predicated execution has been used to reduce the number of branch mispredictions by eliminating hard-to-predict branches. However, the additional instruction overhead and additional data dependencies due to predicated execution sometimes offset the performance advantage of having fewer mispredictions. We propose a mechanism in which the compiler generates code that can be executed either as predicated code or non-predicated code (i.e., code with normal conditional branches). The hardware decides whether the predicated code or the non-predicated code is executed based on a run-time confidence estimation of the branch's prediction. The code generated by the compiler is the same as predicated code, except the predicated conditional branches are NOT removed - they are left intact in the program code. These conditional branches are called wish branches. The goal of wish branches is to use predicated execution for hard-to-predict dynamic branches and branch prediction for easy-to-predict dynamic branches, thereby obtaining the best of both worlds. We also introduce a class of wish branches, called wish loops, which utilize predication to reduce the misprediction penalty for hard-to-predict backward (loop) branches. We describe the semantics, types, and operation of wish branches along with the software and hardware support required to generate and utilize them. Our results show that wish branches decrease the average execution time of a subset of SPEC INT 2000 benchmarks by 14.2% compared to traditional conditional branches and by 13.3% compared to the best-performing predicated code binary},
	author = {{Hyesoon Kim} and {Mutlu}, O. and {Stark}, J. and {Patt}, Y. N.},
	booktitle = {38th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO'05)},
	doi = {10.1109/MICRO.2005.38},
	issn = {2379-3155},
	keywords = {predicated execution},
	month = nov,
	pages = {12 pp.--54},
	title = {Wish branches: combining conditional branching and predication for adaptive predicated execution},
	year = {2005}
}

@inproceedings{kim08_autom_formal_verif_sched_specul_code_motion,
	author = {Kim, Youngsik and Mansouri, Nazanin},
	location = {Orlando, Florida, USA},
	publisher = {ACM},
	url = {https://doi.org/10.1145/1366110.1366134},
	booktitle = {Proceedings of the 18th ACM Great Lakes Symposium on VLSI},
	doi = {10.1145/1366110.1366134},
	isbn = {978-1-59593-999-9},
	keywords = {automated theorem-proving,formal verification,high level synthesis,speculation},
	pages = {95--100},
	series = {GLSVLSI '08},
	title = {Automated Formal Verification of Scheduling with Speculative Code Motions},
	year = {2008}
}

@article{kim21_seman_guided_synth,
	abstract = {This paper develops a new framework for program synthesis, called semantics-guided synthesis (SemGuS), that allows a user to provide both the syntax and the semantics for the constructs in the language. SemGuS accepts a recursively defined big-step semantics, which allows it, for example, to be used to specify and solve synthesis problems over an imperative programming language that may contain loops with unbounded behavior. The customizable nature of SemGuS also allows synthesis problems to be defined over a non-standard semantics, such as an abstract semantics. In addition to the SemGuS framework, we develop an algorithm for solving SemGuS problems that is capable of both synthesizing programs and proving unrealizability, by encoding a SemGuS problem as a proof search over Constrained Horn Clauses: in particular, our approach is the first that we are aware of that can prove unrealizabilty for synthesis problems that involve imperative programs with unbounded loops, over an infinite syntactic search space. We implemented the technique in a tool called MESSY, and applied it to SyGuS problems (i.e., over expressions), synthesis problems over an imperative programming language, and synthesis problems over regular expressions.},
	author = {Kim, Jinwoo and Hu, Qinheping and D'Antoni, Loris and Reps, Thomas},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434311},
	doi = {10.1145/3434311},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {Unrealizability,Program Synthesis,Semantics-Guided Synthesis (SemGuS)},
	month = jan,
	number = {POPL},
	title = {Semantics-Guided Synthesis},
	volume = {5},
	year = {2021}
}

@inproceedings{kim22_binpoin,
	author = {Kim, Sun Hyoung and Zeng, Dongrui and Sun, Cong and Tan, Gang},
	title = {BinPointer: Towards Precise, Sound, and Scalable Binary-Level Pointer Analysis},
	year = {2022},
	isbn = {9781450391832},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497776.3517776},
	doi = {10.1145/3497776.3517776},
	abstract = {Binary-level pointer analysis is critical to binary-level applications such as reverse engineering and binary debloating. In this paper, we propose BinPointer, a new binary-level interprocedural pointer analysis that relies on an offset-sensitive value-tracking analysis to achieve high precision. We also propose a soundness and precision evaluation methodology based on runtime memory accesses triggered by reference input data. Our experimental results demonstrate that BinPointer has higher precision over prior work, while maintaining acceptable scalability. The soundness of BinPointer is also validated through runtime data.},
	booktitle = {Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction},
	pages = {169–180},
	numpages = {12},
	keywords = {static analysis, pointer analysis, binary analysis},
	location = {Seoul, South Korea},
	series = {CC 2022}
}

@article{kim23_unreal_logic,
	author = {Kim, Jinwoo and D'Antoni, Loris and Reps, Thomas},
	title = {Unrealizability Logic},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571216},
	doi = {10.1145/3571216},
	abstract = {We consider the problem of establishing that a program-synthesis problem is unrealizable (i.e., has no solution in a given search space of programs). Prior work on unrealizability has developed some automatic techniques to establish that a problem is unrealizable; however, these techniques are all black-box, meaning that they conceal the reasoning behind why a synthesis problem is unrealizable. In this paper, we present a Hoare-style reasoning system, called unrealizability logic for establishing that a program-synthesis problem is unrealizable. To the best of our knowledge, unrealizability logic is the first proof system for overapproximating the execution of an infinite set of imperative programs. The logic provides a general, logical system for building checkable proofs about unrealizability. Similar to how Hoare logic distills the fundamental concepts behind algorithms and tools to prove the correctness of programs, unrealizability logic distills into a single logical system the fundamental concepts that were hidden within prior tools capable of establishing that a program-synthesis problem is unrealizable.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {23},
	numpages = {30},
	keywords = {Unrealizability, Unrealizability Logic, Program Synthesis}
}

@ARTICLE{kim98_circuit,
	author = {Taewhan Kim and Jao, W. and Tjiang, S.},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	title = {Circuit optimization using carry-save-adder cells},
	year = {1998},
	volume = {17},
	number = {10},
	pages = {974-984},
	abstract = {Carry-save-adder (CSA) is the most often used type of operation in implementing a fast computation of arithmetics of register-transfer-level design in industry. This paper establishes a relationship between the properties of arithmetic computations and several optimizing transformations using CSAs to derive consistently better qualities of results than those of manual implementations. In particular, we introduce two important concepts, operation duplication and operation split, which are the main driving techniques of our algorithm for achieving an extensive utilization of CSAs. Experimental results from a set of typical arithmetic computations found in industry designs indicate that automating CSA optimization with our algorithm produces designs with up to 53% faster timing and up to 42% smaller area.},
	keywords = {arithmetic, carry-save adder, compiler optimisation},
	doi = {10.1109/43.728918},
	ISSN = {1937-4151},
	month = {Oct}
}

@article{kincaid23_when_less_is_more,
	author = {Kincaid, Zachary and Koh, Nicolas and Zhu, Shaowei},
	title = {When Less Is More: Consequence-Finding in a Weak Theory of Arithmetic},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571237},
	doi = {10.1145/3571237},
	abstract = {This paper presents a theory of non-linear integer/real arithmetic and algorithms for reasoning about this theory. The theory can be conceived of as an extension of linear integer/real arithmetic with a weakly-axiomatized multiplication symbol, which retains many of the desirable algorithmic properties of linear arithmetic. In particular, we show that the conjunctive fragment of the theory can be effectively manipulated (analogously to the usual operations on convex polyhedra, the conjunctive fragment of linear arithmetic). As a result, we can solve the following consequence-finding problem: given a ground formula F, find the strongest conjunctive formula that is entailed by F. As an application of consequence-finding, we give a loop invariant generation algorithm that is monotone with respect to the theory and (in a sense) complete. Experiments show that the invariants generated from the consequences are effective for proving safety properties of programs that require non-linear reasoning.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {44},
	numpages = {33},
	keywords = {theory of arithmetic, nonlinear invariant generation, Decision procedures, convex polyhedra, program analysis, polynomial ideals}
}

@article{king76_symbol_execut_progr_testin,
	abstract = {This paper describes the symbolic execution of programs. Instead of supplying the normal inputs to a program (e.g. numbers) one supplies symbols representing arbitrary values. The execution proceeds as in a normal execution except that values may be symbolic formulas over the input symbols. The difficult, yet interesting issues arise during the symbolic execution of conditional branch type statements. A particular system called EFFIGY which provides symbolic execution for program testing and debugging is also described. It interpretively executes programs written in a simple PL/I style programming language. It includes many standard debugging features, the ability to manage and to prove things about symbolic expressions, a simple program testing manager, and a program verifier. A brief discussion of the relationship between symbolic execution and program proving is also included.},
	author = {King, James C.},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/360248.360252},
	doi = {10.1145/360248.360252},
	issn = {0001-0782},
	journaltitle = {Commun. ACM},
	keywords = {program proving,program testing,program debugging,symbolic interpretation,program verification,symbolic execution},
	month = jul,
	number = {7},
	pages = {385--394},
	title = {Symbolic Execution and Program Testing},
	volume = {19},
	year = {1976}
}

@article{kjelstroem22_decid_compl_inter_bidir_dyck_reach,
	author = {Kjelstr\o{}m, Adam Husted and Pavlogiannis, Andreas},
	title = {The Decidability and Complexity of Interleaved Bidirected Dyck Reachability},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498673},
	doi = {10.1145/3498673},
	abstract = {Dyck reachability is the standard formulation of a large domain of static analyses, as it achieves the sweet spot between precision and efficiency, and has thus been studied extensively. Interleaved Dyck reachability (denoted Dk⊙ Dk) uses two Dyck languages for increased precision (e.g., context and field sensitivity) but is well-known to be undecidable. As many static analyses yield a certain type of bidirected graphs, they give rise to interleaved bidirected Dyck reachability problems. Although these problems have seen numerous applications, their decidability and complexity has largely remained open. In a recent work, Li et al. made the first steps in this direction, showing that (i) D1⊙ D1 reachability (i.e., when both Dyck languages are over a single parenthesis and act as counters) is computable in O(n7) time, while (ii) Dk⊙ Dk reachability is NP-hard. However, despite this recent progress, most natural questions about this intricate problem are open. In this work we address the decidability and complexity of all variants of interleaved bidirected Dyck reachability. First, we show that D1⊙ D1 reachability can be computed in O(n3· α(n)) time, significantly improving over the existing O(n7) bound. Second, we show that Dk⊙ D1 reachability (i.e., when one language acts as a counter) is decidable, in contrast to the non-bidirected case where decidability is open. We further consider Dk⊙ D1 reachability where the counter remains linearly bounded. Our third result shows that this bounded variant can be solved in O(n2· α(n)) time, while our fourth result shows that the problem has a (conditional) quadratic lower bound, and thus our upper bound is essentially optimal. Fifth, we show that full Dk⊙ Dk reachability is undecidable. This improves the recent NP-hardness lower-bound, and shows that the problem is equivalent to the non-bidirected case. Our experiments on standard benchmarks show that the new algorithms are very fast in practice, offering many orders-of-magnitude speedups over previous methods.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {12},
	numpages = {26},
	keywords = {complexity, bidirected graphs, static analysis, CFL/Dyck reachability}
}

@inproceedings{klauser98_dynam,
	author = {Klauser, A. and Austin, T. and Grunwald, D. and Calder, B.},
	publisher = {{IEEE} Comput. Soc},
	url = {https://doi.org/10.1109%2Fpact.1998.727261},
	booktitle = {Proceedings. 1998 International Conference on Parallel Architectures and Compilation Techniques (Cat. No.98EX192)},
	doi = {10.1109/pact.1998.727261},
	keywords = {predicated execution,if-conversion},
	title = {Dynamic hammock predication for non-predicated instruction set architectures},
	year = {1998}
}

@article{klimis23_takin_back_contr_inter_repres_gpu_comput,
	author = {Klimis, Vasileios and Clark, Jack and Baker, Alan and Neto, David and Wickerson, John and Donaldson, Alastair F.},
	title = {Taking Back Control in an Intermediate Representation for GPU Computing},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571253},
	doi = {10.1145/3571253},
	abstract = {We describe our experiences successfully applying lightweight formal methods to substantially improve and reformulate an important part of Standard Portable Intermediate Representation SPIRV, an industry-standard language for GPU computing. The formal model that we present has allowed us to (1) identify several ambiguities and needless complexities in the way that structured control flow was defined in the SPIRV specification; (2) interact with the authors of the SPIRV specification to rectify these problems; (3) validate the developer tools and conformance test suites that support the SPIRV language by cross-checking them against our formal model, improving the tools, test suites, and our models in the process; and (4) develop a novel method for fuzzing SPIRV compilers to detect miscompilation bugs that leverages our formal model. The latest release of the SPIRV specification incorporates the revised set of control-flow definitions that have arisen from our work. Furthermore, our novel compiler-fuzzing technique has led to the discovery of twenty distinct, previously unknown bugs in SPIRV compilers from Google, the Khronos Group, Intel, and Mozilla. Our work showcases the practical impact that formal modelling and analysis techniques can have on the design and implementation of industry-standard programming languages.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {60},
	numpages = {30},
	keywords = {control flow, shader/kernel language compilers, fuzz testing, GPUs, SPIR-V}
}

@ARTICLE{knapp92_fasol,
	author = {Knapp, D.W.},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	title = {Fasolt: a program for feedback-driven data-path optimization},
	year = {1992},
	volume = {11},
	number = {6},
	pages = {677-695},
	abstract = {The author describes Fasolt, a system that automatically optimizes the bus topology of a register-level data-path design. The unique aspect of Fasolt is that it uses information taken from a detailed layout model to choose optimizing transformations of the bus topology and the schedule of operations and transfers of an existing design. Such a system is called a feedback-drive system because it uses information derived at a low level (in this case, that of physical layout) to drive the selection of optimizing transformations at a higher level or levels (in this case, the scheduling and allocation levels). This allows the scheduling and topology synthesis steps to take wiring considerations into account in a way that has hitherto not been demonstrated in an automatic synthesis system. Experiments have shown that improvements in area, cycle time, and overall average delay can often be achieved in the same design using this approach.<>},
	keywords = {high-level synthesis, feedback-driven optimisation},
	doi = {10.1109/43.137515},
	ISSN = {1937-4151},
	month = {June}
}

@article{kobayashi23_hfl_z_valid_check_autom_progr_verif,
	author = {Kobayashi, Naoki and Tanahashi, Kento and Sato, Ryosuke and Tsukada, Takeshi},
	title = {HFL(Z) Validity Checking for Automated Program Verification},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571199},
	doi = {10.1145/3571199},
	abstract = {We propose an automated method for checking the validity of a formula of HFL(Z), a higher-order logic with fixpoint operators and integers. Combined with Kobayashi et al.'s reduction from higher-order program verification to HFL(Z) validity checking, our method yields a fully automated, uniform verification method for arbitrary temporal properties of higher-order functional programs expressible in the modal mu-calculus, including termination, non-termination, fair termination, fair non-termination, and also branching-time properties. We have implemented our method and obtained promising experimental results.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {6},
	numpages = {31},
	keywords = {Automated Program Verification, Fixpoint Logic, Higher-Order Functional Programs}
}

@inproceedings{koch22_undec_incom_compl_secon_order_logic_coq,
	author = {Koch, Mark and Kirst, Dominik},
	title = {Undecidability, Incompleteness, and Completeness of Second-Order Logic in Coq},
	year = {2022},
	isbn = {9781450391825},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497775.3503684},
	doi = {10.1145/3497775.3503684},
	abstract = {We mechanise central metatheoretic results about second-order logic (SOL) using the Coq proof assistant. Concretely, we consider undecidability via many-one reduction from Diophantine equations (Hilbert's tenth problem), incompleteness regarding full semantics via categoricity of second-order Peano arithmetic, and completeness regarding Henkin semantics via translation to mono-sorted first-order logic (FOL). Moreover, this translation is used to transport further characteristic properties of FOL to SOL, namely the compactness and L\"{o}wenheim-Skolem theorems.},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {274–290},
	numpages = {17},
	keywords = {second order logic, undecidability, completeness},
	location = {Philadelphia, PA, USA},
	series = {CPP 2022}
}

@inproceedings{koenig21_compc,
	abstract = {Since the introduction of CompCert, researchers have been refining its language semantics and correctness theorem, and used them as components in software verification efforts. Meanwhile, artifacts ranging from CPU designs to network protocols have been successfully verified, and there is interest in making them interoperable to tackle end-to-end verification at an even larger scale. Recent work shows that a synthesis of game semantics, refinement-based methods, and abstraction layers has the potential to serve as a common theory of certified components. Integrating certified compilers to such a theory is a critical goal. However, none of the existing variants of CompCert meets the requirements we have identified for this task. CompCertO extends the correctness theorem of CompCert to characterize compiled program components directly in terms of their interaction with each other. Through a careful and compositional treatment of calling conventions, this is achieved with minimal effort.},
	author = {Koenig, Jérémie and Shao, Zhong},
	location = {Virtual, Canada},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3453483.3454097},
	booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
	doi = {10.1145/3453483.3454097},
	isbn = {9781450383912},
	keywords = {Compositional Compiler Correctness,Simulation Convention,Language Interface,Game Semantics},
	pages = {1095--1109},
	series = {PLDI 2021},
	title = {CompCertO: Compiling Certified Open C Components},
	year = {2021}
}

@inproceedings{kohl23_formal_devel_closed_criter_left,
	author = {Kohl, Christina and Middeldorp, Aart},
	title = {A Formalization of the Development Closedness Criterion for Left-Linear Term Rewrite Systems},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575667},
	doi = {10.1145/3573105.3575667},
	abstract = {Several critical pair criteria are known that guarantee confluence of left-linear term rewrite systems. The correctness of most of these have been formalized in a proof assistant. An important exception has been the development closedness criterion of van Oostrom. Its proof requires a high level of understanding about overlapping redexes and descendants as well as several intermediate results related to these concepts. We present a formalization in the proof assistant Isabelle/HOL. The result has been integrated into the certifier CeTA.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {197–210},
	numpages = {14},
	keywords = {confluence, formalization, term rewriting},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@article{kokologiannakis22_truly_statel_optim_dynam_partial_order_reduc,
	author = {Kokologiannakis, Michalis and Marmanis, Iason and Gladstein, Vladimir and Vafeiadis, Viktor},
	title = {Truly Stateless, Optimal Dynamic Partial Order Reduction},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498711},
	doi = {10.1145/3498711},
	abstract = {Dynamic partial order reduction (DPOR) verifies concurrent programs by exploring all their interleavings up to some equivalence relation, such as the Mazurkiewicz trace equivalence. Doing so involves a complex trade-off between space and time. Existing DPOR algorithms are either exploration-optimal (i.e., explore exactly only interleaving per equivalence class) but may use exponential memory in the size of the program, or maintain polynomial memory consumption but potentially explore exponentially many redundant interleavings. In this paper, we show that it is possible to have the best of both worlds: exploring exactly one interleaving per equivalence class with linear memory consumption. Our algorithm, TruSt, formalized in Coq, is applicable not only to sequential consistency, but also to any weak memory model that satisfies a few basic assumptions, including TSO, PSO, and RC11. In addition, TruSt is embarrassingly parallelizable: its different exploration options have no shared state, and can therefore be explored completely in parallel. Consequently, TruSt outperforms the state-of-the-art in terms of memory and/or time.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {49},
	numpages = {28},
	keywords = {Dynamic Partial Order Reduction, Model Checking, Weak Memory Models}
}

@article{kokologiannakis23_kater,
	author = {Kokologiannakis, Michalis and Lahav, Ori and Vafeiadis, Viktor},
	title = {Kater: Automating Weak Memory Model Metatheory and Consistency Checking},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571212},
	doi = {10.1145/3571212},
	abstract = {The metatheory of axiomatic weak memory models covers questions like the correctness of compilation mappings from one model to another and the correctness of local program transformations according to a given model---topics usually requiring lengthy human investigation. We show that these questions can be solved by answering a more basic question: "Given two memory models, is one weaker than the other?" Moreover, for a wide class of axiomatic memory models, we show that this basic question can be reduced to a language inclusion problem between regular languages, which is decidable. Similarly, implementing an efficient check for whether an execution graph is consistent according to a given memory model has required non-trivial manual effort. Again, we show that such efficient checks can be derived automatically for a wide class of axiomatic memory models, and that incremental consistency checks can be incorporated in GenMC, a state-of-the-art model checker for concurrent programs. As a result, we get the first time- and space-efficient bounded verifier taking the axiomatic memory model as an input parameter.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {19},
	numpages = {29},
	keywords = {Kleene Algebra with Tests, Weak Memory Models, Model Checking}
}

@article{kolosick22_isolat_taxat,
	author = {Kolosick, Matthew and Narayan, Shravan and Johnson, Evan and Watt, Conrad and LeMay, Michael and Garg, Deepak and Jhala, Ranjit and Stefan, Deian},
	title = {Isolation without Taxation: Near-Zero-Cost Transitions for WebAssembly and SFI},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498688},
	doi = {10.1145/3498688},
	abstract = {Software sandboxing or software-based fault isolation (SFI) is a lightweight approach to building secure systems out of untrusted components. Mozilla, for example, uses SFI to harden the Firefox browser by sandboxing third-party libraries, and companies like Fastly and Cloudflare use SFI to safely co-locate untrusted tenants on their edge clouds. While there have been significant efforts to optimize and verify SFI enforcement, context switching in SFI systems remains largely unexplored: almost all SFI systems use heavyweight transitions that are not only error-prone but incur significant performance overhead from saving, clearing, and restoring registers when context switching. We identify a set of zero-cost conditions that characterize when sandboxed code has sufficient structured to guarantee security via lightweight zero-cost transitions (simple function calls). We modify the Lucet Wasm compiler and its runtime to use zero-cost transitions, eliminating the undue performance tax on systems that rely on Lucet for sandboxing (e.g., we speed up image and font rendering in Firefox by up to 29.7% and 10% respectively). To remove the Lucet compiler and its correct implementation of the Wasm specification from the trusted computing base, we (1) develop a static binary verifier, VeriZero, which (in seconds) checks that binaries produced by Lucet satisfy our zero-cost conditions, and (2) prove the soundness of VeriZero by developing a logical relation that captures when a compiled Wasm function is semantically well-behaved with respect to our zero-cost conditions. Finally, we show that our model is useful beyond Wasm by describing a new, purpose-built SFI system, SegmentZero32, that uses x86 segmentation and LLVM with mostly off-the-shelf passes to enforce our zero-cost conditions; our prototype performs on-par with the state-of-the-art Native Client SFI system.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {27},
	numpages = {30},
	keywords = {sandboxing, WebAssembly, verification, software fault isolation}
}

@article{konikowska93_two_over_three,
	keywords = {three-valued logic},
	doi = {10.1080/11663081.1993.10510795},
	url = {https://doi.org/10.1080/11663081.1993.10510795},
	year = {1993},
	month = jan,
	publisher = {Informa {UK} Limited},
	volume = {3},
	number = {1},
	pages = {39--71},
	author = {Beata Konikowska},
	title = {Two Over Three: a Two-Valued Logic for Software Specification and Validation Over a Three-Valued Predicate Calculus},
	journal = {Journal of Applied Non-Classical Logics}
}

@article{koppel22_autom_deriv_contr_flow_graph,
	author = {Koppel, James and Kearl, Jackson and Solar-Lezama, Armando},
	title = {Automatically Deriving Control-Flow Graph Generators from Operational Semantics},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	doi = {10.1145/3547648},
	abstract = {We develop the first theory of control-flow graphs from first principles, and use it to create an algorithm for automatically synthesizing many variants of control-flow graph generators from a language’s operational semantics. Our approach first introduces a new algorithm for converting a large class of small-step operational semantics to an abstract machine. It next uses a technique called ”abstract rewriting” to automatically abstract the semantics of a language, which is used both to directly generate a CFG from a program (”interpreted mode”) and to generate standalone code, similar to a human-written CFG generator, for any program in a language. We show how the choice of two abstraction and projection parameters allow our approach to synthesize several families of CFG-generators useful for different kinds of tools. We prove the correspondence between the generated graphs and the original semantics. We provide and prove an algorithm for automatically proving the termination of interpreted-mode generators. In addition to our theoretical results, we have implemented this algorithm in a tool called Mandate, and show that it produces human-readable code on two medium-size languages with 60−80 rules, featuring nearly all intraprocedural control constructs common in modern languages. We then show these CFG-generators were sufficient to build two static analyses atop them. Our work is a promising step towards the grand vision of being able to synthesize all desired tools from the semantics of a programming language.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {117},
	numpages = {30},
	keywords = {term rewriting, control-flow graphs, abstract machines}
}

@article{koppel22_searc_entan_progr_spaces,
	author = {Koppel, James and Guo, Zheng and de Vries, Edsko and Solar-Lezama, Armando and Polikarpova, Nadia},
	title = {Searching Entangled Program Spaces},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3547622},
	doi = {10.1145/3547622},
	abstract = {Many problem domains, including program synthesis and rewrite-based optimization, require searching astronomically large spaces of programs. Existing approaches often rely on building specialized data structures—version-space algebras, finite tree automata, or e-graphs—to compactly represent such spaces. At their core, all these data structures exploit independence of subterms; as a result, they cannot efficiently represent more complex program spaces, where the choices of subterms are entangled. We introduce equality-constrained tree automata (ECTAs), a new data structure, designed to compactly represent large spaces of programs with entangled subterms. We present efficient algorithms for extracting programs from ECTAs, implemented in a performant Haskell library, ecta. Using the ecta library, we construct Hectare, a type-driven program synthesizer for Haskell. Hectare significantly outperforms a state-of-the-art synthesizer Hoogle+—providing an average speedup of 8\texttimes{}—despite its implementation being an order of magnitude smaller.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {91},
	numpages = {29},
	keywords = {type systems, e-graphs, Haskell, program synthesis}
}

@inproceedings{kosaian23_first_compl_algor_real_quant,
	author = {Kosaian, Katherine and Tan, Yong Kiam and Platzer, Andr\'{e}},
	title = {A First Complete Algorithm for Real Quantifier Elimination in Isabelle/HOL},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575672},
	doi = {10.1145/3573105.3575672},
	abstract = {We formalize a multivariate quantifier elimination (QE) algorithm in the theorem prover Isabelle/HOL. Our algorithm is complete, in that it is able to reduce any quantified formula in the first-order logic of real arithmetic to a logically equivalent quantifier-free formula. The algorithm we formalize is a hybrid mixture of Tarski’s original QE algorithm and the Ben-Or, Kozen, and Reif algorithm, and it is the first complete multivariate QE algorithm formalized in Isabelle/HOL.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {211–224},
	numpages = {14},
	keywords = {multivariate polynomials, theorem proving, real arithmetic, quantifier elimination},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@inproceedings{kostyukov21_beyon_elemen_repres_progr_invar,
	abstract = {First-order logic is a natural way of expressing properties of computation. It is traditionally used in various program logics for expressing the correctness properties and certificates. Although such representations are expressive for some theories, they fail to express many interesting properties of algebraic data types (ADTs). In this paper, we explore three different approaches to represent program invariants of ADT-manipulating programs: tree automata, and first-order formulas with or without size constraints. We compare the expressive power of these representations and prove the negative definability of both first-order representations using the pumping lemmas. We present an approach to automatically infer program invariants of ADT-manipulating programs by a reduction to a finite model finder. The implementation called RInGen has been evaluated against state-of-the-art invariant synthesizers and has been experimentally shown to be competitive. In particular, program invariants represented by automata are capable of expressing more complex properties of computation and their automatic construction is often less expensive.},
	author = {Kostyukov, Yurii and Mordvinov, Dmitry and Fedyukovich, Grigory},
	location = {Virtual, Canada},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3453483.3454055},
	booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
	doi = {10.1145/3453483.3454055},
	isbn = {9781450383912},
	keywords = {tree automata,invariants,invariant representation,first-order definability,finite models,algebraic data types},
	pages = {451--465},
	series = {PLDI 2021},
	title = {Beyond the Elementary Representations of Program Invariants over Algebraic Data Types},
	year = {2021}
}

@article{kountouris02_effic_sched_condit_behav_high_level_synth,
	abstract = {As hardware designs get increasingly complex and time-to-market constraints get tighter there is strong motivation for high-level synthesis (HLS). HLS must efficiently handle both dataflow-dominated and controlflow-dominated designs as well as designs of a mixed nature. In the past efficient tools for the former type have been developed but so far HLS of conditional behaviors lags behind. To bridge this gap an efficient scheduling heuristic for conditional behaviors is presented. Our heuristic and the techniques it utilizes are based on a unifying design representation appropriate for both types of behavioral descriptions, enabling the proposed heuristic to exploit under the same framework several well-established techniques (chaining, multicycling) as well as conditional resource sharing and speculative execution which are essential in efficiently scheduling conditional behaviors. Preliminary experiments confirm the effectiveness of our approach and prompted the development of the CODESIS HLS tool for further experimentation.},
	author = {Kountouris, Apostolos A. and Wolinski, Christophe},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/567270.567272},
	doi = {10.1145/567270.567272},
	issn = {1084-4309},
	journaltitle = {ACM Trans. Des. Autom. Electron. Syst.},
	keywords = {conditional behavior,Design automation,scheduling,high level synthesis (HLS)},
	month = jul,
	number = {3},
	pages = {380--412},
	title = {Efficient Scheduling of Conditional Behaviors for High-Level Synthesis},
	volume = {7},
	year = {2002}
}

@inproceedings{kourta22_caviar,
	author = {Kourta, Smail and Namani, Adel Abderahmane and Benbouzid-Si Tayeb, Fatima and Hazelwood, Kim and Cummins, Chris and Leather, Hugh and Baghdadi, Riyadh},
	title = {Caviar: An e-Graph Based TRS for Automatic Code Optimization},
	year = {2022},
	isbn = {9781450391832},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497776.3517781},
	doi = {10.1145/3497776.3517781},
	abstract = {Term Rewriting Systems (TRSs) are used in compilers to simplify and prove expressions. State-of-the-art TRSs in compilers use a greedy algorithm that applies a set of rewriting rules in a predefined order (where some of the rules are not axiomatic). This leads to a loss of the ability to simplify certain expressions. E-graphs and equality saturation sidestep this issue by representing the different equivalent expressions in a compact manner from which the optimal expression can be extracted. While an e-graph-based TRS can be more powerful than a TRS that uses a greedy algorithm, it is slower because expressions may have a large or sometimes infinite number of equivalent expressions. Accelerating e-graph construction is crucial for making the use of e-graphs practical in compilers. In this paper, we present Caviar, an e-graph-based TRS for proving expressions within compilers. The main advantage of Caviar is its speed. It can prove expressions much faster than base e-graph TRSs. It relies on three techniques: 1) a technique that stops e-graphs from growing when the goal is reached, called Iteration Level Check; 2) a mechanism that balances exploration and exploitation in the equality saturation algorithm, called Pulsing Caviar; 3) a technique to stop e-graph construction before reaching saturation when a non-provable pattern is detected, called Non-Provable Patterns Detection (NPPD). We evaluate caviar on Halide, an optimizing compiler that relies on a greedy-algorithm-based TRS to simplify and prove its expressions. The proposed techniques allow Caviar to accelerate e-graph expansion for the task of proving expressions. They also allow Caviar to prove expressions that Halide’s TRS cannot prove while being only 0.68x slower. Caviar is publicly available at: <a>https://github.com/caviar-trs/caviar</a>.},
	booktitle = {Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction},
	pages = {54–64},
	numpages = {11},
	keywords = {Algebraic expressions simplification, Equality Graphs, Equality Saturation, Term Rewriting Systems},
	location = {Seoul, South Korea},
	series = {CC 2022}
}

@article{koutris23_fine_grain_compl_cfl_reach,
	author = {Koutris, Paraschos and Deep, Shaleen},
	title = {The Fine-Grained Complexity of CFL Reachability},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571252},
	doi = {10.1145/3571252},
	abstract = {Many problems in static program analysis can be modeled as the context-free language (CFL) reachability problem on directed labeled graphs. The CFL reachability problem can be generally solved in time O(n3), where n is the number of vertices in the graph, with some specific cases that can be solved faster. In this work, we ask the following question: given a specific CFL, what is the exact exponent in the monomial of the running time? In other words, for which cases do we have linear, quadratic or cubic algorithms, and are there problems with intermediate runtimes? This question is inspired by recent efforts to classify classic problems in terms of their exact polynomial complexity, known as fine-grained complexity. Although recent efforts have shown some conditional lower bounds (mostly for the class of combinatorial algorithms), a general picture of the fine-grained complexity landscape for CFL reachability is missing. Our main contribution is lower bound results that pinpoint the exact running time of several classes of CFLs or specific CFLs under widely believed lower bound conjectures (e.g., Boolean Matrix Multiplication, k-Clique, APSP, 3SUM). We particularly focus on the family of Dyck-k languages (which are strings with well-matched parentheses), a fundamental class of CFL reachability problems. Remarkably, we are able to show a Ω(n2.5) lower bound for Dyck-2 reachability, which to the best of our knowledge is the first super-quadratic lower bound that applies to all algorithms, and shows that CFL reachability is strictly harder that Boolean Matrix Multiplication. We also present new lower bounds for the case of sparse input graphs where the number of edges m is the input parameter, a common setting in the database literature. For this setting, we show a cubic lower bound for Andersen’s Pointer Analysis which significantly strengthens prior known results.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {59},
	numpages = {27},
	keywords = {Datalog, sparse graphs, fine-grained complexity, Dyck reachability, static pointer analysis}
}

@inproceedings{koutsoukou-argyraki23_formal_balog_szemer_gower_theor_isabel_hol,
	author = {Koutsoukou-Argyraki, Angeliki and Bak\v{s}ys, Mantas and Edmonds, Chelsea},
	title = {A Formalisation of the Balog–Szemer\'{e}di–Gowers Theorem in Isabelle/HOL},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575680},
	doi = {10.1145/3573105.3575680},
	abstract = {We describe our formalisation in the interactive theorem prover Isabelle/HOL of the Balog–Szemer\'{e}di–Gowers Theorem, a profound result in additive combinatorics which played a central role in Gowers’s proof deriving the first effective bounds for Szemer\'{e}di’s Theorem. The proof is of great mathematical interest given that it involves an interplay between different mathematical areas, namely applications of graph theory and probability theory to additive combinatorics involving algebraic objects. This interplay is what made the process of the formalisation, for which we had to develop formalisations of new background material in the aforementioned areas, more rich and technically challenging. We demonstrate how locales, Isabelle’s module system, can be employed to handle such interplays in mathematical formalisations. To treat the graph-theoretic aspects of the proof, we make use of a new, more general undirected graph theory library developed by Edmonds, which is both flexible and extensible. In addition to the main theorem, which, following our source, is formulated for difference sets, we also give an alternative version for sumsets which required a formalisation of an auxiliary triangle inequality. We moreover formalise a few additional results in additive combinatorics that are not used in the proof of the main theorem. This is the first formalisation of the Balog–Szemer\'{e}di–Gowers Theorem in any proof assistant to our knowledge.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {225–238},
	numpages = {14},
	keywords = {Isabelle/HOL, proof assistant, additive combinatorics, graph theory, probabilistic method, formalisation of mathematics, interactive theorem proving},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@article{kovacs22_staged_compil_two_level_type_theor,
	author = {Kov\'{a}cs, Andr\'{a}s},
	title = {Staged Compilation with Two-Level Type Theory},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3547641},
	doi = {10.1145/3547641},
	abstract = {The aim of staged compilation is to enable metaprogramming in a way such that we have guarantees about the well-formedness of code output, and we can also mix together object-level and meta-level code in a concise and convenient manner. In this work, we observe that two-level type theory (2LTT), a system originally devised for the purpose of developing synthetic homotopy theory, also serves as a system for staged compilation with dependent types. 2LTT has numerous good properties for this use case: it has a concise specification, well-behaved model theory, and it supports a wide range of language features both at the object and the meta level. First, we give an overview of 2LTT's features and applications in staging. Then, we present a staging algorithm and prove its correctness. Our algorithm is "staging-by-evaluation", analogously to the technique of normalization-by-evaluation, in that staging is given by the evaluation of 2LTT syntax in a semantic domain. The staging algorithm together with its correctness constitutes a proof of strong conservativity of 2LLT over the object theory. To our knowledge, this is the first description of staged compilation which supports full dependent types and unrestricted staging for types.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {110},
	numpages = {30},
	keywords = {staged compilation, type theory, two-level type theory}
}

@article{krawiec22_provab_correc_asymp_effic_higher,
	author = {Krawiec, Faustyna and Peyton Jones, Simon and Krishnaswami, Neel and Ellis, Tom and Eisenberg, Richard A. and Fitzgibbon, Andrew},
	title = {Provably Correct, Asymptotically Efficient, Higher-Order Reverse-Mode Automatic Differentiation},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498710},
	doi = {10.1145/3498710},
	abstract = {In this paper, we give a simple and efficient implementation of reverse-mode automatic differentiation, which both extends easily to higher-order functions, and has run time and memory consumption linear in the run time of the original program. In addition to a formal description of the translation, we also describe an implementation of this algorithm, and prove its correctness by means of a logical relations argument.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {48},
	numpages = {30},
	keywords = {Reverse-Mode AD, Higher-Order Functions, Wengert List}
}

@inproceedings{krebbers14_formal_c_seman,
	abstract = {We discuss the difference between a formal semantics of the C standard, and a formal semantics of an implementation of C that satisfies the C standard. In this context we extend the CompCert semantics with end-of-array pointers and the possibility to byte-wise copy objects. This is a first and necessary step towards proving that the CompCert semantics refines the formal version of the C standard that is being developed in the Formalin project in Nijmegen.},
	author = {Krebbers, Robbert and Leroy, Xavier and Wiedijk, Freek},
	editor = {Klein, Gerwin and Gamboa, Ruben},
	location = {Cham},
	publisher = {Springer International Publishing},
	booktitle = {Interactive Theorem Proving},
	isbn = {978-3-319-08970-6},
	pages = {543--548},
	title = {Formal C Semantics: CompCert and the C Standard},
	year = {2014}
}

@InProceedings{kroening14_c,
	keywords = {bounded model checking},
	author = {Kroening, Daniel and Tautschnig, Michael},
	editor = "{\'A}brah{\'a}m, Erika
and Havelund, Klaus",
	title = {{CBMC} -- {C} Bounded Model Checker},
	booktitle = "Tools and Algorithms for the Construction and Analysis of Systems",
	year = "2014",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "389--391",
	abstract = "CBMC implements bit-precise bounded model checking for C programs and has been developed and maintained for more than ten years. CBMC verifies the absence of violated assertions under a given loop unwinding bound. Other properties, such as SV-COMP's ERROR labels or memory safety properties are reduced to assertions via automated instrumentation. Only recently support for efficiently checking concurrent programs, including support for weak memory models, has been added. Thus, CBMC is now capable of finding counterexamples in all of SV-COMP's categories. As back end, the competition submission of CBMC uses MiniSat 2.2.0.",
	isbn = "978-3-642-54862-8"
}

@article{krogmeier22_learn_formul_finit_variab_logic,
	author = {Krogmeier, Paul and Madhusudan, P.},
	title = {Learning Formulas in Finite Variable Logics},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498671},
	doi = {10.1145/3498671},
	abstract = {We consider grammar-restricted exact learning of formulas and terms in finite variable logics. We propose a novel and versatile automata-theoretic technique for solving such problems. We first show results for learning formulas that classify a set of positively- and negatively-labeled structures. We give algorithms for realizability and synthesis of such formulas along with upper and lower bounds. We also establish positive results using our technique for other logics and variants of the learning problem, including first-order logic with least fixed point definitions, higher-order logics, and synthesis of queries and terms with recursively-defined functions.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {10},
	numpages = {28},
	keywords = {program synthesis, learning formulas, tree automata, exact learning, version space algebra, interpretable learning}
}

@inproceedings{kropf95,
	author = {Kropf, Thomas and Schneider, Klaus and Kumar, Ramayya},
	editor = {Kumar, Ramayya and Kropf, Thomas},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Theorem Provers in Circuit Design},
	isbn = {978-3-540-49177-4},
	pages = {223--238},
	title = {A formal framework for high level synthesis},
	year = {1995}
}

@INPROCEEDINGS{kumar11_if,
	keywords = {if-conversion, hyperblocks},
	author = {Kumar, Rajendra and Saxena, Abhishek Kumar and Singh, P K},
	booktitle = {2011 3rd International Conference on Electronics Computer Technology},
	title = {A novel heuristic for selection of hyperblock in If-conversion},
	year = {2011},
	volume = {6},
	number = {},
	pages = {232-235},
	doi = {10.1109/ICECTECH.2011.5942088}
}

@inproceedings{kumm13_multip,
	abstract = {The scaling operation, i. e., the multiplication with a single constant is a frequently used operation in many kinds of numeric algorithms. The multiple constant multiplication (MCM) is a generalization where a variable is multiplied by several constants. This kind of operation is heavily used, e. g., in digital filters or discrete transforms. It was shown in recent work that small, fast and power efficient MCM implementations can be realized by using the fast carry chains of FPGAs rather than wasting specialized embedded multipliers. However, in the work so far, only common two-input adders were used. As FPGAs today support ternary adders, i. e., adders with three inputs, this work investigates the optimization of pipelined MCM circuits which include ternary adders. It is shown experimentally that 27% less operations are needed on average by using ternary adders, resulting in 15% slice (Xilinx) and 10% ALM (Altera) reductions, respectively.},
	author = {{Kumm}, M. and {Hardieck}, M. and {Willkomm}, J. and {Zipf}, P. and {Meyer-Baese}, U.},
	booktitle = {2013 23rd International Conference on Field programmable Logic and Applications},
	doi = {10.1109/FPL.2013.6645543},
	issn = {1946-1488},
	keywords = {adders;digital filters;Altera reductions;Xilinx;slice;pipelined MCM circuits;specialized embedded multipliers;FPGA;fast carry chains;discrete transforms;digital filters;generalization;numeric algorithms;single constant;scaling operation;ternary adders;multiple constant multiplication;Adders;Field programmable gate arrays;Table lookup;Pipelines;Topology;Logic gates;Routing},
	month = sep,
	pages = {1--8},
	title = {Multiple constant multiplication with ternary adders},
	year = {2013}
}

@inproceedings{kundu07_autom,
	abstract = {Stepwise refinement is at the core of many approaches to synthesis and optimization of hardware and software systems. For instance, it can be used to build a synthesis approach for digital circuits from high level specifications. It can also be used for post-synthesis modification such as in engineering change orders (ECOs). Therefore, checking if a system, modeled as a set of concurrent processes, is a refinement of another is of tremendous value. In this paper, we focus on concurrent systems modeled as communicating sequential processes (CSP) and show their refinements can be validated using insights from translation validation, automated theorem proving and relational approaches to reasoning about programs. The novelty of our approach is that it handles infinite state spaces in a fully automated manner. We have implemented our refinement checking technique and have applied it to a variety of refinements. We present the details of our algorithm and experimental results. As an example, we were able to automatically check an infinite state space buffer refinement that cannot be checked by current state of the art tools such as FDR. We were also able to check the data part of an industrial case study on the EP2 system.},
	author = {{Sudipta Kundu} and {Lerner}, S. and {Rajesh Gupta}},
	url = {https://doi.org/10.1109/ICCAD.2007.4397284},
	booktitle = {2007 IEEE/ACM International Conference on Computer-Aided Design},
	doi = {10.1109/ICCAD.2007.4397284},
	issn = {1558-2434},
	keywords = {communicating sequential processes;concurrency control;formal specification;high level synthesis;reasoning about programs;state-space methods;theorem proving;automated refinement checking;concurrent systems;stepwise refinement;digital circuits;high level specifications;post-synthesis modification;engineering change orders;communicating sequential processes;translation validation;automated theorem proving;reasoning about programs;state spaces;refinement checking technique;EP2 system;State-space methods;Hardware;Refining;Circuit synthesis;Reasoning about programs;Humans;Manuals;Software systems;Digital circuits;Silicon},
	month = nov,
	pages = {318--325},
	title = {Automated refinement checking of concurrent systems},
	year = {2007}
}

@inproceedings{kundu08_valid_high_level_synth,
	abstract = {The growing design-productivity gap has made designers shift toward using high-level languages like C, C++ and Java to do system-level design. High-Level Synthesis (HLS) is the process of generating Register Transfer Level (RTL) design from these initial high-level programs. Unfortunately, this translation process itself can be buggy, which can create a mismatch between what a designer intends and what is actually implemented in the circuit. In this paper, we present an approach to validate the result of HLS against the initial high-level program using insights from translation validation, automated theorem proving and relational approaches to reasoning about programs. We have implemented our validating technique and have applied it to a highly parallelizing HLS framework called SPARK. We present the details of our algorithm and experimental results.},
	author = {Kundu, Sudipta and Lerner, Sorin and Gupta, Rajesh},
	editor = {Gupta, Aarti and Malik, Sharad},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Computer Aided Verification},
	isbn = {978-3-540-70545-1},
	pages = {459--472},
	title = {Validating High-Level Synthesis},
	year = {2008}
}

@article{kundu09_high_level_verif,
	author = {Kundu, Sudipta and Lerner, Sorin and Gupta, Rajesh},
	url = {https://doi.org/10.2197/ipsjtsldm.2.131},
	doi = {10.2197/ipsjtsldm.2.131},
	journaltitle = {IPSJ Transactions on System LSI Design Methodology},
	pages = {131--144},
	title = {High-Level Verification},
	volume = {2},
	year = {2009}
}

@inproceedings{kundu09_provin_optim_correc_using_param_progr_equiv,
	abstract = {Translation validation is a technique for checking that, after an optimization has run, the input and output of the optimization are equivalent. Traditionally, translation validation has been used to prove concrete, fully specified programs equivalent. In this paper we present Parameterized Equivalence Checking (PEC), a generalization of translation validation that can prove the equivalence of parameterized programs. A parameterized program is a partially specified program that can represent multiple concrete programs. For example, a parameterized program may contain a section of code whose only known property is that it does not modify certain variables. By proving parameterized programs equivalent, PEC can prove the correctness of transformation rules that represent complex optimizations once and for all, before they are ever run. We implemented our PEC technique in a tool that can establish the equivalence of two parameterized programs. To highlight the power of PEC, we designed a language for implementing complex optimizations using many-to-many rewrite rules, and used this language to implement a variety of optimizations including software pipelining, loop unrolling, loop unswitching, loop interchange, and loop fusion. Finally, to demonstrate the effectiveness of PEC, we used our PEC implementation to verify that all the optimizations we implemented in our language preserve program behavior.},
	author = {Kundu, Sudipta and Tatlock, Zachary and Lerner, Sorin},
	location = {Dublin, Ireland},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/1542476.1542513},
	booktitle = {Proceedings of the 30th ACM SIGPLAN Conference on Programming Language Design and Implementation},
	doi = {10.1145/1542476.1542513},
	isbn = {9781605583921},
	keywords = {correctness,translation validation,compiler optimization},
	pages = {327--337},
	series = {PLDI '09},
	title = {Proving Optimizations Correct Using Parameterized Program Equivalence},
	year = {2009}
}

@article{kundu10_trans_valid_high_level_synth,
	author = {{Kundu}, S. and {Lerner}, S. and {Gupta}, R. K.},
	url = {https://doi.org/10.1109/TCAD.2010.2042889},
	doi = {10.1109/TCAD.2010.2042889},
	issn = {1937-4151},
	journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {formal specification;high level synthesis;integrated circuit design;integrated circuit testing;translation validation;high-level specifications;abstraction;automated theorem;concurrent systems modeled;parallelizing high-level synthesis framework;Spark;communicating sequential processes;High level synthesis;Hardware;Fabrication;Silicon;Circuits;Reasoning about programs;Sparks;Chip scale packaging;Process design;Productivity;Communicating sequential processes;correctness;equivalence checking;high-level synthesis;refinement checking;translation validation},
	month = apr,
	number = {4},
	pages = {566--579},
	title = {Translation Validation of High-Level Synthesis},
	volume = {29},
	year = {2010}
}

@article{kuperberg21_cyclic_proof_system_power_contr,
	abstract = {We study a cyclic proof system C over regular expression types, inspired by linear logic and non-wellfounded proof theory. Proofs in C can be seen as strongly typed goto programs. We show that they denote computable total functions and we analyse the relative strength of C and Gödel’s system T. In the general case, we prove that the two systems capture the same functions on natural numbers. In the affine case, i.e., when contraction is removed, we prove that they capture precisely the primitive recursive functions—providing an alternative and more general proof of a result by Dal Lago, about an affine version of system T. Without contraction, we manage to give a direct and uniform encoding of C into T, by analysing cycles and translating them into explicit recursions. Whether such a direct and uniform translation from C to T can be given in the presence of contraction remains open. We obtain the two upper bounds on the expressivity of C using a different technique: we formalise weak normalisation of a small step reduction semantics in subsystems of second-order arithmetic: ACA0 and RCA0.},
	author = {Kuperberg, Denis and Pinault, Laureline and Pous, Damien},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434282},
	doi = {10.1145/3434282},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {cyclic type system,reverse mathematics,linear logic,second order arithmetic,system T,primitive recursion,Cyclic proofs,regular expressions},
	month = jan,
	number = {POPL},
	title = {Cyclic Proofs, System t, and the Power of Contraction},
	volume = {5},
	year = {2021}
}

@InProceedings{lahiri03_symbol_approac_predic_abstr,
	doi = {10.1007/978-3-540-45069-6_15},
	keywords = {predicated execution, if-conversion, abstract interpretation},
	author = {Lahiri, Shuvendu K. and Bryant, Randal E. and Cook, Byron},
	editor = "Hunt, Warren A.
and Somenzi, Fabio",
	title = "A Symbolic Approach to Predicate Abstraction",
	booktitle = "Computer Aided Verification",
	year = "2003",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "141--153",
	abstract = "Predicate abstraction is a useful form of abstraction for the verification of transition systems with large or infinite state spaces. One of the main bottlenecks of this approach is the extremely large number of decision procedures calls that are required to construct the abstract state space. In this paper we propose the use of a symbolic decision procedure and its application for predicate abstraction. The advantage of the approach is that it reduces the number of calls to the decision procedure exponentially and also provides for reducing the re-computations inherent in the current approaches. We provide two implementations of the symbolic decision procedure: one based on BDDs which leverages the current advances in early quantification algorithms, and the other based on SAT-solvers. We also demonstrate our approach with quantified predicates for verifying parameterized systems. We illustrate the effectiveness of this approach on benchmarks from the verification of microprocessors, communication protocols, parameterized systems, and Microsoft Windows device drivers.",
	isbn = "978-3-540-45069-6"
}

@article{lahti19_are_we_there_yet,
	abstract = {To increase productivity in designing digital hardware components, high-level synthesis (HLS) is seen as the next step in raising the design abstraction level. However, the quality of results (QoRs) of HLS tools has tended to be behind those of manual register-transfer level (RTL) flows. In this paper, we survey the scientific literature published since 2010 about the QoR and productivity differences between the HLS and RTL design flows. Altogether, our survey spans 46 papers and 118 associated applications. Our results show that on average, the QoR of RTL flow is still better than that of the state-of-the-art HLS tools. However, the average development time with HLS tools is only a third of that of the RTL flow, and a designer obtains over four times as high productivity with HLS. Based on our findings, we also present a model case study to sum up the best practices in comparative studies between HLS and RTL. The outcome of our case study is also in line with the survey results, as using an HLS tool is seen to increase the productivity by a factor of six. In addition, to help close the QoR gap, we present a survey of literature focused on improving HLS. Our results let us conclude that HLS is currently a viable option for fast prototyping and for designs with short time to market.},
	author = {{Lahti}, S. and {Sjövall}, P. and {Vanne}, J. and {Hämäläinen}, T. D.},
	url = {https://doi.org/10.1109/TCAD.2018.2834439},
	doi = {10.1109/TCAD.2018.2834439},
	issn = {1937-4151},
	journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {electronic design automation;field programmable gate arrays;high level synthesis;logic design;RTL design flows;average development time;HLS tool;QoR gap;high-level synthesis;digital hardware components;design abstraction level;productivity differences;manual register-transfer level flows;quality of results;Tools;Field programmable gate arrays;Measurement;Hardware;Productivity;Matlab;Hardware design languages;Electronic design automation and methodology;field-programmable gate array (FPGA);hardware description languages (HDLs);high-level synthesis (HLS);reconfigurable logic},
	month = may,
	number = {5},
	pages = {898--911},
	title = {Are We There Yet? a Study on the State of High-Level Synthesis},
	volume = {38},
	year = {2019}
}

@inproceedings{lam88_softw_pipel,
	author = {Lam, M.},
	title = {Software Pipelining: An Effective Scheduling Technique for VLIW Machines},
	year = {1988},
	isbn = {0897912691},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/53990.54022},
	doi = {10.1145/53990.54022},
	abstract = {This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors. In software pipelining, iterations of a loop in the source program are continuously initiated at constant intervals, before the preceding iterations complete. The advantage of software pipelining is that optimal performance can be achieved with compact object code.This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations. Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained.The techniques proposed have been validated by an implementation of a compiler for Warp, a systolic array consisting of 10 VLIW processors. This compiler has been used for developing a large number of applications in the areas of image, signal and scientific processing.},
	booktitle = {Proceedings of the ACM SIGPLAN 1988 Conference on Programming Language Design and Implementation},
	pages = {318–328},
	numpages = {11},
	location = {Atlanta, Georgia, USA},
	series = {PLDI '88}
}

@inproceedings{lamiaux23_comput_cohom_rings_cubic_agda,
	author = {Lamiaux, Thomas and Ljungstr\"{o}m, Axel and M\"{o}rtberg, Anders},
	title = {Computing Cohomology Rings in Cubical Agda},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575677},
	doi = {10.1145/3573105.3575677},
	abstract = {In Homotopy Type Theory, cohomology theories are studied synthetically using higher inductive types and univalence. This paper extends previous developments by providing the first fully mechanized definition of cohomology rings. These rings may be defined as direct sums of cohomology groups together with a multiplication induced by the cup product, and can in many cases be characterized as quotients of multivariate polynomial rings. To this end, we introduce appropriate definitions of direct sums and graded rings, which we then use to define both cohomology rings and multivariate polynomial rings. Using this, we compute the cohomology rings of some classical spaces, such as the spheres and the Klein bottle. The formalization is constructive so that it can be used to do concrete computations, and it relies on the Cubical Agda system which natively supports higher inductive types and computational univalence.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {239–252},
	numpages = {14},
	keywords = {Polynomials, Cohomology Rings, Synthetic Cohomology Theory, Homotopy Type Theory},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@inproceedings{lasser21_costar,
	abstract = {Parsers are security-critical components of many software systems, and verified parsing therefore has a key role to play in secure software design. However, existing verified parsers for context-free grammars are limited in their expressiveness, termination properties, or performance characteristics. They are only compatible with a restricted class of grammars, they are not guaranteed to terminate on all inputs, or they are not designed to be performant on grammars for real-world programming languages and data formats. In this work, we present CoStar, a verified parser that addresses these limitations. The parser is implemented with the Coq Proof Assistant and is based on the ALL(*) parsing algorithm. CoStar is sound and complete for all non-left-recursive grammars; it produces a correct parse tree for its input whenever such a tree exists, and it correctly detects ambiguous inputs. CoStar also provides strong termination guarantees; it terminates without error on all inputs when applied to a non-left-recursive grammar. Finally, CoStar achieves linear-time performance on a range of unambiguous grammars for commonly used languages and data formats.},
	author = {Lasser, Sam and Casinghino, Chris and Fisher, Kathleen and Roux, Cody},
	location = {Virtual, Canada},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3453483.3454053},
	booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
	doi = {10.1145/3453483.3454053},
	isbn = {9781450383912},
	keywords = {interactive theorem proving,parsing},
	pages = {420--434},
	series = {PLDI 2021},
	title = {CoStar: A Verified ALL(*) Parser},
	year = {2021}
}

@INPROCEEDINGS{lattner04_llvm,
	author = {Lattner, C. and Adve, V.},
	booktitle = {International Symposium on Code Generation and Optimization, 2004. CGO 2004.},
	title = {LLVM: a compilation framework for lifelong program analysis \& transformation},
	year = {2004},
	volume = {},
	number = {},
	pages = {75-86},
	doi = {10.1109/CGO.2004.1281665}
}

@inproceedings{lattner21_mlir,
	abstract = {This work presents MLIR, a novel approach to building reusable and extensible compiler infrastructure. MLIR addresses software fragmentation, compilation for heterogeneous hardware, significantly reducing the cost of building domain specific compilers, and connecting existing compilers together. MLIR facilitates the design and implementation of code generators, translators and optimizers at different levels of abstraction and across application domains, hardware targets and execution environments. The contribution of this work includes (1) discussion of MLIR as a research artifact, built for extension and evolution, while identifying the challenges and opportunities posed by this novel design, semantics, optimization specification, system, and engineering. (2) evaluation of MLIR as a generalized infrastructure that reduces the cost of building compilers-describing diverse use-cases to show research and educational opportunities for future programming languages, compilers, execution environments, and computer architecture. The paper also presents the rationale for MLIR, its original design principles, structures and semantics.},
	author = {Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
	booktitle = {2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},
	doi = {10.1109/CGO51591.2021.9370308},
	keywords = {,mlir},
	month = feb,
	pages = {2--14},
	title = {MLIR: Scaling Compiler Infrastructure for Domain Specific Computation},
	year = {2021}
}

@INPROCEEDINGS{lattuada15_ctbsss,
	keywords = {bambu},
	author = {Lattuada, Marco and Ferrandi, Fabrizio},
	booktitle = {2015 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
	title = {Code Transformations Based on Speculative {SDC} Scheduling},
	year = {2015},
	volume = {},
	number = {},
	pages = {71-77},
	doi = {10.1109/ICCAD.2015.7372552}
}

@article{laurel22_dual_number_abstr_static_analy_clark_jacob,
	author = {Laurel, Jacob and Yang, Rem and Singh, Gagandeep and Misailovic, Sasa},
	title = {A Dual Number Abstraction for Static Analysis of Clarke Jacobians},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498718},
	doi = {10.1145/3498718},
	abstract = {We present a novel abstraction for bounding the Clarke Jacobian of a Lipschitz continuous, but not necessarily differentiable function over a local input region. To do so, we leverage a novel abstract domain built upon dual numbers, adapted to soundly over-approximate all first derivatives needed to compute the Clarke Jacobian. We formally prove that our novel forward-mode dual interval evaluation produces a sound, interval domain-based over-approximation of the true Clarke Jacobian for a given input region. Due to the generality of our formalism, we can compute and analyze interval Clarke Jacobians for a broader class of functions than previous works supported – specifically, arbitrary compositions of neural networks with Lipschitz, but non-differentiable perturbations. We implement our technique in a tool called DeepJ and evaluate it on multiple deep neural networks and non-differentiable input perturbations to showcase both the generality and scalability of our analysis. Concretely, we can obtain interval Clarke Jacobians to analyze Lipschitz robustness and local optimization landscapes of both fully-connected and convolutional neural networks for rotational, contrast variation, and haze perturbations, as well as their compositions.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {56},
	numpages = {30},
	keywords = {Differentiable Programming, Robustness, Abstract Interpretation}
}

@inproceedings{lavery95_unrol,
	author = {Lavery, DM and Hwu, WW},
	booktitle = {Proceedings of the 28th Annual International Symposium on Microarchitecture},
	title = {Unrolling-based optimizations for software pipelining},
	year = {1995}
}

@article{le14_compil_valid_equiv_input,
	abstract = {We introduce equivalence modulo inputs (EMI), a simple, widely applicable methodology for validating optimizing compilers. Our key insight is to exploit the close interplay between (1) dynamically executing a program on some test inputs and (2) statically compiling the program to work on all possible inputs. Indeed, the test inputs induce a natural collection of the original program's EMI variants, which can help differentially test any compiler and specifically target the difficult-to-find miscompilations.To create a practical implementation of EMI for validating C compilers, we profile a program's test executions and stochastically prune its unexecuted code. Our extensive testing in eleven months has led to 147 confirmed, unique bug reports for GCC and LLVM alone. The majority of those bugs are miscompilations, and more than 100 have already been fixed.Beyond testing compilers, EMI can be adapted to validate program transformation and analysis systems in general. This work opens up this exciting, new direction.},
	author = {Le, Vu and Afshari, Mehrdad and Su, Zhendong},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/2666356.2594334},
	doi = {10.1145/2666356.2594334},
	issn = {0362-1340},
	journaltitle = {SIGPLAN Not.},
	keywords = {miscompilation,automated testing,compiler testing,equivalent program variants},
	month = jun,
	number = {6},
	pages = {216--226},
	title = {Compiler Validation via Equivalence modulo Inputs},
	volume = {49},
	year = {2014}
}

@article{le22_quant_inter_separ_conjun_local,
	author = {Le, Xuan-Bach and Lin, Shang-Wei and Sun, Jun and Sanan, David},
	title = {A Quantum Interpretation of Separating Conjunction for Local Reasoning of Quantum Programs Based on Separation Logic},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498697},
	doi = {10.1145/3498697},
	abstract = {It is well-known that quantum programs are not only complicated to design but also challenging to verify because the quantum states can have exponential size and require sophisticated mathematics to encode and manipulate. To tackle the state-space explosion problem for quantum reasoning, we propose a Hoare-style inference framework that supports local reasoning for quantum programs. By providing a quantum interpretation of the separating conjunction, we are able to infuse separation logic into our framework and apply local reasoning using a quantum frame rule that is similar to the classical frame rule. For evaluation, we apply our framework to verify various quantum programs including Deutsch–Jozsa’s algorithm and Grover's algorithm.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {36},
	numpages = {27},
	keywords = {Quantum Computing, Verification, Formal Semantics}
}

@inproceedings{lechenet20_fast_verif_liven_analy_ssa_form,
	abstract = {Liveness analysis is a standard compiler analysis, enabling several optimizations such as deadcode elimination. The SSA form is a popular compiler intermediate language allowing for simple and fast optimizations. Boissinot et al. [7] designed a fast liveness analysis by combining the specific properties of SSA with graph-theoretic ideas such as depth-first search and dominance. We formalize their approach in the Coq proof assistant, inside the CompCertSSA verified C compiler. We also compare experimentally this approach on CompCert's benchmarks with respect to the classic data-flow-based liveness analysis, and observe performance gains.},
	author = {Léchenet, Jean-Christophe and Blazy, Sandrine and Pichardie, David},
	editor = {Peltier, Nicolas and Sofronie-Stokkermans, Viorica},
	location = {Cham},
	publisher = {Springer},
	booktitle = {Automated Reasoning},
	isbn = {978-3-030-51054-1},
	keywords = {CompCertSSA},
	pages = {324--340},
	title = {A Fast Verified Liveness Analysis in SSA Form},
	year = {2020}
}

@inproceedings{lee11_equiv,
	author = {{Lee}, C. and {Shih}, C. and {Huang}, J. and {Jou}, J.},
	url = {https://doi.org/10.1109/ASPDAC.2011.5722241},
	booktitle = {16th Asia and South Pacific Design Automation Conference (ASP-DAC 2011)},
	doi = {10.1109/ASPDAC.2011.5722241},
	issn = {2153-6961},
	keywords = {finite state machines;high level synthesis;scheduling;scheduling equivalence checking;speculative code transformations;high-level synthesis;finite state machine;common subexpression extraction;basic block boundaries;Scheduling;Runtime;Algorithm design and analysis;Merging;Automata;Scheduling algorithm},
	month = jan,
	pages = {497--502},
	title = {Equivalence checking of scheduling with speculative code transformations in high-level synthesis},
	year = {2011}
}

@article{lee21_combin_top_propag_bottom_enumer,
	abstract = {We present an effective method for scalable and general-purpose inductive program synthesis. There have been two main approaches for inductive synthesis: enumerative search, which repeatedly enumerates possible candidate programs, and the top-down propagation (TDP), which recursively decomposes a given large synthesis problem into smaller subproblems. Enumerative search is generally applicable but limited in scalability, and the TDP is efficient but only works for special grammars or applications. In this paper, we synergistically combine the two approaches. We generate small program subexpressions via enumerative search and put them together into the desired program by using the TDP. Enumerative search enables to bring the power of TDP into arbitrary grammars, and the TDP helps to overcome the limited scalability of enumerative search. We apply our approach to a standard formulation, syntax-guided synthesis (SyGuS), thereby supporting a broad class of inductive synthesis problems. We have implemented our approach in a tool called Duet and evaluate it on SyGuS benchmark problems from various domains. We show that Duet achieves significant performance gains over existing general-purpose as well as domain-specific synthesizers.},
	author = {Lee, Woosuk},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434335},
	doi = {10.1145/3434335},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {Programming by example,Syntax-guided Synthesis},
	month = jan,
	number = {POPL},
	title = {Combining the Top-down Propagation and Bottom-up Enumeration for Inductive Program Synthesis},
	volume = {5},
	year = {2021}
}

@inproceedings{lee22_effic_profil_guided_size_optim,
	author = {Lee, Kyungwoo and Hoag, Ellis and Tillmann, Nikolai},
	title = {Efficient Profile-Guided Size Optimization for Native Mobile Applications},
	year = {2022},
	isbn = {9781450391832},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497776.3517764},
	doi = {10.1145/3497776.3517764},
	abstract = {Positive user experience of mobile apps demands they not only launch fast and run fluidly, but are also small in order to reduce network bandwidth from regular updates. Conventional optimizations often trade off size regressions for performance wins, making them impractical in the mobile space. Indeed, profile-guided optimization (PGO) is successful in server workloads, but is not effective at reducing size and page faults for mobile apps. Also, profiles must be collected from instrumenting builds that are up to 2X larger, so they cannot run normally on real mobile devices. In this paper, we first introduce Machine IR Profile (MIP), a lightweight instrumentation that runs at the machine IR level. Unlike the existing LLVM IR instrumentation counterpart, MIP withholds static metadata from the instrumenting binaries leading to a 2/3 reduction in size overhead. In addition, MIP collects profile data that is more relevant to optimizations in the mobile space. Then we propose three improvements to the LLVM machine outliner: (i) the global outliner overcomes the local scope of the machine outliner when using ThinLTO, (ii) the frame outliner effectively outlines irregular prologues and epilogues, and (iii) the custom outliner outlines frequent patterns occurring in Objective-C and Swift. Lastly, we present our PGO that orders hot start-up functions to minimize page faults, and controls the size optimization level (-Os vs -Oz) for functions based on their estimated execution time driven from MIP. We also order cold functions based on similarity to minimize the compressed app size. Our work improves both the size and performance of real-world mobile apps when compared to the MinSize (-Oz) optimization level: (i) in SocialApp, we reduced the compressed app size by 5.2%, the uncompressed app size by 9.6% and the page faults by 20.6%, and (ii) in ChatApp, we reduced them by 2.4%, 4.6% and 36.4%, respectively.},
	booktitle = {Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction},
	pages = {243–253},
	numpages = {11},
	keywords = {profile-guided optimizations, iOS, size optimizations, machine outlining, mobile applications},
	location = {Seoul, South Korea},
	series = {CC 2022}
}

@article{lee23_induc_synth_struc_recur_funct,
	author = {Lee, Woosuk and Cho, Hangyeol},
	title = {Inductive Synthesis of Structurally Recursive Functional Programs from Non-Recursive Expressions},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571263},
	doi = {10.1145/3571263},
	abstract = {We present a novel approach to synthesizing recursive functional programs from input-output examples. Synthesizing a recursive function is challenging because recursive subexpressions should be constructed while the target function has not been fully defined yet. We address this challenge by using a new technique we call block-based pruning. A block refers to a recursion- and conditional-free expression (i.e., straight-line code) that yields an output from a particular input. We first synthesize as many blocks as possible for each input-output example, and then we explore the space of recursive programs, pruning candidates that are inconsistent with the blocks. Our method is based on an efficient version space learning, thereby effectively dealing with a possibly enormous number of blocks. In addition, we present a method that uses sampled input-output behaviors of library functions to enable a goal-directed search for a recursive program using the library. We have implemented our approach in a system called Trio and evaluated it on synthesis tasks from prior work and on new tasks. Our experiments show that Trio outperforms prior work by synthesizing a solution to 98% of the benchmarks in our benchmark suite.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {70},
	numpages = {31},
	keywords = {Synthesis, Programming by Example, Recursive Functional Programs}
}

@article{lee23_smoot_analy_probab_progr_applic,
	author = {Lee, Wonyeol and Rival, Xavier and Yang, Hongseok},
	title = {Smoothness Analysis for Probabilistic Programs with Application to Optimised Variational Inference},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571205},
	doi = {10.1145/3571205},
	abstract = {We present a static analysis for discovering differentiable or more generally smooth parts of a given probabilistic program, and show how the analysis can be used to improve the pathwise gradient estimator, one of the most popular methods for posterior inference and model learning. Our improvement increases the scope of the estimator from differentiable models to non-differentiable ones without requiring manual intervention of the user; the improved estimator automatically identifies differentiable parts of a given probabilistic program using our static analysis, and applies the pathwise gradient estimator to the identified parts while using a more general but less efficient estimator, called score estimator, for the rest of the program. Our analysis has a surprisingly subtle soundness argument, partly due to the misbehaviours of some target smoothness properties when viewed from the perspective of program analysis designers. For instance, some smoothness properties, such as partial differentiability and partial continuity, are not preserved by function composition, and this makes it difficult to analyse sequential composition soundly without heavily sacrificing precision. We formulate five assumptions on a target smoothness property, prove the soundness of our analysis under those assumptions, and show that our leading examples satisfy these assumptions. We also show that by using information from our analysis instantiated for differentiability, our improved gradient estimator satisfies an important differentiability requirement and thus computes the correct estimate on average (i.e., returns an unbiased estimate) under a regularity condition. Our experiments with representative probabilistic programs in the Pyro language show that our static analysis is capable of identifying smooth parts of those programs accurately, and making our improved pathwise gradient estimator exploit all the opportunities for high performance in those programs.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {12},
	numpages = {32},
	keywords = {smoothness, variational inference, probabilistic programming, static analysis}
}

@article{leijen23_tail_recur_modul_contex,
	author = {Leijen, Daan and Lorenzen, Anton},
	title = {Tail Recursion Modulo Context: An Equational Approach},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571233},
	doi = {10.1145/3571233},
	abstract = {The tail-recursion modulo cons transformation can rewrite functions that are not quite tail-recursive into a tail-recursive form that can be executed efficiently. In this article we generalize tail recursion modulo cons (TRMc) to modulo contexts (TRMC), and calculate a general TRMC algorithm from its specification. We can instantiate our general algorithm by providing an implementation of application and composition on abstract contexts, and showing that our context laws_ hold. We provide some known instantiations of TRMC, namely modulo evaluation contexts (CPS), and associative operations, and further instantiantions not so commonly associated with TRMC, such as defunctionalized evaluation contexts, monoids, semirings, exponents, and cons products. We study the modulo cons instantiation in particular and prove that an instantiation using Minamide’s hole calculus is sound. We also calculate a second instantiation in terms of the Perceus heap semantics to precisely reason about the soundness of in-place update. While all previous approaches to TRMc fail in the presence of non-linear control (for example induced by call/cc, shift/reset or algebraic effect handlers), we can elegantly extend the heap semantics to a hybrid approach which dynamically adapts to non-linear control flow. We have a full implementation of hybrid TRMc in the Koka language and our benchmark shows the TRMc transformed functions are always as fast or faster than using manual alternatives.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {40},
	numpages = {30},
	keywords = {Tail Recursion Modulo Cons, Non-Linear Control, Equational Reasoning}
}

@InProceedings{leino10_d,
	author = "Leino, K. Rustan M.",
	editor = "Clarke, Edmund M.
and Voronkov, Andrei",
	title = "Dafny: An Automatic Program Verifier for Functional Correctness",
	booktitle = "Logic for Programming, Artificial Intelligence, and Reasoning",
	year = "2010",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "348--370",
	abstract = "Traditionally, the full verification of a program's functional correctness has been obtained with pen and paper or with interactive proof assistants, whereas only reduced verification tasks, such as extended static checking, have enjoyed the automation offered by satisfiability-modulo-theories (SMT) solvers. More recently, powerful SMT solvers and well-designed program verifiers are starting to break that tradition, thus reducing the effort involved in doing full verification.",
	isbn = "978-3-642-17511-4"
}

@article{leiserson91_retim,
	keywords = {modulo scheduling, loop scheduling, hardware scheduling},
	doi = {10.1007/bf01759032},
	url = {https://doi.org/10.1007/bf01759032},
	year = {1991},
	month = jun,
	publisher = {Springer Science and Business Media {LLC}},
	volume = {6},
	number = {1-6},
	pages = {5--35},
	author = {Charles E. Leiserson and James B. Saxe},
	title = {Retiming synchronous circuitry},
	journal = {Algorithmica}
}

@article{lemerre23_ssa_trans_is_abstr_inter,
	author = {Lemerre, Matthieu},
	title = {SSA Translation Is an Abstract Interpretation},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571258},
	doi = {10.1145/3571258},
	abstract = {Static single assignment (SSA) form is a popular intermediate representation that helps implement useful static analyses, including global value numbering (GVN), sparse dataflow analyses, or SMT-based abstract interpretation or model checking. However, the precision of the SSA translation itself depends on static analyses, and a priori static analysis is even indispensable in the case of low-level input languages like machine code. To solve this chicken-and-egg problem, we propose to turn the SSA translation into a standard static analysis based on abstract interpretation. This allows the SSA translation to be combined with other static analyses in a single pass, taking advantage of the fact that it is more precise to combine analyses than applying passes in sequence. We illustrate the practicality of these results by writing a simple dataflow analysis that performs SSA translation, optimistic global value numbering, sparse conditional constant propagation, and loop-invariant code motion in a single small pass; and by presenting a multi-language static analyzer for both C and machine code that uses the SSA abstract domain as its main intermediate representation.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {65},
	numpages = {30},
	keywords = {Static Single Assignment (SSA), Abstract interpretation, Cyclic term graph}
}

@article{lepigre22_vip,
	author = {Lepigre, Rodolphe and Sammler, Michael and Memarian, Kayvan and Krebbers, Robbert and Dreyer, Derek and Sewell, Peter},
	title = {VIP: Verifying Real-World C Idioms with Integer-Pointer Casts},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498681},
	doi = {10.1145/3498681},
	abstract = {Systems code often requires fine-grained control over memory layout and pointers, expressed using low-level (e.g., bitwise) operations on pointer values. Since these operations go beyond what basic pointer arithmetic in C allows, they are performed with the help of integer-pointer casts. Prior work has explored increasingly realistic memory object models for C that account for the desired semantics of integer-pointer casts while also being sound w.r.t. compiler optimisations, culminating in PNVI, the preferred memory object model in ongoing discussions within the ISO WG14 C standards committee. However, its complexity makes it an unappealing target for verification, and no tools currently exist to verify C programs under PNVI. In this paper, we introduce VIP, a new memory object model aimed at supporting C verification. VIP sidesteps the complexities of PNVI with a simple but effective idea: a new construct that lets programmers express the intended provenances of integer-pointer casts explicitly. At the same time, we prove VIP compatible with PNVI, thus enabling verification on top of VIP to benefit from PNVI’s validation with respect to practice. In particular, we build a verification tool, RefinedC-VIP, for verifying programs under VIP semantics. As the name suggests, RefinedC-VIP extends the recently developed RefinedC tool, which is automated yet also produces foundational proofs in Coq. We evaluate RefinedC-VIP on a range of systems-code idioms, and validate VIP’s expressiveness via an implementation in the Cerberus C semantics.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {20},
	numpages = {32},
	keywords = {Coq, proof automation, Iris, separation logic, memory model, pointer provenance, C programming language}
}

@inproceedings{leroy06_coind_big_step_operat_seman,
	abstract = {This paper illustrates the use of coinductive definitions and proofs in big-step operational semantics, enabling the latter to describe diverging evaluations in addition to terminating evaluations. We show applications to proofs of type soundness and to proofs of semantic preservation for compilers.},
	author = {Leroy, Xavier},
	editor = {Sestoft, Peter},
	location = {Berlin, Heidelberg},
	publisher = {Springer},
	booktitle = {Programming Languages and Systems},
	isbn = {978-3-540-33096-7},
	pages = {54--68},
	title = {Coinductive Big-Step Operational Semantics},
	year = {2006}
}

@inproceedings{leroy06_formal_certif_compil_back_end,
	author = {Leroy, Xavier},
	location = {Charleston, South Carolina, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/1111037.1111042},
	booktitle = {Conference Record of the 33rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
	doi = {10.1145/1111037.1111042},
	isbn = {1595930272},
	keywords = {the Coq theorem prover,certified compilation,compiler transformations and optimizations,program proof,semantic preservation},
	pages = {42--54},
	series = {POPL '06},
	title = {Formal Certification of a Compiler Back-End or: Programming a Compiler with a Proof Assistant},
	year = {2006}
}

@article{leroy09_formal_verif_compil_back_end,
	abstract = {This article describes the development and formal verification (proof of semantic preservation) of a compiler back-end from Cminor (a simple imperative intermediate language) to PowerPC assembly code, using the Coq proof assistant both for programming the compiler and for proving its soundness. Such a verified compiler is useful in the context of formal methods applied to the certification of critical software: the verification of the compiler guarantees that the safety properties proved on the source code hold for the executable compiled code as well.},
	author = {Leroy, Xavier},
	doi = {10.1007/s10817-009-9155-4},
	issn = {1573-0670},
	journaltitle = {Journal of Automated Reasoning},
	number = {4},
	pages = {363},
	title = {A Formally Verified Compiler Back-End},
	volume = {43},
	year = {2009}
}

@article{leroy09_formal_verif_realis_compil,
	author = {Leroy, Xavier},
	location = {New York, NY, USA},
	publisher = {ACM},
	doi = {10.1145/1538788.1538814},
	issn = {0001-0782},
	journaltitle = {Commun. ACM},
	month = jul,
	number = {7},
	pages = {107--115},
	title = {Formal Verification of a Realistic Compiler},
	volume = {52},
	year = {2009}
}

@inproceedings{leroy16_cfvoc,
	TITLE = {{CompCert - A Formally Verified Optimizing Compiler}},
	AUTHOR = {Leroy, Xavier and Blazy, Sandrine and K{\"a}stner, Daniel and Schommer, Bernhard and Pister, Markus and Ferdinand, Christian},
	URL = {https://inria.hal.science/hal-01238879},
	BOOKTITLE = {{ERTS 2016: Embedded Real Time Software and Systems, 8th European Congress}},
	ADDRESS = {Toulouse, France},
	ORGANIZATION = {{SEE}},
	YEAR = {2016},
	MONTH = Jan,
	PDF = {https://inria.hal.science/hal-01238879/file/erts2016_compcert.pdf},
	HAL_ID = {hal-01238879},
	HAL_VERSION = {v1}
}

@inproceedings{lescuyer08_sat_coq,
	author = {Lescuyer, Stéphane and Conchon, Sylvain},
	organization = {Citeseer},
	booktitle = {21 st International Conference on Theorem Proving in Higher Order Logics},
	keywords = {sat,reflection,coq, verification},
	pages = {64},
	title = {A reflexive formalization of a SAT solver in Coq},
	year = {2008}
}

@InProceedings{lescuyer09_improv_coq_propos_reason_using,
	keywords = {SAT, coq, verification, reflection},
	doi = {10.1007/978-3-642-04222-5_18},
	author = {Lescuyer, Stéphane and Conchon, Sylvain},
	editor = "Ghilardi, Silvio
and Sebastiani, Roberto",
	title = "Improving Coq Propositional Reasoning Using a Lazy CNF Conversion Scheme",
	booktitle = "Frontiers of Combining Systems",
	year = "2009",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "287--303",
	abstract = "In an attempt to improve automation capabilities in the Coq proof assistant, we develop a tactic for the propositional fragment based on the DPLL procedure. Although formulas naturally arising in interactive proofs do not require a state-of-the-art SAT solver, the conversion to clausal form required by DPLL strongly damages the performance of the procedure. In this paper, we present a reflexive DPLL algorithm formalized in Coq which outperforms the existing tactics. It is tightly coupled with a lazy CNF conversion scheme which, unlike Tseitin-style approaches, does not disrupt the procedure. This conversion relies on a lazy mechanism which requires slight adaptations of the original DPLL. As far as we know, this is the first formal proof of this mechanism and its Coq implementation raises interesting challenges.",
	isbn = "978-3-642-04222-5"
}

@phdthesis{lescuyer11_formal_implem_reflex_tactic_autom_deduc_coq,
	url = {http://www.theses.fr/2011PA112363},
	title = {Formalizing and Implementing a Reflexive Tactic for Automated Deduction in Coq},
	author = {Lescuyer, Stephane},
	year = {2011},
	note = {Thèse de doctorat dirigée par Contejean, Evelyne Informatique Paris 11 2011},
	note = {2011PA112363},
	url = {http://www.theses.fr/2011PA112363/document}
}

@inproceedings{letouzey08_extrac_coq,
	abstract = {The extraction mechanism of Coq allows one to transform Coq proofs and functions into functional programs. We illustrate the behavior of this tool by reviewing several variants of Coq definitions for Euclidean division, as well as some more advanced examples. We then continue with a more general description of this tool: key features, main examples, strengths, limitations and perspectives.},
	author = {Letouzey, Pierre},
	editor = {Beckmann, Arnold and Dimitracopoulos, Costas and Löwe, Benedikt},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Logic and Theory of Algorithms},
	isbn = {978-3-540-69407-6},
	pages = {359--369},
	title = {Extraction in Coq: An Overview},
	year = {2008}
}

@inproceedings{leung15_c_veril,
	author = {{Leung}, A. and {Bounov}, D. and {Lerner}, S.},
	url = {https://doi.org/10.1109/MEMCOD.2015.7340466},
	booktitle = {2015 ACM/IEEE International Conference on Formal Methods and Models for Codesign (MEMOCODE)},
	doi = {10.1109/MEMCOD.2015.7340466},
	keywords = {C language;formal specification;hardware description languages;high level synthesis;program compilers;program verification;C-to-Verilog translation validation;digital circuit design;high-level languages;C language;C++ language;high-level synthesis tools;HLS tools;high-level specifications;hardware description language;bugs;correctness verification;HLS translation process;C program;Verilog code generation;VTV tool;Xilinx Vivado HLS compiler;Hardware design languages;Hardware;Semantics;Syntactics;Program processors;Protocols;Ports (Computers)},
	month = sep,
	pages = {42--47},
	title = {C-to-Verilog translation validation},
	year = {2015}
}

@article{lew23_adev,
	author = {Lew, Alexander K. and Huot, Mathieu and Staton, Sam and Mansinghka, Vikash K.},
	title = {ADEV: Sound Automatic Differentiation of Expected Values of Probabilistic Programs},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571198},
	doi = {10.1145/3571198},
	abstract = {Optimizing the expected values of probabilistic processes is a central problem in computer science and its applications, arising in fields ranging from artificial intelligence to operations research to statistical computing. Unfortunately, automatic differentiation techniques developed for deterministic programs do not in general compute the correct gradients needed for widely used solutions based on gradient-based optimization. In this paper, we present ADEV, an extension to forward-mode AD that correctly differentiates the expectations of probabilistic processes represented as programs that make random choices. Our algorithm is a source-to-source program transformation on an expressive, higher-order language for probabilistic computation, with both discrete and continuous probability distributions. The result of our transformation is a new probabilistic program, whose expected return value is the derivative of the original program’s expectation. This output program can be run to generate unbiased Monte Carlo estimates of the desired gradient, that can be used within the inner loop of stochastic gradient descent. We prove ADEV correct using logical relations over the denotations of the source and target probabilistic programs. Because it modularly extends forward-mode AD, our algorithm lends itself to a concise implementation strategy, which we exploit to develop a prototype in just a few dozen lines of Haskell (https://github.com/probcomp/adev).},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {5},
	numpages = {33},
	keywords = {machine learning theory, functional programming, denotational semantics, automatic differentiation, probabilistic programming, correctness, logical relations}
}

@inproceedings{li15_equiv,
	author = {{Li}, T. and {Hu}, J. and {Guo}, Y. and {Li}, S. and {Tan}, Q.},
	url = {https://doi.org/10.1109/ISQED.2015.7085435},
	booktitle = {Sixteenth International Symposium on Quality Electronic Design},
	doi = {10.1109/ISQED.2015.7085435},
	issn = {1948-3295},
	keywords = {graph theory;high level synthesis;processor scheduling;high level synthesis tools;electronic system level designs;design-productivity gap;scheduling optimizations;shared-value graphs techniques;cut-point;translation validation;equivalence checking;synthesis process;high-quality hardware system;Sparks;Algorithm design and analysis;Optimization;Scheduling;High level synthesis;Processor scheduling;System-level design},
	month = mar,
	pages = {257--262},
	title = {Equivalence checking of scheduling in high-level synthesis},
	year = {2015}
}

@inproceedings{li15_resour_aware_throug_optim_high_level_synth,
	author = {Li, Peng and Zhang, Peng and Pouchet, Louis-Noel and Cong, Jason},
	location = {Monterey, California, USA},
	publisher = {ACM},
	url = {https://doi.org/10.1145/2684746.2689065},
	booktitle = {Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	doi = {10.1145/2684746.2689065},
	isbn = {978-1-4503-3315-3},
	keywords = {area constraint,high-level synthesis,resource sharing,throughput optimization},
	pages = {200--209},
	series = {FPGA '15},
	title = {Resource-Aware Throughput Optimization for High-Level Synthesis},
	year = {2015}
}

@inproceedings{li16_gated_graph_sequen_neural_networ,
	author = {Li, Yujia and Tarlow, Daniel and Brockschmidt, Marc and Zemel, Richard S.},
	editor = {Bengio, Yoshua and LeCun, Yann},
	url = {http://arxiv.org/abs/1511.05493},
	booktitle = {4th International Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
	title = {Gated Graph Sequence Neural Networks},
	year = {2016}
}

@article{li22_effic_algor_dynam_bidir_dyck_reach,
	author = {Li, Yuanbo and Satya, Kris and Zhang, Qirun},
	title = {Efficient Algorithms for Dynamic Bidirected Dyck-Reachability},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498724},
	doi = {10.1145/3498724},
	abstract = {Dyck-reachability is a fundamental formulation for program analysis, which has been widely used to capture properly-matched-parenthesis program properties such as function calls/returns and field writes/reads. Bidirected Dyck-reachability is a relaxation of Dyck-reachability on bidirected graphs where each edge u→(iv labeled by an open parenthesis “(i” is accompanied with an inverse edge v→)iu labeled by the corresponding close parenthesis “)i”, and vice versa. In practice, many client analyses such as alias analysis adopt the bidirected Dyck-reachability formulation. Bidirected Dyck-reachability admits an optimal reachability algorithm. Specifically, given a graph with n nodes and m edges, the optimal bidirected Dyck-reachability algorithm computes all-pairs reachability information in O(m) time. This paper focuses on the dynamic version of bidirected Dyck-reachability. In particular, we consider the problem of maintaining all-pairs Dyck-reachability information in bidirected graphs under a sequence of edge insertions and deletions. Dynamic bidirected Dyck-reachability can formulate many program analysis problems in the presence of code changes. Unfortunately, solving dynamic graph reachability problems is challenging. For example, even for maintaining transitive closure, the fastest deterministic dynamic algorithm requires O(n2) update time to achieve O(1) query time. All-pairs Dyck-reachability is a generalization of transitive closure. Despite extensive research on incremental computation, there is no algorithmic development on dynamic graph algorithms for program analysis with worst-case guarantees. Our work fills the gap and proposes the first dynamic algorithm for Dyck reachability on bidirected graphs. Our dynamic algorithms can handle each graph update (i.e., edge insertion and deletion) in O(n·α(n)) time and support any all-pairs reachability query in O(1) time, where α(n) is the inverse Ackermann function. We have implemented and evaluated our dynamic algorithm on an alias analysis and a context-sensitive data-dependence analysis for Java. We compare our dynamic algorithms against a straightforward approach based on the O(m)-time optimal bidirected Dyck-reachability algorithm and a recent incremental Datalog solver. Experimental results show that our algorithm achieves orders of magnitude speedup over both approaches.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {62},
	numpages = {29},
	keywords = {Dynamic Graph Algorithms, Incremental Analysis, Bidirected Graphs, Dyck-Reachability}
}

@article{li22_progr_adver_tl_embed,
	author = {Li, Yao and Weirich, Stephanie},
	title = {Program Adverbs and Tl\"{o}n Embeddings},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3547632},
	doi = {10.1145/3547632},
	abstract = {Free monads (and their variants) have become a popular general-purpose tool for representing the semantics of effectful programs in proof assistants. These data structures support the compositional definition of semantics parameterized by uninterpreted events, while admitting a rich equational theory of equivalence. But monads are not the only way to structure effectful computation, why should we limit ourselves? In this paper, inspired by applicative functors, selective functors, and other structures, we define a collection of data structures and theories, which we call program adverbs, that capture a variety of computational patterns. Program adverbs are themselves composable, allowing them to be used to specify the semantics of languages with multiple computation patterns. We use program adverbs as the basis for a new class of semantic embeddings called Tl\"{o}n embeddings. Compared with embeddings based on free monads, Tl\"{o}n embeddings allow more flexibility in computational modeling of effects, while retaining more information about the program's syntactic structure.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {101},
	numpages = {31},
	keywords = {applicative}
}

@article{li23_singl_sourc_singl_target_inter,
	author = {Li, Yuanbo and Zhang, Qirun and Reps, Thomas},
	title = {Single-Source-Single-Target Interleaved-Dyck Reachability via Integer Linear Programming},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571228},
	doi = {10.1145/3571228},
	abstract = {An interleaved-Dyck (InterDyck) language consists of the interleaving of two or more Dyck languages, where each Dyck language represents a set of strings of balanced parentheses.InterDyck-reachability is a fundamental framework for program analyzers that simultaneously track multiple properly-matched pairs of actions such as call/return, lock/unlock, or write-data/read-data.Existing InterDyck-reachability algorithms are based on the well-known tabulation technique. This paper presents a new perspective on solving InterDyck-reachability. Our key observation is that for the single-source-single-target InterDyck-reachability variant, it is feasible to summarize all paths from the source node to the target node based on path expressions. Therefore, InterDyck-reachability becomes an InterDyck-path-recognition problem over path expressions. Instead of computing summary edges as in traditional tabulation algorithms, this new perspective enables us to express InterDyck-reachability as a parenthesis-counting problem, which can be naturally formulated via integer linear programming (ILP). We implemented our ILP-based algorithm and performed extensive evaluations based on two client analyses (a reachability analysis for concurrent programs and a taint analysis). In particular, we evaluated our algorithm against two types of algorithms: (1) the general all-pairs InterDyck-reachability algorithms based on linear conjunctive language (LCL) reachability and synchronized pushdown system (SPDS) reachability, and (2) two domain-specific algorithms for both client analyses. The experimental results are encouraging. Our algorithm achieves 1.42\texttimes{}, 28.24\texttimes{}, and 11.76\texttimes{} speedup for the concurrency-analysis benchmarks compared to all-pair LCL-reachability, SPDS-reachability, and domain-specific tools, respectively; 1.2\texttimes{}, 69.9\texttimes{}, and 0.98\texttimes{} speedup for the taint-analysis benchmarks. Moreover, the algorithm also provides precision improvements, particularly for taint analysis, where it achieves 4.55%, 11.1%, and 6.8% improvement, respectively.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {35},
	numpages = {24},
	keywords = {Single-Source-Single-Target Variant, Interleaved-Dyck Reachability, Path Expressions, Integer Linear Programming}
}

@article{li23_type_preser_depen_aware_guide,
	author = {Li, Jianlin and Ven, Leni and Shi, Pengyuan and Zhang, Yizhou},
	title = {Type-Preserving, Dependence-Aware Guide Generation for Sound, Effective Amortized Probabilistic Inference},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571243},
	doi = {10.1145/3571243},
	abstract = {In probabilistic programming languages (PPLs), a critical step in optimization-based inference methods is constructing, for a given model program, a trainable guide program. Soundness and effectiveness of inference rely on constructing good guides, but the expressive power of a universal PPL poses challenges. This paper introduces an approach to automatically generating guides for deep amortized inference in a universal PPL. Guides are generated using a type-directed translation per a novel behavioral type system. Guide generation extracts and exploits independence structures using a syntactic approach to conditional independence, with a semantic account left to further work. Despite the control-flow expressiveness allowed by the universal PPL, generated guides are guaranteed to satisfy a critical soundness condition and moreover, consistently improve training and inference over state-of-the-art baselines for a suite of benchmarks.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {50},
	numpages = {29},
	keywords = {Probabilistic programming, type systems, amortized inference, guide generation}
}

@article{liang18_hi_dmm,
	abstract = {High-level synthesis (HLS) of field programmable gate array (FPGA)-based accelerators has been proposed in order to simplify accelerator design process with respect to design time and complexity. However, modern HLS tools do not consider dynamic memory allocation constructs in high-level programming languages like C and limit themselves to static memory allocation. This paper proposes a dynamic memory allocation and management scheme, called Hi-DMM, for inclusion in commercial HLS design flows. Hi-DMM performs source-to-source transformation of user C code with dynamic memory constructs into C-source code with the dynamic memory allocator and management scheme developed in this paper. The transformed C-source code is amenable to synthesis by commercial tools like Vivado HLS. Relying on buddy tree-based allocation schemes and efficient hardware implementation of the allocators, Hi-DMM achieves 4x speed-up in both fine-grained and coarse-grained memory allocation compared to previous works. Experimental results obtained by including Hi-DMM with Vivado-HLS show that dynamic memory allocation of FPGA memory resources can be achieved at a much lower latency with minimal resource overhead, paving the way for synthesis of dynamic memory constructs in commercial HLS flows.},
	author = {{Liang}, T. and {Zhao}, J. and {Feng}, L. and {Sinha}, S. and {Zhang}, W.},
	url = {https://doi.org/10.1109/TCAD.2018.2857040},
	doi = {10.1109/TCAD.2018.2857040},
	issn = {1937-4151},
	journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {field programmable gate arrays;high level synthesis;storage management;coarse-grained memory allocation;Vivado-HLS;FPGA memory resources;high-performance dynamic memory management;accelerator design process;high-level programming languages;static memory allocation;dynamic memory allocator;Vivado HLS;buddy tree-based allocation schemes;dynamic memory allocation;source-to-source transformation;C-source code;Hi-DMM;Resource management;Memory management;Field programmable gate arrays;Dynamic scheduling;Tools;Hardware;Transforms;Field programmable gate array (FPGA);high-level synthesis (HLS);memory management},
	month = nov,
	number = {11},
	pages = {2555--2566},
	title = {Hi-Dmm: High-Performance Dynamic Memory Management in High-Level Synthesis},
	volume = {37},
	year = {2018}
}

@INPROCEEDINGS{liang19_hi_clock,
	author = {Liang, Tingyuan and Zhao, Jieru and Feng, Liang and Sinha, Sharad and Zhang, Wei},
	booktitle = {2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
	title = {Hi-ClockFlow: Multi-Clock Dataflow Automation and Throughput Optimization in High-Level Synthesis},
	year = {2019},
	volume = {},
	number = {},
	pages = {1-6},
	doi = {10.1109/ICCAD45719.2019.8942136}
}

@inproceedings{lidbury15_many_core_compil_fuzzin,
	author = {Lidbury, Christopher and Lascu, Andrei and Chong, Nathan and Donaldson, Alastair F.},
	location = {Portland, OR, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/2737924.2737986},
	booktitle = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation},
	doi = {10.1145/2737924.2737986},
	isbn = {9781450334686},
	keywords = {random testing,Compilers,OpenCL,metamorphic testing,GPUs,concurrency},
	pages = {65--76},
	series = {PLDI '15},
	title = {Many-Core Compiler Fuzzing},
	year = {2015}
}

@article{lim21_approac_gener_correc_round_math,
	abstract = {Given the importance of floating point (FP) performance in numerous domains, several new variants of FP and its alternatives have been proposed (e.g., Bfloat16, TensorFloat32, and posits). These representations do not have correctly rounded math libraries. Further, the use of existing FP libraries for these new representations can produce incorrect results. This paper proposes a novel approach for generating polynomial approximations that can be used to implement correctly rounded math libraries. Existing methods generate polynomials that approximate the real value of an elementary function 𝑓 (𝑥) and produce wrong results due to approximation errors and rounding errors in the implementation. In contrast, our approach generates polynomials that approximate the correctly rounded value of 𝑓 (𝑥) (i.e., the value of 𝑓 (𝑥) rounded to the target representation). It provides more margin to identify efficient polynomials that produce correctly rounded results for all inputs. We frame the problem of generating efficient polynomials that produce correctly rounded results as a linear programming problem. Using our approach, we have developed correctly rounded, yet faster, implementations of elementary functions for multiple target representations.},
	author = {Lim, Jay P. and Aanjaneya, Mridul and Gustafson, John and Nagarakatte, Santosh},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434310},
	doi = {10.1145/3434310},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {correctly rounded math libraries,floating point,posits},
	month = jan,
	number = {POPL},
	title = {An Approach to Generate Correctly Rounded Math Libraries for New Floating Point Variants},
	volume = {5},
	year = {2021}
}

@article{lim22_one_polyn_approx_produc_correc,
	author = {Lim, Jay P. and Nagarakatte, Santosh},
	title = {One Polynomial Approximation to Produce Correctly Rounded Results of an Elementary Function for Multiple Representations and Rounding Modes},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498664},
	doi = {10.1145/3498664},
	abstract = {Mainstream math libraries for floating point (FP) do not produce correctly rounded results for all inputs. In contrast, CR-LIBM and RLIBM provide correctly rounded implementations for a specific FP representation with one rounding mode. Using such libraries for a representation with a new rounding mode or with different precision will result in wrong results due to double rounding. This paper proposes a novel method to generate a single polynomial approximation that produces correctly rounded results for all inputs for multiple rounding modes and multiple precision configurations. To generate a correctly rounded library for n-bits, our key idea is to generate a polynomial approximation for a representation with n+2-bits using the round-to-odd mode. We prove that the resulting polynomial approximation will produce correctly rounded results for all five rounding modes in the standard and for multiple representations with k-bits such that |E| +1 &lt; k ≤ n, where |E| is the number of exponent bits in the representation. Similar to our prior work in the RLIBM project, we approximate the correctly rounded result when we generate the library with n+2-bits using the round-to-odd mode. We also generate polynomial approximations by structuring it as a linear programming problem but propose enhancements to polynomial generation to handle the round-to-odd mode. Our prototype is the first 32-bit float library that produces correctly rounded results with all rounding modes in the IEEE standard for all inputs with a single polynomial approximation. It also produces correctly rounded results for any FP configuration ranging from 10-bits to 32-bits while also being faster than mainstream libraries.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {3},
	numpages = {28},
	keywords = {round-to-odd, floating point, correctly rounded math libraries}
}

@inproceedings{limperg23_aesop,
	author = {Limperg, Jannis and From, Asta Halkj\ae{}r},
	title = {Aesop: White-Box Best-First Proof Search for Lean},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575671},
	doi = {10.1145/3573105.3575671},
	abstract = {We present Aesop, a proof search tactic for the Lean 4 interactive theorem prover. Aesop performs a tree-based search over a user-specified set of proof rules. It supports safe and unsafe rules and uses a best-first search strategy with customisable prioritisation. Aesop also allows users to register custom normalisation rules and integrates Lean's simplifier to support equational reasoning. Many details of Aesop's search procedure are designed to make it a white-box proof automation tactic, meaning that users should be able to easily predict how their rules will be applied, and thus how powerful and fast their Aesop invocations will be. Since we use a best-first search strategy, it is not obvious how to handle metavariables which appear in multiple goals. The most common strategy for dealing with metavariables relies on backtracking and is therefore not suitable for best-first search. We give an algorithm which addresses this issue. The algorithm works with any search strategy, is independent of the underlying logic and makes few assumptions about how rules interact with metavariables. We conjecture that with a fair search strategy, the algorithm is as complete as the given set of rules allows.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {253–266},
	numpages = {14},
	keywords = {Lean, type theory, interactive theorem proving, deductive verification, proof search, tactic},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@inproceedings{liu15_offlin_synth_onlin_depen_testin,
	abstract = {Loop pipelining is probably the most important optimization method in high-level synthesis (HLS), allowing multiple loop iterations to execute in a pipeline. In this paper, we extend the capability of loop pipelining in HLS to handle loops with uncertain memory behaviours. We extend polyhedral synthesis techniques to the parametric case, offloading the uncertainty to parameter values determined at run time. Our technique then synthesizes lightweight runtime checks to detect the case where a low initiation interval (II) is achievable, resulting in a run-time switch between aggressive (fast) and conservative (slow) execution modes. This optimization is implemented into an automated source-to-source code transformation framework with Xilinx Vivado HLS as one RTL generation backend. Over a suite of benchmarks, experiments show that our optimization can implement transformed pipelines at almost same clock frequency as that generated directly with Vivado HLS, but with approximately 10× faster initiation interval in the fast case, while consuming approximately 60% more resource.},
	author = {Liu, Junyi and Bayliss, Samuel and Constantinides, George A.},
	booktitle = {2015 IEEE 23rd Annual International Symposium on Field-Programmable Custom Computing Machines},
	doi = {10.1109/FCCM.2015.31},
	month = may,
	pages = {159--162},
	title = {Offline Synthesis of Online Dependence Testing: Parametric Loop Pipelining for HLS},
	year = {2015}
}

@inproceedings{liu16_effic_high_level_synth_desig,
	abstract = {This paper presents a dedicated High-Level Synthesis (HLS) Design Space Explorer (DSE) for FPGAs. C-based VLSI design has the advantage over conventional RTL design that it allows the generation of micro-architectures with unique area vs. performance trade-offs without having to modify the original behavioral description (in this work area vs. latency). This is typically done by modifying the Functional Unit (FU) constraint file or setting different synthesis directives e.g. unroll loops or synthesize arrays as RAM or registers. The result of the design space exploration is a set of Pareto-optimal designs. In this work, we first investigate the quality of the exploration results when using the results reported after HLS (in particular the area) to guide the explorer in finding Pareto-optimal designs. We found that due to the nature of how HLS tools pre-characterize, the area and delay of basic logic primitives and the FPGAs internal structure the area results are not accurate and hence making it necessary to perform a logic synthesis after each newly generated design. This in turn leads to unacceptable long running time. This work therefore presents a dedicated DSE for FPGAs based on a pruning with adaptive windowing method to extract the design candidates to be further (logic) synthesized after HLS. The adaptive windowing is based on a learning method inspired from Rival Penalized Competitive Learning (RPCL) model in order to classify which designs need to be synthesized to find the true Pareto-optimal designs. Results show that our method leads to similar results compared to an explorer which performs a logic synthesis for each newly generated design, while being much faster.},
	author = {{Dong Liu} and {Schafer}, B. C.},
	url = {https://doi.org/10.1109/FPL.2016.7577370},
	booktitle = {2016 26th International Conference on Field Programmable Logic and Applications (FPL)},
	doi = {10.1109/FPL.2016.7577370},
	issn = {1946-1488},
	keywords = {circuit optimisation;field programmable gate arrays;high level synthesis;integrated circuit design;Pareto optimisation;unsupervised learning;VLSI;high-level synthesis;HLS;design space explorer;DSE;field programmable gate arrays;FPGA;C-based VLSI design;RTL design;microarchitectures;functional unit;FU constraint file;Pareto-optimal designs;logic primitives;logic synthesis;adaptive windowing method;learning method;rival penalized competitive learning;RPCL model;Lead;Libraries;Algorithm design and analysis},
	month = aug,
	pages = {1--8},
	title = {Efficient and reliable High-Level Synthesis Design Space Explorer for FPGAs},
	year = {2016}
}

@inproceedings{liu16_loop_split_effic_pipel_high_level_synth,
	abstract = {Loop pipelining is widely adopted as a key optimization method in high-level synthesis (HLS). However, when complex memory dependencies appear in a loop, commercial HLS tools are still not able to maximize pipeline performance. In this paper, we leverage parametric polyhedral analysis to reason about memory dependence patterns that are uncertain (i.e., parameterised by an undetermined variable) and/or non-uniform (i.e., varying between loop iterations). We develop an automated source-to-source code transformation to split the loop into pieces, which are then synthesised by Vivado HLS as the hardware generation back-end. Our technique allows generated loops to run with a minimal interval, automatically inserting statically-determined parametric pipeline breaks at those iterations violating dependencies. Our experiments on seven representative benchmarks show that, compared to default loop pipelining, our parametric loop splitting improves pipeline performance by 4.3× in terms of clock cycles per iteration. The optimized pipelines consume 2.0× as many LUTs, 1.8× as many registers, and 1.1× as many DSP blocks. Hence the area-time product is improved by nearly a factor of 2.},
	author = {Liu, Junyi and Wickerson, John and Constantinides, George A.},
	booktitle = {2016 IEEE 24th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
	doi = {10.1109/FCCM.2016.27},
	month = may,
	pages = {72--79},
	title = {Loop Splitting for Efficient Pipelining in High-Level Synthesis},
	year = {2016}
}

@article{liu18_polyh_based_dynam_loop_pipel,
	abstract = {Loop pipelining is one of the most important optimization methods in high-level synthesis (HLS) for increasing loop parallelism. There has been considerable work on improving loop pipelining, which mainly focuses on optimizing static operation scheduling and parallel memory accesses. Nonetheless, when loops contain complex memory dependencies, current techniques cannot generate high performance pipelines. In this paper, we extend the capability of loop pipelining in HLS to handle loops with uncertain dependencies (i.e., parameterized by an undetermined variable) and/or nonuniform dependencies (i.e., varying between loop iterations). Our optimization allows a pipeline to be statically scheduled without the aforementioned memory dependencies, but an associated controller will change the execution speed of loop iterations at runtime. This allows the augmented pipeline to process each loop iteration as fast as possible without violating memory dependencies. We use a parametric polyhedral analysis to generate the control logic for when to safely run all loop iterations in the pipeline and when to break the pipeline execution to resolve memory conflicts. Our techniques have been prototyped in an automated source-to-source code transformation framework, with Xilinx Vivado HLS, a leading HLS tool, as the RTL generation backend. Over a suite of benchmarks, experiments show that our optimization can implement optimized pipelines at almost the same clock speed as without our transformations, running approximately 3.7-10× faster, with a reasonable resource overhead.},
	author = {Liu, Junyi and Wickerson, John and Bayliss, Samuel and Constantinides, George A.},
	doi = {10.1109/TCAD.2017.2783363},
	issn = {1937-4151},
	journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	month = sep,
	number = {9},
	pages = {1802--1815},
	title = {Polyhedral-Based Dynamic Loop Pipelining for High-Level Synthesis},
	volume = {37},
	year = {2018}
}

@INPROCEEDINGS{liu18_sfthsc,
	keywords = {motivation},
	author = {Liu, Kun and Kong, Weiqiang and Hou, Gang and Fukuda, Akira},
	booktitle = {2018 7th International Congress on Advanced Applied Informatics (IIAI-AAI)},
	title = {A Survey of Formal Techniques for Hardware/Software Co-verification},
	year = {2018},
	volume = {},
	number = {},
	pages = {125-128},
	doi = {10.1109/IIAI-AAI.2018.00033}
}

@article{liu22_verif_tensor_progr_optim_high,
	author = {Liu, Amanda and Bernstein, Gilbert Louis and Chlipala, Adam and Ragan-Kelley, Jonathan},
	title = {Verified Tensor-Program Optimization via High-Level Scheduling Rewrites},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498717},
	doi = {10.1145/3498717},
	abstract = {We present a lightweight Coq framework for optimizing tensor kernels written in a pure, functional array language. Optimizations rely on user scheduling using series of verified, semantics-preserving rewrites. Unusually for compilation targeting imperative code with arrays and nested loops, all rewrites are source-to-source within a purely functional language. Our language comprises a set of core constructs for expressing high-level computation detail and a set of what we call reshape operators, which can be derived from core constructs but trigger low-level decisions about storage patterns and ordering. We demonstrate that not only is this system capable of deriving the optimizations of existing state-of-the-art languages like Halide and generating comparably performant code, it is also able to schedule a family of useful program transformations beyond what is reachable in Halide.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {55},
	numpages = {28},
	keywords = {array programming, formal verification, optimization, proof assistants}
}

@report{loaiza19_partial_survey_ai_techn_applic,
	author = {Loaiza, Francisco L and Wheeler, David A and Birdwell, John D and Bechtold, Ronald G and Agre, Jonathan R},
	file = {::},
	title = {{A Partial Survey on AI Technologies Applicable to Automated Source Code Generation}},
	type = {techreport},
	year = {2019}
}

@article{loehr22_safe_modul_packet_pipel_progr,
	author = {Loehr, Devon and Walker, David},
	title = {Safe, Modular Packet Pipeline Programming},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498699},
	doi = {10.1145/3498699},
	abstract = {The P4 language and programmable switch hardware, like the Intel Tofino, have made it possible for network engineers to write new programs that customize operation of computer networks, thereby improving performance, fault-tolerance, energy use, and security. Unfortunately, possible does not mean easy—there are many implicit constraints that programmers must obey if they wish their programs to compile to specialized networking hardware. In particular, all computations on the same switch must access data structures in a consistent order, or it will not be possible to lay that data out along the switch’s packet-processing pipeline. In this paper, we define Lucid 2.0, a new language and type system that guarantees programs access data in a consistent order and hence are pipeline-safe. Lucid 2.0 builds on top of the original Lucid language, which is also pipeline-safe, but lacks the features needed for modular construction of data structure libraries. Hence, Lucid 2.0 adds (1) polymorphism and ordering constraints for code reuse; (2) abstract, hierarchical pipeline locations and data types to support information hiding; (3) compile-time constructors, vectors and loops to allow for construction of flexible data structures; and (4) type inference to lessen the burden of program annotations. We develop the meta-theory of Lucid 2.0, prove soundness, and show how to encode constraint checking as an SMT problem. We demonstrate the utility of Lucid 2.0 by developing a suite of useful networking libraries and applications that exploit our new language features, including Bloom filters, sketches, cuckoo hash tables, distributed firewalls, DNS reflection defenses, network address translators (NATs) and a probabilistic traffic monitoring service.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {38},
	numpages = {28},
	keywords = {type and effect systems, Network programming languages, PISA, P4}
}

@inproceedings{lopes21_alive2,
	abstract = {We designed, implemented, and deployed Alive2: a bounded translation validation tool for the LLVM compiler’s intermediate representation (IR). It limits resource consumption by, for example, unrolling loops up to some bound, which means there are circumstances in which it misses bugs. Alive2 is designed to avoid false alarms, is fully automatic through the use of an SMT solver, and requires no changes to LLVM. By running Alive2 over LLVM’s unit test suite, we discovered and reported 47 new bugs, 28 of which have been fixed already. Moreover, our work has led to eight patches to the LLVM Language Reference—the definitive description of the semantics of its IR—and we have participated in numerous discussions with the goal of clarifying ambiguities and fixing errors in these semantics. Alive2 is open source and we also made it available on the web, where it has active users from the LLVM community.},
	author = {Lopes, Nuno P. and Lee, Juneyoung and Hur, Chung-Kil and Liu, Zhengyang and Regehr, John},
	location = {Virtual, Canada},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3453483.3454030},
	booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
	doi = {10.1145/3453483.3454030},
	isbn = {9781450383912},
	keywords = {Translation Validation,Automatic Software Verification,IR Semantics,Compilers},
	pages = {65--79},
	series = {PLDI 2021},
	title = {Alive2: Bounded Translation Validation for LLVM},
	year = {2021}
}

@article{lorenzen22_refer_count_frame_limit_reuse,
	author = {Lorenzen, Anton and Leijen, Daan},
	title = {Reference Counting with Frame Limited Reuse},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3547634},
	doi = {10.1145/3547634},
	abstract = {The recently introduced _Perceus_ algorithm can automatically insert reference count instructions such that the resulting (cycle-free) program is _garbage free_: objects are freed at the very moment they can no longer be referenced. An important extension is reuse analysis. This optimization pairs objects of known size with fresh allocations of the same size and tries to reuse the object in-place at runtime if it happens to be unique. Unfortunately, current implementations of reuse analysis are fragile with respect to small program transformations, or can cause an arbitrary increase in the peak heap usage. We present a novel _drop-guided_ reuse algorithm that is simpler and more robust than previous approaches. Moreover, we generalize the linear resource calculus to precisely characterize garbage-free and frame-limited evaluations. On each function call, a frame-limited evaluation may hold on to memory longer if the size is bounded by a constant factor. Using this framework we show that our drop-guided reuse _is_ frame-limited and find that an implementation of our new reuse approach in Koka can provide significant speedups.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {103},
	numpages = {24},
	keywords = {Reuse, Koka, Reference Counting, Frame Limited}
}

@article{love12_proof_carry_hardw_intel_proper,
	author = {{Love}, E. and {Jin}, Y. and {Makris}, Y.},
	url = {https://doi.org/10.1109/TIFS.2011.2160627},
	doi = {10.1109/TIFS.2011.2160627},
	journaltitle = {IEEE Transactions on Information Forensics and Security},
	keywords = {codes;industrial property;security of data;theorem proving;proof-carrying trustworthy hardware intellectual property;trusted module acquisition;proof-carrying code;security-related property validation;temporal logic;Coq theorem-proving language;security compliance;Hardware;Hardware design languages;Security;Semantics;IP networks;Syntactics;Field programmable gate arrays;Hardware intellectual property (IP);hardware security;proof-carrying code (PCC);proof-carrying hardware (PCH);trusted integrated circuit},
	month = feb,
	number = {1},
	pages = {25--40},
	title = {Proof-Carrying Hardware Intellectual Property: a Pathway To Trusted Module Acquisition},
	volume = {7},
	year = {2012}
}

@article{lozano19_survey_combin_regis_alloc_instr_sched,
	abstract = {Register allocation (mapping variables to processor registers or memory) and instruction scheduling (reordering instructions to increase instruction-level parallelism) are essential tasks for generating efficient assembly code in a compiler. In the past three decades, combinatorial optimization has emerged as an alternative to traditional, heuristic algorithms for these two tasks. Combinatorial optimization approaches can deliver optimal solutions according to a model, can precisely capture trade-offs between conflicting decisions, and are more flexible at the expense of increased compilation time.This article provides an exhaustive literature review and a classification of combinatorial optimization approaches to register allocation and instruction scheduling, with a focus on the techniques that are most applied in this context: integer programming, constraint programming, partitioned Boolean quadratic programming, and enumeration. Researchers in compilers and combinatorial optimization can benefit from identifying developments, trends, and challenges in the area; compiler practitioners may discern opportunities and grasp the potential benefit of applying combinatorial optimization.},
	author = {Lozano, Roberto Castañeda and Schulte, Christian},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3200920},
	doi = {10.1145/3200920},
	issn = {0360-0300},
	journaltitle = {ACM Comput. Surv.},
	keywords = {static scheduling,survey},
	month = jun,
	number = {3},
	title = {Survey on Combinatorial Register Allocation and Instruction Scheduling},
	volume = {52},
	year = {2019}
}

@article{lu23_griset,
	author = {Lu, Sirui and Bod\'{\i}k, Rastislav},
	title = {Grisette: Symbolic Compilation as a Functional Programming Library},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571209},
	doi = {10.1145/3571209},
	abstract = {The development of constraint solvers simplified automated reasoning about programs and shifted the engineering burden to implementing symbolic compilation tools that translate programs into efficiently solvable constraints. We describe Grisette, a reusable symbolic evaluation framework for implementing domain-specific symbolic compilers. Grisette evaluates all execution paths and merges their states into a normal form that avoids making guards mutually exclusive. This ordered-guards representation reduces the constraint size 5-fold and the solving time more than 2-fold. Grisette is designed entirely as a library, which sidesteps the complications of lifting the host language into the symbolic domain. Grisette is purely functional, enabling memoization of symbolic compilation as well as monadic integration with host libraries. Grisette is statically typed, which allows catching programming errors at compile time rather than delaying their detection to the constraint solver. We implemented Grisette in Haskell and evaluated it on benchmarks that stress both the symbolic evaluation and constraint solving.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {16},
	numpages = {33},
	keywords = {State Merging, Symbolic Compilation}
}

@report{lu_codex,
	year = {2019},
	abstract = {Benchmark datasets have a significant impact on accelerating research in programming language tasks. In this paper, we introduce CodeXGLUE, a benchmark dataset to foster machine learning research for program understanding and generation. CodeXGLUE includes a collection of 10 tasks across 14 datasets and a platform for model evaluation and comparison. CodeXGLUE also features three baseline systems, including the BERT-style, GPT-style, and Encoder-Decoder models, to make it easy for researchers to use the platform. The availability of such data and baselines can help the development and validation of new methods that can be applied to various program understanding and generation problems 1 .},
	author = {Lu, Shuai and Guo, Daya and Ren, Shuo and Huang, Junjie and Microsoft, Alexey Svyatkovskiy and Blanco, Ambrosio and Microsoft, Colin Clement and Daxin, Microsoft and Microsoft, Jiang and Tang, Duyu and Li, Ge and Zhou, Lidong and Zhou, Microsoft Long and Microsoft, Michele Tufano and Microsoft, Ming Gong and Zhou, Ming and Duan, Nan and Sundaresan, Neel and Shao, Microsoft and Deng, Kun and Shengyu, Microsoft and Microsoft, Fu and Liu, Shujie},
	url = {https://evansdata.com/press/viewRelease.php?pressID=278},
	eprint = {2102.04664v2},
	eprinttype = {arXiv},
	keywords = {machine learning,naturalness of software,program understanding},
	title = {{CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation; CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation}},
	type = {techreport}
}

@article{lundqvist99_integ_path_timin_analy_method,
	title = {An Integrated Path and Timing Analysis Method based on Cycle-Level Symbolic Execution},
	keywords = {hardware abstract interpretation},
	doi = {10.1023/a:1008138407139},
	url = {https://doi.org/10.1023/a:1008138407139},
	year = {1999},
	publisher = {Springer Science and Business Media {LLC}},
	volume = {17},
	number = {2/3},
	pages = {183--207},
	author = {Thomas Lundqvist and Per Stenstr\"{o}m},
	journal = {Real-Time Systems}
}

@article{lynch95_forwar_backw_simul,
	keywords = {simulation proof},
	author = {Lynch, Nancy and Vaandrager, Frits},
	publisher = {Elsevier {BV}},
	url = {https://doi.org/10.1006/inco.1995.1134},
	doi = {10.1006/inco.1995.1134},
	journaltitle = {Information and Computation},
	month = sep,
	number = {2},
	pages = {214--233},
	title = {Forward and Backward Simulations},
	volume = {121},
	year = {1995}
}

@article{lynch96_forwar_backw_simul,
	title = {Forward and Backward Simulations: II. Timing-Based Systems},
	journal = {Information and Computation},
	volume = {128},
	number = {1},
	pages = {1-25},
	year = {1996},
	issn = {0890-5401},
	doi = {https://doi.org/10.1006/inco.1996.0060},
	url = {https://www.sciencedirect.com/science/article/pii/S0890540196900607},
	author = {Lynch, Nancy and Vaandrager, Frits},
	abstract = {A general automaton model for timing-based systems is presented and is used as the context for developing a variety of simulation proof techniques for such systems. These techniques include (1) refinements, (2) forward and backward simulations, (3) hybrid forward–backward and backward–forward simulations, and (4) history and prophecy relations. Relationships between the different types of simulations, as well as soundness and completeness results, are stated and proved. These results are (with one exception) analogous to the results for untimed systems in Part I of this paper. In fact, many of the results for the timed case are obtained as consequences of the analogous results for the untimed case.}
}

@inproceedings{lööw19_proof_trans_veril_devel_hol,
	author = {Lööw, Andreas and Myreen, Magnus O.},
	location = {Montreal, Quebec, Canada},
	publisher = {IEEE Press},
	booktitle = {Proceedings of the 7th International Workshop on Formal Methods in Software Engineering},
	doi = {10.1109/FormaliSE.2019.00020},
	pages = {99--108},
	series = {FormaliSE '19},
	title = {A Proof-producing Translator for Verilog Development in HOL},
	year = {2019}
}

@inproceedings{lööw19_verif_compil_verif_proces,
	author = {Lööw, Andreas and Kumar, Ramana and Tan, Yong Kiam and Myreen, Magnus O. and Norrish, Michael and Abrahamsson, Oskar and Fox, Anthony},
	location = {Phoenix, AZ, USA},
	publisher = {ACM},
	booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
	doi = {10.1145/3314221.3314622},
	isbn = {978-1-4503-6712-7},
	keywords = {compiler verification,hardware verification,program verification,verified stack},
	pages = {1041--1053},
	series = {PLDI 2019},
	title = {Verified Compilation on a Verified Processor},
	year = {2019}
}

@inproceedings{lööw21_lutsig,
	abstract = {We report on a new verified Verilog compiler called Lutsig. Lutsig currently targets (a class of) FPGAs and is capable of producing technology mapped netlists for FPGAs. We have connected Lutsig to existing Verilog development tools, and in this paper we show how Lutsig, as a consequence of this connection, fits into a hardware development methodology for verified circuits in the HOL4 theorem prover. One important step in the methodology is transporting properties proved at the behavioral Verilog level down to technology mapped netlists, and Lutsig is the component in the methodology that enables such transportation.},
	author = {Lööw, Andreas},
	location = {Virtual, Denmark},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 10th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	doi = {10.1145/3437992.3439916},
	isbn = {9781450382991},
	keywords = {hardware verification,hardware synthesis,compiler verification},
	pages = {46--60},
	series = {CPP 2021},
	title = {Lutsig: A Verified Verilog Compiler for Verified Circuit Development},
	year = {2021}
}

@inproceedings{ma22_debug_brave_new_world_recon_hardw,
	author = {Ma, Jiacheng and Zuo, Gefei and Loughlin, Kevin and Zhang, Haoyang and Quinn, Andrew and Kasikci, Baris},
	title = {Debugging in the Brave New World of Reconfigurable Hardware},
	year = {2022},
	isbn = {9781450392051},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3503222.3507701},
	doi = {10.1145/3503222.3507701},
	abstract = {Software and hardware development cycles have traditionally been quite distinct. Software allows post-deployment patches, which leads to a rapid development cycle. In contrast, hardware bugs that are found after fabrication are extremely costly to fix (and sometimes even unfixable), so the traditional hardware development cycle involves massive investment in extensive simulation and formal verification. Reconfigurable hardware, such as a Field Programmable Gate Array (FPGA), promises to propel hardware development towards an agile software-like development approach, since it enables a hardware developer to patch bugs that are detected during on-chip testing or in production. Unfortunately, FPGA programmers lack bug localization tools amenable to this rapid development cycle, since past tools mainly find bugs via simulation and verification. To develop hardware bug localization tools for a rapid development cycle, a thorough understanding of the symptoms, root causes, and fixes of hardware bugs is needed.  In this paper, we first study bugs in existing FPGA designs and produce a testbed of reliably-reproducible bugs. We classify the bugs according to their intrinsic properties, symptoms, and root causes. We demonstrate that many hardware bugs are comparable to software bug counterparts, and would benefit from similar techniques for bug diagnosis and repair. Based upon our findings, we build a novel collection of hybrid static/dynamic program analysis and monitoring tools for debugging FPGA designs, showing that our tools enable a software-like development cycle by effectively reducing developers' manual efforts for bug localization.},
	booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
	pages = {946–962},
	numpages = {17},
	keywords = {Reconfigurable Hardware, Bug Study, Debugging, FPGA},
	location = {Lausanne, Switzerland},
	series = {ASPLOS '22}
}

@InProceedings{macqueen94_seman_higher_order_funct,
	author = "MacQueen, David B.
and Tofte, Mads",
	editor = "Sannella, Donald",
	title = {A Semantics for Higher-Order Functors},
	booktitle = "Programming Languages and Systems --- ESOP '94",
	year = "1994",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "409--423",
	abstract = "Standard ML has a module system that allows one to define parametric modules, called functors. Functors are ``first-order,'' meaning that functors themselves cannot be passed as parameters or returned as results of functor applications. This paper presents a semantics for a higher-order module system which generalizes the module system of Standard ML. The higher-order functors described here are implemented in the current version of Standard ML of New Jersey and have proved useful in programming practice.",
	isbn = "978-3-540-48376-2"
}

@article{madiot22_separ_logic_heap_space_garbag_collec,
	author = {Madiot, Jean-Marie and Pottier, Fran\c{c}ois},
	title = {A Separation Logic for Heap Space under Garbage Collection},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498672},
	doi = {10.1145/3498672},
	abstract = {We present SL♢, a Separation Logic that allows controlling the heap space consumption of a program in the presence of dynamic memory allocation and garbage collection. A user of the logic works with space credits, a resource that is consumed when an object is allocated and produced when a group of objects is logically deallocated, that is, when the user is able to prove that it has become unreachable and therefore can be collected. To prove such a fact, the user maintains pointed-by assertions that record the immediate predecessors of every object. Our calculus, SpaceLang, has mutable state, shared-memory concurrency, and code pointers. We prove that SL♢ is sound and present several simple examples of its use.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {11},
	numpages = {28},
	keywords = {tracing garbage collection, program verification, separation logic, live data}
}

@article{mahlke92_effec_compil_suppor_predic_execut_using_hyper,
	author = {Mahlke, Scott A. and Lin, David C. and Chen, William Y. and Hank, Richard E. and Bringmann, Roger A.},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	doi = {10.1145/144965.144998},
	issn = {1050-916X},
	journaltitle = {SIGMICRO Newsl.},
	keywords = {speculative execution,static scheduling,hyperblocks},
	month = dec,
	number = {1-2},
	pages = {45--54},
	title = {Effective Compiler Support for Predicated Execution Using the Hyperblock},
	volume = {23},
	year = {1992}
}

@article{mahlke93_sentin_sched,
	abstract = {Speculative execution is an important source of parallelism for VLIW and superscalar processors. A serious challenge with compiler-controlled speculative execution is to efficiently handle exceptions for speculative instructions. In this article, a set of architectural features and compile-time scheduling support collectively referred to as sentinel scheduling is introduced. Sentinel scheduling provides an effective framework for both compiler-controlled speculative execution and exception handling. All program exceptions are accurately detected and reported in a timely manner with sentinel scheduling. Recovery from exceptions is also ensured with the model. Experimental results show the effectiveness of sentinel scheduling for exploiting instruction-level parallelism and overhead associated with exception handling.},
	author = {Mahlke, Scott A. and Chen, William Y. and Bringmann, Roger A. and Hank, Richard E. and Hwu, Wen-Mei W. and Rau, B. Ramakrishna and Schlansker, Michael S.},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/161541.159765},
	doi = {10.1145/161541.159765},
	issn = {0734-2071},
	journaltitle = {ACM Trans. Comput. Syst.},
	keywords = {speculative execution,static scheduling,hyperblocks},
	month = nov,
	number = {4},
	pages = {376--408},
	title = {Sentinel Scheduling: A Model for Compiler-Controlled Speculative Execution},
	volume = {11},
	year = {1993}
}

@inproceedings{mahlke94_charac_impac_predic_execut_branc_predic,
	abstract = {Branch instructions are recognized as a major impediment to exploiting instruction level parallelism. Even with sophisticated branch prediction techniques, many frequently executed branches remain difficult to predict. An architecture supporting predicated execution may allow the compiler to remove many of these hard-to-predict branches, reducing the number of branch mispredictions and thereby improving performance. We present an in-depth analysis of the characteristics of those branches which are frequently mispredicted and examine the effectiveness of an advanced compiler to eliminate these branches. Over the benchmarks studied, an average of 27% of the dynamic branches and 56% of the dynamic branch mispredictions are eliminated with predicated execution support.},
	author = {Mahlke, Scott A. and Hank, Richard E. and Bringmann, Roger A. and Gyllenhaal, John C. and Gallagher, David M. and Hwu, Wen-mei W.},
	location = {San Jose, California, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/192724.192755},
	booktitle = {Proceedings of the 27th Annual International Symposium on Microarchitecture},
	doi = {10.1145/192724.192755},
	isbn = {0897917073},
	keywords = {speculative execution,static scheduling,hyperblocks},
	pages = {217--227},
	series = {MICRO 27},
	title = {Characterizing the Impact of Predicated Execution on Branch Prediction},
	year = {1994}
}

@article{malik12_effor_resour_abstr_perfor_high_level_synth,
	abstract = {This work provides new perspectives on impact of design effort, consumed resources and design abstraction on hardware performance in a high-level synthesis flow. We have shown that counter to published literature as well as intuition; more design effort may not always result in better performance. We developed a kernel that simulates Brownian motion, and investigated improvement in hardware performance with design effort at various abstraction levels. Our results indicate that a designer should be careful in putting more effort at a particular abstraction level. In our case, we achieved best performance/effort ratio at algorithm level rather than lower abstraction levels. This strongly suggests that design effort is not always proportional to corresponding improvement in performance.},
	author = {Malik, Jamshaid Sarwar and Palazzari, Paolo and Hemani, Ahmed},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/2460216.2460228},
	doi = {10.1145/2460216.2460228},
	issn = {0163-5964},
	journaltitle = {SIGARCH Comput. Archit. News},
	keywords = {high-level synthesis,study},
	month = mar,
	number = {5},
	pages = {64--69},
	title = {Effort, Resources, and Abstraction vs Performance in High-Level Synthesis: Finding New Answers to an Old Question},
	volume = {40},
	year = {2012}
}

@inproceedings{mansouri98_method_autom_verif_synth_rtl,
	abstract = {High-level synthesis tools generate rtl designs from algorithmic behavioral specifications and consist of well defined tasks. Widely used algorithms for these tasks retain the overall control flow structure of the behavioral specification allowing limited code motion. Further, hls algorithms are oblivious to the mathematical properties of arithmetic and logic operators, selecting and sharing rtl library modules solely based on matching uninterpreted function symbols and constants. This paper reports a Verification methodology that effectively exploits these features to achieve efficient and fully automated Verification of synthesized designs and its incorporation in a relatively mature hls tool.},
	author = {Mansouri, Nazanin and Vemuri, Ranga},
	editor = {Gopalakrishnan, Ganesh and Windley, Phillip},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Formal Methods in Computer-Aided Design},
	isbn = {978-3-540-49519-2},
	pages = {204--221},
	title = {A Methodology for Automated Verification of Synthesized RTL Designs and Its Integration with a High-Level Synthesis Tool},
	year = {1998}
}

@article{martin09_high_level_synth,
	author = {{Martin}, G. and {Smith}, G.},
	url = {https://doi.org/10.1109/MDT.2009.83},
	doi = {10.1109/MDT.2009.83},
	journaltitle = {IEEE Design Test of Computers},
	keywords = {high level synthesis;high-level synthesis;industry adoption;HLS tools;system-level design;High level synthesis;Electronic design automation and methodology;Computer industry;History;Electronics industry;Commercialization;Hardware;Machinery production industries;Physics computing;Algorithm design and analysis;High-level synthesis;behavioral synthesis;ESL synthesis;commercial use;history;design and test},
	month = jul,
	number = {4},
	pages = {18--25},
	title = {High-Level Synthesis: Past, Present, and Future},
	volume = {26},
	year = {2009}
}

@inproceedings{masud22_comput_inter_weak_contr_closur,
	author = {Masud, Abu Naser and Lisper, Bj\"{o}rn},
	title = {On the Computation of Interprocedural Weak Control Closure},
	year = {2022},
	isbn = {9781450391832},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497776.3517782},
	doi = {10.1145/3497776.3517782},
	abstract = {Many program analysis techniques depend on capturing the control dependencies of the program. Most existing control dependence algorithms either compute intraprocedural control dependencies only, or they compute control dependence relations that are not precise in general including nonterminating systems. Weak control closure (WCC) subsumes all known nontermination insensitive control dependence relations, including those that are appropriate for nonterminating systems. In this paper, we provide the first formal development of an algorithm to compute the WCC for interprocedural programs capturing the weak form of interprocedural control dependencies. The method is widely applicable due to the generality of WCC. Theorems on the theoretical results of soundness, precision, and the worst-case complexity of our method are also included. We have compared our algorithm with a WCC computation algorithm based on a state-of-the-art interprocedural control dependence computation algorithm. The latter algorithm loses soundness, and we improve the precision by 15.21% on all our experimental benchmarks. This empirical evidence suggests that our algorithm is more effective for any client application of WCC requiring interprocedural program analysis.},
	booktitle = {Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction},
	pages = {65–76},
	numpages = {12},
	keywords = {Control dependency, weak control closure, debugging, nontermination insensitive, program slicing},
	location = {Seoul, South Korea},
	series = {CC 2022}
}

@article{materzok22_gener_circuit_gener,
	author = {Materzok, Marek},
	title = {Generating Circuits with Generators},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3549821},
	doi = {10.1145/3549821},
	abstract = {The most widely used languages and methods used for designing digital hardware fall into two rough categories. One of them, register transfer level (RTL), requires specifying each and every component in the designed circuit. This gives the designer full control, but burdens the designer with many trivial details. The other, the high-level synthesis (HLS) method, allows the designer to abstract the details of hardware away and focus on the problem being solved. This method however cannot be used for a class of hardware design problems because the circuit's clock is also abstracted away. We present YieldFSM, a hardware description language that uses the generator abstraction to represent clock-level timing in a digital circuit. It represents a middle ground between the RTL and HLS approaches: the abstraction level is higher than in RTL, but thanks to explicit information about clock-level timing, it can be used in applications where RTL is traditionally used. We also present the YieldFSM compiler, which uses methods developed by the functional programming community -- including continuation-passsing style translation and defunctionalization -- to translate YieldFSM programs to Mealy machines. It is implemented using Template Haskell and the Clash functional hardware description language. We show that this approach leads to short and conceptually simple hardware descriptions.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {92},
	numpages = {28},
	keywords = {hardware description languages, circuit synthesis, generators}
}

@article{mathiasen21_fine_grain_paral_compl_ander_point_analy,
	abstract = {Pointer analysis is one of the fundamental problems in static program analysis. Given a set of pointers, the task is to produce a useful over-approximation of the memory locations that each pointer may point-to at runtime. The most common formulation is Andersen’s Pointer Analysis (APA), defined as an inclusion-based set of m pointer constraints over a set of n pointers. Scalability is extremely important, as points-to information is a prerequisite to many other components in the static-analysis pipeline. Existing algorithms solve APA in O(n2· m) time, while it has been conjectured that the problem has no truly sub-cubic algorithm, with a proof so far having remained elusive. It is also well-known that APA can be solved in O(n2) time under certain sparsity conditions that hold naturally in some settings. Besides these simple bounds, the complexity of the problem has remained poorly understood. In this work we draw a rich fine-grained and parallel complexity landscape of APA, and present upper and lower bounds. First, we establish an O(n3) upper-bound for general APA, improving over O(n2· m) as n=O(m). Second, we show that even on-demand APA (“may a specific pointer a point to a specific location b?”) has an Ω(n3) (combinatorial) lower bound under standard complexity-theoretic hypotheses. This formally establishes the long-conjectured “cubic bottleneck” of APA, and shows that our O(n3)-time algorithm is optimal. Third, we show that under mild restrictions, APA is solvable in Õ(nω) time, where ω&lt;2.373 is the matrix-multiplication exponent. It is believed that ω=2+o(1), in which case this bound becomes quadratic. Fourth, we show that even under such restrictions, even the on-demand problem has an Ω(n2) lower bound under standard complexity-theoretic hypotheses, and hence our algorithm is optimal when ω=2+o(1). Fifth, we study the parallelizability of APA and establish lower and upper bounds: (i) in general, the problem is P-complete and hence unlikely parallelizable, whereas (ii) under mild restrictions, the problem is parallelizable. Our theoretical treatment formalizes several insights that can lead to practical improvements in the future.},
	author = {Mathiasen, Anders Alnor and Pavlogiannis, Andreas},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434315},
	doi = {10.1145/3434315},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {inclusion-based pointer analysis,Dyck reachability,static pointer analysis,fine-grained complexity},
	month = jan,
	number = {POPL},
	title = {The Fine-Grained and Parallel Complexity of Andersen’s Pointer Analysis},
	volume = {5},
	year = {2021}
}

@article{mathur09_funct_equiv_verif_tools_high,
	author = {{Mathur}, A. and {Fujita}, M. and {Clarke}, E. and {Urard}, P.},
	url = {https://doi.org/10.1109/MDT.2009.79},
	doi = {10.1109/MDT.2009.79},
	journaltitle = {IEEE Design Test of Computers},
	keywords = {formal verification;high level synthesis;functional equivalence verification tool;high-level synthesis flows;formal verification methodologies;RTL model;sequential equivalence checking techniques;High level synthesis;Algorithm design and analysis;Computer bugs;Testing;Software algorithms;Hardware;Software performance;Software tools;Partitioning algorithms;State-space methods;system-level model;sequential equivalence;functional equivalence;design and test;formal analysis;correctness},
	month = jul,
	number = {4},
	pages = {88--95},
	title = {Functional Equivalence Verification Tools in High-Level Synthesis Flows},
	volume = {26},
	year = {2009}
}

@inproceedings{maziarz21_hashin_alpha_equiv,
	abstract = {In many applications one wants to identify identical subtrees of a program syntax tree. This identification should ideally be robust to alpha-renaming of the program, but no existing technique has been shown to achieve this with good efficiency (better than O(n2) in expression size). We present a new, asymptotically efficient way to hash modulo alpha-equivalence. A key insight of our method is to use a weak (commutative) hash combiner at exactly one point in the construction, which admits an algorithm with O(n (logn)2) time complexity. We prove that the use of the commutative combiner nevertheless yields a strong hash with low collision probability. Numerical benchmarks attest to the asymptotic behaviour of the method.},
	author = {Maziarz, Krzysztof and Ellis, Tom and Lawrence, Alan and Fitzgibbon, Andrew and Peyton Jones, Simon},
	location = {Virtual, Canada},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3453483.3454088},
	booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
	doi = {10.1145/3453483.3454088},
	isbn = {9781450383912},
	keywords = {equivalence,abstract syntax tree,hashing},
	pages = {960--973},
	series = {PLDI 2021},
	title = {Hashing modulo Alpha-Equivalence},
	year = {2021}
}

@article{mcdowell02_reason_higher_order_abstr_syntax_logic_framew,
	author = {McDowell, Raymond C. and Miller, Dale A.},
	title = {Reasoning with Higher-Order Abstract Syntax in a Logical Framework},
	year = {2002},
	issue_date = {January 2002},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {3},
	number = {1},
	issn = {1529-3785},
	url = {https://doi.org/10.1145/504077.504080},
	doi = {10.1145/504077.504080},
	abstract = {Logical frameworks based on intuitionistic or linear logics with higher-type quantification have been successfully used to give high-level, modular, and formal specifications of many important judgments in the area of programming languages and inference systems. Given such specifications, it is natural to consider proving properties about the specified systems in the framework: for example, given the specification of evaluation for a functional programming language, prove that the language is deterministic or that evaluation preserves types. One challenge in developing a framework for such reasoning is that higher-order abstract syntax (HOAS), an elegant and declarative treatment of object-level abstraction and substitution, is difficult to treat in proofs involving induction. In this article, we present a meta-logic that can be used to reason about judgments coded using HOAS; this meta-logic is an extension of a simple intuitionistic logic that admits higher-order quantification over simply typed λ-terms (key ingredients for HOAS) as well as induction and a notion of definition. The latter concept of definition is a proof-theoretic device that allows certain theories to be treated as "closed" or as defining fixed points. We explore the difficulties of formal meta-theoretic analysis of HOAS encodings by considering encodings of intuitionistic and linear logics, and formally derive the admissibility of cut for important subsets of these logics. We then propose an approach to avoid the apparent trade-off between the benefits of higher-order abstract syntax and the ability to analyze the resulting encodings. We illustrate this approach through examples involving the simple functional and imperative programming languages PCF and PCF:=. We formally derive such properties as unicity of typing, subject reduction, determinacy of evaluation, and the equivalence of transition semantics and natural semantics presentations of evaluation.},
	journal = {ACM Trans. Comput. Logic},
	month = {jan},
	pages = {80–136},
	numpages = {57},
	keywords = {logical frameworks, induction, higher-order abstract syntax, Definitions}
}

@inproceedings{mckeever01_towar_provab_correc_hardw_compil,
	abstract = {This paper presents a framework for verifying compilation tools based on parametrised hardware libraries expressed in Pebble, a simple declarative language. An approach based on pass separation techniques is described for specifying and verifying Pebble abstraction mechanisms, such as the loop statement. We show how this approach can be used to verify the correctness of the flattening procedure in the Pebble compiler, which also results in a more efficient implementation than a non-verified version. The approach is useful for guiding compiler implementations for Pebble and related languages such as VHDL; it may also form the basis for automating the generation of provably-correct tools for hardware development.},
	author = {McKeever, Steve and Luk, Wayne},
	editor = {Margaria, Tiziana and Melham, Tom},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Correct Hardware Design and Verification Methods},
	isbn = {978-3-540-44798-6},
	pages = {212--227},
	title = {Towards Provably-Correct Hardware Compilation Tools Based on Pass Separation Techniques},
	year = {2001}
}

@article{mckeever06_provab_correc_hardw_compil_tools,
	abstract = {This paper presents a framework for verifying compilation tools for parametrised hardware designs with placement information. The framework involves Pebble, a simple declarative language based on Structural VHDL which supports the use of placement information to guide circuit layout; such information often leads to efficient designs that are particularly important for hardware libraries. Relative placement information enables control of circuit layout at a higher level of abstraction than placement information in the form of explicit coordinates. An approach based on pass separation techniques is adopted for specifying and verifying two Pebble abstraction mechanisms: a flattening procedure and a relative placement method. For the flattening procedure, which takes a set of parametrised blocks and unfolds the circuit description into a netlist, we provide semantic descriptions of both the hierarchical and the flattened Pebble languages to prove its functional correctness. For the relative placement method, we specify the compilation procedure from Pebble programs with relative placement information to Pebble programs with explicit coordinate expressions, often in the form of symbolic placement constraints. This compilation procedure can be used in conjunction with partial evaluation to optimise the size and speed of parametrised circuit descriptions using relative placement, without flattening the original hierarchical descriptions. Our approach has been used for optimising a pattern matcher design, which results in a 33{\ \%} reduction in resource usage. For DES encryption, our method can reduce the size of a DES design by 60{\ \%}.},
	author = {McKeever, Steve and Luk, Wayne},
	url = {https://doi.org/10.1007/s00165-005-0075-8},
	date = {2006-06-01},
	doi = {10.1007/s00165-005-0075-8},
	issn = {1433-299X},
	journaltitle = {Formal Aspects of Computing},
	number = {2},
	pages = {120--142},
	title = {Provably-Correct Hardware Compilation Tools Based on Pass Separation Techniques},
	volume = {18}
}

@article{meeus12_overv_today_high_level_synth_tools,
	abstract = {High-level synthesis (HLS) is an increasingly popular approach in electronic design automation (EDA) that raises the abstraction level for designing digital circuits. With the increasing complexity of embedded systems, these tools are particularly relevant in embedded systems design. In this paper, we present our evaluation of a broad selection of recent HLS tools in terms of capabilities, usability and quality of results. Even though HLS tools are still lacking some maturity, they are constantly improving and the industry is now starting to adopt them into their design flows.},
	author = {Meeus, Wim and Van Beeck, Kristof and Goedemé, Toon and Meel, Jan and Stroobandt, Dirk},
	url = {https://doi.org/10.1007/s10617-012-9096-8},
	date = {2012-09-01},
	doi = {10.1007/s10617-012-9096-8},
	issn = {1572-8080},
	journaltitle = {Design Automation for Embedded Systems},
	number = {3},
	pages = {31--51},
	title = {An Overview of Today's High-Level Synthesis Tools},
	volume = {16}
}

@inproceedings{mehta22_softw_pre_execut_irreg_memor,
	author = {Mehta, Sanyam and Elsesser, Gary and Greyzck, Terry},
	title = {Software Pre-Execution for Irregular Memory Accesses in the HBM Era},
	year = {2022},
	isbn = {9781450391832},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497776.3517783},
	doi = {10.1145/3497776.3517783},
	abstract = {The introduction of High Bandwidth Memory (HBM) necessitates the use of intelligent software prefetching in irregular applications to utilize the surplus bandwidth. In this work, we propose Software Pre-execution (SPE), a technique that relies on pre-executing a minimal copy of the loop of concern (we call the pre-execution loop) for the purpose of prefetching irregular accesses. This is complemented by the compiler's enforcing a certain prefetch distance through apriori strip-mining of the original loop such that the execution of the pre-execution loop is interspersed with the main loop to ensure timeliness of prefetches. We find that this approach provides natural advantages over prior art such as preservation of loop vectorization, handling short loops, avoiding performance bottlenecks, amenability to threading and most importantly, effective coverage. We demonstrate these advantages using a variety of benchmarks on Fujitsu's A64FX processor with HBM2 memory - we outperform prior art by 1.3x and 1.2x when using small and huge pages, respectively. Simulations further show that our approach holds stronger promise on upcoming processors with HBM2e.},
	booktitle = {Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction},
	pages = {231–242},
	numpages = {12},
	keywords = {Pre-execution, HBM, Irregular memory access, Software prefetching, Graph applications},
	location = {Seoul, South Korea},
	series = {CC 2022}
}

@inproceedings{mehta23_formal_shark_theor_proof_pearl,
	author = {Mehta, Bhavik},
	title = {Formalising Sharkovsky’s Theorem (Proof Pearl)},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575689},
	doi = {10.1145/3573105.3575689},
	abstract = {Sharkovsky's theorem is a celebrated result by Ukrainian mathematician Oleksandr Sharkovsky in the theory of discrete dynamical systems, including the fact that if a continuous function of reals has a point of period 3, it must have points of any period. We formalise the proof in the Lean theorem prover, giving a characterisation of the possible sets of periods a continuous function on the real numbers may have. We further include the converse of the theorem, showing that the aforementioned sets are achievable under mild conditions.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {267–274},
	numpages = {8},
	keywords = {proof assistant, real analysis, Lean, formal math},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@misc{mentor20_catap_high_level_synth,
	author = {Siemens},
	title = {Catapult High-Level Synthesis},
	url = {https://eda.sw.siemens.com/en-US/ic/catapult-high-level-synthesis/hls/c-cplus/},
	urldate = {2023-11-14},
	year = 2021
}

@inproceedings{meredith10_veril,
	author = {{Meredith}, P. and {Katelman}, M. and {Meseguer}, J. and {Roşu}, G.},
	url = {https://doi.org/10.1109/MEMCOD.2010.5558634},
	booktitle = {Eighth ACM/IEEE International Conference on Formal Methods and Models for Codesign (MEMOCODE 2010)},
	doi = {10.1109/MEMCOD.2010.5558634},
	issn = {null},
	keywords = {hardware description languages;programming language semantics;formal executable semantics;Verilog hardware description language;mathematically rigorous reference;official language standard;Hardware design languages;Semantics;Equations;Delay;Integrated circuit modeling;Mathematical model;Computational modeling},
	month = jul,
	pages = {179--188},
	title = {A formal executable semantics of {Verilog}},
	year = {2010}
}

@article{michael23_mswas,
	author = {Michael, Alexandra E. and Gollamudi, Anitha and Bosamiya, Jay and Johnson, Evan and Denlinger, Aidan and Disselkoen, Craig and Watt, Conrad and Parno, Bryan and Patrignani, Marco and Vassena, Marco and Stefan, Deian},
	title = {MSWasm: Soundly Enforcing Memory-Safe Execution of Unsafe Code},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571208},
	doi = {10.1145/3571208},
	abstract = {Most programs compiled to WebAssembly (Wasm) today are written in unsafe languages like C and C++. Unfortunately, memory-unsafe C code remains unsafe when compiled to Wasm—and attackers can exploit buffer overflows and use-after-frees in Wasm almost as easily as they can on native platforms. Memory- Safe WebAssembly (MSWasm) proposes to extend Wasm with language-level memory-safety abstractions to precisely address this problem. In this paper, we build on the original MSWasm position paper to realize this vision. We give a precise and formal semantics of MSWasm, and prove that well-typed MSWasm programs are, by construction, robustly memory safe. To this end, we develop a novel, language-independent memory-safety property based on colored memory locations and pointers. This property also lets us reason about the security guarantees of a formal C-to-MSWasm compiler—and prove that it always produces memory-safe programs (and preserves the semantics of safe programs). We use these formal results to then guide several implementations: Two compilers of MSWasm to native code, and a C-to-MSWasm compiler (that extends Clang). Our MSWasm compilers support different enforcement mechanisms, allowing developers to make security-performance trade-offs according to their needs. Our evaluation shows that on the PolyBenchC suite, the overhead of enforcing memory safety in software ranges from 22% (enforcing spatial safety alone) to 198% (enforcing full memory safety), and 51.7% when using hardware memory capabilities for spatial safety and pointer integrity. More importantly, MSWasm’s design makes it easy to swap between enforcement mechanisms; as fast (especially hardware-based) enforcement techniques become available, MSWasm will be able to take advantage of these advances almost for free.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {15},
	numpages = {30},
	keywords = {Memory-safety, WebAssembly, Semantics, Secure Compilation}
}

@inproceedings{milehins22_exten_framew_types_sets_isabel_hol,
	author = {Milehins, Mihails},
	title = {An Extension of the Framework Types-to-Sets for Isabelle/HOL},
	year = {2022},
	isbn = {9781450391825},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497775.3503674},
	doi = {10.1145/3497775.3503674},
	abstract = {In their article titled From Types to Sets by Local Type Definitions in Higher-Order Logic and published in the proceedings of the conference Interactive Theorem Proving in 2016, Ond\v{r}ej Kun\v{c}ar and Andrei Popescu propose an extension of the logic Isabelle/HOL and an associated algorithm for the relativization of the type-based theorems to more flexible set-based theorems, collectively referred to as Types-To-Sets. One of the aims of their work was to open an opportunity for the development of a software tool for applied relativization in the implementation of the logic Isabelle/HOL of the proof assistant Isabelle. In this article, we provide a description of a software framework for the interactive automated relativization of definitions and theorems in Isabelle/HOL, developed as an extension of the proof language Isabelle/Isar, building upon some of the ideas for further work expressed in the original article on Types-To-Sets by Ond\v{r}ej Kun\v{c}ar and Andrei Popescu and the subsequent article Smooth Manifolds and Types to Sets for Linear Algebra in Isabelle/HOL, which was written by Fabian Immler and Bohua Zhan and published in the proceedings of the International Conference on Certified Programs and Proofs in 2019.},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {180–196},
	numpages = {17},
	keywords = {Higher Order Logic, Proof Assistants, Isabelle, Formalization of Mathematics},
	location = {Philadelphia, PA, USA},
	series = {CPP 2022}
}

@article{miltner22_bottom_synth_recur_funct_progr,
	author = {Miltner, Anders and Nu\~{n}ez, Adrian Trejo and Brendel, Ana and Chaudhuri, Swarat and Dillig, Isil},
	title = {Bottom-up Synthesis of Recursive Functional Programs Using Angelic Execution},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498682},
	doi = {10.1145/3498682},
	abstract = {We present a novel bottom-up method for the synthesis of functional recursive programs. While bottom-up synthesis techniques can work better than top-down methods in certain settings, there is no prior technique for synthesizing recursive programs from logical specifications in a purely bottom-up fashion. The main challenge is that effective bottom-up methods need to execute sub-expressions of the code being synthesized, but it is impossible to execute a recursive subexpression of a program that has not been fully constructed yet. In this paper, we address this challenge using the concept of angelic semantics. Specifically, our method finds a program that satisfies the specification under angelic semantics (we refer to this as angelic synthesis), analyzes the assumptions made during its angelic execution, uses this analysis to strengthen the specification, and finally reattempts synthesis with the strengthened specification. Our proposed angelic synthesis algorithm is based on version space learning and therefore deals effectively with many incremental synthesis calls made during the overall algorithm. We have implemented this approach in a prototype called Burst and evaluate it on synthesis problems from prior work. Our experiments show that Burst is able to synthesize a solution to 94% of the benchmarks in our benchmark suite, outperforming prior work.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {21},
	numpages = {29},
	keywords = {Angelic Execution, Logical Specifications, Program Synthesis}
}

@report{misra94,
	url = {http://www.cs.uu.nl/docs/vakken/pv/resources/SafetyProgress.pdf},
	author = {Misra, Jayadev},
	institution = {Technical report, The University of Texas at Austin},
	keywords = {unity},
	title = {A logic for concurrent programming},
	type = {techreport},
	year = {1994}
}

@inproceedings{mogers22_mappin_paral_funct_ir_const_satis,
	author = {Mogers, Naums and Li, Lu and Radu, Valentin and Dubach, Christophe},
	title = {Mapping Parallelism in a Functional IR through Constraint Satisfaction: A Case Study on Convolution for Mobile GPUs},
	year = {2022},
	isbn = {9781450391832},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497776.3517777},
	doi = {10.1145/3497776.3517777},
	abstract = {Graphics Processing Units (GPUs) are notoriously hard to optimize for manually. What is needed are good automatic code generators and optimizers. Accelerate, Futhark and Lift demonstrated that a functional approach is well suited for this challenge. Lift, for instance, uses a system of rewrite rules with a multi-stage approach. Algorithmic optimizations are first explored, followed by hardware-specific optimizations such as using shared memory and mapping parallelism. While the algorithmic exploration leads to correct transformed programs by construction, it is not necessarily true for the latter phase. Exploiting shared memory and mapping parallelism while ensuring correct synchronization is a delicate balancing act, and is hard to encode in a rewrite system. Currently, Lift relies on heuristics with ad-hoc mechanisms to check for correctness. Although this practical approach eventually produces high-performance code, it is not an ideal state of affairs. This paper proposes to extract parallelization constraints automatically from a functional IR and use a solver to identify valid rewriting. Using a convolutional neural network on a mobile GPU as a use case, this approach matches the performance of the ARM Compute Library GEMM convolution and the TVM-generated kernel consuming between 2.7x and 3.6x less memory on average. Furthermore, a speedup of 12x is achieved over the ARM Compute Library direct convolution implementation.},
	booktitle = {Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction},
	pages = {218–230},
	numpages = {13},
	keywords = {convolution, mobile GPU, parallelism, code generation},
	location = {Seoul, South Korea},
	series = {CC 2022}
}

@inproceedings{moine22_specif_verif_trans_stack,
	author = {Moine, Alexandre and Chargu\'{e}raud, Arthur and Pottier, Fran\c{c}ois},
	title = {Specification and Verification of a Transient Stack},
	year = {2022},
	isbn = {9781450391825},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497775.3503677},
	doi = {10.1145/3497775.3503677},
	abstract = {A transient data structure is a package of an ephemeral data structure, a persistent data structure, and fast conversions between them. We describe the specification and proof of a transient stack and its iterators. This data structure is a scaled-down version of the general-purpose transient sequence data structure implemented in the OCaml library Sek. Internally, it relies on fixed-capacity arrays, or chunks, which can be shared between several ephemeral and persistent stacks. Dynamic tests are used to determine whether a chunk can be updated in place or must be copied: a chunk can be updated if it is uniquely owned or if the update is monotonic. Using CFML, which implements Separation Logic with Time Credits inside Coq, we verify the functional correctness and the amortized time complexity of this data structure. Our verification effort covers iterators, which involve direct pointers to internal chunks. The specification of iterators describes what the operations on iterators do, how much they cost, and under what circumstances an iterator is invalidated.},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {82–99},
	numpages = {18},
	keywords = {program verification, transient data structure},
	location = {Philadelphia, PA, USA},
	series = {CPP 2022}
}

@article{moine23_high_level_separ_logic_heap,
	author = {Moine, Alexandre and Chargu\'{e}raud, Arthur and Pottier, Fran\c{c}ois},
	title = {A High-Level Separation Logic for Heap Space under Garbage Collection},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571218},
	doi = {10.1145/3571218},
	abstract = {We present a Separation Logic with space credits for reasoning about heap space in a sequential call-by-value lambda-calculus equipped with garbage collection and mutable state. A key challenge is to design sound, modular, lightweight mechanisms for establishing the unreachability of a block. Prior work in this area uses pointed-by assertions to keep track of the predecessors of every block, but is carried out in the setting of an assembly-like programming language. We take up the challenge in the setting of a high-level language, where a key problem is to identify and reason about the memory locations that the garbage collector considers as roots. For this purpose, we propose novel "stackable" assertions, which keep track of the existence of stack-to-heap pointers without explicitly recording their origin. Furthermore, we explain how to reason about closures -- concrete heap-allocated data structures that implement the abstract concept of a first-class function. We demonstrate the expressiveness and tractability of our program logic via a range of examples, including recursive functions on linked lists, objects implemented using closures and mutable internal state, recursive functions in continuation-passing style, and three stack implementations that exhibit different space bounds. These last three examples illustrate reasoning about the reachability of the items stored in a container as well as amortized reasoning about space. All of our results are proved in Coq on top of Iris.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {25},
	numpages = {30},
	keywords = {tracing garbage collection, separation logic, live data, program verification}
}

@article{mokhov18_build_system_la_carte,
	author = {Mokhov, Andrey and Mitchell, Neil and Peyton Jones, Simon},
	title = {Build Systems \`{a} La Carte},
	year = {2018},
	issue_date = {September 2018},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {2},
	number = {ICFP},
	url = {https://doi.org/10.1145/3236774},
	doi = {10.1145/3236774},
	abstract = {Build systems are awesome, terrifying -- and unloved. They are used by every developer around the world, but are rarely the object of study. In this paper we offer a systematic, and executable, framework for developing and comparing build systems, viewing them as related points in landscape rather than as isolated phenomena. By teasing apart existing build systems, we can recombine their components, allowing us to prototype new build systems with desired properties.},
	journal = {Proc. ACM Program. Lang.},
	month = {jul},
	articleno = {79},
	numpages = {29},
	keywords = {algorithms, functional programming, build systems, applicative}
}

@article{mokhov19_selec_applic_funct,
	author = {Mokhov, Andrey and Lukyanov, Georgy and Marlow, Simon and Dimino, Jeremie},
	title = {Selective Applicative Functors},
	year = {2019},
	issue_date = {August 2019},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {3},
	number = {ICFP},
	url = {https://doi.org/10.1145/3341694},
	doi = {10.1145/3341694},
	abstract = {Applicative functors and monads have conquered the world of functional programming by providing general and powerful ways of describing effectful computations using pure functions. Applicative functors provide a way to compose independent effects that cannot depend on values produced by earlier computations, and all of which are declared statically. Monads extend the applicative interface by making it possible to compose dependent effects, where the value computed by one effect determines all subsequent effects, dynamically. This paper introduces an intermediate abstraction called selective applicative functors that requires all effects to be declared statically, but provides a way to select which of the effects to execute dynamically. We demonstrate applications of the new abstraction on several examples, including two industrial case studies.},
	journal = {Proc. ACM Program. Lang.},
	month = {jul},
	articleno = {90},
	numpages = {29},
	keywords = {monads, applicative}
}

@article{monniaux22_formal_verif_loop_invar_code,
	author = {Monniaux, David and Six, Cyril},
	title = {Formally Verified Loop-Invariant Code Motion and Assorted Optimizations},
	year = {2022},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {22},
	number = {1},
	issn = {1539-9087},
	url = {https://doi.org/10.1145/3529507},
	doi = {10.1145/3529507},
	abstract = {We present an approach for implementing a formally certified loop-invariant code motion optimization by composing an unrolling pass and a formally certified yet efficient global subexpression elimination. This approach is lightweight: each pass comes with a simple and independent proof of correctness. Experiments show the approach significantly narrows the performance gap between the CompCert certified compiler and state-of-the-art optimizing compilers. Our static analysis employs an efficient yet verified hashed set structure, resulting in the fast compilation.},
	journal = {ACM Trans. Embed. Comput. Syst.},
	month = {dec},
	articleno = {3},
	numpages = {27},
	keywords = {CompCert, Verified compilation, common subexpression elimination}
}

@inproceedings{monson15_using_sourc_level_trans_improv,
	author = {Monson, Joshua S. and Hutchings, Brad L.},
	location = {Monterey, California, USA},
	publisher = {ACM},
	url = {https://doi.org/10.1145/2684746.2689087},
	booktitle = {Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	doi = {10.1145/2684746.2689087},
	isbn = {978-1-4503-3315-3},
	keywords = {debugging,fpga,high-level synthesis,hls,simulation},
	pages = {5--8},
	series = {FPGA '15},
	title = {Using Source-Level Transformations to Improve High-Level Synthesis Debug and Validation on FPGAs},
	year = {2015}
}

@inproceedings{montagu21_trace_based_contr_flow_analy,
	abstract = {We define a small-step semantics for the untyped λ-calculus, that traces the β-reductions that occur during evaluation. By abstracting the computation traces, we reconstruct k-CFA using abstract interpretation, and justify constraint-based k-CFA in a semantic way. The abstract interpretation of the trace semantics also paves the way for introducing widening operators in CFA that go beyond existing analyses, that are all based on exploring a finite state space. We define ∇CFA, a widening-based analysis that limits the cycles in call stacks, and can achieve better precision than k-CFA at a similar cost.},
	author = {Montagu, Benoı̂t and Jensen, Thomas},
	location = {Virtual, Canada},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3453483.3454057},
	booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
	doi = {10.1145/3453483.3454057},
	isbn = {9781450383912},
	keywords = {control flow analysis,program traces,lambda-calculus,abstract interpretation,widening},
	pages = {482--496},
	series = {PLDI 2021},
	title = {Trace-Based Control-Flow Analysis},
	year = {2021}
}

@inproceedings{moreira12_decid_regul_expres_in_equiv_coq,
	abstract = {This work presents a mechanically verified implementation of an algorithm for deciding regular expression (in-)equivalence within the Coq proof assistant. This algorithm decides regular expression equivalence through an iterated process of testing the equivalence of their partial derivatives and also does not construct the underlying automata. Our implementation has a refutation step that improves the general efficiency of the decision procedure by enforcing the in-equivalence of regular expressions at early stages of computation. Recent theoretical and experimental research provide evidence that this method is, on average, more efficient than the classical methods based in automata. We present some performance tests and comparisons with similar approaches.},
	author = {Moreira, Nelma and Pereira, David and Melo de Sousa, Simão},
	editor = {Kahl, Wolfram and Griffin, Timothy G.},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Relational and Algebraic Methods in Computer Science},
	isbn = {978-3-642-33314-9},
	keywords = {regex,coq,path expressions,gated-SSA},
	pages = {98--113},
	title = {Deciding Regular Expressions (In-)Equivalence in Coq},
	year = {2012}
}

@article{morvan13_polyh_bubbl_inser,
	abstract = {High-level synthesis (HLS) allows hardware to be directly produced from behavioral description in C/C++, thus accelerating the design process. Loop pipelining is a key transformation of HLS, as it improves the throughput of the design at the price of a small hardware overhead. However, for small loops, its use often results in a poor hardware utilization due to the pipeline latency overhead. Overlapping the iterations of the whole loop nest instead of only overlapping the innermost loop is a way to overcome this difficulty, but currently available techniques are restricted to perfectly nested loops with constant bounds, involving uniform dependences only. Using the polyhedral model, we extend the applicability of the nested loop pipelining transformation by proposing a new legality check and a new loop correction technique, called polyhedral bubble insertion. This method was implemented in a source-to-source compiler targeting HLS, and results on benchmark kernels show that polyhedral bubble insertion is effective in practice on a much larger class of loop nests.},
	author = {Morvan, Antoine and Derrien, Steven and Quinton, Patrice},
	doi = {10.1109/TCAD.2012.2228270},
	issn = {1937-4151},
	journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	month = mar,
	number = {3},
	pages = {339--352},
	title = {Polyhedral Bubble Insertion: A Method to Improve Nested Loop Pipelining for High-Level Synthesis},
	volume = {32},
	year = {2013}
}

@inproceedings{moskewicz01_chaff,
	author = {Moskewicz, Matthew W. and Madigan, Conor F. and Zhao, Ying and Zhang, Lintao and Malik, Sharad},
	title = {Chaff: Engineering an Efficient SAT Solver},
	year = {2001},
	isbn = {1581132972},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/378239.379017},
	doi = {10.1145/378239.379017},
	abstract = {Boolean Satisfiability is probably the most studied of combinatorial optimization/search problems. Significant effort has been devoted to trying to provide practical solutions to this problem for problem instances encountered in a range of applications in Electronic Design Automation (EDA), as well as in Artificial Intelligence (AI). This study has culminated in the development of several SAT packages, both proprietary and in the public domain (e.g. GRASP, SATO) which find significant use in both research and industry. Most existing complete solvers are variants of the Davis-Putnam (DP) search algorithm. In this paper we describe the development of a new complete solver, Chaff, which achieves significant performance gains through careful engineering of all aspects of the search - especially a particularly efficient implementation of Boolean constraint propagation (BCP) and a novel low overhead decision strategy. Chaff has been able to obtain one to two orders of magnitude performance improvement on difficult SAT benchmarks in comparison with other solvers (DP or otherwise), including GRASP and SATO.},
	booktitle = {Proceedings of the 38th Annual Design Automation Conference},
	pages = {530–535},
	numpages = {6},
	keywords = {boolean satisfiability, design verification, SAT},
	location = {Las Vegas, Nevada, USA},
	series = {DAC '01}
}

@InProceedings{moura08_z,
	keywords = {SMT},
	author = {de Moura, Leonardo and Bj{\o}rner, Nikolaj},
	editor = "Ramakrishnan, C. R.
and Rehof, Jakob",
	title = {Z3: An Efficient SMT Solver},
	booktitle = "Tools and Algorithms for the Construction and Analysis of Systems",
	year = "2008",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "337--340",
	abstract = "Satisfiability Modulo Theories (SMT) problem is a decision problem for logical first order formulas with respect to combinations of background theories such as: arithmetic, bit-vectors, arrays, and uninterpreted functions. Z3 is a new and efficient SMT Solver freely available from Microsoft Research. It is used in various software verification and analysis applications.",
	isbn = "978-3-540-78800-3"
}

@InProceedings{moura15_l,
	doi = {10.1007/978-3-319-21401-6_26},
	author = {de Moura, Leonardo and Kong, Soonho and Avigad, Jeremy and van Doorn, Floris and von Raumer, Jakob},
	editor = {Felty, Amy P. and Middeldorp, Aart},
	title = "The Lean Theorem Prover (System Description)",
	booktitle = "Automated Deduction - CADE-25",
	year = "2015",
	publisher = "Springer International Publishing",
	address = "Cham",
	pages = "378--388",
	abstract = "Lean is a new open source theorem prover being developed at Microsoft Research and Carnegie Mellon University, with a small trusted kernel based on dependent type theory. It aims to bridge the gap between interactive and automated theorem proving, by situating automated tools and methods in a framework that supports user interaction and the construction of fully specified axiomatic proofs. Lean is an ongoing and long-term effort, but it already provides many useful components, integrated development environments, and a rich API which can be used to embed it into other systems. It is currently being used to formalize category theory, homotopy type theory, and abstract algebra. We describe the project goals, system architecture, and main features, and we discuss applications and continuing work.",
	isbn = "978-3-319-21401-6"
}

@article{moy21_corps_reviv,
	abstract = {Gradually typed programming languages permit the incremental addition of static types to untyped programs. To remain sound, languages insert run-time checks at the boundaries between typed and untyped code. Unfortunately, performance studies have shown that the overhead of these checks can be disastrously high, calling into question the viability of sound gradual typing. In this paper, we show that by building on existing work on soft contract verification, we can reduce or eliminate this overhead. Our key insight is that while untyped code cannot be trusted by a gradual type system, there is no need to consider only the worst case when optimizing a gradually typed program. Instead, we statically analyze the untyped portions of a gradually typed program to prove that almost all of the dynamic checks implied by gradual type boundaries cannot fail, and can be eliminated at compile time. Our analysis is modular, and can be applied to any portion of a program. We evaluate this approach on a dozen existing gradually typed programs previously shown to have prohibitive performance overhead—with a median overhead of 2.5\texttimes{} and up to 80.6\texttimes{} in the worst case—and eliminate all overhead in most cases, suffering only 1.5\texttimes{} overhead in the worst case.},
	author = {Moy, Cameron and Nguyundefinedn, Phúc C. and Tobin-Hochstadt, Sam and Van Horn, David},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434334},
	doi = {10.1145/3434334},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {gradual typing,Typed Racket,contract verification},
	month = jan,
	number = {POPL},
	title = {Corpse Reviver: Sound and Efficient Gradual Typing via Contract Verification},
	volume = {5},
	year = {2021}
}

@InProceedings{muijnck-hughes23_wirin_circuit_is_easy_is_it,
	author = {de Muijnck-Hughes, Jan and Vanderbauwhede, Wim},
	title = {{Wiring Circuits Is Easy as \{0,1,\omega\}, or Is It...}},
	booktitle = {37th European Conference on Object-Oriented Programming (ECOOP 2023)},
	pages = {8:1--8:28},
	series = {Leibniz International Proceedings in Informatics (LIPIcs)},
	ISBN = {978-3-95977-281-5},
	ISSN = {1868-8969},
	year = {2023},
	volume = {263},
	editor = {Ali, Karim and Salvaneschi, Guido},
	publisher = {Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
	address = {Dagstuhl, Germany},
	URL = {https://drops.dagstuhl.de/opus/volltexte/2023/18201},
	URN = {urn:nbn:de:0030-drops-182010},
	doi = {10.4230/LIPIcs.ECOOP.2023.8},
	annote = {Keywords: Hardware Design, Linear Types, Dependent Types, DSLs, Idris, SystemVerilog, Netlists}
}

@INPROCEEDINGS{mukherjee15_hardw_verif_using_softw_analy,
	keywords = {hardware abstract interpretation},
	author = {Mukherjee, Rajdeep and Kroening, Daniel and Melham, Tom},
	booktitle = {2015 IEEE Computer Society Annual Symposium on VLSI},
	title = {Hardware Verification Using Software Analyzers},
	year = {2015},
	volume = {},
	number = {},
	pages = {7-12},
	doi = {10.1109/ISVLSI.2015.107}
}

@misc{mukherjee20_hardw_softw_co_using_path_symbol_execut,
	doi = {10.48550/ARXIV.2001.01324},
	url = {https://arxiv.org/abs/2001.01324},
	author = {Mukherjee, Rajdeep and Joshi, Saurabh and O'Leary, John and Kroening, Daniel and Melham, Tom},
	keywords = {hardware abstract interpretation},
	title = {Hardware/Software Co-verification Using Path-based Symbolic Execution},
	publisher = {arXiv},
	year = {2020},
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{muller21_model_analy_evaluat_cost_cuda_kernel,
	abstract = {General-purpose programming on GPUs (GPGPU) is becoming increasingly in vogue as applications such as machine learning and scientific computing demand high throughput in vector-parallel applications. NVIDIA's CUDA toolkit seeks to make GPGPU programming accessible by allowing programmers to write GPU functions, called kernels, in a small extension of C/C++. However, due to CUDA's complex execution model, the performance characteristics of CUDA kernels are difficult to predict, especially for novice programmers. This paper introduces a novel quantitative program logic for CUDA kernels, which allows programmers to reason about both functional correctness and resource usage of CUDA kernels, paying particular attention to a set of common but CUDA-specific performance bottlenecks. The logic is proved sound with respect to a novel operational cost semantics for CUDA kernels. The semantics, logic and soundness proofs are formalized in Coq. An inference algorithm based on LP solving automatically synthesizes symbolic resource bounds by generating derivations in the logic. This algorithm is the basis of RaCuda, an end-to-end resource-analysis tool for kernels, which has been implemented using an existing resource-analysis tool for imperative programs. An experimental evaluation on a suite of CUDA benchmarks shows that the analysis is effective in aiding the detection of performance bugs in CUDA kernels.},
	author = {Muller, Stefan K. and Hoffmann, Jan},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434306},
	doi = {10.1145/3434306},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {CUDA,thread-level parallelism,performance analysis,program logics,resource-aware type system},
	month = jan,
	number = {POPL},
	title = {Modeling and Analyzing Evaluation Cost of CUDA Kernels},
	volume = {5},
	year = {2021}
}

@article{muller22_static_predic_paral_comput_graph,
	author = {Muller, Stefan K.},
	title = {Static Prediction of Parallel Computation Graphs},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498708},
	doi = {10.1145/3498708},
	abstract = {Many algorithms for analyzing parallel programs, for example to detect deadlocks or data races or to calculate the execution cost, are based on a model variously known as a cost graph, computation graph or dependency graph, which captures the parallel structure of threads in a program. In modern parallel programs, computation graphs are highly dynamic and depend greatly on the program inputs and execution details. As such, most analyses that use these graphs are either dynamic analyses or are specialized static analyses that gather a subset of dependency information for a specific purpose. This paper introduces graph types, which compactly represent all of the graphs that could arise from program execution. Graph types are inferred from a parallel program using a graph type system and inference algorithm, which we present drawing on ideas from Hindley-Milner type inference, affine logic and region type systems. We have implemented the inference algorithm over a subset of OCaml, extended with parallelism primitives, and we demonstrate how graph types can be used to accelerate the development of new graph-based static analyses by presenting proof-of-concept analyses for deadlock detection and cost analysis.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {46},
	numpages = {31},
	keywords = {type inference, cost graphs, parallel programs, type systems, computation graphs, graph types}
}

@book{munkres00_topol,
	note = {Central Library, 2nd Floor, 515.1 MUN},
	publisher = {Prentice Hall, Inc.},
	title = {Topology },
	year = {2000},
	author = {Munkres, James R.},
	address = {Upper Saddle River, NJ},
	booktitle = {Topology},
	edition = {2nd ed.},
	isbn = {0131816292},
	keywords = {topology, book},
	language = {eng},
	lccn = {99052942}
}

@inbook{mycroft13_notion_alias_owner,
	abstract = {We survey notions of aliasing and ownership. An extreme but conceptually useful model is that of pure linear languages where each object is constructed once and read, being consumed, once. We see more realistic programming languages as relaxing this to allow multiple references to an object (spatial aliasing) or multiple sequenced operations on a single live reference (temporal aliasing) before the object is deallocated. Concurrency complicates things (concurrent aliasing) because spatial aliasing may only happen under certain scheduling conditions. We argue that this view of aliasing is closely related to that of type tags in low-level implementations of dynamic types.},
	author = {Mycroft, Alan and Voigt, Janina},
	editor = {Clarke, Dave and Noble, James and Wrigstad, Tobias},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	url = {https://doi.org/10.1007/978-3-642-36946-9_4},
	booktitle = {Aliasing in Object-Oriented Programming. Types, Analysis and Verification},
	doi = {10.1007/978-3-642-36946-9_4},
	isbn = {978-3-642-36946-9},
	keywords = {memory aliasing,ownership types},
	pages = {59--83},
	title = {Notions of Aliasing and Ownership},
	year = {2013}
}

@article{myreen09_trans_progr_recur_funct,
	abstract = {This paper presents a new proof-assistant based approach to program verification: programs are translated, via fully-automatic deduction, into tail-recursive function defined in the logic of a theorem prover. This approach improves on well-established methods based on Hoare logic and verification condition generation (VCG) by removing the need to annotate programs with assertions, making the proof natural to the theorem prover and being easier to implement than a trusted VCG. Our tool has been implemented in the HOL4 theorem prover.},
	author = {Myreen, Magnus O. and Gordon, Michael J.C.},
	url = {http://www.sciencedirect.com/science/article/pii/S1571066109001741},
	doi = {https://doi.org/10.1016/j.entcs.2009.05.052},
	issn = {1571-0661},
	journaltitle = {Electronic Notes in Theoretical Computer Science},
	keywords = {program verification,theorem proving},
	note = {Proceedings of the Eleventh Brazilian Symposium on Formal Methods (SBMF 2008)},
	pages = {185--200},
	title = {Transforming Programs into Recursive Functions},
	volume = {240},
	year = {2009}
}

@InProceedings{müller16_autom_verif_iterat_separ_conjun,
	keywords = {symbolic execution, predicated execution, memory model},
	doi = {10.1007/978-3-319-41528-4_22},
	author = "Müller, Peter and Schwerhoff, Malte and Summers, Alexander J.",
	editor = "Chaudhuri, Swarat and Farzan, Azadeh",
	title = "Automatic Verification of Iterated Separating Conjunctions Using Symbolic Execution",
	booktitle = "Computer Aided Verification",
	year = "2016",
	publisher = "Springer International Publishing",
	address = "Cham",
	pages = "405--425",
	abstract = "In permission logics such as separation logic, the iterated separating conjunction is a quantifier denoting access permission to an unbounded set of heap locations. In contrast to recursive predicates, iterated separating conjunctions do not prescribe a structure on the locations they range over, and so do not restrict how to traverse and modify these locations. This flexibility is important for the verification of random-access data structures such as arrays and data structures that can be traversed in multiple ways such as graphs. Despite its usefulness, no automatic program verifier natively supports iterated separating conjunctions; they are especially difficult to incorporate into symbolic execution engines, the prevalent technique for building verifiers for these logics.",
	isbn = "978-3-319-41528-4"
}

@INPROCEEDINGS{müller21_formal_approac_confid_verif_socs,
	author = {Müller, Johannes and Fadiheh, Mohammad Rahmani and Antón, Anna Lena Duque and Eisenbarth, Thomas and Stoffel, Dominik and Kunz, Wolfgang},
	booktitle = {2021 58th ACM/IEEE Design Automation Conference (DAC)},
	title = {A Formal Approach to Confidentiality Verification in SoCs at the Register Transfer Level},
	year = {2021},
	volume = {},
	number = {},
	pages = {991-996},
	doi = {10.1109/DAC18074.2021.9586248}
}

@inproceedings{nane12_light_specul_predic_schem_hardw_execut,
	abstract = {If-conversion is a known software technique to speedup applications containing conditional expressions and targeting processors with predication support. However, the success of this scheme is highly dependent on the structure of the if-statements, i.e., if they are balanced or unbalanced, as well as on the path taken. Therefore, the predication scheme does not always provide a better execution time than the conventional jump scheme. In this paper, we present an algorithm that leverages the benefits of both jump and predication schemes adapted for hardware execution. The results show that performance degradation is not possible anymore for the unbalanced if-statements as well as a speedup for all test cases between 4% and 21%.},
	author = {{Nane}, R. and {Sima}, V. and {Bertels}, K.},
	booktitle = {2012 International Conference on Reconfigurable Computing and FPGAs},
	doi = {10.1109/ReConFig.2012.6416721},
	issn = {2325-6532},
	keywords = {high-level synthesis,speculative execution,predicated execution},
	month = dec,
	pages = {1--6},
	title = {A Lightweight Speculative and Predicative Scheme for Hardware Execution},
	year = {2012}
}

@article{nane16_survey_evaluat_fpga_high_level_synth_tools,
	abstract = {High-level synthesis (HLS) is increasingly popular for the design of high-performance and energy-efficient heterogeneous systems, shortening time-to-market and addressing today's system complexity. HLS allows designers to work at a higher-level of abstraction by using a software program to specify the hardware functionality. Additionally, HLS is particularly interesting for designing field-programmable gate array circuits, where hardware implementations can be easily refined and replaced in the target device. Recent years have seen much activity in the HLS research community, with a plethora of HLS tool offerings, from both industry and academia. All these tools may have different input languages, perform different internal optimizations, and produce results of different quality, even for the very same input description. Hence, it is challenging to compare their performance and understand which is the best for the hardware to be implemented. We present a comprehensive analysis of recent HLS tools, as well as overview the areas of active interest in the HLS research community. We also present a first-published methodology to evaluate different HLS tools. We use our methodology to compare one commercial and three academic tools on a common set of C benchmarks, aiming at performing an in-depth evaluation in terms of performance and the use of resources.},
	author = {{Nane}, R. and {Sima}, V. and {Pilato}, C. and {Choi}, J. and {Fort}, B. and {Canis}, A. and {Chen}, Y. T. and {Hsiao}, H. and {Brown}, S. and {Ferrandi}, F. and {Anderson}, J. and {Bertels}, K.},
	url = {https://doi.org/10.1109/TCAD.2015.2513673},
	doi = {10.1109/TCAD.2015.2513673},
	issn = {1937-4151},
	journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {if-conversion,predicated execution,high-level synthesis,compiler optimisation,legup},
	month = oct,
	number = {10},
	pages = {1591--1604},
	title = {A Survey and Evaluation of Fpga High-Level Synthesis Tools},
	volume = {35},
	year = {2016}
}

@inproceedings{narasimhan98,
	author = {Narasimhan, Naren and Vemuri, Ranga},
	editor = {Grundy, Jim and Newey, Malcolm},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Theorem Proving in Higher Order Logics},
	isbn = {978-3-540-49801-8},
	pages = {367--386},
	title = {On the effectiveness of theorem proving guided discovery of formal assertions for a register allocator in a high-level synthesis system},
	year = {1998}
}

@inproceedings{nash22_formal_lie_algeb,
	author = {Nash, Oliver},
	title = {Formalising Lie Algebras},
	year = {2022},
	isbn = {9781450391825},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497775.3503672},
	doi = {10.1145/3497775.3503672},
	abstract = {Lie algebras are an important class of algebras which arise throughout mathematics and physics. We report on the formalisation of Lie algebras in Lean's Mathlib library. Although basic knowledge of Lie theory will benefit the reader, none is assumed; the intention is that the overall themes will be accessible even to readers unfamiliar with Lie theory. Particular attention is paid to the construction of the classical and exceptional Lie algebras. Thanks to these constructions, it is possible to state the classification theorem for finite-dimensional semisimple Lie algebras over an algebraically closed field of characteristic zero. In addition to the focus on Lie theory, we also aim to highlight the unity of Mathlib. To this end, we include examples of achievements made possible only by leaning on several branches of the library simultaneously.},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {239–250},
	numpages = {12},
	keywords = {proof assistant, Lean, formal math, Lie theory, algebra},
	location = {Philadelphia, PA, USA},
	series = {CPP 2022}
}

@InProceedings{necsulescu11_agvhc,
	keywords = {control-flow, data-flow},
	author = {Necsulescu, Philip I. and Groza, Voicu},
	booktitle = {2011 6th IEEE International Symposium on Applied Computational Intelligence and Informatics (SACI)},
	title = {Automatic Generation of {VHDL} Hardware Code From Data Flow Graphs},
	year = {2011},
	volume = {},
	number = {},
	pages = {523-528},
	doi = {10.1109/SACI.2011.5873059}
}

@article{necula00_trans_valid_optim_compil,
	author = {Necula, George C.},
	location = {New York, NY, USA},
	publisher = {ACM},
	url = {https://doi.org/10.1145/358438.349314},
	doi = {10.1145/358438.349314},
	issn = {0362-1340},
	journaltitle = {SIGPLAN Not.},
	keywords = {verification,translation validation,compiler optimisation},
	month = may,
	number = {5},
	pages = {83--94},
	title = {Translation Validation for an Optimizing Compiler},
	volume = {35},
	year = {2000}
}

@inproceedings{necula98_desig_implem_certif_compil,
	author = {Necula, George C. and Lee, Peter},
	location = {Montreal, Quebec, Canada},
	publisher = {ACM},
	url = {https://doi.org/10.1145/277650.277752},
	booktitle = {Proceedings of the ACM SIGPLAN 1998 Conference on Programming Language Design and Implementation},
	doi = {10.1145/277650.277752},
	isbn = {0-89791-987-4},
	keywords = {translation validation,verification,compiler optimisation},
	pages = {333--344},
	series = {PLDI '98},
	title = {The Design and Implementation of a Certifying Compiler},
	year = {1998}
}

@inproceedings{neshatpour18_archit_consid_fpga_accel_machin,
	keywords = {FPGA, motivation},
	author = {Neshatpour, Katayoun and Mokrani, Hosein Mohammadi and Sasan, Avesta and Ghasemzadeh, Hassan and Rafatirad, Setareh and Homayoun, Houman},
	title = {Architectural Considerations for FPGA Acceleration of Machine Learning Applications in MapReduce},
	year = {2018},
	isbn = {9781450364942},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3229631.3229639},
	abstract = {While demand for data center computational resources continues to grow as the size of data grows, the semiconductor industry has reached scaling limits and is no longer able to reduce power consumption in new chips. Unfortunately, the promise of analytics running on large systems (e.g., data-centers) over huge data, and scaling those systems into the future, coincides with an era when the end of Dennard scaling brings into serious question our ability to provide scalable computational power without prohibitive power and energy costs.},
	booktitle = {Proceedings of the 18th International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation},
	pages = {89–96},
	numpages = {8},
	location = {Pythagorion, Greece},
	series = {SAMOS '18}
}

@article{nguyen22_modul_probab_model_algeb_effec,
	author = {Nguyen, Minh and Perera, Roly and Wang, Meng and Wu, Nicolas},
	title = {Modular Probabilistic Models via Algebraic Effects},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3547635},
	doi = {10.1145/3547635},
	abstract = {Probabilistic programming languages (PPLs) allow programmers to construct statistical models and then simulate data or perform inference over them. Many PPLs restrict models to a particular instance of simulation or inference, limiting their reusability. In other PPLs, models are not readily composable. Using Haskell as the host language, we present an embedded domain specific language based on algebraic effects, where probabilistic models are modular, first-class, and reusable for both simulation and inference. We also demonstrate how simulation and inference can be expressed naturally as composable program transformations using algebraic effect handlers.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {104},
	numpages = {30},
	keywords = {modularity, effect handlers, functional programming, probabilistic programming, embedded domain-specific languages}
}

@inproceedings{ni23_asn1,
	author = {Ni, Haobin and Delignat-Lavaud, Antoine and Fournet, C\'{e}dric and Ramananandro, Tahina and Swamy, Nikhil},
	title = {ASN1*: Provably Correct, Non-Malleable Parsing for ASN.1 DER},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575684},
	doi = {10.1145/3573105.3575684},
	abstract = {Abstract Syntax Notation One (ASN.1) is a language for structured data exchange between computers, standardized by both ITU-T and ISO/IEC since 1984. The Distinguished Encoding Rules (DER) specify its non-malleable binary format: for a given ASN.1 data type, every value has a distinct, unique binary representation. ASN.1 DER is used in many security-critical interfaces for telecommunications and networking, such as the X.509 public key infrastructure, where non-malleability is essential. However, due to the expressiveness and flexibility of the general-purpose ASN.1 language, correctly parsing ASN.1 DER data formats is still considered a serious security challenge in practice. We present ASN1*, the first formalization of ASN.1 DER with a mechanized proof of non-malleability. Our development provides a shallow embedding of ASN.1 in the F* proof assistant and formalizes its DER semantics within the EverParse parser generator framework. It guarantees that any ASN.1 data encoded using our DER semantics is non-malleable. It yields verified code that parses valid binary representations into values of the corresponding ASN.1 data type while rejecting invalid ones. We empirically confirm that our semantics models ASN.1 DER usage in practice by evaluating ASN1* parsers extracted to OCaml on both positive and negative test cases involving X.509 certificates and Certificate Revocation Lists (CRLs).},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {275–289},
	numpages = {15},
	keywords = {ASN.1, Parsing, Formal verification, Domain-specific Language},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@inproceedings{nielsen23_formal_decen_exchan_coq,
	author = {Nielsen, Eske Hoy and Annenkov, Danil and Spitters, Bas},
	title = {Formalising Decentralised Exchanges in Coq},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575685},
	doi = {10.1145/3573105.3575685},
	abstract = {The number of attacks and accidents leading to significant losses of crypto-assets is growing. According to Chainalysis, in 2021, approx. \$14 billion has been lost due to various incidents, and this number is dominated by Decentralized Finance (DeFi) applications. To address these issues, one can use a collection of tools ranging from auditing to formal methods. We use formal verification and provide the first formalisation of a DeFi contract in a foundational proof assistant capturing contract interactions. We focus on Dexter2, a decentralized, non-custodial exchange for the Tezos network similar to Uniswap on Ethereum. The Dexter implementation consists of several smart contracts. This poses unique challenges for formalisation due to the complex contract interactions. Our formalisation includes proofs of functional correctness with respect to an informal specification for the contracts involved in Dexter’s implementation. Moreover, our formalisation is the first to feature proofs of safety properties of the interacting smart contracts of a decentralized exchange. We have extracted our contract from Coq into CameLIGO code, so it can be deployed on the Tezos blockchain. Uniswap and Dexter are paradigmatic for a collection of similar contracts. Our methodology thus allows us to implement and verify DeFi applications featuring similar interaction patterns.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {290–302},
	numpages = {13},
	keywords = {software correctness, blockchain, Coq, smart contracts, decentralized finance},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@InProceedings{niemetz18_bbb,
	keywords = {SMT},
	author = {Niemetz, Aina and Preiner, Mathias and Wolf, Clifford and Biere, Armin},
	editor = "Chockler, Hana
and Weissenbacher, Georg",
	title = "Btor2 , BtorMC and Boolector 3.0",
	booktitle = "Computer Aided Verification",
	year = "2018",
	publisher = "Springer International Publishing",
	address = "Cham",
	pages = "587--595",
	abstract = "We describe Btor2, a word-level model checking format for capturing models of hardware and potentially software in a bit-precise manner. This simple, line-based and easy to parse format can be seen as a sorted extension of the word-level format Btor. It uses design principles from the bit-level format Aiger and follows semantics of the Smt-Lib logics of bit-vectors with arrays. This intermediate format can be used in various verification flows and is perfectly suited to establish a word-level model checking competition. It is supported by our new open source model checker BtorMC, which is built on top of version 3.0 of our SMT solver Boolector. We further provide new word-level benchmarks on which these open source tools are evaluated.",
	isbn = "978-3-319-96145-3"
}

@inproceedings{niemetz19_towar_bit_width_indep_proof_smt_solver,
	keywords = {SMT, modulo solver},
	abstract = {Many SMT solvers implement efficient SAT-based procedures for solving fixed-size bit-vector formulas. These approaches, however, cannot be used directly to reason about bit-vectors of symbolic bit-width. To address this shortcoming, we propose a translation from bit-vector formulas with parametric bit-width to formulas in a logic supported by SMT solvers that includes non-linear integer arithmetic, uninterpreted functions, and universal quantification. While this logic is undecidable, this approach can still solve many formulas by capitalizing on advances in SMT solving for non-linear arithmetic and universally quantified formulas. We provide several case studies in which we have applied this approach with promising results, including the bit-width independent verification of invertibility conditions, compiler optimizations, and bit-vector rewrites.},
	address = {Cham},
	author = {Niemetz, Aina and Preiner, Mathias and Reynolds, Andrew and Zohar, Yoni and Barrett, Clark and Tinelli, Cesare},
	booktitle = {Automated Deduction -- CADE 27},
	editor = {Fontaine, Pascal},
	isbn = {978-3-030-29436-6},
	pages = {366--384},
	publisher = {Springer International Publishing},
	title = {Towards Bit-Width-Independent Proofs in SMT Solvers},
	year = {2019}
}

@inproceedings{nigam20_predic_accel_desig_time_sensit_affin_types,
	abstract = {Field-programmable gate arrays (FPGAs) provide an opportunity to co-design applications with hardware accelerators, yet they remain difficult to program. High-level synthesis (HLS) tools promise to raise the level of abstraction by compiling C or C++ to accelerator designs. Repurposing legacy software languages, however, requires complex heuristics to map imperative code onto hardware structures. We find that the black-box heuristics in HLS can be unpredictable: changing parameters in the program that should improve performance can counterintuitively yield slower and larger designs. This paper proposes a type system that restricts HLS to programs that can predictably compile to hardware accelerators. The key idea is to model consumable hardware resources with a time-sensitive affine type system that prevents simultaneous uses of the same hardware structure. We implement the type system in Dahlia, a language that compiles to HLS C++, and show that it can reduce the size of HLS parameter spaces while accepting Pareto-optimal designs.},
	author = {Nigam, Rachit and Atapattu, Sachille and Thomas, Samuel and Li, Zhijing and Bauer, Theodore and Ye, Yuwei and Koti, Apurva and Sampson, Adrian and Zhang, Zhiru},
	location = {London, UK},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3385412.3385974},
	booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
	doi = {10.1145/3385412.3385974},
	isbn = {9781450376136},
	keywords = {affine types,high-level synthesis},
	pages = {393--407},
	series = {PLDI 2020},
	title = {Predictable Accelerator Design with Time-Sensitive Affine Types},
	year = {2020}
}

@InProceedings{niqui08_modul_devel_hybrid_system_verif_coq,
	keywords = {predicate abstraction, abstract interpretation, coq},
	doi = {10.1007/978-3-540-78929-1_53},
	author = {Niqui, Milad and Tveretina, Olga},
	editor = "Egerstedt, Magnus
and Mishra, Bud",
	title = "Modular Development of Hybrid Systems for Verification in Coq",
	booktitle = "Hybrid Systems: Computation and Control",
	year = "2008",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "638--641",
	abstract = "In this paper we present a formalization of the theory of hybrid automata and algorithms for building trajectory trees using module types and functors in the Coq proof assistant.",
	isbn = "978-3-540-78929-1"
}

@article{niu22_cost_aware_logic_framew,
	author = {Niu, Yue and Sterling, Jonathan and Grodin, Harrison and Harper, Robert},
	title = {A Cost-Aware Logical Framework},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498670},
	doi = {10.1145/3498670},
	abstract = {We present calf, a cost-aware logical framework for studying quantitative aspects of functional programs. Taking inspiration from recent work that reconstructs traditional aspects of programming languages in terms of a modal account of phase distinctions, we argue that the cost structure of programs motivates a phase distinction between intension and extension. Armed with this technology, we contribute a synthetic account of cost structure as a computational effect in which cost-aware programs enjoy an internal noninterference property: input/output behavior cannot depend on cost. As a full-spectrum dependent type theory, calf presents a unified language for programming and specification of both cost and behavior that can be integrated smoothly with existing mathematical libraries available in type theoretic proof assistants. We evaluate calf as a general framework for cost analysis by implementing two fundamental techniques for algorithm analysis: the method of recurrence relations and physicist’s method for amortized analysis. We deploy these techniques on a variety of case studies: we prove a tight, closed bound for Euclid’s algorithm, verify the amortized complexity of batched queues, and derive tight, closed bounds for the sequential and parallel complexity of merge sort, all fully mechanized in the Agda proof assistant. Lastly we substantiate the soundness of quantitative reasoning in calf by means of a model construction.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {9},
	numpages = {31},
	keywords = {behavioral verification, recurrence relations, noninterference, phase distinction, intensional property, amortized analysis, cost models, mechanized proof, parallel algorithms, proof assistants, modal type theory, equational reasoning, algorithm analysis}
}

@inproceedings{noronha17_rapid_fpga,
	author = {{Noronha}, D. H. and {Pinilla}, J. P. and {Wilton}, S. J. E.},
	booktitle = {2017 International Conference on ReConFigurable Computing and FPGAs (ReConFig)},
	doi = {10.1109/RECONFIG.2017.8279807},
	keywords = {high-level synthesis,FPGA,inlining,compiler optimisation},
	pages = {1--6},
	title = {Rapid circuit-specific inlining tuning for FPGA high-level synthesis},
	year = {2017}
}

@article{ntlatlapa98_high_level_synth_using_depen,
	author = {Ntlatlapa, Ntsibane},
	journaltitle = {Department of Computer science and engineering, Auburn University},
	keywords = {high-level synthesis},
	title = {High-Level Synthesis Using Dependence Flow Graphs As the Intermediate Form},
	year = {1998}
}

@ARTICLE{numan20_t,
	author = {Numan, Mostafa W. and Phillips, Braden J. and Puddy, Gavin S. and Falkner, Katrina},
	journal = {IEEE Access},
	title = {Towards Automatic High-Level Code Deployment on Reconfigurable Platforms: A Survey of High-Level Synthesis Tools and Toolchains},
	year = {2020},
	volume = {8},
	number = {},
	pages = {174692-174722},
	doi = {10.1109/ACCESS.2020.3024098}
}

@inproceedings{oancea15_scalab_civ,
	abstract = {Subscripts using induction variables that cannot be expressed as a formula in terms of the enclosing-loop indices appear in the low-level implementation of common programming abstractions such as Alter, or stack operations and pose significant challenges to automatic parallelization. Because the complexity of such induction variables is often due to their conditional evaluation across the iteration space of loops we name them Conditional Induction Variables (CIV). This paper presents a flow-sensitive technique that summarizes both such CIV-based and affine subscripts to program level, using the same representation. Our technique requires no modifications of our dependence tests, which is agnostic to the original shape of the subscripts, and is more powerful than previously reported dependence tests that rely on the pairwise disambiguation of read-write references. We have implemented the CIV analysis in our parallelizing compiler and evaluated its impact on five Fortran benchmarks. We have found that that there are many important loops using CIV subscripts and that our analysis can lead to their scalable parallelization. This in turn has led to the parallelization of the benchmark programs they appear in.},
	author = {Oancea, Cosmin E. and Rauchwerger, Lawrence},
	booktitle = {2015 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},
	doi = {10.1109/CGO.2015.7054201},
	keywords = {,gated-SSA,optimization,application},
	month = feb,
	pages = {213--224},
	title = {Scalable conditional induction variables (CIV) analysis},
	year = {2015}
}

@InProceedings{oe12_versat,
	doi = {10.1007/978-3-642-27940-9_24},
	author = {Oe, Duckki and Stump, Aaron and Oliver, Corey and Clancy, Kevin},
	editor = "Kuncak, Viktor
and Rybalchenko, Andrey",
	title = {Versat: A Verified Modern SAT Solver},
	booktitle = "Verification, Model Checking, and Abstract Interpretation",
	year = "2012",
	publisher = {Springer},
	address = "Berlin, Heidelberg",
	pages = "363--378",
	abstract = "This paper presents versat, a formally verified SAT solver incorporating the essential features of modern SAT solvers, including clause learning, watched literals, optimized conflict analysis, non-chronological backtracking, and decision heuristics. Unlike previous related work on SAT-solver verification, our implementation uses efficient low-level data structures like mutable C arrays for clauses and other solver state, and machine integers for literals. The implementation and proofs are written in Guru, a verified-programming language. We compare versat to a state-of-the-art SAT solver that produces certified ``unsat'' answers. We also show through an empirical evaluation that versat can solve SAT problems on the modern scale.",
	isbn = "978-3-642-27940-9"
}

@misc{oezkan20_anyhl,
	author = {Özkan, M. Akif and Pérard-Gayot, Arsène and Membarth, Richard and Slusallek, Philipp and Leissa, Roland and Hack, Sebastian and Teich, Jürgen and Hannig, Frank},
	eprint = {2002.05796},
	eprintclass = {cs.PL},
	eprinttype = {arXiv},
	keywords = {high-level synthesis},
	title = {AnyHLS: High-Level Synthesis with Partial Evaluation},
	year = {2020}
}

@Article{oh08_specul_loop_pipel_binar_trans_hardw_accel,
	keywords = {speculative execution, predicated execution, high-level synthesis},
	author = {Oh, Sejong and Kim, Tag Gon and Cho, Jeonghun and Bozorgzadeh, Elaheh},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	title = {Speculative Loop-Pipelining in Binary Translation for Hardware Acceleration},
	year = {2008},
	volume = {27},
	number = {3},
	pages = {409-422},
	doi = {10.1109/TCAD.2008.915533}
}

@article{ohman22_visib_reason_concur_snaps_algor,
	author = {\"{O}hman, Joakim and Nanevski, Aleksandar},
	title = {Visibility Reasoning for Concurrent Snapshot Algorithms},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498694},
	doi = {10.1145/3498694},
	abstract = {Visibility relations have been proposed by Henzinger et al. as an abstraction for proving linearizability of concurrent algorithms that obtains modular and reusable proofs. This is in contrast to the customary approach based on exhibiting the algorithm's linearization points. In this paper we apply visibility relations to develop modular proofs for three elegant concurrent snapshot algorithms of Jayanti. The proofs are divided by signatures into components of increasing level of abstraction; the components at higher abstraction levels are shared, i.e., they apply to all three algorithms simultaneously. Importantly, the interface properties mathematically capture Jayanti's original intuitions that have previously been given only informally.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {33},
	numpages = {30},
	keywords = {concurrent snapshots, visibility relations, linearizability}
}

@article{oleary97_verif_compil_commun_proces_into_clock_circuit,
	abstract = {We have previously developed a verified algorithm for compiling programs written in an occam-like language into delay-insensitive circuits. In this paper we show how to retarget our compiler for clocked circuits. Since verifying a hardware compiler is a huge effort, it is significant that we are able to retarget our compiler proof without recreating that effort.},
	author = {O'Leary, John and Brown, Geoffrey and Luk, Wayne},
	url = {https://doi.org/10.1007/BF01211459},
	date = {1997-09-01},
	doi = {10.1007/BF01211459},
	issn = {1433-299X},
	journaltitle = {Formal Aspects of Computing},
	number = {5},
	pages = {537--559},
	title = {Verified Compilation of Communicating Processes Into Clocked Circuits},
	volume = {9}
}

@article{oliveira22_layer_objec_based_game_seman,
	author = {Oliveira Vale, Arthur and Melli\`{e}s, Paul-Andr\'{e} and Shao, Zhong and Koenig, J\'{e}r\'{e}mie and Stefanesco, L\'{e}o},
	title = {Layered and Object-Based Game Semantics},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498703},
	doi = {10.1145/3498703},
	abstract = {Large-scale software verification relies critically on the use of compositional languages, semantic models, specifications, and verification techniques. Recent work on certified abstraction layers synthesizes game semantics, the refinement calculus, and algebraic effects to enable the composition of heterogeneous components into larger certified systems. However, in existing models of certified abstraction layers, compositionality is restricted by the lack of encapsulation of state. In this paper, we present a novel game model for certified abstraction layers where the semantics of layer interfaces and implementations are defined solely based on their observable behaviors. Our key idea is to leverage Reddy's pioneer work on modeling the semantics of imperative languages not as functions on global states but as objects with their observable behaviors. We show that a layer interface can be modeled as an object type (i.e., a layer signature) plus an object strategy. A layer implementation is then essentially a regular map, in the sense of Reddy, from an object with the underlay signature to that with the overlay signature. A layer implementation is certified when its composition with the underlay object strategy implements the overlay object strategy. We also describe an extension that allows for non-determinism in layer interfaces. After formulating layer implementations as regular maps between object spaces, we move to concurrency and design a notion of concurrent object space, where sequential traces may be identified modulo permutation of independent operations. We show how to express protected shared object concurrency, and a ticket lock implementation, in a simple model based on regular maps between concurrent object spaces.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {42},
	numpages = {32},
	keywords = {object-based semantics, game semantics, program refinement, certified abstraction layers}
}

@article{oliveira23_compos_theor_linear,
	author = {Oliveira Vale, Arthur and Shao, Zhong and Chen, Yixuan},
	title = {A Compositional Theory of Linearizability},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571231},
	doi = {10.1145/3571231},
	abstract = {Compositionality is at the core of programming languages research and has become an important goal toward scalable verification of large systems. Despite that, there is no compositional account of linearizability, the gold standard of correctness for concurrent objects. In this paper, we develop a compositional semantics for linearizable concurrent objects. We start by showcasing a common issue, which is independent of linearizability, in the construction of compositional models of concurrent computation: interaction with the neutral element for composition can lead to emergent behaviors, a hindrance to compositionality. Category theory provides a solution for the issue in the form of the Karoubi envelope. Surprisingly, and this is the main discovery of our work, this abstract construction is deeply related to linearizability and leads to a novel formulation of it. Notably, this new formulation neither relies on atomicity nor directly upon happens-before ordering and is only possible because of compositionality, revealing that linearizability and compositionality are intrinsically related to each other. We use this new, and compositional, understanding of linearizability to revisit much of the theory of linearizability, providing novel, simple, algebraic proofs of the locality property and of an analogue of the equivalence with observational refinement. We show our techniques can be used in practice by connecting our semantics with a simple program logic that is nonetheless sound concerning this generalized linearizability.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {38},
	numpages = {32},
	keywords = {linearizability, game semantics, program logic, concurrency}
}

@inproceedings{oppermann16_ilp_based_sched_high_level_synth,
	abstract = {In high-level synthesis, loop pipelining is a technique to improve the throughput and utilisation of hardware datapaths by starting new loop iterations after a fixed amount of time, called the initiation interval (II), allowing to overlap subsequent iterations. The problem is to find the smallest II and corresponding operation schedule that fulfils all data dependencies and resource constraints, both of which are usually found by modulo scheduling.We propose Moovac1, a novel integer linear program (ILP) formulation of the modulo scheduling problem based on overlap variables to model exact resource constraints. Given enough time, Moovac will find a mimimum-II solution. This is in contrast to Canis' state-of-the-art Modulo SDC approach, which requires heuristic simplifications of the resource constraints. Moovac can thus be used as a reference to evaluate heuristics, or in a time-limited mode as a heuristic itself to provide a best-so-far solution.We schedule kernels from the CHStone and MachSuite benchmarks for loop pipelining with Moovac, Modulo SDC and a prior exact formulation by Eichenberger.Moovac has competitive performance in its time-limited mode, and delivers better results faster than the Modulo SDC scheduler for some loops. Often its structure leads to quicker solution times than Eichenberger's formulation.Using the Moovac-computed optimal solutions as a reference, we can confirm that the Modulo SDC heuristic is indeed capable of finding optimal or near-optimal solutions for the maqjority of small- to medium-sized loops. However, for larger loops the two algorithms begin to diverge, with Moovac often being significantly faster to prove the infeasibility of a candidate II. This can be exploited by running both schedulers synergistically, leading to a quicker convergence to the final II.},
	author = {Oppermann, Julian and Koch, Andreas and Reuter-Oppermann, Melanie and Sinnen, Oliver},
	location = {Pittsburgh, Pennsylvania},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/2968455.2968512},
	booktitle = {Proceedings of the International Conference on Compilers, Architectures and Synthesis for Embedded Systems},
	doi = {10.1145/2968455.2968512},
	isbn = {9781450344821},
	series = {CASES '16},
	title = {ILP-Based modulo Scheduling for High-Level Synthesis},
	year = {2016}
}

@article{oppermann19_exact_pract_modul_sched_high_level_synth,
	abstract = {Loop pipelining is an essential technique in high-level synthesis to increase the throughput and resource utilisation of field-programmable gate array--based accelerators. It relies on modulo schedulers to compute an operator schedule that allows subsequent loop iterations to overlap partially when executed while still honouring all precedence and resource constraints. Modulo schedulers face a bi-criteria problem: minimise the initiation interval (II; i.e., the number of timesteps after which new iterations are started) and minimise the schedule length.We present Moovac, a novel exact formulation that models all aspects (including the II minimisation) of the modulo scheduling problem as a single integer linear program, and discuss simple measures to prevent excessive runtimes, to challenge the old preconception that exact modulo scheduling is impractical.We substantiate this claim by conducting an experimental study covering 188 loops from two established high-level synthesis benchmark suites, four different time limits, and three bounds for the schedule length, to compare our approach against a highly tuned exact formulation and a state-of-the-art heuristic algorithm. In the fastest configuration, an accumulated runtime of under 16 minutes is spent on scheduling all loops, and proven optimal IIs are found for 179 test instances.},
	author = {Oppermann, Julian and Reuter-Oppermann, Melanie and Sommer, Lukas and Koch, Andreas and Sinnen, Oliver},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3317670},
	doi = {10.1145/3317670},
	issn = {1936-7406},
	journaltitle = {ACM Trans. Reconfigurable Technol. Syst.},
	keywords = {high-level synthesis, modulo scheduling},
	month = may,
	number = {2},
	title = {Exact and Practical Modulo Scheduling for High-Level Synthesis},
	volume = {12},
	year = {2019}
}

@article{ostermann22_introd_elimin_left_right,
	author = {Ostermann, Klaus and Binder, David and Skupin, Ingo and S\"{u}berkr\"{u}b, Tim and Downen, Paul},
	title = {Introduction and Elimination, Left and Right},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3547637},
	doi = {10.1145/3547637},
	abstract = {Functional programming language design has been shaped by the framework of natural deduction, in which language constructs are divided into introduction and elimination rules for producers of values. In sequent calculus-based languages, left introduction rules replace (right) elimination rules and provide a dedicated sublanguage for consumers of values. In this paper, we present and analyze a wider design space of programming languages which encompasses four kinds of rules: Introduction and elimination, both left and right. We analyze the influence of rule choice on program structure and argue that having all kinds of rules enriches a programmer’s modularity arsenal. In particular, we identify four ways of adhering to the principle that ”the structure of the program follows the structure of the data“ and show that they correspond to the four possible choices of rules. We also propose the principle of bi-expressibility to guide and validate the design of rules for a connective. Finally, we deepen the well-known dualities between different connectives by means of the proof/refutation duality.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {106},
	numpages = {28},
	keywords = {Duality, Natural Deduction, Sequent Calculus}
}

@inproceedings{ottenstein90_progr_depen_web,
	abstract = {The Program Dependence Web (PDW) is a program representation that can be directly interpreted using control-, data-, or demand-driven models of execution. A PDW combines a single-assignment version of the program with explicit operators that manage the flow of data values. The PDW can be viewed as an augmented Program Dependence Graph. Translation to the PDW representation provides the basis for projects to compile Fortran onto dynamic dataflow architectures and simulators. A second application of the PDW is the construction of various compositional semantics for program dependence graphs.},
	author = {Ottenstein, Karl J. and Ballance, Robert A. and MacCabe, Arthur B.},
	location = {White Plains, New York, USA},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the ACM SIGPLAN 1990 Conference on Programming Language Design and Implementation},
	doi = {10.1145/93542.93578},
	isbn = {0897913647},
	keywords = {gated-SSA,SSA,program dependence graph},
	pages = {257--271},
	series = {PLDI '90},
	title = {The Program Dependence Web: A Representation Supporting Control-, Data-, and Demand-Driven Interpretation of Imperative Languages},
	year = {1990}
}

@inproceedings{ozen22_perfor_portab_openm,
	author = {Ozen, Guray and Wolfe, Michael},
	title = {Performant Portable OpenMP},
	year = {2022},
	isbn = {9781450391832},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497776.3517780},
	doi = {10.1145/3497776.3517780},
	abstract = {Accelerated computing has increased the need to specialize how a program is parallelized depending on the target. Fully exploiting a highly parallel accelerator, such as a GPU, demands more parallelism and sometimes more levels of parallelism than a multicore CPU. OpenMP has a directive for each level of parallelism, but choosing directives for each target can incur a significant productivity cost. We argue that using the new OpenMP loop directive with an appropriate compiler decision process can achieve the same performance benefits of target-specific parallelization with the productivity advantage of a single directive for all targets. In this paper, we introduce a fully descriptive model and demonstrate its benefits with an implementation of the loop directive, comparing performance, productivity, and portability against other production compilers using the SPEC ACCEL benchmark suite. We provide an implementation of our proposal in NVIDIA's HPC compiler. It yields up to 56X speedup and an average of 1.91x-1.79x speedup compared to the baseline performance (depending on the host system) on GPUs, and preserves CPU performance. In addition, our proposal requires 60% fewer parallelism directives.},
	booktitle = {Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction},
	pages = {156–168},
	numpages = {13},
	keywords = {Parallel Programming Languages, Compilers, OpenMP, GPUs},
	location = {Seoul, South Korea},
	series = {CC 2022}
}

@inproceedings{pace05_verif,
	author = {Pace, Gordon J and Claessen, Koen},
	booktitle = {Proceedings of CSAW'05},
	pages = {23},
	title = {Verifying hardware compilers},
	year = {2005}
}

@article{padon22_induc_dualit,
	abstract = {Many invariant inference techniques reason simultaneously about states and predicates, and it is well-known that these two kinds of reasoning are in some sense dual to each other. We present a new formal duality between states and predicates, and use it to derive a new primal-dual invariant inference algorithm. The new induction duality is based on a notion of provability by incremental induction that is formally dual to reachability, and the duality is surprisingly symmetric. The symmetry allows us to derive the dual of the well-known Houdini algorithm, and by combining Houdini with its dual image we obtain primal-dual Houdini, the first truly primal-dual invariant inference algorithm. An early prototype of primal-dual Houdini for the domain of distributed protocol verification can handle difficult benchmarks from the literature.},
	author = {Padon, Oded and Wilcox, James R. and Koenig, Jason R. and McMillan, Kenneth L. and Aiken, Alex},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3498712},
	doi = {10.1145/3498712},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {induction duality,counterexample-guided abstraction refinement,primal-dual Houdini,IC3,Houdini,property directed reachability,invariant inference},
	month = jan,
	number = {POPL},
	title = {Induction Duality: Primal-Dual Search for Invariants},
	volume = {6},
	year = {2022}
}

@inproceedings{page91_compil_occam,
	author = {Page, Ian and Luk, Wayne},
	organization = {Citeseer},
	booktitle = {FPGAs, Oxford Workshop on Field Programmable Logic and Applications},
	pages = {271--283},
	title = {Compiling Occam into field-programmable gate arrays},
	volume = {15},
	year = {1991}
}

@article{palmkvist23_static_resol_ambig,
	author = {Palmkvist, Viktor and Castegren, Elias and Haller, Philipp and Broman, David},
	title = {Statically Resolvable Ambiguity},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571251},
	doi = {10.1145/3571251},
	abstract = {Traditionally, a grammar defining the syntax of a programming language is typically both context free and unambiguous. However, recent work suggests that an attractive alternative is to use ambiguous grammars,thus postponing the task of resolving the ambiguity to the end user. If all programs accepted by an ambiguous grammar can be rewritten unambiguously, then the parser for the grammar is said to be resolvably ambiguous. Guaranteeing resolvable ambiguity statically---for all programs---is hard, where previous work only solves it partially using techniques based on property-based testing. In this paper, we present the first efficient, practical, and proven correct solution to the statically resolvable ambiguity problem. Our approach introduces several key ideas, including splittable productions, operator sequences, and the concept of a grouper that works in tandem with a standard parser. We prove static resolvability using a Coq mechanization and demonstrate its efficiency and practical applicability by implementing and integrating resolvable ambiguity into an essential part of the standard OCaml parser.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {58},
	numpages = {27},
	keywords = {Parser, Resolvable Ambiguity, OCaml, Coq}
}

@article{pangrle87_desig_tools_intel_silic_compil,
	author = {Barry M. Pangrle and
                  Daniel D. Gajski},
	title = {Design Tools for Intelligent Silicon Compilation},
	journal = {{IEEE} Trans. Comput. Aided Des. Integr. Circuits Syst.},
	volume = {6},
	number = {6},
	pages = {1098--1112},
	year = {1987},
	doi = {10.1109/TCAD.1987.1270350},
	timestamp = {Thu, 24 Sep 2020 11:28:40 +0200},
	biburl = {https://dblp.org/rec/journals/tcad/PangrleG87.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{pardalos22_resour_sharin_verif_high_level_synth,
	publisher = {IEEE},
	author = {Pardalos, Michalis and Herklotz, Yann and Wickerson, John},
	booktitle = {2022 IEEE 30th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
	title = {Resource Sharing for Verified High-Level Synthesis},
	year = {2022},
	volume = {},
	number = {},
	pages = {1-6},
	doi = {10.1109/FCCM53951.2022.9786208}
}

@inproceedings{park13_paral_progr_big_operat,
	abstract = {In the sciences, it is common to use the so-called "big operator" notation to express the iteration of a binary operator (the reducer) over a collection of values. Such a notation typically assumes that the reducer is associative and abstracts the iteration process. Consequently, from a programming point-of-view, we can organize the reducer operations to minimize the depth of the overall reduction, allowing a potentially parallel evaluation of a big operator expression. We believe that the big operator notation is indeed an effective construct to express parallel computations in the Generate/Map/Reduce programming model, and our goal is to introduce it in programming languages to support parallel programming. The effective definition of such a big operator expression requires a simple way to generate elements, and a simple way to declare algebraic properties of the reducer (such as its identity, or its commutativity). In this poster, we want to present an extension of Scala with support for big operator expressions. We show how big operator expressions are defined and how the API is organized to support the simple definition of reducers with their algebraic properties.},
	author = {Park, Changhee and Steele, Guy L. and Tristan, Jean-Baptiste},
	location = {Shenzhen, China},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/2442516.2442551},
	booktitle = {Proceedings of the 18th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
	doi = {10.1145/2442516.2442551},
	isbn = {9781450319225},
	keywords = {scala,parallelism,mathematical notation},
	pages = {293--294},
	series = {PPoPP '13},
	title = {Parallel Programming with Big Operators},
	year = {2013}
}

@article{park13_predic_model_polyh_optim_space,
	author = {Park, Eunjung and Cavazos, John and Pouchet, Louis-Noël and Bastoul, Cédric and Cohen, Albert and Sadayappan, P.},
	publisher = {Springer Science and Business Media {LLC}},
	url = {https://doi.org/10.1007/s10766-013-0241-1},
	doi = {10.1007/s10766-013-0241-1},
	journaltitle = {International Journal of Parallel Programming},
	keywords = {polyhedral model,polyhedral analysis},
	month = feb,
	number = {5},
	pages = {704--750},
	title = {Predictive Modeling in a Polyhedral Optimization Space},
	volume = {41},
	year = {2013}
}

@inproceedings{park19_inter_analy_multic_shared_resour,
	keywords = {RTOS, predictable execution, avionics},
	doi = {10.1109/dasc43569.2019.9081704},
	url = {https://doi.org/10.1109%2Fdasc43569.2019.9081704},
	year = 2019,
	month = {sep},
	publisher = {{IEEE}},
	author = {Sihyeong Park and Daeyoung Song and Hyeoksoo Jang and Mi-Young Kwon and Sang-Hun Lee and Hoon-Kyu Kim and Hyungshin Kim},
	title = {Interference Analysis of Multicore Shared Resources with a Commercial Avionics {RTOS}},
	booktitle = {2019 {IEEE}/{AIAA} 38th Digital Avionics Systems Conference ({DASC})}
}

@TechReport{park91_predic_execut,
	url = {https://www.cs.princeton.edu/courses/archive/spr04/cos598C/papers/HPL-91-58.pdf},
	keywords = {if-conversion, gated-SSA},
	institution = {Hewlett Packard},
	title = {On Predicated Execution},
	author = {Park, Joseph C. H. and Schlansker, Mike},
	year = {1991},
	publisher = {Hewlett-Packard Laboratories Palo Alto, California}
}

@INPROCEEDINGS{parkinson06_variab_resour_hoare_logic,
	keywords = {verification, symbolic execution, predicated execution},
	author = {Parkinson, M. and Bornat, R. and Calcagno, C.},
	booktitle = {21st Annual IEEE Symposium on Logic in Computer Science (LICS'06)},
	title = {Variables as Resource in Hoare Logics},
	year = {2006},
	volume = {},
	number = {},
	pages = {137-146},
	doi = {10.1109/LICS.2006.52}
}

@inbook{parkinson13_separ_logic_objec_orien_progr,
	abstract = {In this article we propose techniques based on separation logic to reason about object-oriented programs. This leads to a modular proof system that can deal with features considered core to object-oriented programming, including object encapsulation, subclassing, inheritance, and dynamic dispatch.},
	author = {Parkinson, Matthew and Bierman, Gavin},
	editor = {Clarke, Dave and Noble, James and Wrigstad, Tobias},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	url = {https://doi.org/10.1007/978-3-642-36946-9_13},
	booktitle = {Aliasing in Object-Oriented Programming. Types, Analysis and Verification},
	doi = {10.1007/978-3-642-36946-9_13},
	isbn = {978-3-642-36946-9},
	keywords = {memory aliasing,ownership types},
	pages = {366--406},
	title = {Separation Logic for Object-Oriented Programming},
	year = {2013}
}

@ARTICLE{parnas93_predic,
	keywords = {predicated execution, if-conversion},
	author = {Parnas, D.L.},
	journal = {IEEE Transactions on Software Engineering},
	title = {Predicate logic for software engineering},
	year = {1993},
	volume = {19},
	number = {9},
	pages = {856-862},
	doi = {10.1109/32.241769}
}

@book{patterson16_comput_arm,
	title = {Computer organization and design ARM edition: the hardware software interface},
	author = {Patterson, David A and Hennessy, John L},
	year = {2016},
	publisher = {Morgan kaufmann}
}

@inproceedings{pauget18_towar_compil_imper_languag_fpgas,
	author = {Pauget, Baptiste and Pearce, David J. and Potanin, Alex},
	location = {Boston, MA, USA},
	publisher = {ACM},
	url = {https://doi.org/10.1145/3281287.3281291},
	booktitle = {Proceedings of the 10th ACM SIGPLAN International Workshop on Virtual Machines and Intermediate Languages},
	doi = {10.1145/3281287.3281291},
	isbn = {978-1-4503-6071-5},
	keywords = {Compilers,Field-Programmable Gate Arrays,Hardware Description Languages},
	pages = {47--56},
	series = {VMIL 2018},
	title = {Towards Compilation of an Imperative Language for FPGAs},
	year = {2018}
}

@incollection{paulin-mohring15_introd_calcul_induc_const,
	author = {Paulin-Mohring, Christine},
	editor = {Paleo, Bruno Woltzenlogel and Delahaye, David},
	publisher = {College Publications},
	url = {https://hal.inria.fr/hal-01094195},
	booktitle = {{All about Proofs, Proofs for All}},
	file = {https://hal.inria.fr/hal-01094195/file/CIC.pdf},
	keywords = {Coq proof assistant ; Calculus of Inductive Constructions},
	month = jan,
	series = {Studies in Logic (Mathematical logic and foundations)},
	title = {{Introduction to the Calculus of Inductive Constructions}},
	volume = {55},
	year = {2015}
}

@inproceedings{paulin86_hal,
	author = {Paulin, P.G. and Knight, J.P. and Girczyc, E.F.},
	booktitle = {23rd ACM/IEEE Design Automation Conference},
	doi = {10.1109/DAC.1986.1586099},
	pages = {263--270},
	title = {HAL: A Multi-Paradigm Approach to Automatic Data Path Synthesis},
	year = {1986}
}

@inproceedings{paulin89_sched_bindin_algor_high_level_synth,
	author = {Paulin, P. G. and Knight, J. P.},
	location = {Las Vegas, Nevada, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/74382.74383},
	booktitle = {Proceedings of the 26th ACM/IEEE Design Automation Conference},
	doi = {10.1145/74382.74383},
	isbn = {0897913108},
	keywords = {scheduling,high-level synthesis},
	pages = {1--6},
	series = {DAC '89},
	title = {Scheduling and Binding Algorithms for High-Level Synthesis},
	year = {1989}
}

@article{paulson00_mechan_unity_isabel,
	abstract = {UNITY is an abstract formalism for proving properties of concurrent systems, which typically are expressed using guarded assignments [Chandy and Misra 1988]. UNITY has been mechanized in higher-order logic using Isabelle, a proof assistant. Safety and progress primitives, their weak forms (for the substitution axiom), and the program composition operator (union) have been formalized. To give a feel for the concrete syntax, this article presents a few extracts from the Isabelle definitions and proofs. It discusses a small example, two-process mutual exclusion. A mechanical theory of unions of programs supports a degree of compositional reasoning. Original work on extending program states is presented and then illustrated through a simple example involving an array of processes.},
	author = {Paulson, Lawrence C.},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/343369.343370},
	doi = {10.1145/343369.343370},
	issn = {1529-3785},
	journaltitle = {ACM Trans. Comput. Logic},
	keywords = {concurrency,isabelle,unity},
	month = jul,
	number = {1},
	pages = {3--32},
	title = {Mechanizing UNITY in Isabelle},
	volume = {1},
	year = {2000}
}

@article{paulson93_isabel,
	abstract = {Isabelle is a generic theorem prover, designed for interactive reasoning in a variety of formal theories. At present it provides useful proof procedures for Constructive Type Theory, various first-order logics, Zermelo-Fraenkel set theory, and higher-order logic. This survey of Isabelle serves as an introduction to the literature. It explains why generic theorem proving is beneficial. It gives a thorough history of Isabelle, beginning with its origins in the LCF system. It presents an account of how logics are represented, illustrated using classical logic. The approach is compared with the Edinburgh Logical Framework. Several of the Isabelle object-logics are presented.},
	author = {Paulson, Lawrence C},
	publisher = {arXiv},
	title = {Isabelle: The next 700 theorem provers},
	year = {1993}
}

@book{paulson94_i,
	title = {Isabelle: A generic theorem prover},
	author = {Paulson, Lawrence C},
	year = {1994},
	publisher = {Springer}
}

@article{pauwels95_formal_multi_precis_arith_high,
	author = {Pauwels, Marc and Goossens, Gert and Catthoor, Francky and Man, Hugo De},
	publisher = {Springer Science and Business Media {LLC}},
	url = {https://doi.org/10.1007/bf02106825},
	doi = {10.1007/bf02106825},
	journaltitle = {Journal of {VLSI} signal processing systems for signal, image and video technology},
	month = oct,
	number = {1-2},
	pages = {97--112},
	title = {Formalisation of Multi-Precision Arithmetic for High-Level Synthesis of {DSP} Architectures},
	volume = {11},
	year = {1995}
}

@article{pavlinovic21_data_flow_refin_type_infer,
	abstract = {Refinement types enable lightweight verification of functional programs. Algorithms for statically inferring refinement types typically work by reduction to solving systems of constrained Horn clauses extracted from typing derivations. An example is Liquid type inference, which solves the extracted constraints using predicate abstraction. However, the reduction to constraint solving in itself already signifies an abstraction of the program semantics that affects the precision of the overall static analysis. To better understand this issue, we study the type inference problem in its entirety through the lens of abstract interpretation. We propose a new refinement type system that is parametric with the choice of the abstract domain of type refinements as well as the degree to which it tracks context-sensitive control flow information. We then derive an accompanying parametric inference algorithm as an abstract interpretation of a novel data flow semantics of functional programs. We further show that the type system is sound and complete with respect to the constructed abstract semantics. Our theoretical development reveals the key abstraction steps inherent in refinement type inference algorithms. The trade-off between precision and efficiency of these abstraction steps is controlled by the parameters of the type system. Existing refinement type systems and their respective inference algorithms, such as Liquid types, are captured by concrete parameter instantiations. We have implemented our framework in a prototype tool and evaluated it for a range of new parameter instantiations (e.g., using octagons and polyhedra for expressing type refinements). The tool compares favorably against other existing tools. Our evaluation indicates that our approach can be used to systematically construct new refinement type inference algorithms that are both robust and precise.},
	author = {Pavlinovic, Zvonimir and Su, Yusen and Wies, Thomas},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434300},
	doi = {10.1145/3434300},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {refinement type inference,abstract interpretation,Liquid types},
	month = jan,
	number = {POPL},
	title = {Data Flow Refinement Type Inference},
	volume = {5},
	year = {2021}
}

@book{pedroni13_finit_state_machin_hardw,
	abstract = {Modern, complex digital systems invariably include hardware-implemented finite state machines. The correct design of such parts is crucial for attaining proper system performance. This book offers detailed, comprehensive coverage of the theory and design for any category of hardware-implemented finite state machines. It describes crucial design problems that lead to incorrect or far from optimal implementation and provides examples of finite state machines developed in both VHDL and SystemVerilog (the successor of Verilog) hardware description languages. Important features include: extensive review of design practices for sequential digital circuits; a new division of all state machines into three hardware-based categories, encompassing all possible situations, with numerous practical examples provided in all three categories; the presentation of complete designs, with detailed VHDL and SystemVerilog codes, comments, and simulation results, all tested in FPGA devices; and exercise examples, all of which can be synthesized, simulated, and physically implemented in FPGA boards. Additional material is available on the book's Website. Designing a state machine in hardware is more complex than designing it in software. Although interest in hardware for finite state machines has grown dramatically in recent years, there is no comprehensive treatment of the subject. This book offers the most detailed coverage of finite state machines available. It will be essential for industrial designers of digital systems and for students of electrical engineering and computer science.},
	author = {Pedroni, Volnei A},
	address = {Cambridge, Massachusetts},
	booktitle = {Finite State Machines in Hardware},
	copyright = {2013 Massachusetts Institute of Technology},
	isbn = {0262319098},
	keywords = {FSMD, finite state machine},
	language = {eng},
	publisher = {MIT Press},
	title = {Finite State Machines in Hardware: Theory and Design (with VHDL and SystemVerilog)},
	year = {2013}
}

@inproceedings{peduri22_qssa,
	author = {Peduri, Anurudh and Bhat, Siddharth and Grosser, Tobias},
	title = {QSSA: An SSA-Based IR for Quantum Computing},
	year = {2022},
	isbn = {9781450391832},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497776.3517772},
	doi = {10.1145/3497776.3517772},
	abstract = {Quantum computing hardware has progressed rapidly. Simultaneously, there has been a proliferation of programming languages and program optimization tools for quantum computing. Existing quantum compilers use intermediate representations (IRs) where quantum programs are described as circuits. Such IRs fail to leverage existing work on compiler optimizations. In such IRs, it is non-trivial to statically check for physical constraints such as the no-cloning theorem, which states that qubits cannot be copied. We introduce QSSA, a novel quantum IR based on static single assignment (SSA) that enables decades of research in compiler optimizations to be applied to quantum compilation. QSSA models quantum operations as being side-effect-free. The inputs and outputs of the operation are in one-to-one correspondence; qubits cannot be created or destroyed. As a result, our IR supports a static analysis pass that verifies no-cloning at compile-time. The quantum circuit is fully encoded within the def-use chain of the IR, allowing us to leverage existing optimization passes on SSA representations such as redundancy elimination and dead-code elimination. Running our QSSA-based compiler on the QASMBench and IBM Quantum Challenge datasets, we show that our optimizations perform comparably to IBM’s Qiskit quantum compiler infrastructure. QSSA allows us to represent, analyze, and transform quantum programs using the robust theory of SSA representations, bringing quantum compilation into the realm of well-understood theory and practice.},
	booktitle = {Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction},
	pages = {2–14},
	numpages = {13},
	keywords = {optimization, quantum circuits, compilers, intermediate representations, SSA},
	location = {Seoul, South Korea},
	series = {CC 2022}
}

@misc{pelayo13,
	author = {Pelayo, Álvaro and Voevodsky, Vladimir and Warren, Michael A.},
	eprint = {1302.1207},
	eprintclass = {math.LO},
	eprinttype = {arXiv},
	title = {A preliminary univalent formalization of the p-adic numbers},
	year = {2013}
}

@inproceedings{pelcat16_desig_hdl,
	author = {Pelcat, Maxime and Bourrasset, Cédric and Maggiani, Luca and Berry, François},
	booktitle = {2016 International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation (SAMOS)},
	doi = {10.1109/SAMOS.2016.7818341},
	pages = {140--147},
	title = {Design productivity of a high level synthesis compiler versus HDL},
	year = {2016}
}

@article{perera22_linked_visual_galois_depen,
	author = {Perera, Roly and Nguyen, Minh and Petricek, Tomas and Wang, Meng},
	title = {Linked Visualisations via Galois Dependencies},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498668},
	doi = {10.1145/3498668},
	abstract = {We present new language-based dynamic analysis techniques for linking visualisations and other structured outputs to data in a fine-grained way, allowing users to explore how data attributes and visual or other output elements are related by selecting (focusing on) substructures of interest. Our approach builds on bidirectional program slicing techiques based on Galois connections, which provide desirable round-tripping properties. Unlike the prior work, our approach allows selections to be negated, equipping the bidirectional analysis with a De Morgan dual which can be used to link different outputs generated from the same input. This offers a principled language-based foundation for a popular view coordination feature called brushing and linking where selections in one chart automatically select corresponding elements in another related chart.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {7},
	numpages = {29},
	keywords = {Galois connections, data provenance}
}

@thesis{perna10_handel_c,
	author = {Perna, Juan Ignacio},
	institution = {University of York},
	title = {A verified compiler for Handel-C},
	type = {phdthesis},
	year = {2010}
}

@article{perna11_correc_hardw_synth,
	abstract = {This paper presents an algebraic compilation approach to the correct synthesis (compilation into hardware) of a synchronous language with shared variables and parallelism. The synthesis process generates a hardware component that implements the source program by means of gradually reducing it into a highly parallel state-machine. The correctness of the compiler follows by construction from the correctness of the transformations involved in the synthesis process. Each transformation is proved sound from more basic algebraic laws of the source language; the laws are themselves formally derived from a denotational semantics expressed in the Unified Theories of Programming. The proposed approach is based on previous efforts that handle both software and hardware compilation, in a pure algebraic style, but the complexity of our source language demanded significant adaptations and extensions to the existing approaches.},
	author = {Perna, Juan and Woodcock, Jim and Sampaio, Augusto and Iyoda, Juliano},
	url = {https://doi.org/10.1007/s00236-011-0142-y},
	date = {2011-12-01},
	doi = {10.1007/s00236-011-0142-y},
	issn = {1432-0525},
	journaltitle = {Acta Informatica},
	number = {7},
	pages = {363--396},
	title = {Correct Hardware Synthesis},
	volume = {48}
}

@article{perna12_mechan_wire_wise_verif_handel_c_synth,
	author = {Perna, Juan and Woodcock, Jim},
	doi = {https://doi.org/10.1016/j.scico.2010.02.007},
	issn = {0167-6423},
	journaltitle = {Science of Computer Programming},
	keywords = {Handel-C synthesis,Denotational semantics,Correctness,Mechanical verification,HOL},
	number = {4},
	pages = {424--443},
	title = {Mechanised Wire-Wise Verification of {Handel-C} Synthesis},
	volume = {77},
	year = {2012}
}

@inproceedings{peterson23_p4cub,
	author = {Peterson, Rudy and Campbell, Eric Hayden and Chen, John and Isak, Natalie and Shyu, Calvin and Doenges, Ryan and Ataei, Parisa and Foster, Nate},
	title = {P4Cub: A Little Language for Big Routers},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575670},
	doi = {10.1145/3573105.3575670},
	abstract = {P4Cub is a new intermediate representation (IR) for the P4 programming language. It has been designed with the goal of facilitating development of certified tools. To achieve this, P4Cub is organized around a small set of core constructs and avoids side effects in expressions, which avoids mutual recursion between the semantics of expressions and statements. Still, it retains the essential domain-specific features of P4 itself. P4Cub has a front-end based on Petr4, and has been fully mechanized in Coq including big-step and small-step semantics and a type system. As case studies, we have engineered several certified tools with P4Cub including proofs of type soundness, a verified compilation pass, and an automated verification tool.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {303–319},
	numpages = {17},
	keywords = {Coq, domain-specific languages, formal verification, P4, formal semantics, intermediate representations},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@article{petrik07_quineex,
	keywords = {boolean simplification, multi-valued logic},
	doi = {10.1007/s00500-007-0175-x},
	url = {https://doi.org/10.1007/s00500-007-0175-x},
	year = {2007},
	month = apr,
	publisher = {Springer Science and Business Media {LLC}},
	volume = {12},
	number = {4},
	pages = {393--402},
	author = {Milan Petr{\'{\i}}k},
	title = {Quine--McCluskey method for many-valued logical functions},
	journal = {Soft Computing}
}

@article{pfenning88_higher_order_abstr_syntax,
	abstract = {We describe motivation, design, use, and implementation of higher-order abstract syntax as a central representation for programs, formulas, rules, and other syntactic objects in program manipulation and other formal systems where matching and substitution or unification are central operations. Higher-order abstract syntax incorporates name binding information in a uniform and language generic way. Thus it acts as a powerful link integrating diverse tools in such formal environments. We have implemented higher-order abstract syntax, a supporting matching and unification algorithm, and some clients in Common Lisp in the framework of the Ergo project at Carnegie Mellon University.},
	author = {Pfenning, F. and Elliott, C.},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/960116.54010},
	doi = {10.1145/960116.54010},
	issn = {0362-1340},
	journaltitle = {SIGPLAN Not.},
	month = jun,
	number = {7},
	pages = {199--208},
	title = {Higher-Order Abstract Syntax},
	volume = {23},
	year = {1988}
}

@article{pick23_psym,
	author = {Pick, Lauren and Desai, Ankush and Gupta, Aarti},
	title = {Psym: Efficient Symbolic Exploration of Distributed Systems},
	year = {2023},
	issue_date = {June 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {PLDI},
	url = {https://dl.acm.org/doi/pdf/10.1145/3591247},
	doi = {10.1145/3591247},
	abstract = {Verification of distributed systems using systematic exploration is daunting because of the many possible interleavings of messages and failures. When faced with this scalability challenge, existing approaches have traditionally mitigated state space explosion by avoiding exploration of redundant states (e.g., via state hashing) and redundant interleavings of transitions (e.g., via partial-order reductions). In this paper, we present an efficient symbolic exploration method that not only avoids redundancies in states and interleavings, but additionally avoids redundant computations that are performed during updates to states on transitions. Our symbolic explorer leverages a novel, fine-grained, canonical representation of distributed system configurations (states) to identify opportunities for avoiding such redundancies on-the-fly. The explorer also includes an interface that is compatible with abstractions for state-space reduction and with partial-order and other reductions for avoiding redundant interleavings. We implement our approach in the tool Psym and empirically demonstrate that it outperforms a state-of-the-art exploration tool, can successfully verify many common distributed protocols, and can scale to multiple real-world industrial case studies across},
	journal = {Proc. ACM Program. Lang.},
	month = {jun},
	articleno = {133},
	numpages = {26},
	keywords = {value summaries, symbolic execution, hyperblocks}
}

@inproceedings{pilato13_bambu,
	author = {{Pilato}, C. and {Ferrandi}, F.},
	booktitle = {2013 23rd International Conference on Field programmable Logic and Applications},
	doi = {10.1109/FPL.2013.6645550},
	pages = {1--4},
	title = {Bambu: A modular framework for the high level synthesis of memory-intensive applications},
	year = {2013}
}

@article{pilato18_secur_hardw_accel,
	abstract = {High-level synthesis (HLS) tools have made significant progress in the past few years, improving the design productivity for hardware accelerators and becoming mainstream in industry to create specialized system-on-chip architectures. Increasing the level of security of these heterogeneous architectures is becoming critical. However, state-of-the-art security countermeasures are still applied only to the code executing on the processor cores or manually implemented into the generated components, leading to suboptimal and sometimes even insecure designs. This letter discusses extensions to HLS tools for creating secure heterogeneous architectures.},
	author = {{Pilato}, C. and {Garg}, S. and {Wu}, K. and {Karri}, R. and {Regazzoni}, F.},
	url = {https://doi.org/10.1109/LES.2017.2774800},
	doi = {10.1109/LES.2017.2774800},
	issn = {1943-0671},
	journaltitle = {IEEE Embedded Systems Letters},
	keywords = {high level synthesis;security of data;system-on-chip;HLS tools;secure heterogeneous architectures;hardware accelerators;high-level synthesis;design productivity;system-on-chip architectures;suboptimal designs;security countermeasures;processor cores;Hardware;Optimization;Microarchitecture;Side-channel attacks;High level synthesis;Reverse engineering;Computer security;Hardware security;high-level synthesis (HLS)},
	month = sep,
	number = {3},
	pages = {77--80},
	title = {Securing Hardware Accelerators: a New Challenge for High-Level Synthesis},
	volume = {10},
	year = {2018}
}

@article{pilato19_taint,
	abstract = {Dynamic information flow tracking (DIFT) is a technique to track potential security vulnerabilities in software and hardware systems at run time. Untrusted data are marked with tags (tainted), which are propagated through the system and their potential for unsafe use is analyzed to prevent them. DIFT is not supported in heterogeneous systems especially hardware accelerators. Currently, DIFT is manually generated and integrated into the accelerators. This process is error-prone, potentially hurting the process of identifying security violations in heterogeneous systems. We present TaintHLS, to automatically generate a micro-architecture to support baseline operations and a shadow microarchitecture for intrinsic DIFT support in hardware accelerators while providing variable granularity of taint tags. TaintHLS offers a companion high-level synthesis (HLS) methodology to automatically generate such DIFT-enabled accelerators from a high-level specification. We extended a state-of-the-art HLS tool to generate DIFT-enhanced accelerators and demonstrated the approach on numerous benchmarks. The DIFT-enabled accelerators have negligible performance and no more than 30 \% hardware overhead.},
	author = {{Pilato}, C. and {Wu}, K. and {Garg}, S. and {Karri}, R. and {Regazzoni}, F.},
	url = {https://doi.org/10.1109/TCAD.2018.2834421},
	doi = {10.1109/TCAD.2018.2834421},
	issn = {1937-4151},
	journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {high level synthesis;security of data;security vulnerabilities;high-level synthesis methodology;microarchitecture;DIFT-enabled accelerators;taint tags;hardware accelerators;dynamic information flow tracking;TaintHLS;Hardware;Software;Security;Computer architecture;Random access memory;Registers;Central Processing Unit;Dynamic information flow tracking (DIFT);hardware security;high-level synthesis (HLS)},
	month = may,
	number = {5},
	pages = {798--808},
	title = {Tainthls: High-Level Synthesis for Dynamic Information Flow Tracking},
	volume = {38},
	year = {2019}
}

@inproceedings{pinilla16_enhan_fpga_high_level_synth,
	abstract = {High-Level Synthesis (HLS) has emerged as a leading technology to reduce the design time and complexity that is associated with reconfigurable systems. In order to maintain the productivity promised by HLS, it is important that the designer can debug the system in the context of the high-level code. Currently, software simulations offer a quick and familiar method to target logic and syntax bugs, while software/hardware co-simulations are useful for synthesis verification. However, to analyze the behaviour of the circuit as it is running, the user is forced to understand waveforms from the synthesized design.},
	author = {{Pinilla}, J. P. and {Wilton}, S. J. E.},
	url = {https://doi.org/10.1109/FPT.2016.7929514},
	booktitle = {2016 International Conference on Field-Programmable Technology (FPT)},
	doi = {10.1109/FPT.2016.7929514},
	keywords = {circuit simulation;field programmable gate arrays;logic design;source-level instrumentation;FPGA in-system debug;high-level synthesis designs;HLS;reconfigurable systems;high-level code;software simulations;logic bug;syntax bug;software-hardware co-simulations;synthesis verification;design synthesis;Instruments;Hardware;Tools;Software;Computer bugs;Debugging;Optimization},
	month = dec,
	pages = {109--116},
	title = {Enhanced source-level instrumentation for FPGA in-system debug of High-Level Synthesis designs},
	year = {2016}
}

@inproceedings{pit-claudel21_effec_simul_debug_high_level,
	abstract = {Rule-based hardware-design languages (RHDLs) promise to enhance developer productivity by offering convenient abstractions. Advanced compiler technology keeps the cost of these abstractions low, generating circuits with excellent area and timing properties. Unfortunately, comparatively little effort has been spent on building simulators and debuggers for these languages, so users often simulate and debug their designs at the RTL level. This is problematic because generated circuits typically suffer from poor readability, as compiler optimizations can break high-level abstractions. Worse, optimizations that operate under the assumption that concurrency is essentially free yield faster circuits but often actively hurt simulation performance on platforms with limited concurrency, like desktop computers or servers. This paper demonstrates the benefits of completely separating the simulation and synthesis pipelines. We propose a new approach, yielding the first compiler designed for effective simulation and debugging of a language in the Bluespec family. We generate cycle-accurate C++ models that are readable, compatible with a wide range of traditional software-debugging tools, and fast (often two to three times faster than circuit-level simulation). We achieve these results by optimizing for sequential performance and using static analysis to minimize redundant work. The result is a vastly improved hardware-design experience, which we demonstrate on embedded processor designs and DSP building blocks using performance benchmarks and debugging case studies.},
	author = {Pit-Claudel, Clément and Bourgeat, Thomas and Lau, Stella and Arvind and Chlipala, Adam},
	location = {Virtual, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3445814.3446720},
	booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
	doi = {10.1145/3445814.3446720},
	isbn = {9781450383172},
	keywords = {hardware debugging,Hardware simulation,compilation},
	pages = {789--803},
	series = {ASPLOS 2021},
	title = {Effective Simulation and Debugging for a High-Level Hardware Language Using Software Compilers},
	year = {2021}
}

@article{pitts23_local_namel_sets,
	author = {Pitts, Andrew M.},
	title = {Locally Nameless Sets},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571210},
	doi = {10.1145/3571210},
	abstract = {This paper provides a new mathematical foundation for the locally nameless representation of syntax with binders, one informed by nominal techniques. It gives an equational axiomatization of two key locally nameless operations, "variable opening" and "variable closing" and shows that a lot of the locally nameless infrastructure can be defined from that in a syntax-independent way, including crucially a "shift" functor for name binding. That functor operates on a category whose objects we call locally nameless sets. Functors combining shift with sums and products have initial algebras that recover the usual locally nameless representation of syntax with binders in the finitary case. We demonstrate this by uniformly constructing such an initial locally nameless set for each instance of Plotkin's notion of binding signature. We also show by example that the shift functor is useful for locally nameless sets of a semantic rather than a syntactic character. The category of locally nameless sets is proved to be isomorphic to a known topos of finitely supported M-sets, where M is the full transformation monoid on a countably infinite set. A corollary of the proof is that several categories that have been used in the literature to model variable renaming operations on syntax with binders are all equivalent to each other and to the category of locally nameless sets.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {17},
	numpages = {27},
	keywords = {initial algebra, Agda, metatheory of syntax, cofinite quantification, locally nameless, category theory, name binding}
}

@inproceedings{pnueli98_trans,
	abstract = {We present the notion of translation validation as a new approach to the verification of translators (compilers, code generators). Rather than proving in advance that the compiler always produces a target code which correctly implements the source code (compiler verification), each individual translation (i.e. a run of the compiler) is followed by a validation phase which verifies that the target code produced on this run correctly implements the submitted source program. Several ingredients are necessary to set up the --- fully automatic --- translation validation process, among which are:1.A common semantic framework for the representation of the source code and the generated target code.2.A formalization of the notion of ``correct implementation'' as a refinement relation.3.A syntactic simulation-based proof method which allows to automatically verify that one model of the semantic framework, representing the produced target code, correctly implements another model which represents the source.},
	author = {Pnueli, A. and Siegel, M. and Singerman, E.},
	editor = {Steffen, Bernhard},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Tools and Algorithms for the Construction and Analysis of Systems},
	isbn = {978-3-540-69753-4},
	pages = {151--166},
	title = {Translation validation},
	year = {1998}
}

@article{popescu23_admis_types_pers_relat_higher_order_logic,
	author = {Popescu, Andrei and Traytel, Dmitriy},
	title = {Admissible Types-to-PERs Relativization in Higher-Order Logic},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571235},
	doi = {10.1145/3571235},
	abstract = {Relativizing statements in Higher-Order Logic (HOL) from types to sets is useful for improving productivity when working with HOL-based interactive theorem provers such as HOL4, HOL Light and Isabelle/HOL. This paper provides the first comprehensive definition and study of types-to-sets relativization in HOL, done in the more general form of types-to-PERs (partial equivalence relations). We prove that, for a large practical fragment of HOL which includes container types such as datatypes and codatatypes, types-to-PERs relativization is admissible, in that the provability of the original, type-based statement implies the provability of its relativized, PER-based counterpart. Our results also imply the admissibility of a previously proposed axiomatic extension of HOL with local type definitions. We have implemented types-to-PERs relativization as an Isabelle tool that performs relativization of HOL theorems on demand.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {42},
	numpages = {32},
	keywords = {higher-order logic (HOL), Isabelle/HOL, proof theory, relativization, type definition, interactive theorem proving, partial equivalence relation}
}

@article{porncharoenwase22_formal_found_symbol_evaluat_mergin,
	abstract = {Reusable symbolic evaluators are a key building block of solver-aided verification and synthesis tools. A reusable evaluator reduces the semantics of all paths in a program to logical constraints, and a client tool uses these constraints to formulate a satisfiability query that is discharged with SAT or SMT solvers. The correctness of the evaluator is critical to the soundness of the tool and the domain properties it aims to guarantee. Yet so far, the trust in these evaluators has been based on an ad-hoc foundation of testing and manual reasoning. This paper presents the first formal framework for reasoning about the behavior of reusable symbolic evaluators. We develop a new symbolic semantics for these evaluators that incorporates state merging. Symbolic evaluators use state merging to avoid path explosion and generate compact encodings. To accommodate a wide range of implementations, our semantics is parameterized by a symbolic factory, which abstracts away the details of merging and creation of symbolic values. The semantics targets a rich language that extends Core Scheme with assumptions and assertions, and thus supports branching, loops, and (first-class) procedures. The semantics is designed to support reusability, by guaranteeing two key properties: legality of the generated symbolic states, and the reducibility of symbolic evaluation to concrete evaluation. Legality makes it simpler for client tools to formulate queries, and reducibility enables testing of client tools on concrete inputs. We use the Lean theorem prover to mechanize our symbolic semantics, prove that it is sound and complete with respect to the concrete semantics, and prove that it guarantees legality and reducibility. To demonstrate the generality of our semantics, we develop Leanette, a reference evaluator written in Lean, and Rosette 4, an optimized evaluator written in Racket. We prove Leanette correct with respect to the semantics, and validate Rosette 4 against Leanette via solver-aided differential testing. To demonstrate the practicality of our approach, we port 16 published verification and synthesis tools from Rosette 3 to Rosette 4. Rosette 3 is an existing reusable evaluator that implements the classic merging semantics, adopted from bounded model checking. Rosette 4 replaces the semantic core of Rosette 3 but keeps its optimized symbolic factory. Our results show that Rosette 4 matches the performance of Rosette 3 across a wide range of benchmarks, while providing a cleaner interface that simplifies the implementation of client tools.},
	author = {Porncharoenwase, Sorawee and Nelson, Luke and Wang, Xi and Torlak, Emina},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3498709},
	doi = {10.1145/3498709},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {state merging,symbolic evaluation},
	month = jan,
	number = {POPL},
	title = {A Formal Foundation for Symbolic Evaluation with Merging},
	volume = {6},
	year = {2022}
}

@inproceedings{pouchet13_polyh,
	author = {Pouchet, Louis-Noel and Zhang, Peng and Sadayappan, Ponnuswamy and Cong, Jason},
	booktitle = {Proceedings of the ACM/SIGDA international symposium on Field programmable gate arrays},
	doi = {https://doi.org/10.1145/2435264.2435273},
	pages = {29--38},
	title = {Polyhedral-based data reuse optimization for configurable computing},
	year = {2013}
}

@misc{pouchet20_polyb_c,
	author = {Pouchet, Louis-No\"el},
	title = {PolyBench/C: the Polyhedral Benchmark suite},
	url = {http://web.cse.ohio-state.edu/~pouchet.2/software/polybench/},
	year = {2020}
}

@inproceedings{prieto-cubides22_homot_walks_spher_maps_homot_type_theor,
	author = {Prieto-Cubides, Jonathan},
	title = {On Homotopy of Walks and Spherical Maps in Homotopy Type Theory},
	year = {2022},
	isbn = {9781450391825},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497775.3503671},
	doi = {10.1145/3497775.3503671},
	abstract = {We work with combinatorial maps to represent graph embeddings into surfaces up to isotopy. The surface in which the graph is embedded is left implicit in this approach. The constructions herein are proof-relevant and stated with a subset of the language of homotopy type theory. This article presents a refinement of one characterisation of embeddings in the sphere, called spherical maps, of connected and directed multigraphs with discrete node sets. A combinatorial notion of homotopy for walks and the normal form of walks under a reduction relation is introduced. The first characterisation of spherical maps states that a graph can be embedded in the sphere if any pair of walks with the same endpoints are merely walk-homotopic. The refinement of this definition filters out any walk with inner cycles. As we prove in one of the lemmas, if a spherical map is given for a graph with a discrete node set, then any walk in the graph is merely walk-homotopic to a normal form. The proof assistant Agda contributed to formalising the results recorded in this article.},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {338–351},
	numpages = {14},
	keywords = {walk normal forms, graph maps, HoTT, Agda},
	location = {Philadelphia, PA, USA},
	series = {CPP 2022}
}

@article{pujet22_obser_equal,
	abstract = {Building on the recent extension of dependent type theory with a universe of definitionally proof-irrelevant types, we introduce TTobs, a new type theory based on the setoidal interpretation of dependent type theory. TTobs equips every type with an identity relation that satisfies function extensionality, propositional extensionality, and definitional uniqueness of identity proofs (UIP). Compared to other existing proposals to enrich dependent type theory with these principles, our theory features a notion of reduction that is normalizing and provides an algorithmic canonicity result, which we formally prove in Agda using the logical relation framework of Abel et al. Our paper thoroughly develops the meta-theoretical properties of TTobs, such as the decidability of the conversion and of the type checking, as well as consistency. We also explain how to extend our theory with quotient types, and we introduce a setoidal version of Swan's Id types that turn it into a proper extension of MLTT with inductive equality.},
	author = {Pujet, Loı̈c and Tabareau, Nicolas},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3498693},
	doi = {10.1145/3498693},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {termination,type theory,confluence,dependent types,rewriting theory},
	month = jan,
	number = {POPL},
	title = {Observational Equality: Now for Good},
	volume = {6},
	year = {2022}
}

@article{pujet23_impred_obser_equal,
	author = {Pujet, Lo\"{\i}c and Tabareau, Nicolas},
	title = {Impredicative Observational Equality},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571739},
	doi = {10.1145/3571739},
	abstract = {In dependent type theory, impredicativity is a powerful logical principle that allows the definition of propositions that quantify over arbitrarily large types, potentially resulting in self-referential propositions. Impredicativity can provide a system with increased logical strength and flexibility, but in counterpart it comes with multiple incompatibility results. In particular, Abel and Coquand showed that adding definitional uniqueness of identity proofs (UIP) to the main proof assistants that support impredicative propositions (Coq and Lean) breaks the normalization procedure, and thus the type-checking algorithm. However, it was not known whether this stems from a fundamental incompatibility between UIP and impredicativity or if a more suitable algorithm could decide type-checking for a type theory that supports both. In this paper, we design a theory that handles both UIP and impredicativity by extending the recently introduced observational type theory TTobs with an impredicative universe of definitionally proof-irrelevant types, as initially proposed in the seminal work on observational equality of Altenkirch et al. We prove decidability of conversion for the resulting system, that we call CCobs, by harnessing proof-irrelevance to avoid computing with impredicative proof terms. Additionally, we prove normalization for CCobs in plain Martin-L\"{o}f type theory, thereby showing that adding proof-irrelevant impredicativity does not increase the computational content of the theory.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {74},
	numpages = {26},
	keywords = {rewriting theory, termination, type theory, dependent types, confluence}
}

@article{pulte23_cn,
	author = {Pulte, Christopher and Makwana, Dhruv C. and Sewell, Thomas and Memarian, Kayvan and Sewell, Peter and Krishnaswami, Neel},
	title = {CN: Verifying Systems C Code with Separation-Logic Refinement Types},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571194},
	doi = {10.1145/3571194},
	abstract = {Despite significant progress in the verification of hypervisors, operating systems, and compilers, and in verification tooling, there exists a wide gap between the approaches used in verification projects and conventional development of systems software. We see two main challenges in bringing these closer together: verification handling the complexity of code and semantics of conventional systems software, and verification usability. We describe an experiment in verification tool design aimed at addressing some aspects of both: we design and implement CN, a separation-logic refinement type system for C systems software, aimed at predictable proof automation, based on a realistic semantics of ISO C. CN reduces refinement typing to decidable propositional logic reasoning, uses first-class resources to support pointer aliasing and pointer arithmetic, features resource inference for iterated separating conjunction, and uses a novel syntactic restriction of ghost variables in specifications to guarantee their successful inference. We implement CN and formalise key aspects of the type system, including a soundness proof of type checking. To demonstrate the usability of CN we use it to verify a substantial component of Google's pKVM hypervisor for Android.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {1},
	numpages = {32},
	keywords = {separation logic, pKVM, verification, refinement types, Android, C}
}

@inproceedings{putnam08_chimp,
	abstract = {This paper describes CHiMPS, a C-based accelerator compiler for hybrid CPU-FPGA computing platforms. CHiMPSpsilas goal is to facilitate FPGA programming for high-performance computing developers. It inputs generic ANSIC code and automatically generates VHDL blocks for an FPGA. The accelerator architecture is customized with multiple caches that are tuned to the application. Speedups of 2.8x to 36.9x (geometric mean 6.7x) are achieved on a variety of HPC benchmarks with minimal source code changes.},
	author = {{Putnam}, A. and {Bennett}, D. and {Dellinger}, E. and {Mason}, J. and {Sundararajan}, P. and {Eggers}, S.},
	booktitle = {2008 International Conference on Field Programmable Logic and Applications},
	doi = {10.1109/FPL.2008.4629927},
	issn = {1946-1488},
	keywords = {compiler generators;field programmable gate arrays;hardware description languages;hybrid CPU-FPGA architectures;CHiMPS;C-level compilation flow;C-based accelerator compiler;FPGA programming;generic ANSIC code;VHDL blocks;multiple caches;source code;Field programmable gate arrays;Hardware;Program processors;Programming;Parallel processing;Benchmark testing;Computer architecture},
	month = sep,
	pages = {173--178},
	title = {CHiMPS: A C-level compilation flow for hybrid CPU-FPGA architectures},
	year = {2008}
}

@article{păsăreanu09_survey_new_trend_symbol_execut,
	author = {Păsăreanu, Corina S. and Visser, Willem},
	publisher = {Springer Science and Business Media {LLC}},
	url = {https://doi.org/10.1007/s10009-009-0118-1},
	doi = {10.1007/s10009-009-0118-1},
	journaltitle = {International Journal on Software Tools for Technology Transfer},
	keywords = {symbolic execution,symbolic evaluation},
	month = aug,
	number = {4},
	pages = {339--353},
	title = {A Survey of New Trends in Symbolic Execution for Software Testing and Analysis},
	volume = {11},
	year = {2009}
}

@inproceedings{quinones07_improv_branc_predic_predic_execut,
	author = {Quinones, Eduardo and Parcerisa, Joan-Manuel and Gonzailez, Antonio},
	publisher = {IEEE},
	url = {https://doi.org/10.1109%2Fhpca.2007.346186},
	booktitle = {2007 {IEEE} 13th International Symposium on High Performance Computer Architecture},
	doi = {10.1109/hpca.2007.346186},
	keywords = {predicated execution},
	title = {Improving Branch Prediction and Predicated Execution in Out-of-Order Processors},
	year = {2007}
}

@article{quiring22_analy_bindin_exten,
	author = {Quiring, Benjamin and Reppy, John and Shivers, Olin},
	title = {Analyzing Binding Extent in 3CPS},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3547645},
	doi = {10.1145/3547645},
	abstract = {To date, the most effective approach to compiling strict, higher-order functional languages (such as OCaml, Scheme, and SML) has been to use whole-program techniques to convert the program to a first-order monomorphic representation that can be optimized using traditional compilation techniques. This approach, popularized by MLton, has limitations, however. We are interested in exploring a different approach to compiling such languages, one that preserves the higher-order and polymorphic character of the program throughout optimization. To enable such an approach, we must have effective analyses that both provide precise information about higher-order programs and that scale to larger units of compilation. This paper describes one such analysis for determining the extent of variable bindings. We classify the extent of variables as either register (only one binding instance can be live at any time), stack (the lifetimes of binding instances obey a LIFO order), or heap (binding lifetimes are arbitrary). These extents naturally connect variables to the machine resources required to represent them. We believe that precise information about binding extents will enable efficient management of environments, which is a key problem in the efficient compilation of higher-order programs. At the core of the paper is the 3CPS intermediate representation, which is a factored CPS-based intermediate representation (IR) that statically marks variables to indicate their binding extent. We formally specify the management of this binding structure by means of a small-step operational semantics and define a static analysis that determines the extents of the variables in a program. We evaluate our analysis using a standard suite of SML benchmark programs. Our implementation gets surprisingly high yield and exhibits scalable performance. While this paper uses a CPS-based IR, the algorithm and results are easily transferable to other λ-calculus IRs, such as ANF.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {114},
	numpages = {29},
	keywords = {Higher-Order Flow Analysis, Compilers, Intermediate Representation, Continuation-Passing Style}
}

@article{raad22_concur_incor_separ_logic,
	author = {Raad, Azalea and Berdine, Josh and Dreyer, Derek and O'Hearn, Peter W.},
	title = {Concurrent Incorrectness Separation Logic},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498695},
	doi = {10.1145/3498695},
	abstract = {Incorrectness separation logic (ISL) was recently introduced as a theory of under-approximate reasoning, with the goal of proving that compositional bug catchers find actual bugs. However, ISL only considers sequential programs. Here, we develop concurrent incorrectness separation logic (CISL), which extends ISL to account for bug catching in concurrent programs. Inspired by the work on Views, we design CISL as a parametric framework, which can be instantiated for a number of bug catching scenarios, including race detection, deadlock detection, and memory safety error detection. For each instance, the CISL meta-theory ensures the soundness of incorrectness reasoning for free, thereby guaranteeing that the bugs detected are true positives.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {34},
	numpages = {29},
	keywords = {separation logic, bug catching, Concurrency, program logics}
}

@article{raad22_exten_intel_x86_consis_persis,
	author = {Raad, Azalea and Maranget, Luc and Vafeiadis, Viktor},
	title = {Extending Intel-X86 Consistency and Persistency: Formalising the Semantics of Intel-X86 Memory Types and Non-Temporal Stores},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498683},
	doi = {10.1145/3498683},
	abstract = {Existing semantic formalisations of the Intel-x86 architecture cover only a small fragment of its available features that are relevant for the consistency semantics of multi-threaded programs as well as the persistency semantics of programs interfacing with non-volatile memory. We extend these formalisations to cover: (1) non-temporal writes, which provide higher performance and are used to ensure that updates are flushed to memory; (2) reads and writes to other Intel-x86 memory types, namely uncacheable, write-combined, and write-through; as well as (3) the interaction between these features. We develop our formal model in both operational and declarative styles, and prove that the two characterisations are equivalent. We have empirically validated our formalisation of the consistency semantics of these additional features and their subtle interactions by extensive testing on different Intel-x86 implementations.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {22},
	numpages = {31},
	keywords = {non-volatile memory, non-temporal accesses, Intel-x86, memory types, memory persistency, memory consistency, cacheability, weak memory}
}

@inproceedings{radhakrishnan00,
	author = {{Radhakrishnan}, R. and {Teica}, E. and {Vermuri}, R.},
	url = {https://doi.org/10.1109/HLDVT.2000.889564},
	booktitle = {Proceedings IEEE International High-Level Design Validation and Test Workshop (Cat. No.PR00786)},
	doi = {10.1109/HLDVT.2000.889564},
	keywords = {high level synthesis;formal verification;logic design;high-level synthesis system validation;formally verified transformations;software errors;design correctness;transformational synthesis;behavior-preserving register transfer level transformations;High level synthesis;Algorithm design and analysis;Formal verification;Error correction;Software tools;Software algorithms},
	month = nov,
	pages = {80--85},
	title = {An approach to high-level synthesis system validation using formally verified transformations},
	year = {2000}
}

@article{radul23_you_only_linear_once,
	author = {Radul, Alexey and Paszke, Adam and Frostig, Roy and Johnson, Matthew J. and Maclaurin, Dougal},
	title = {You Only Linearize Once: Tangents Transpose to Gradients},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571236},
	doi = {10.1145/3571236},
	abstract = {Automatic differentiation (AD) is conventionally understood as a family of distinct algorithms, rooted in two “modes”—forward and reverse—which are typically presented (and implemented) separately. Can there be only one? Following up on the AD systems developed in the JAX and Dex projects, we formalize a decomposition of reverse-mode AD into (i) forward-mode AD followed by (ii) unzipping the linear and non-linear parts and then (iii) transposition of the linear part. To that end, we define a (substructurally) linear type system that can prove a class of functions are (algebraically) linear. Our main results are that forward-mode AD produces such linear functions, and that we can unzip and transpose any such linear function, conserving cost, size, and linearity. Composing these three transformations recovers reverse-mode AD. This decomposition also sheds light on checkpointing, which emerges naturally from a free choice in unzipping let expressions. As a corollary, checkpointing techniques are applicable to general-purpose partial evaluation, not just AD. We hope that our formalization will lead to a deeper understanding of automatic differentiation and that it will simplify implementations, by separating the concerns of differentiation proper from the concerns of gaining efficiency (namely, separating the derivative computation from the act of running it backward).},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {43},
	numpages = {29},
	keywords = {decomposition, partial evaluation, automatic differentiation, transpose}
}

@article{rahmouni94_loop_based_sched_algor_for,
	author = {Rahmouni, Maher and O'Brien, Kevin and Jerraya, Ahmed A.},
	publisher = {World Scientific Pub Co Pte Lt},
	url = {https://doi.org/10.1142/s0129626494000326},
	doi = {10.1142/s0129626494000326},
	journaltitle = {Parallel Processing Letters},
	keywords = {hardware description language,hardware scheduling},
	month = sep,
	number = {03},
	pages = {351--364},
	title = {A {Loop}-{Based} {Scheduling} {Algorithm} {For} {Hardware} {Description} {Languages}},
	volume = {04},
	year = {1994}
}

@inproceedings{rainey21_task_paral_assem_languag_uncom_paral,
	abstract = {Achieving parallel performance and scalability involves making compromises between parallel and sequential computation. If not contained, the overheads of parallelism can easily outweigh its benefits, sometimes by orders of magnitude. Today, we expect programmers to implement this compromise by optimizing their code manually. This process is labor intensive, requires deep expertise, and reduces code quality. Recent work on heartbeat scheduling shows a promising approach that manifests the potentially vast amounts of available, latent parallelism, at a regular rate, based on even beats in time. The idea is to amortize the overheads of parallelism over the useful work performed between the beats. Heartbeat scheduling is promising in theory, but the reality is complicated: it has no known practical implementation. In this paper, we propose a practical approach to heartbeat scheduling that involves equipping the assembly language with a small set of primitives. These primitives leverage existing kernel and hardware support for interrupts to allow parallelism to remain latent, until a heartbeat, when it can be manifested with low cost. Our Task Parallel Assembly Language (TPAL) is a compact, RISC-like assembly language. We specify TPAL through an abstract machine and implement the abstract machine as compiler transformations for C/C++ code and a specialized run-time system. We present an evaluation on both the Linux and the Nautilus kernels, considering a range of heartbeat interrupt mechanisms. The evaluation shows that TPAL can dramatically reduce the overheads of parallelism without compromising scalability.},
	author = {Rainey, Mike and Newton, Ryan R. and Hale, Kyle and Hardavellas, Nikos and Campanoni, Simone and Dinda, Peter and Acar, Umut A.},
	location = {Virtual, Canada},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3453483.3460969},
	booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
	doi = {10.1145/3453483.3460969},
	isbn = {9781450383912},
	keywords = {parallel programming languages,granularity control},
	pages = {1064--1079},
	series = {PLDI 2021},
	title = {Task Parallel Assembly Language for Uncompromising Parallelism},
	year = {2021}
}

@article{rajani21_unify_type_theor_higher_order,
	abstract = {This paper presents λ-amor, a new type-theoretic framework for amortized cost analysis of higher-order functional programs and shows that existing type systems for cost analysis can be embedded in it. λ-amor introduces a new modal type for representing potentials – costs that have been accounted for, but not yet incurred, which are central to amortized analysis. Additionally, λ-amor relies on standard type-theoretic concepts like affineness, refinement types and an indexed cost monad. λ-amor is proved sound using a rather simple logical relation. We embed two existing type systems for cost analysis in λ-amor showing that, despite its simplicity, λ-amor can simulate cost analysis for different evaluation strategies (call-by-name and call-by-value), in different styles (effect-based and coeffect-based), and with or without amortization. One of the embeddings also implies that λ-amor is relatively complete for all terminating PCF programs.},
	author = {Rajani, Vineet and Gaboardi, Marco and Garg, Deepak and Hoffmann, Jan},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434308},
	doi = {10.1145/3434308},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {type theory,relative completeness,amortized cost analysis},
	month = jan,
	number = {POPL},
	title = {A Unifying Type-Theory for Higher-Order (Amortized) Cost Analysis},
	volume = {5},
	year = {2021}
}

@article{ramalingam99_solvin_system_differ_const_increm,
	abstract = {Difference constraints systems consisting of inequalities of the form xi- xj{\$} {\backslash}leq {\$} bi,joccur in many applications, most notably those involving temporal reasoning. Often, it is necessary to maintain a solution to such a system as constraints are added, modified, and deleted. Existing algorithms handle modifications by solving the resulting system anew each time, which is inefficient. The best known algorithm to determine if a system of difference constraints is feasible (i.e., if it has a solution) and to compute a solution runs in $\Theta$ (mn) time, where n is the number of variables and m is the number of constraints.},
	author = {Ramalingam, G. and Song, J. and Joskowicz, L. and Miller, R. E.},
	url = {https://doi.org/10.1007/PL00009261},
	date = {1999-03-01},
	doi = {10.1007/PL00009261},
	issn = {1432-0541},
	journaltitle = {Algorithmica},
	number = {3},
	pages = {261--275},
	title = {Solving Systems of Difference Constraints Incrementally},
	volume = {23}
}

@article{ramsey22_beyon_reloop,
	author = {Ramsey, Norman},
	title = {Beyond Relooper: Recursive Translation of Unstructured Control Flow to Structured Control Flow (Functional Pearl)},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	doi = {10.1145/3547621},
	abstract = {In many compilers, control flow is represented using an arbitrary directed graph. But in some interesting target languages, including JavaScript and WebAssembly, intraprocedural control flow can be expressed only in structured ways, using loops, conditionals, and multilevel breaks or exits. As was shown by Peterson, Kasami, and Tokura in 1973, such structured control flow can be obtained by translating arbitrary control flow. The translation uses two standard analyses, but as published, it takes three passes—which may explain why it was overlooked by Emscripten, a popular compiler from C to JavaScript. By tweaking the analyses and by applying fundamental ideas from functional programming (recursive functions and immutable abstract-syntax trees), the translation, along with a couple of code improvements, can be implemented in a single pass. This new implementation is slated to be added to the Glasgow Haskell Compiler. Its single-pass translation, its immutable representation, and its use of dominator trees make it much easier to reason about than the original translation.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {90},
	numpages = {22},
	keywords = {Haskell, reverse postorder numbering, WebAssembly, control-flow analysis, dominator tree}
}

@INPROCEEDINGS{ratha97_fpga,
	keywords = {FPGA, motivation},
	author = {Ratha, N.K. and Jain, A.K.},
	booktitle = {Proceedings Fourth IEEE International Workshop on Computer Architecture for Machine Perception. CAMP'97},
	title = {FPGA-based computing in computer vision},
	year = {1997},
	volume = {},
	number = {},
	pages = {128-137},
	doi = {10.1109/CAMP.1997.631921}
}

@inproceedings{rau92_code_gener_schem_sched_loops,
	author = {Rau, B. Ramakrishna and Schlansker, Michael S. and Tirumalai, P. P.},
	location = {Portland, Oregon, USA},
	publisher = {IEEE Computer Society Press},
	booktitle = {Proceedings of the 25th Annual International Symposium on Microarchitecture},
	isbn = {0818631759},
	keywords = {modulo scheduling,code motion,loop scheduling,software pipelining,rotating registers},
	pages = {158--169},
	series = {MICRO 25},
	title = {Code Generation Schema for modulo Scheduled Loops},
	year = {1992}
}

@inproceedings{rau92_regis_alloc_softw_pipel_loops,
	abstract = {Software pipelining is an important instruction scheduling technique for efficiently overlapping successive iterations of loops and executing them in parallel. This paper studies the task of register allocation for software pipelined loops, both with and without hardware features that are specifically aimed at supporting software pipelines. Register allocation for software pipelines presents certain novel problems leading to unconventional solutions, especially in the presence of hardware support. This paper formulates these novel problems and presents a number of alternative solution strategies. These alternatives are comprehensively tested against over one thousand loops to determine the best register allocation strategy, both with and without the hardware support for software pipelining.},
	author = {Rau, B. R. and Lee, M. and Tirumalai, P. P. and Schlansker, M. S.},
	location = {San Francisco, California, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/143095.143141},
	booktitle = {Proceedings of the ACM SIGPLAN 1992 Conference on Programming Language Design and Implementation},
	doi = {10.1145/143095.143141},
	isbn = {0897914759},
	keywords = {software pipelining,loop scheduling,register allocation,rotating registers,compiler optimisation},
	pages = {283--299},
	series = {PLDI '92},
	title = {Register Allocation for Software Pipelined Loops},
	year = {1992}
}

@inproceedings{rau94_iterat_sched,
	abstract = {Modulo scheduling is a framework within which a wide variety of algorithms and heuristics may be defined for software pipelining innermost loops. This paper presents a practical algorithm, iterative modulo scheduling, that is capable of dealing with realistic machine models. This paper also characterizes the algorithm in terms of the quality of the generated schedules as well the computational expense incurred.},
	author = {Rau, B. Ramakrishna},
	location = {San Jose, California, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/192724.192731},
	booktitle = {Proceedings of the 27th Annual International Symposium on Microarchitecture},
	doi = {10.1145/192724.192731},
	isbn = {0897917073},
	keywords = {software pipelining,loop scheduling,rotating registers,code motion,compiler optimisation,modulo scheduling},
	pages = {63--74},
	series = {MICRO 27},
	title = {Iterative modulo Scheduling: An Algorithm for Software Pipelining Loops},
	year = {1994}
}

@article{rau96_iterat_modul_sched,
	doi = {10.1007/BF03356742},
	abstract = {Modulo scheduling is a framework within which algorithms for software pipelining innermost loops may be defined. The framework specifies a set of constraints that must be met in order to achieve a legal modulo schedule. A wide variety of algorithms and heuristics can be defined within this framework. Little work has been done to evaluate and compare alternative algorithms and heuristics for modulo scheduling from the viewpoints of schedule quality as well as computational complexity. This, along with a vague and unfounded perception that modulo scheduling is computationally expensive as well as difficult to implement, have inhibited its incorporation into product compilers. This paper presents iterative modulo scheduling, a practical algorithm that is capable of dealing with realistic machine models. The paper also characterizes the algorithm in terms of the quality of the generated schedules as well the computational expense incurred.},
	author = {Rau, B. Ramakrishna},
	date = {1996-02-01},
	issn = {1573-7640},
	journaltitle = {International Journal of Parallel Programming},
	keywords = {loop scheduling,software pipelining,code motion,compiler optimisation,rotating registers,modulo scheduling},
	number = {1},
	pages = {3--64},
	title = {Iterative Modulo Scheduling},
	volume = {24}
}

@inproceedings{ravi16_open_hls,
	abstract = {This paper presents a comprehensive survey on commonly used High-Level synthesis (HLS) tools that are available as open source. The HLS tools considered in this paper include Bambu, GAUT, Icarus Verilog, LegUp, and MyHDL. This paper also presents the process steps involved in each tool in detail and the major issues and challenges that are to be addressed in these tools in order to make the tools to become the most suitable choice for modern electronic system design.},
	author = {{Ravi}, S. and {Joseph}, M.},
	url = {https://doi.org/10.1109/ICCIC.2016.7919615},
	booktitle = {2016 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC)},
	doi = {10.1109/ICCIC.2016.7919615},
	issn = {2473-943X},
	keywords = {CAD;hardware description languages;high level synthesis;public domain software;open source HLS tools;modern electronic CAD;high-level synthesis tools;Bambu;GAUT;Icarus Verilog;LegUp;MyHDL;modern electronic system design;Hardware design languages;Hardware;Optimization;Linux;Resource management;Algorithm design and analysis;Field programmable gate arrays;Bambu;Behavioral synthesis;GAUT;High-Level Synthesis;Icarus Verilog;LegUp;MyHDL},
	month = dec,
	pages = {1--8},
	title = {Open source HLS tools: A stepping stone for modern electronic CAD},
	year = {2016}
}

@Inbook{reetz95_fgsv,
	keywords = {data-flow, control-flow},
	author = {Reetz, Ralf and Kropf, Thomas},
	editor = "Kloos, Carlos Delgado
and Breuer, Peter T.",
	title = "A Flow Graph Semantics of VHDL: A Basis for Hardware Verification with VHDL",
	bookTitle = "Formal Semantics for VHDL",
	year = "1995",
	publisher = {Springer US},
	address = "Boston, MA",
	pages = "205--238",
	abstract = "VHDL-based verification methods require a formal semantics of the language as a starting point. It has been shown recently that flow graphs are an excellent means for capturing such semantics. Our approach differs importantly from earlier work in that we use flow graphs as an intermediate for the ``deep'' embedding of VHDL in higher order logic. I.e., each VHDL program becomes a well-formed formula of the logic. This leads to a transparent semantics in which all constructs are explicitly conservative extensions to the underlying logic, which in turn allows reasoning about VHDL constructs to be carried out in the setting. As we provide a general framework, different verification techniques such as model-checking, first-order theorem proving or invariant-based approaches may be applied, depending on the verification task at hand. In particular, the separation of a flow graph into control flow and data flow components helps to structure the verification process.",
	isbn = "978-1-4615-2237-9",
	doi = "10.1007/978-1-4615-2237-9_8"
}

@inproceedings{ren14_high_level_synth,
	abstract = {This paper tries to give a brief introduction on contemporary High-Level Synthesis (HLS). It covers following areas on HLS: design languages, main algorithms and flow, available commercial and academic tools, HLS application space, benefits of using HLS, and remaining challenges. The goals of this paper are to give reader an overview of HLS area, promote the adoption of HLS in the industry, and point out interesting research directions.},
	author = {{Ren}, H.},
	url = {https://doi.org/10.1109/ICICDT.2014.6838614},
	booktitle = {2014 IEEE International Conference on IC Design Technology},
	doi = {10.1109/ICICDT.2014.6838614},
	issn = {2381-3555},
	keywords = {electronic engineering computing;hardware description languages;high level synthesis;HLS area;HLS application space;academic tools;commercial tools;design languages;contemporary HLS;contemporary high-level synthesis;Hardware;Algorithm design and analysis;Timing;Optimization;Software;Field programmable gate arrays;Pipeline processing;High-Level Synthesis},
	month = may,
	pages = {1--4},
	title = {A brief introduction on contemporary High-Level Synthesis},
	year = {2014}
}

@INPROCEEDINGS{reuther20_survey_machin_learn_accel,
	keywords = {motivation},
	author = {Reuther, Albert and Michaleas, Peter and Jones, Michael and Gadepally, Vijay and Samsi, Siddharth and Kepner, Jeremy},
	booktitle = {2020 IEEE High Performance Extreme Computing Conference (HPEC)},
	title = {Survey of Machine Learning Accelerators},
	year = {2020},
	volume = {},
	number = {},
	pages = {1-12},
	doi = {10.1109/HPEC43674.2020.9286149}
}

@article{reynaud21_pract_mode_system_recur_defin,
	abstract = {In call-by-value languages, some mutually-recursive definitions can be safely evaluated to build recursive functions or cyclic data structures, but some definitions (let rec x = x + 1) contain vicious circles and their evaluation fails at runtime. We propose a new static analysis to check the absence of such runtime failures. We present a set of declarative inference rules, prove its soundness with respect to the reference source-level semantics of Nordlander, Carlsson, and Gill [2008], and show that it can be directed into an algorithmic backwards analysis check in a surprisingly simple way. Our implementation of this new check replaced the existing check used by the OCaml programming language, a fragile syntactic criterion which let several subtle bugs slip through as the language kept evolving. We document some issues that arise when advanced features of a real-world functional language (exceptions in first-class modules, GADTs, etc.) interact with safety checking for recursive definitions.},
	author = {Reynaud, Alban and Scherer, Gabriel and Yallop, Jeremy},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434326},
	doi = {10.1145/3434326},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {ML,call-by-value,types,semantics,functional programming,recursion},
	month = jan,
	number = {POPL},
	title = {A Practical Mode System for Recursive Definitions},
	volume = {5},
	year = {2021}
}

@ARTICLE{rinker01_apcdg,
	keywords = {control-flow, data-flow, semantics, FPGA},
	author = {Rinker, R. and Carter, M. and Patel, A. and Chawathe, M. and Ross, C. and Hammes, J. and Najjar, W.A. and Bohm, W.},
	journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	title = {An Automated Process for Compiling Dataflow Graphs Into Reconfigurable Hardware},
	year = {2001},
	volume = {9},
	number = {1},
	pages = {130-139},
	doi = {10.1109/92.920828}
}

@article{rioux23_bowtie_beast,
	author = {Rioux, Nick and Huang, Xuejing and Oliveira, Bruno C. d. S. and Zdancewic, Steve},
	title = {A Bowtie for a Beast: Overloading, Eta Expansion, and Extensible Data Types in F⋈},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571211},
	doi = {10.1145/3571211},
	abstract = {The typed merge operator offers the promise of a compositional style of statically-typed programming in which solutions to the expression problem arise naturally. This approach, dubbed compositional programming, has recently been demonstrated by Zhang et al. Unfortunately, the merge operator is an unwieldy beast. Merging values from overlapping types may be ambiguous, so disjointness relations have been introduced to rule out undesired nondeterminism and obtain a well-behaved semantics. Past type systems using a disjoint merge operator rely on intersection types, but extending such systems to include union types or overloaded functions is problematic: naively adding either reintroduces ambiguity. In a nutshell: the elimination forms of unions and overloaded functions require values to be distinguishable by case analysis, but the merge operator can create exotic values that violate that requirement. This paper presents F⋈, a core language that demonstrates how unions, intersections, and overloading can all coexist with a tame merge operator. The key is an underlying design principle that states that any two inhabited types can support either the deterministic merging of their values, or the ability to distinguish their values, but never both. To realize this invariant, we decompose previously studied notions of disjointness into two new, dual relations that permit the operation that best suits each pair of types. This duality respects the polarization of the type structure, yielding an expressive language that we prove to be both type safe and deterministic.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {18},
	numpages = {29},
	keywords = {polymorphism, extensibility, type systems}
}

@article{rivas17_notion,
	keywords = {applicative},
	title = {Notions of computation as monoids},
	volume = {27},
	DOI = {10.1017/S0956796817000132},
	journal = {Journal of Functional Programming},
	publisher = {Cambridge University Press},
	author = {Rivas, Exequiel and Jaskelioff, Mauro},
	year = {2017},
	pages = {e21}
}

@article{rivera22_struc_pipel_compos_higher_order,
	author = {Rivera, Elijah and Krishnamurthi, Shriram},
	title = {Structural versus Pipeline Composition of Higher-Order Functions (Experience Report)},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3547633},
	doi = {10.1145/3547633},
	abstract = {In teaching students to program with compositions of higher-order functions, we have encountered a sharp distinction in the difficulty of problems as perceived by students. This distinction especially matters as growing numbers of programmers learn about functional programming for data processing. We have made initial progress on identifying this distinction, which appears counter-intuitive to some. We describe the phenomenon, provide some preliminary evidence of the difference in difficulty, and suggest consequences for functional programming pedagogy.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {102},
	numpages = {14},
	keywords = {plans, program structure, composition, higher-order functions}
}

@InProceedings{rizzi23_iterat_method_mappin_aware_frequen,
	keywords = {delay prediction},
	author = {Rizzi, Carmine and Guerrieri, Andrea and Josipović, Lana},
	booktitle = {Proceedings of the 60rd ACM/IEEE Design Automation Conference},
	title = {An Iterative Method for Mapping-Aware Frequency Regulation in Dataflow Circuits},
	year = {2023},
	month = jul,
	address = {San Francisco, CA}
}

@TechReport{roane23_autom_hw_sw_co_desig,
	institution = {Cadence},
	urldate = {2023-12-14},
	keywords = {high-level synthesis},
	url = {https://www.cadence.com/en_US/home/resources/white-papers/automated-hw-sw-co-design-of-dsp-systems-composed-of-processors-and-hardware-accelerators-wp.html},
	year = {2023},
	title = {Automated HW/SW Co-Design of DSP Systems Composed of Processors and Hardware Accelerators},
	author = {Roane, Jeff}
}

@inproceedings{robert12_formal_verif_alias_analy,
	abstract = {This paper reports on the formalization and proof of soundness, using the Coq proof assistant, of an alias analysis: a static analysis that approximates the flow of pointer values. The alias analysis considered is of the points-to kind and is intraprocedural, flow-sensitive, field-sensitive, and untyped. Its soundness proof follows the general style of abstract interpretation. The analysis is designed to fit in the CompCert C verified compiler, supporting future aggressive optimizations over memory accesses.},
	author = {Robert, Valentin and Leroy, Xavier},
	editor = {Hawblitzel, Chris and Miller, Dale},
	location = {Berlin, Heidelberg},
	publisher = {Springer},
	booktitle = {Certified Programs and Proofs},
	isbn = {978-3-642-35308-6},
	pages = {11--26},
	title = {A Formally-Verified Alias Analysis},
	year = {2012}
}

@inproceedings{rong13_alloc_rotat_regis_sched,
	abstract = {A rotating alias register file is a scalable hardware support to detect memory aliases at run-time. It has been shown that it can enable instruction-level parallelism to be effectively exploited from sequential code. Yet it is unknown how to apply it to loops.This paper presents an elegant and efficient solution that allocates rotating alias registers for a software-pipelined schedule of a loop. We show that surprisingly, this specific register allocation problem can be reduced to another software pipelining problem, for which numerous efficient algorithms are available. This is interesting in both theory and practice. We propose an algorithmic framework to solve the problem. We also present a simple software pipelining algorithm that specially targets register allocation. Comparison with a few other algorithms shows that it usually achieves the best allocation at the least time cost.Finally, we generalize the approach to allocate general-purpose (integer/floating-point/predicate) rotating registers by showing that it is also a software pipelining problem.},
	author = {Rong, Hongbo and Park, Hyunchul and Wang, Cheng and Wu, Youfeng},
	location = {Davis, California},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/2540708.2540738},
	booktitle = {Proceedings of the 46th Annual IEEE/ACM International Symposium on Microarchitecture},
	doi = {10.1145/2540708.2540738},
	isbn = {9781450326384},
	keywords = {register allocation,static scheduling,software pipelining,memory aliasing},
	pages = {346--358},
	series = {MICRO-46},
	title = {Allocating Rotating Registers by Scheduling},
	year = {2013}
}

@article{rosemann21_abstr_inter_spmd_diver_reduc,
	abstract = {Vectorizing compilers employ divergence analysis to detect at which program point a specific variable is uniform, i.e. has the same value on all SPMD threads that execute this program point. They exploit uniformity to retain branching to counter branch divergence and defer computations to scalar processor units. Divergence is a hyper-property and is closely related to non-interference and binding time. There exist several divergence, binding time, and non-interference analyses already but they either sacrifice precision or make significant restrictions to the syntactical structure of the program in order to achieve soundness. In this paper, we present the first abstract interpretation for uniformity that is general enough to be applicable to reducible CFGs and, at the same time, more precise than other analyses that achieve at least the same generality. Our analysis comes with a correctness proof that is to a large part mechanized in Coq. Our experimental evaluation shows that the compile time and the precision of our analysis is on par with LLVM's default divergence analysis that is only sound on more restricted CFGs. At the same time, our analysis is faster and achieves better precision than a state-of-the-art non-interference analysis that is sound and at least as general as our analysis.},
	author = {Rosemann, Julian and Moll, Simon and Hack, Sebastian},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434312},
	doi = {10.1145/3434312},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {Hyper-Property,Divergence Analysis,Vectorization,Non-Interference,Abstract Interpretation,Dependence,Binding Time},
	month = jan,
	number = {POPL},
	title = {An Abstract Interpretation for SPMD Divergence on Reducible Control Flow Graphs},
	volume = {5},
	year = {2021}
}

@article{rouvoet21_intrin_typed_compil_namel_label,
	abstract = {To avoid compilation errors it is desirable to verify that a compiler is type correct—i.e., given well-typed source code, it always outputs well-typed target code. This can be done intrinsically by implementing it as a function in a dependently typed programming language, such as Agda. This function manipulates data types of well-typed source and target programs, and is therefore type correct by construction. A key challenge in implementing an intrinsically typed compiler is the representation of labels in bytecode. Because label names are global, bytecode typing appears to be inherently a non-compositional, whole-program property. The individual operations of the compiler do not preserve this property, which requires the programmer to reason about labels, which spoils the compiler definition with proof terms. In this paper, we address this problem using a new nameless and co-contextual representation of typed global label binding, which is compositional. Our key idea is to use linearity to ensure that all labels are defined exactly once. To write concise compilers that manipulate programs in our representation, we develop a linear, dependently typed, shallowly embedded language in Agda, based on separation logic. We show that this language enables the concise specification and implementation of intrinsically typed operations on bytecode, culminating in an intrinsically typed compiler for a language with structured control-flow.},
	author = {Rouvoet, Arjen and Krebbers, Robbert and Visser, Eelco},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434303},
	doi = {10.1145/3434303},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {Type safety,Proof relevance,Co-contextual typing,Agda,Nameless,Compilation,Intrinsically typed,Code transformations,Dependent types},
	month = jan,
	number = {POPL},
	title = {Intrinsically Typed Compilation with Nameless Labels},
	volume = {5},
	year = {2021}
}

@inproceedings{roy19_high_cover_concol_equiv_check,
	author = {{Roy}, P. and {Chaki}, S. and {Chauhan}, P.},
	url = {https://doi.org/10.23919/DATE.2019.8715131},
	booktitle = {2019 Design, Automation Test in Europe Conference Exhibition (DATE)},
	doi = {10.23919/DATE.2019.8715131},
	keywords = {formal verification;hardware description languages;sequential equivalence checking;random simulation;complex designs;concolic approach;control signals;depth-first lexicographic manner;user-constrained designs;search depth;user-specified design constraints;high coverage concolic equivalence checking;high-level hardware description;RTL;nonincremental SLEC-CF;incremental SLEC-CF;Hardware;Testing;C++ languages;Wires;Hardware design languages;Tools;Inspection},
	month = mar,
	pages = {462--467},
	title = {High Coverage Concolic Equivalence Checking},
	year = {2019}
}

@ARTICLE{rudell87_multip_valued_minim_pla_optim,
	keywords = {boolean simplification, multi-valued logic},
	author = {Rudell, R.L. and Sangiovanni-Vincentelli, A.},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	title = {Multiple-Valued Minimization for PLA Optimization},
	year = {1987},
	volume = {6},
	number = {5},
	pages = {727-750},
	doi = {10.1109/TCAD.1987.1270318}
}

@inproceedings{ryu22_one_shot_tuner_deep_learn_compil,
	author = {Ryu, Jaehun and Park, Eunhyeok and Sung, Hyojin},
	title = {One-Shot Tuner for Deep Learning Compilers},
	year = {2022},
	isbn = {9781450391832},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497776.3517774},
	doi = {10.1145/3497776.3517774},
	abstract = {Auto-tuning DL compilers are gaining ground as an optimizing back-end for DL frameworks. While existing work can generate deep learning models that exceed the performance of hand-tuned libraries, they still suffer from prohibitively long auto-tuning time due to repeated hardware measurements in large search spaces. In this paper, we take a neural-predictor inspired approach to reduce the auto-tuning overhead and show that a performance predictor model trained prior to compilation can produce optimized tensor operation codes without repeated search and hardware measurements. To generate a sample-efficient training dataset, we extend input representation to include task-specific information and to guide data sampling methods to focus on learning high-performing codes. We evaluated the resulting predictor model, One-Shot Tuner, against AutoTVM and other prior work, and the results show that One-Shot Tuner speeds up compilation by 2.81x to 67.7x compared to prior work while providing comparable or improved inference time for CNN and Transformer models.},
	booktitle = {Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction},
	pages = {89–103},
	numpages = {15},
	keywords = {autotuning, optimizing compilers, deep neural networks, performance models},
	location = {Seoul, South Korea},
	series = {CC 2022}
}

@inproceedings{sabelfeld01_seman_trans_formal_synth_system_level,
	abstract = {In formal synthesis methodology, circuit implementations are derived from specifications by means of elementary logical transformation steps, which are performed within a theorem prover. In this approach, additionally to the circuit implementation, the proof that the result is a correct implementation of a given specification is obtained automatically. In the paper, we formally describe the functional semantics of system specifications in higher order logic.This semantics builds the basis for formal synthesis at system level. Further, theorems for circuit optimisation at this level are proposed.},
	author = {Sabelfeld, Viktor and Blumenröhr, Christian and Kapp, Kai},
	editor = {Bj{ø}rner, Dines and Broy, Manfred and Zamulin, Alexandre V.},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Perspectives of System Informatics},
	isbn = {978-3-540-45575-2},
	pages = {149--156},
	title = {Semantics and Transformations in Formal Synthesis at System Level},
	year = {2001}
}

@article{sabok21_probab_progr_seman_name_gener,
	abstract = {We make a formal analogy between random sampling and fresh name generation. We show that quasi-Borel spaces, a model for probabilistic programming, can soundly interpret the ν-calculus, a calculus for name generation. Moreover, we prove that this semantics is fully abstract up to first-order types. This is surprising for an ‘off-the-shelf’ model, and requires a novel analysis of probability distributions on function spaces. Our tools are diverse and include descriptive set theory and normal forms for the ν-calculus.},
	author = {Sabok, Marcin and Staton, Sam and Stein, Dario and Wolman, Michael},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434292},
	doi = {10.1145/3434292},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {standard Borel spaces,quasi-Borel spaces,Borel on Borel,name generation,nu-calculus,synthetic probability theory,denotational semantics,descriptive set theory,probabilistic programming},
	month = jan,
	number = {POPL},
	title = {Probabilistic Programming Semantics for Name Generation},
	volume = {5},
	year = {2021}
}

@inproceedings{sahebolamri22_seaml_deduc_infer_macros,
	author = {Sahebolamri, Arash and Gilray, Thomas and Micinski, Kristopher},
	title = {Seamless Deductive Inference via Macros},
	year = {2022},
	isbn = {9781450391832},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497776.3517779},
	doi = {10.1145/3497776.3517779},
	abstract = {We present an approach to integrating state-of-art bottom-up logic programming within the Rust ecosystem, demonstrating it with Ascent, an extension of Datalog that performs well against comparable systems. Rust’s powerful macro system permits Ascent to be compiled uniformly with the Rust code it’s embedded in and to interoperate with arbitrary user-defined components written in Rust, addressing a challenge in real-world use of logic programming languages: the fact that logical programs are parts of bigger software systems and need to interoperate with other components written in imperative programming languages. We leverage Rust’s trait system to extend Datalog semantics with non-powerset lattices, much like Flix, and with user-defined data types much like Formulog and Souffle. We use Ascent to re-implement the Rust borrow checker, a static analysis required by the Rust compiler. We evaluate our performance against Datafrog, Flix, and Souffl\'{e} using the borrow checker and other benchmarks, observing comparable performance to Datafrog and Souffl\'{e}, and speedups of around two orders of magnitude compared to Flix.},
	booktitle = {Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction},
	pages = {77–88},
	numpages = {12},
	keywords = {Static Analysis, Rust, Datalog, Ascent, Program Analysis, Logic Programming},
	location = {Seoul, South Korea},
	series = {CC 2022}
}

@inproceedings{sammler21_refin,
	abstract = {Given the central role that C continues to play in systems software, and the difficulty of writing safe and correct C code, it remains a grand challenge to develop effective formal methods for verifying C programs. In this paper, we propose a new approach to this problem: a type system we call RefinedC, which combines ownership types (for modular reasoning about shared state and concurrency) with refinement types (for encoding precise invariants on C data types and Hoare-style specifications for C functions). RefinedC is both automated (requiring minimal user intervention) and foundational (producing a proof of program correctness in Coq), while at the same time handling a range of low-level programming idioms such as pointer arithmetic. In particular, following the approach of RustBelt, the soundness of the RefinedC type system is justified semantically by interpretation into the Coq-based Iris framework for higher-order concurrent separation logic. However, the typing rules of RefinedC are also designed to be encodable in a new “separation logic programming” language we call Lithium. By restricting to a carefully chosen (yet expressive) fragment of separation logic, Lithium supports predictable, automatic, goal-directed proof search without backtracking. We demonstrate the effectiveness of RefinedC on a range of representative examples of C code.},
	author = {Sammler, Michael and Lepigre, Rodolphe and Krebbers, Robbert and Memarian, Kayvan and Dreyer, Derek and Garg, Deepak},
	location = {Virtual, Canada},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3453483.3454036},
	booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
	doi = {10.1145/3453483.3454036},
	isbn = {9781450383912},
	keywords = {proof automation,Coq,ownership types,Iris,C programming language,separation logic,refinement types},
	pages = {158--174},
	series = {PLDI 2021},
	title = {RefinedC: Automating the Foundational Verification of C Code with Refined Ownership Types},
	year = {2021}
}

@article{sammler23_dimsum,
	author = {Sammler, Michael and Spies, Simon and Song, Youngju and D'Osualdo, Emanuele and Krebbers, Robbert and Garg, Deepak and Dreyer, Derek},
	title = {DimSum: A Decentralized Approach to Multi-Language Semantics and Verification},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571220},
	doi = {10.1145/3571220},
	abstract = {Prior work on multi-language program verification has achieved impressive results, including the compositional verification of complex compilers. But the existing approaches to this problem impose a variety of restrictions on the overall structure of multi-language programs (e.g. fixing the source language, fixing the set of involved languages, fixing the memory model, or fixing the semantics of interoperation). In this paper, we explore the problem of how to avoid such global restrictions. Concretely, we present DimSum: a new, decentralized approach to multi-language semantics and verification, which we have implemented in the Coq proof assistant. Decentralization means that we can define and reason about languages independently from each other (as independent modules communicating via events), but also combine and translate between them when necessary (via a library of combinators). We apply DimSum to a high-level imperative language Rec (with an abstract memory model and function calls), a low-level assembly language Asm (with a concrete memory model, arbitrary jumps, and syscalls), and a mathematical specification language Spec. We evaluate DimSum on two case studies: an Asm library extending Rec with support for pointer comparison, and a coroutine library for Rec written in Asm. In both cases, we show how DimSum allows the Asm libraries to be abstracted to Rec-level specifications, despite the behavior of the Asm libraries not being syntactically expressible in Rec itself. We also verify an optimizing multi-pass compiler from Rec to Asm, showing that it is compatible with these Asm libraries.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {27},
	numpages = {31},
	keywords = {separation logic, compilers, verification, multi-language semantics, non-determinism, Iris, Coq}
}

@article{sangiorgi22_from_enhan_coind_enhan_induc,
	abstract = {There exist a rich and well-developed theory of enhancements of the coinduction proof method, widely used on behavioural relations such as bisimilarity. We study how to develop an analogous theory for inductive behaviour relations, i.e., relations defined from inductive observables. Similarly to the coinductive setting, our theory makes use of (semi)-progressions of the form R-&gt;F(R), where R is a relation on processes and F is a function on relations, meaning that there is an appropriate match on the transitions that the processes in R can perform in which the process derivatives are in F(R). For a given preorder, an enhancement corresponds to a sound function, i.e., one for which R-&gt;F(R) implies that R is contained in the preorder; and similarly for equivalences. We introduce weights on the observables of an inductive relation, and a weight-preserving condition on functions that guarantees soundness. We show that the class of functions contains non-trivial functions and enjoys closure properties with respect to desirable function constructors, so to be able to derive sophisticated sound functions (and hence sophisticated proof techniques) from simpler ones. We consider both strong semantics (in which all actions are treated equally) and weak semantics (in which one abstracts from internal transitions). We test our enhancements on a few non-trivial examples.},
	author = {Sangiorgi, Davide},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3498679},
	doi = {10.1145/3498679},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {process calculi,proof techniques,behavioural relations,coinduction},
	month = jan,
	number = {POPL},
	title = {From Enhanced Coinduction towards Enhanced Induction},
	volume = {6},
	year = {2022}
}

@article{sangiorgi98,
	keywords = {simulation proof, bisimulation},
	title = {On the bisimulation proof method},
	volume = {8},
	DOI = {10.1017/S0960129598002527},
	number = {5},
	journal = {Mathematical Structures in Computer Science},
	publisher = {Cambridge University Press},
	author = {Sangiorgi, Davide},
	year = {1998},
	pages = {447–479}
}

@inproceedings{saravanakumaran12_survey_optim_techn_high_level_synth,
	abstract = {This paper provides a detailed survey of optimization techniques available in high level synthesis. This survey contemplates on two parts. The first part deals with the applicability of optimization techniques available in high level language compiler into high level synthesis. The second part address the topics such as Area optimization, Resource optimization, Power optimization and Optimization issues pertaining to the notions value-grouping, value to register assignment, Transfer to wire assignment and wire to FU port assignment.},
	author = {Saravanakumaran, B. and Joseph, M.},
	editor = {Meghanathan, Natarajan and Chaki, Nabendu and Nagamalai, Dhinaharan},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Advances in Computer Science and Information Technology. Computer Science and Engineering},
	isbn = {978-3-642-27308-7},
	pages = {11--21},
	title = {Survey on Optimization Techniques in High Level Synthesis},
	year = {2012}
}

@inproceedings{sarkar98_enabl_spars_const_propag_array,
	doi = {10.1007/3-540-49727-7_3},
	abstract = {We present a new static analysis technique based on Array SSA form [6]. Compared to traditional SSA form, the key enhancement in Array SSA form is that it deals with arrays at the element level instead of as monolithic objects. In addition, Array SSA form improves the $\phi$ function used for merging scalar or array variables in traditional SSA form. The computation of a $\phi$ function in traditional SSA form depends on the program's control flow in addition to the arguments of the $\phi$ function. Our improved $\phi$ function (referred to as a $\phi$ function) includes the relevant control flow information explicitly as arguments through auxiliary variables that are called @ variables.},
	author = {Sarkar, Vivek and Knobe, Kathleen},
	editor = {Levi, Giorgio},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Static Analysis},
	isbn = {978-3-540-49727-1},
	pages = {33--56},
	title = {Enabling Sparse Constant Propagation of Array Elements via Array SSA Form},
	year = {1998}
}

@InProceedings{saïdi99_abstr_model_check_you_prove,
	doi = {10.1007/3-540-48683-6_38},
	author = {Saïdi, Hassen and Shankar, Natarajan},
	editor = "Halbwachs, Nicolas
and Peled, Doron",
	title = "Abstract and Model Check while You Prove",
	booktitle = "Computer Aided Verification",
	year = "1999",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "443--454",
	abstract = "The construction of abstractions is essential for reducing large or infinite state systems to small or finite state systems. Boolean abstractions, where boolean variables replace concrete predicates, are an important class that subsume several abstraction schemes. We show how boolean abstractions can be constructed simply, efficiently, and precisely for infinite state systems while preserving properties in the full {\textmu}-calculus. We also propose an automatic refinement algorithm which refines the abstraction until the property is verified or a counterexample is found. Our algorithm is implemented as a proof rule in the PVS verification system. With the abstraction proof rule, proof strategies combining deductive proof construction, model checking, and abstraction can be defined entirely within the PVS framework.",
	isbn = "978-3-540-48683-1"
}

@article{schafer16_sourc_code_error_detec_high,
	author = {{Carrion Schafer}, B.},
	url = {https://doi.org/10.1109/TVLSI.2015.2397036},
	doi = {10.1109/TVLSI.2015.2397036},
	issn = {1557-9999},
	journaltitle = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	keywords = {C language;error detection;formal verification;hardware description languages;high level synthesis;source code (software);source code error detection;high level synthesis functional verification;dynamic functional verification method;synthesizable high-level synthesis;behavioral descriptions ANSI-C;internal signal;timed simulation;functional software simulation;Probes;Generators;Synthesizers;Very large scale integration;Hardware;Timing;Finite impulse response filters;Error detection latency (EDL);functional verification;high-level synthesis (HLS).;Error detection latency (EDL);functional verification;high-level synthesis (HLS)},
	month = jan,
	number = {1},
	pages = {301--312},
	title = {Source Code Error Detection in High-Level Synthesis Functional Verification},
	volume = {24},
	year = {2016}
}

@Inbook{schaumont13_dfish,
	keywords = {control-flow, data-flow},
	author = "Schaumont, Patrick R.",
	title = "Data Flow Implementation in Software and Hardware",
	bookTitle = "A Practical Introduction to Hardware/Software Codesign",
	year = "2013",
	publisher = "Springer US",
	address = "Boston, MA",
	pages = "61--88",
	abstract = "The data flow model of computation is fully concurrent, and it is not committed to either a sequential or parallel implementation. Therefore, a single specification can be used to target software as well as hardware. This chapter describes the implementation of data flow systems in hardware and software on three different design targets. The first target is a software implementation of the data flow graph. Appropriate software scheduling techniques are needed to implement the data flow schedule. The second target is a fully parallel hardware implementation of the data flow graph. In this implementation, each actor maps into a separate hardware module, and individual modules synchronize through their token communications. The third target is hybrid: a combination of software and a hardware coprocessor. In this case, a data flow graph is partitioned over hardware and software. This chapter demonstrates the flexibility and convenience of a data flow specification under multiple targets.",
	isbn = "978-1-4614-3737-6",
	doi = "10.1007/978-1-4614-3737-6_3"
}

@InProceedings{scheurer16_gener_lattic_model_mergin_symbol_execut_branc,
	doi = {10.1007/978-3-319-47846-3_5},
	keywords = {value summaries, symbolic execution, hyperblocks},
	author = "Scheurer, Dominic
and H{\"a}hnle, Reiner
and Bubel, Richard",
	editor = "Ogata, Kazuhiro
and Lawford, Mark
and Liu, Shaoying",
	title = "A General Lattice Model for Merging Symbolic Execution Branches",
	booktitle = "Formal Methods and Software Engineering",
	year = "2016",
	publisher = "Springer International Publishing",
	address = "Cham",
	pages = "57--73",
	abstract = "Symbolic execution is a software analysis technique that has been used with success in the past years in program testing and verification. A main bottleneck of symbolic execution is the path explosion problem: the number of paths in a symbolic execution tree is exponential in the number of static branches of the executed program. Here we put forward an abstraction-based framework for state merging in symbolic execution. We show that it subsumes existing approaches and prove soundness. The method was implemented in the verification system KeY. Our empirical evaluation shows that reductions in proof size of up to 80 {\%} are possible by state merging when applied to complex verification problems; new proofs become feasible that were out of reach so far.",
	isbn = "978-3-319-47846-3"
}

@inproceedings{schultz22_formal_verif_distr_dynam_recon_protoc,
	author = {Schultz, William and Dardik, Ian and Tripakis, Stavros},
	title = {Formal Verification of a Distributed Dynamic Reconfiguration Protocol},
	year = {2022},
	isbn = {9781450391825},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497775.3503688},
	doi = {10.1145/3497775.3503688},
	abstract = {We present a formal, machine checked TLA+ safety proof of MongoRaftReconfig, a distributed dynamic reconfiguration protocol. MongoRaftReconfig was designed for and implemented in MongoDB, a distributed database whose replication protocol is derived from the Raft consensus algorithm. We present an inductive invariant for MongoRaftReconfig that is formalized in TLA+ and formally proved using the TLA+ proof system (TLAPS). We also present a formal TLAPS proof of two key safety properties of MongoRaftReconfig, LeaderCompleteness and StateMachineSafety. To our knowledge, these are the first machine checked inductive invariant and safety proof of a dynamic reconfiguration protocol for a Raft based replication system.},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {143–152},
	numpages = {10},
	keywords = {Dynamic Reconfiguration, Raft, Formal Verification, TLA+, Theorem Proving, Distributed Systems},
	location = {Philadelphia, PA, USA},
	series = {CPP 2022}
}

@InProceedings{schurr21_reliab_recon_fine_proof_proof_assis,
	keywords = {verification, proof-carrying, SAT},
	doi = {10.1007/978-3-030-79876-5_26},
	author = {Schurr, Hans-J{\"o}rg and Fleury, Mathias and Desharnais, Martin},
	editor = "Platzer, Andr{\'e}
and Sutcliffe, Geoff",
	title = "Reliable Reconstruction of Fine-grained Proofs in a Proof Assistant",
	booktitle = "Automated Deduction -- CADE 28",
	year = "2021",
	publisher = "Springer International Publishing",
	address = "Cham",
	pages = "450--467",
	abstract = "We present a fast and reliable reconstruction of proofs generated by the SMT solver veriT in Isabelle. The fine-grained proof format makes the reconstruction simple and efficient. For typical proof steps, such as arithmetic reasoning and skolemization, our reconstruction can avoid expensive search. By skipping proof steps that are irrelevant for Isabelle, the performance of proof checking is improved. Our method increases the success rate of Sledgehammer by halving the failure rate and reduces the checking time by 13{\%}. We provide a detailed evaluation of the reconstruction time for each rule. The runtime is influenced by both simple rules that appear very often and common complex rules.",
	isbn = "978-3-030-79876-5"
}

@article{schutten96_list,
	title = {List scheduling revisited},
	journal = {Operations Research Letters},
	volume = {18},
	number = {4},
	pages = {167-170},
	year = {1996},
	issn = {0167-6377},
	doi = {https://doi.org/10.1016/0167-6377(95)00057-7},
	url = {https://www.sciencedirect.com/science/article/pii/0167637795000577},
	author = {J.M.J. Schutten},
	keywords = {Scheduling, List scheduling, Parallel machines, Setup times, Regular cost functions},
	abstract = {We consider the problem of scheduling n jobs on m identical parallel machines to minimize a regular cost function. The standard list scheduling algorithm converts a list into a feasible schedule by focusing on the job start times. We prove that list schedules are dominant for this type of problem. Furthermore, we prove that an alternative list scheduling algorithm, focusing on the completion times rather than the start times, yields also dominant list schedules for problems with sequence dependent setup times.}
}

@INPROCEEDINGS{schwartz10_all_you_ever_wanted_know,
	keywords = {symbolic execution, taint analysis},
	author = {Schwartz, Edward J. and Avgerinos, Thanassis and Brumley, David},
	booktitle = {2010 IEEE Symposium on Security and Privacy},
	title = {All You Ever Wanted to Know about Dynamic Taint Analysis and Forward Symbolic Execution (but Might Have Been Afraid to Ask)},
	year = {2010},
	volume = {},
	number = {},
	pages = {317-331},
	doi = {10.1109/SP.2010.26}
}

@article{sekiyama23_tempor_verif_answer_effec_modif,
	author = {Sekiyama, Taro and Unno, Hiroshi},
	title = {Temporal Verification with Answer-Effect Modification: Dependent Temporal Type-and-Effect System with Delimited Continuations},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571264},
	doi = {10.1145/3571264},
	abstract = {Type-and-effect systems are a widely used approach to program verification, verifying the result of a computation using types, and its behavior using effects. This paper extends an effect system for verifying temporal, value-dependent properties on event sequences yielded by programs, to the delimited control operators shift0/reset0. While these delimited control operators enable useful and powerful programming techniques, they hinder reasoning about the behavior of programs because of their ability to suspend, resume, discard, and duplicate delimited continuations. This problem is more serious in effect systems for temporal properties because these systems must be capable of identifying what event sequences are yielded by captured continuations. Our key observation for achieving effective reasoning in the presence of the delimited control operators is that their use modifies answer effects, which are temporal effects of the continuations. Based on this observation, we extend an effect system for temporal verification to accommodate answer-effect modification. Allowing answer-effect modification enables easily reasoning about traces that captured continuations yield. Another novel feature of our effect system is the support for dependently typed continuations, which allows us to reason about programs more precisely. We prove soundness of the effect system for finite event sequences via type safety and that for infinite event sequences using a logical relation.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {71},
	numpages = {32},
	keywords = {answer-effect modification, temporal verification, delimited continuations, type-and-effect system}
}

@inproceedings{sen15_multis,
	author = {Sen, Koushik and Necula, George and Gong, Liang and Choi, Wontae},
	title = {MultiSE: Multi-Path Symbolic Execution Using Value Summaries},
	year = {2015},
	isbn = {9781450336758},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/2786805.2786830},
	abstract = {Dynamic symbolic execution (DSE) has been proposed to effectively generate test inputs for real-world programs. Unfortunately, DSE techniques do not scale well for large realistic programs, because often the number of feasible execution paths of a program increases exponentially with the increase in the length of an execution path. In this paper, we propose MultiSE, a new technique for merging states incrementally during symbolic execution, without using auxiliary variables. The key idea of MultiSE is based on an alternative representation of the state, where we map each variable, including the program counter, to a set of guarded symbolic expressions called a value summary. MultiSE has several advantages over conventional DSE and conventional state merging techniques: value summaries enable sharing of symbolic expressions and path constraints along multiple paths and thus avoid redundant execution. MultiSE does not introduce auxiliary symbolic variables, which enables it to 1) make progress even when merging values not supported by the constraint solver, 2) avoid expensive constraint solver calls when resolving function calls and jumps, and 3) carry out most operations concretely. Moreover, MultiSE updates value summaries incrementally at every assignment instruction, which makes it unnecessary to identify the join points and to keep track of variables to merge at join points. We have implemented MultiSE for JavaScript programs in a publicly available open-source tool. Our evaluation of MultiSE on several programs shows that 1) value summaries are an eective technique to take advantage of the sharing of value along multiple execution path, that 2) MultiSE can run significantly faster than traditional dynamic symbolic execution and, 3) MultiSE saves a substantial number of state merges compared to conventional state-merging techniques.},
	booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
	pages = {842–853},
	numpages = {12},
	keywords = {symbolic execution, value summaries, predicated execution, hyperblocks},
	location = {Bergamo, Italy},
	series = {ESEC/FSE 2015}
}

@article{sevcik13_compc,
	abstract = {In this article, we consider the semantic design and verified compilation of a C-like programming language for concurrent shared-memory computation on x86 multiprocessors. The design of such a language is made surprisingly subtle by several factors: the relaxed-memory behavior of the hardware, the effects of compiler optimization on concurrent code, the need to support high-performance concurrent algorithms, and the desire for a reasonably simple programming model. In turn, this complexity makes verified compilation both essential and challenging.We describe ClightTSO, a concurrent extension of CompCert’s Clight in which the TSO-based memory model of x86 multiprocessors is exposed for high-performance code, and CompCertTSO, a formally verified compiler from ClightTSO to x86 assembly language, building on CompCert. CompCertTSO is verified in Coq: for any well-behaved and successfully compiled ClightTSO source program, any permitted observable behavior of the generated assembly code (if it does not run out of memory) is also possible in the source semantics. We also describe some verified fence-elimination optimizations, integrated into CompCertTSO.},
	author = {Ševčı́k, Jaroslav and Vafeiadis, Viktor and Zappa Nardelli, Francesco and Jagannathan, Suresh and Sewell, Peter},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/2487241.2487248},
	doi = {10.1145/2487241.2487248},
	issn = {0004-5411},
	journaltitle = {J. ACM},
	keywords = {semantics,Relaxed memory models,verified compilation},
	month = jun,
	number = {3},
	title = {CompCertTSO: A Verified Compiler for Relaxed-Memory Concurrency},
	volume = {60},
	year = {2013}
}

@article{shankar85_towar,
	doi = {10.1007/bf00244278},
	url = {https://doi.org/10.1007%2Fbf00244278},
	year = 1985,
	publisher = {Springer Science and Business Media {LLC}},
	volume = {1},
	number = {4},
	author = {N. Shankar},
	title = {Towards mechanical metamathematics},
	journal = {Journal of Automated Reasoning}
}

@inproceedings{sharma20_java_ranger,
	author = {Sharma, Vaibhav and Hussein, Soha and Whalen, Michael W. and McCamant, Stephen and Visser, Willem},
	title = {Java Ranger: Statically Summarizing Regions for Efficient Symbolic Execution of Java},
	year = {2020},
	isbn = {9781450370431},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3368089.3409734},
	doi = {10.1145/3368089.3409734},
	abstract = {Merging execution paths is a powerful technique for reducing path explosion in symbolic execution. One approach, introduced and dubbed “veritesting” by Avgerinos et al., works by translating abounded control flow region into a single constraint. This approach is a convenient way to achieve path merging as a modification to a pre-existing single-path symbolic execution engine. Previous work evaluated this approach for symbolic execution of binary code, but different design considerations apply when building tools for other languages. In this paper, we extend the previous approach for symbolic execution of Java. Because Java code typically contains many small dynamically dispatched methods, it is important to include them in multi-path regions; we introduce dynamic inlining of method-regions to do so modularly. Java’s typed memory structure is very different from the binary representation, but we show how the idea of static single assignment (SSA) form can be applied to object references to statically account for aliasing. We have implemented our algorithms in Java Ranger, an extension to the widely used Symbolic Pathfinder tool. In a set of nine benchmarks, Java Ranger reduces the running time and number of execution paths by a total of 38\% and 71\% respectively as compared to SPF. Our results are a significant improvement over the performance of JBMC, a recently released verification tool for Java bytecode. We also participated in a static verification competition at a top theory conference where other participants included state-of-the-art Java verifiers. JR won first place in the competition’s Java verification track.},
	booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	pages = {123–134},
	numpages = {12},
	keywords = {symbolic execution, path merging, value summaries, hyperblocks},
	location = {Virtual Event, USA},
	series = {ESEC/FSE 2020}
}

@InProceedings{sheng20_polit_theor_algeb_datat,
	keywords = {SAT, proof-carrying},
	doi = {10.1007/978-3-030-51074-9_14},
	author = {Sheng, Ying and Zohar, Yoni and Ringeissen, Christophe and Lange, Jane and Fontaine, Pascal and Barrett, Clark},
	editor = "Peltier, Nicolas
and Sofronie-Stokkermans, Viorica",
	title = "Politeness for the Theory of Algebraic Datatypes",
	booktitle = "Automated Reasoning",
	year = "2020",
	publisher = "Springer International Publishing",
	address = "Cham",
	pages = "238--255",
	abstract = "Algebraic datatypes, and among them lists and trees, have attracted a lot of interest in automated reasoning and Satisfiability Modulo Theories (SMT). Since its latest stable version, the SMT-LIB standard defines a theory of algebraic datatypes, which is currently supported by several mainstream SMT solvers. In this paper, we study this particular theory of datatypes and prove that it is strongly polite, showing also how it can be combined with other arbitrary disjoint theories using polite combination. Our results cover both inductive and finite datatypes, as well as their union. The combination method uses a new, simple, and natural notion of additivity, that enables deducing strong politeness from (weak) politeness.",
	isbn = "978-3-030-51074-9"
}

@article{sherman21,
	abstract = {Deep learning is moving towards increasingly sophisticated optimization objectives that employ higher-order functions, such as integration, continuous optimization, and root-finding. Since differentiable programming frameworks such as PyTorch and TensorFlow do not have first-class representations of these functions, developers must reason about the semantics of such objectives and manually translate them to differentiable&nbsp;code. We present a differentiable programming language, λS, that is the first to deliver a semantics for higher-order functions, higher-order derivatives, and Lipschitz but nondifferentiable functions. Together, these features enableλS to expose differentiable, higher-order functions for integration, optimization, and root-finding as first-class functions with automatically computed derivatives. λS’s semantics is computable, meaning that values can be computed to arbitrary precision, and we implement λS as an embedded language in Haskell. We use λS to construct novel differentiable libraries for representing probability distributions, implicit surfaces, and generalized parametric surfaces – all as instances of higher-order datatypes – and present case studies that rely on computing the derivatives of these higher-order functions and datatypes. In addition to modeling existing differentiable algorithms, such as a differentiable ray tracer for implicit surfaces, without requiring any user-level differentiation code, we demonstrate new differentiable algorithms, such as the Hausdorff distance of generalized parametric surfaces.},
	author = {Sherman, Benjamin and Michel, Jesse and Carbin, Michael},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434284},
	doi = {10.1145/3434284},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {Automatic Differentiation,Constructive Analysis,Diffeological Spaces},
	month = jan,
	number = {POPL},
	title = {𝜆ₛ: Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes},
	volume = {5},
	year = {2021}
}

@inproceedings{shi21_coqqf,
	abstract = {We present a certified SMT  QF{\_}BV solver CoqQFBV built from a verified bit blasting algorithm, Kissat, and the verified SAT certificate checker GratChk in this paper. Our verified bit blasting algorithm supports the full QF{\_}BV logic of SMT-LIB; it is specified and formally verified in the proof assistant Coq . We compare CoqQFBV with CVC4, Bitwuzla, and Boolector on benchmarks from the QF{\_}BV division of the single query track in the 2020 SMT Competition, and real-world cryptographic program verification problems. CoqQFBV surprisingly solves more program verification problems with certification than the 2020 SMT  QF{\_}BV division winner Bitwuzla without certification.},
	author = {Shi, Xiaomu and Fu, Yu-Fu and Liu, Jiaxiang and Tsai, Ming-Hsien and Wang, Bow-Yaw and Yang, Bo-Yin},
	editor = {Silva, Alexandra and Leino, K. Rustan M.},
	location = {Cham},
	publisher = {Springer International Publishing},
	booktitle = {Computer Aided Verification},
	isbn = {978-3-030-81688-9},
	keywords = {coq,sat},
	pages = {149--171},
	title = {CoqQFBV: A Scalable Certified SMT Quantifier-Free Bit-Vector Solver},
	year = {2021}
}

@inproceedings{shi21_path_sensit_spars_analy_path_condit,
	abstract = {Sparse program analysis is fast as it propagates data flow facts via data dependence, skipping unnecessary control flows. However, when path-sensitively checking millions of lines of code, it is still prohibitively expensive because a huge number of path conditions have to be computed and solved via an SMT solver. This paper presents Fusion, a fused approach to inter-procedurally path-sensitive sparse analysis. In Fusion, the SMT solver does not work as a standalone tool on path conditions but directly on the program together with the sparse analysis. Such a fused design allows us to determine the path feasibility without explicitly computing path conditions, not only saving the cost of computing path conditions but also providing an opportunity to enhance the SMT solving algorithm. To the best of our knowledge, Fusion, for the first time, enables whole program bug detection on millions of lines of code in a common personal computer, with the precision of inter-procedural path-sensitivity. Compared to two state-of-the-art tools, Fusion is 10\texttimes{} faster but consumes only 10% of memory on average. Fusion has detected over a hundred bugs in mature open-source software, some of which have even been assigned CVE identifiers due to their security impact.},
	author = {Shi, Qingkai and Yao, Peisen and Wu, Rongxin and Zhang, Charles},
	location = {Virtual, Canada},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3453483.3454086},
	booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
	doi = {10.1145/3453483.3454086},
	isbn = {9781450383912},
	keywords = {Sparse analysis,SMT solving,program dependence graph,path sensitivity},
	pages = {930--943},
	series = {PLDI 2021},
	title = {Path-Sensitive Sparse Analysis without Path Conditions},
	year = {2021}
}

@inproceedings{shobaki22_graph_trans_regis_press_aware_instr_sched,
	author = {Shobaki, Ghassan and Bassett, Justin and Heffernan, Mark and Kerbow, Austin},
	title = {Graph Transformations for Register-Pressure-Aware Instruction Scheduling},
	year = {2022},
	isbn = {9781450391832},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497776.3517771},
	doi = {10.1145/3497776.3517771},
	abstract = {This paper presents graph transformation algorithms for register-pressure-aware instruction scheduling. The proposed transformations add edges to the data dependence graph (DDG) to eliminate solutions that are either redundant or sub-optimal. Register-pressure-aware instruction scheduling aims at balancing two conflicting objectives: maximizing instruction-level parallelism (ILP) and minimizing register pressure (RP). Graph transformations have been previously proposed for the problem of maximizing ILP without considering RP, which is a problem of limited practical value. In the current paper, we extend that work by proposing graph transformations for the RP minimization objective, which is an important objective in practice. Various cost functions are considered for representing RP, and we show that the proposed transformations preserve optimality with respect to each of them. The proposed transformations are used to reduce the size of the solution space before applying a Branch-and-Bound (B&amp;B) algorithm that exhaustively searches for an optimal solution. The proposed transformations and the B&amp;B algorithm were implemented in the LLVM compiler, and their performance was evaluated experimentally on a CPU target and a GPU target. The SPEC CPU2017 floating-point benchmarks were used on the CPU and the PlaidML benchmarks were used on the GPU. The results show that the proposed transformations significantly reduce the compile time while giving approximately the same execution-time performance.},
	booktitle = {Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction},
	pages = {41–53},
	numpages = {13},
	keywords = {graph transformations, instruction scheduling, register-pressure reduction, branch and bound, dominance},
	location = {Seoul, South Korea},
	series = {CC 2022}
}

@inproceedings{sias00_accur_effic_predic_analy_binar_decis_diagr,
	keywords = {if-conversion, predicated execution},
	author = {Sias, John W. and Hwu, Wen-Mei W. and August, David I.},
	title = {Accurate and Efficient Predicate Analysis with Binary Decision Diagrams},
	year = {2000},
	isbn = {1581131968},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/360128.360141},
	doi = {10.1145/360128.360141},
	booktitle = {Proceedings of the 33rd Annual ACM/IEEE International Symposium on Microarchitecture},
	pages = {112–123},
	numpages = {12},
	location = {Monterey, California, USA},
	series = {MICRO 33}
}

@Software{siemens23_ss,
	urldate = {2023-12-21},
	url = {https://eda.sw.siemens.com/en-US/ic/catapult-high-level-synthesis/hls-verification/slec/},
	year = {2023},
	title = {SLEC System},
	author = {Siemens}
}

@inproceedings{silva11_rtos,
	keywords = {RTOS, predictable execution, avionics},
	doi = {10.1109/iolts.2011.5993805},
	url = {https://doi.org/10.1109%2Fiolts.2011.5993805},
	year = 2011,
	month = {jul},
	publisher = {{IEEE}},
	author = {Dhiego Silva and Leticia Bolzani and Fabian Vargas},
	title = {An intellectual property core to detect task schedulling-related faults in {RTOS}-based embedded systems},
	booktitle = {2011 {IEEE} 17th International On-Line Testing Symposium}
}

@inproceedings{silva15_area_desig_space_explor_high_level_synth,
	abstract = {To assure good quality of synthesis results (QoR) in the current High-Level Synthesis (HLS) practice is still a big challenge. To address the issue of very large design space possibilities in digital systems design, this paper presents an iterative method for Design Space Exploration (DSE). We target FPGA (field-programmable gate array) devices and use an off-the-shelf standard HLS tool in our experiments. We present our methodology, which includes: code optimization checkpoints detection, automatic optimization directives insertion and results parsing/analysis aiming at a highest QoR, in terms of area. Experimental results on HLS compilation of a VLIW (Very Large Instruction Word) processor obtains up to 68% on flip-flops (FFs) reduction and 32% of lookup-tables (LUTs) reduction, compared to a baseline HLS flow. Using a FIR filter HLS as a test-case, our DSE method with the same Vivado™ tool results in more than 3X lower FF utilization, with practically the same LUTs consumption and performance, comparing with the non-guided HLS flow. Our results, in terms of QoR, represent an increase of 50% and 43%, respectively, for VLIW and FIR filter benchmarks.},
	author = {{da Silva}, J. S. and {Bampi}, S.},
	booktitle = {2015 IEEE 6th Latin American Symposium on Circuits Systems (LASCAS)},
	doi = {10.1109/LASCAS.2015.7250447},
	keywords = {field programmable gate arrays;FIR filters;flip-flops;high level synthesis;instruction sets;iterative methods;table lookup;area-oriented iterative method;design space exploration;DSE;high-level synthesis;QoR;digital systems design;FPGA devices;field-programmable gate array;code optimization checkpoint detection;automatic optimization directive insertion;VLIW processor;very large instruction word processor;flip-flop reduction;lookup-table reduction;FIR filter HLS;Vivado tool;Field programmable gate arrays;Arrays;Table lookup;Optimization;Measurement;Benchmark testing;Space exploration},
	month = feb,
	pages = {1--4},
	title = {Area-oriented iterative method for Design Space Exploration with High-Level Synthesis},
	year = {2015}
}

@article{silver21_dijks_monad_forev,
	abstract = {This paper extends the Dijkstra monad framework, designed for writing specifications over effectful programs using monadic effects, to handle termination sensitive specifications over interactive programs. We achieve this by introducing base specification monads for non-terminating programs with uninterpreted events. We model such programs using interaction trees, a coinductive datatype for representing programs with algebraic effects in Coq, which we further develop by adding trace semantics. We show that this approach subsumes typical, simple proof principles. The framework is implemented as an extension of the Interaction Trees Coq library.},
	author = {Silver, Lucas and Zdancewic, Steve},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434307},
	doi = {10.1145/3434307},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {verification,specification,Hoare logic,monads,coinduction},
	month = jan,
	number = {POPL},
	title = {Dijkstra Monads Forever: Termination-Sensitive Specifications for Interaction Trees},
	volume = {5},
	year = {2021}
}

@inproceedings{simbuerger14,
	author = {Simbürger, Andreas and Grö{ß}linger, Armin},
	keywords = {polyhedral analysis,polyhedral model,algebraic model},
	series = {IMPACT},
	title = {On the variety of static control parts in real-world programs: from affine via multi-dimensional to polynomial and just-in-time},
	year = {2014}
}

@article{simbuerger19_polyj,
	abstract = {While polyhedral optimization appeared in mainstream compilers during the past decade, its profitability in scenarios outside its classic domain of linear-algebra programs has remained in question. Recent implementations, such as the LLVM plugin Polly, produce promising speedups, but the restriction to affine loop programs with control flow known at compile time continues to be a limiting factor. PolyJIT combines polyhedral optimization with multi-versioning at run time, at which one has access to knowledge enabling polyhedral optimization, which is not available at compile time. By means of a fully-fledged implementation of a light-weight just-in-time compiler and a series of experiments on a selection of real-world and benchmark programs, we demonstrate that the consideration of run-time knowledge helps in tackling compile-time violations of affinity and, consequently, offers new opportunities of optimization at run time.},
	author = {Simbuerger, Andreas and Apel, Sven and Groesslinger, Armin and Lengauer, Christian},
	publisher = {Springer},
	url = {https://doi.org/10.1007/s10766-018-0597-3},
	doi = {10.1007/s10766-018-0597-3},
	issn = {15737640},
	journaltitle = {International Journal of Parallel Programming},
	keywords = {polyhedral analysis,just-in-time compilation},
	number = {5-6},
	pages = {874--906},
	title = {{PolyJIT}: Polyhedral Optimization Just in Time},
	volume = {47},
	year = {2019}
}

@Book{singer18_static_singl_assig_book,
	url = {https://pfalcon.github.io/ssabook/latest/book-full.pdf},
	keywords = {gated-SSA, SSA},
	volume = {1},
	year = {2018},
	title = {Static Single Assignment Book},
	author = {Singer, J.}
}

@inproceedings{singh08_kiwi,
	abstract = {We describe the Kiwi parallel programming library and its associated synthesis system which is used to transform C# parallel programs into circuits for realization on FPGAs. The Kiwi system is targeted at making reconfigurable computing technology accessible to software engineers that are willing to express their computations as parallel programs. Although there has been much work on compiling sequential C-like programs to hardware by automatically `discovering¿ parallelism, we work by exploiting the parallel architecture communicated by the designer through the choice of parallel and concurrent programming language constructs. Specifically, we describe a system that takes .NET assembly language with suitable custom attributes as input and produces Verilog output which is mapped to FPGAs. We can then choose to apply analysis and verification techniques to either the highlevel representation in C# or other .NET languages or to the generated RTL netlists. A distinctive aspect of our approach is the exploitation of existing language constructs for concurrent programming and synchronization which contrasts with other schemes which introduce specialized concurrency control constructs to extend a sequential language.},
	author = {{Singh}, S. and {Greaves}, D. J.},
	booktitle = {2008 16th International Symposium on Field-Programmable Custom Computing Machines},
	doi = {10.1109/FCCM.2008.46},
	keywords = {high-level synthesis,FPGA},
	month = apr,
	pages = {3--12},
	title = {Kiwi: Synthesis of FPGA Circuits from Parallel Programs},
	year = {2008}
}

@misc{singh21_silver_oak,
	author = {Singh, Satnam},
	url = {https://github.com/project-oak/silveroak},
	title = {{Silver Oak}},
	year = {2021}
}

@article{singh23_operat_approac_librar_abstr_relax_memor_concur,
	author = {Singh, Abhishek Kr and Lahav, Ori},
	title = {An Operational Approach to Library Abstraction under Relaxed Memory Concurrency},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571246},
	doi = {10.1145/3571246},
	abstract = {Concurrent data structures and synchronization mechanisms implemented by expert developers are indispensable for modular software development. In this paper, we address the fundamental problem of library abstraction under weak memory concurrency, and identify a general library correctness condition allowing clients of the library to reason about program behaviors using the specification code, which is often much simpler than the concrete implementation. We target (a fragment of) the RC11 memory model, and develop an equivalent operational presentation that exposes knowledge propagation between threads, and is sufficiently expressive to capture library behaviors as totally ordered operational execution traces. We further introduce novel access modes to the language that allow intricate specifications accounting for library internal synchronization that is not exposed to the client, as well as the library's demands on external synchronization by the client. We illustrate applications of our approach in several examples of different natures.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {53},
	numpages = {31},
	keywords = {Relaxed memory consistency, Linearizability, Library abstraction, Concurrent objects}
}

@inproceedings{sittel20_modul_sched_ration_initiat_inter,
	abstract = {In modulo scheduling, the number of clock cycles between successive inputs (the initiation interval, II) is traditionally an integer, but in this paper, we explore the benefits of allowing it to be a rational number. This rational II can be interpreted as the average number of clock cycles between successive inputs. As the minimum rational II can be less than the minimum integer II, this translates to higher throughput. We formulate rational-II modulo scheduling as an integer linear programming (ILP) problem that is able to find latency-optimal schedules for a fixed rational II. We have applied our scheduler to a standard benchmark of hardware designs, and our results demonstrate a significant speedup compared to state-of-the-art integer-II and rational-II formulations.},
	author = {{Sittel}, P. and {Wickerson}, J. and {Kuimm}, M. and {Zipf}, P.},
	url = {https://doi.org/10.1109/ASP-DAC47756.2020.9045616},
	booktitle = {2020 25th Asia and South Pacific Design Automation Conference (ASP-DAC)},
	doi = {10.1109/ASP-DAC47756.2020.9045616},
	issn = {2153-697X},
	keywords = {integer programming;linear programming;scheduling;rational initiation intervals;custom hardware design;clock cycles;successive inputs;initiation interval;rational number;minimum rational II;minimum integer II;rational-II modulo scheduling;integer linear programming problem;latency-optimal schedules;fixed rational II;hardware designs;state-of-the-art integer-II;rational-II formulations;Clocks;Schedules;Hardware;Throughput;Processor scheduling;Indexes;Standards},
	month = jan,
	pages = {568--573},
	title = {Modulo Scheduling with Rational Initiation Intervals in Custom Hardware Design},
	year = {2020}
}

@article{six20_certif_effic_instr_sched,
	abstract = {CompCert is a moderately optimizing C compiler with a formal, machine-checked, proof of correctness: after successful compilation, the assembly code has a behavior faithful to the source code. Previously, it only supported target instruction sets with sequential semantics, and did not attempt reordering instructions for optimization. We present here a CompCert backend for a VLIW core (i.e. with explicit parallelism at the instruction level), the first CompCert backend providing scalable and efficient instruction scheduling. Furthermore, its highly modular implementation can be easily adapted to other VLIW or non-VLIW pipelined processors.},
	author = {Six, Cyril and Boulmé, Sylvain and Monniaux, David},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	doi = {10.1145/3428197},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {coq,translation validation,scheduling,static scheduling,verification,VLIW,operational semantics},
	month = nov,
	number = {OOPSLA},
	title = {Certified and Efficient Instruction Scheduling: Application to Interlocked VLIW Processors},
	volume = {4},
	year = {2020}
}

@unpublished{six21_verif_super_sched_relat_optim,
	author = {Six, Cyril and Gourdin, Léo and Boulmé, Sylvain and Monniaux, David},
	url = {https://hal.archives-ouvertes.fr/hal-03200774},
	file = {https://hal.archives-ouvertes.fr/hal-03200774/file/hal_prepass_scheduling.pdf},
	keywords = {coq,translation validation,scheduling,static scheduling,verification,VLIW,operational semantics},
	month = apr,
	note = {working paper or preprint},
	title = {{Verified Superblock Scheduling with Related Optimizations}},
	year = {2021}
}

@inproceedings{six22_formal_verif_super_sched,
	abstract = {On in-order processors, without dynamic instruction scheduling, program running times may be significantly reduced by compile-time instruction scheduling. We present here the first effective certified instruction scheduler that operates over superblocks (it may move instructions across branches), along with its performance evaluation. It is integrated within the CompCert C compiler, providing a complete machine-checked proof of semantic preservation from C to assembly. Our optimizer composes several passes designed by translation validation: program transformations are proposed by untrusted oracles, which are then validated by certified and scalable checkers. Our main checker is an architecture-independent simulation-test over superblocks modulo register liveness, which relies on hash-consed symbolic execution.},
	author = {Six, Cyril and Gourdin, Léo and Boulmé, Sylvain and Monniaux, David and Fasse, Justus and Nardino, Nicolas},
	location = {Philadelphia, PA, USA},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	doi = {10.1145/3497775.3503679},
	isbn = {9781450391825},
	keywords = {Symbolic execution,Instruction-level parallelism,Translation validation,the COQ proof assistant},
	pages = {40--54},
	series = {CPP 2022},
	title = {Formally Verified Superblock Scheduling},
	year = {2022}
}

@article{slind07_proof_produc_synth_arith_crypt_hardw,
	abstract = {A compiler from a synthesisable subset of higher order logic to clocked synchronous hardware is described. It is being used to create coprocessors for cryptographic and arithmetic applications. The compiler automatically translates a function f defined in higher order logic (typically using recursion) into a device that computes f via a four-phase handshake circuit. Compilation is by fully automatic proof in the HOL4 system, and generates a correctness theorem for each compiled function. Synthesised circuits can be directly translated to Verilog, and then input to design automation tools. A fully-expansive `LCF methodology' allows users to safely modify and extend the compiler's theorem proving scripts to add optimisations or to enlarge the synthesisable subset of higher order logic.},
	author = {Slind, Konrad and Owens, Scott and Iyoda, Juliano and Gordon, Mike},
	url = {https://doi.org/10.1007/s00165-007-0028-5},
	date = {2007-08-01},
	doi = {10.1007/s00165-007-0028-5},
	issn = {1433-299X},
	journaltitle = {Formal Aspects of Computing},
	number = {3},
	pages = {343--362},
	title = {Proof Producing Synthesis of Arithmetic and Cryptographic Hardware},
	volume = {19}
}

@inproceedings{slind08_brief_overv_hol4,
	abstract = {The HOLF proof assistant supports specification and proof in classical higher order logic. It is the latest in a long line of similar systems. In this short overview, we give an outline of the HOLF system and how it may be applied in formal verification.},
	author = {Slind, Konrad and Norrish, Michael},
	editor = {Mohamed, Otmane Ait and Muñoz, César and Tahar, Sofiène},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Theorem Proving in Higher Order Logics},
	isbn = {978-3-540-71067-7},
	keywords = {theorem prover},
	pages = {28--32},
	title = {A Brief Overview of HOL4},
	year = {2008}
}

@article{smeding23_effic_dual_number_rever_ad,
	author = {Smeding, Tom J. and V\'{a}k\'{a}r, Matthijs I. L.},
	title = {Efficient Dual-Numbers Reverse AD via Well-Known Program Transformations},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571247},
	doi = {10.1145/3571247},
	abstract = {Where dual-numbers forward-mode automatic differentiation (AD) pairs each scalar value with its tangent value, dual-numbers reverse-mode AD attempts to achieve reverse AD using a similarly simple idea: by pairing each scalar value with a backpropagator function. Its correctness and efficiency on higher-order input languages have been analysed by Brunel, Mazza and Pagani, but this analysis used a custom operational semantics for which it is unclear whether it can be implemented efficiently. We take inspiration from their use of linear factoring to optimise dual-numbers reverse-mode AD to an algorithm that has the correct complexity and enjoys an efficient implementation in a standard functional language with support for mutable arrays, such as Haskell. Aside from the linear factoring ingredient, our optimisation steps consist of well-known ideas from the functional programming community. We demonstrate the use of our technique by providing a practical implementation that differentiates most of Haskell98.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {54},
	numpages = {28},
	keywords = {source transformation, functional programming, automatic differentiation}
}

@INPROCEEDINGS{smelyanskiy03_predic,
	keywords = {trace scheduling, static scheduling, predicated execution, hyperblocks},
	author = {Smelyanskiy, M. and Mahlke, S.A. and Davidson, E.S. and Lee, H.-H.S.},
	booktitle = {International Symposium on Code Generation and Optimization, 2003. CGO 2003.},
	title = {Predicate-aware scheduling: a technique for reducing resource constraints},
	year = {2003},
	volume = {},
	number = {},
	pages = {169-178},
	doi = {10.1109/CGO.2003.1191543}
}

@INPROCEEDINGS{smith06_dataf_predic,
	publisher = {IEEE},
	author = {Smith, Aaron and Nagarajan, Ramadass and Sankaralingam, Karthikeyan and McDonald, Robert and Burger, Doug and Keckler, Stephen W. and McKinley, Kathryn S.},
	booktitle = {2006 39th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO'06)},
	title = {Dataflow Predication},
	year = {2006},
	volume = {},
	number = {},
	pages = {89-102},
	doi = {10.1109/MICRO.2006.17}
}

@InProceedings{soegaard-andersen93_comput,
	doi = {10.1007/3-540-56922-7_25},
	keywords = {simulation proof},
	author = {S{\o}gaard-Andersen, J{\o}rgen F. and Garland, Stephen J. and Guttag, John V. and Lynch, Nancy A. and Pogosyants, Anna},
	editor = "Courcoubetis, Costas",
	title = "Computer-assisted simulation proofs",
	booktitle = "Computer Aided Verification",
	year = "1993",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "305--319",
	abstract = "This paper presents a scalable approach to reasoning formally about distributed algorithms. It uses results about I/O automata to extract a set of proof obligations for showing that the behaviors of one algorithm are among those of another, and it uses the Larch tools for specification and deduction to discharge these obligations in a natural and easy-to-read fashion. The approach is demonstrated by proving the behavior equivalence of two high-level specifications for a communication protocol.",
	isbn = "978-3-540-47787-7"
}

@article{song19_compc,
	author = {Song, Youngju and Cho, Minki and Kim, Dongjoo and Kim, Yonghyun and Kang, Jeehoon and Hur, Chung-Kil},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3371091},
	doi = {10.1145/3371091},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {verification,CompCert,compositional compiler},
	month = dec,
	number = {POPL},
	title = {{CompCertM}: {CompCert} With {C}-Assembly Linking and Lightweight Modular Verification},
	volume = {4},
	year = {2019}
}

@article{song23_condit_contex_refin,
	author = {Song, Youngju and Cho, Minki and Lee, Dongjae and Hur, Chung-Kil and Sammler, Michael and Dreyer, Derek},
	title = {Conditional Contextual Refinement},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571232},
	doi = {10.1145/3571232},
	abstract = {Much work in formal verification of low-level systems is based on one of two approaches: refinement or separation logic. These two approaches have complementary benefits: refinement supports the use of programs as specifications, as well as transitive composition of proofs, whereas separation logic supports conditional specifications, as well as modular ownership reasoning about shared state. A number of verification frameworks employ these techniques in tandem, but in all such cases the benefits of the two techniques remain separate. For example, in frameworks that use relational separation logic to prove contextual refinement, the relational separation logic judgment does not support transitive composition of proofs, while the contextual refinement judgment does not support conditional specifications. In this paper, we propose Conditional Contextual Refinement (or CCR, for short), the first verification system to not only combine refinement and separation logic in a single framework but also to truly marry them together into a unified mechanism enjoying all the benefits of refinement and separation logic simultaneously. Specifically, unlike in prior work, CCR’s refinement specifications are both conditional (with separation logic pre- and post-conditions) and transitively composable. We implement CCR in Coq and evaluate its effectiveness on a range of interesting examples.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {39},
	numpages = {31},
	keywords = {contextual refinement, verification, Coq, separation logic}
}

@inproceedings{spall22_forwar_build_system_formal,
	author = {Spall, Sarah and Mitchell, Neil and Tobin-Hochstadt, Sam},
	title = {Forward Build Systems, Formally},
	year = {2022},
	isbn = {9781450391825},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497775.3503687},
	doi = {10.1145/3497775.3503687},
	abstract = {Build systems are a fundamental part of software construction, but their correctness has received comparatively little attention, relative to more prominent parts of the toolchain. In this paper, we address the correctness of forward build systems, which automatically determine the dependency structure of the build, rather than having it specified by the programmer. We first define what it means for a forward build system to be correct---it must behave identically to simply executing the programmer-specified commands in order. Of course, realistic build systems avoid repeated work, stop early when possible, and run commands in parallel, and we prove that these optimizations, as embodied in the recent forward build system Rattle, preserve our definition of correctness. Along the way, we show that other forward build systems, such as Fabricate and Memoize, are also correct. We carry out all of our work in Agda, and describe in detail the assumptions underlying both Rattle itself and our modeling of it.},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {130–142},
	numpages = {13},
	keywords = {build systems, program verification, concurrency, functional programming, systems, verified applications, agda},
	location = {Philadelphia, PA, USA},
	series = {CPP 2022}
}

@article{spies21_trans_step_index_termin,
	abstract = {Step-indexed logical relations are an extremely useful technique for building operational-semantics-based models and program logics for realistic, richly-typed programming languages. They have proven to be indispensable for modeling features like higher-order state, which many languages support but which were difficult to accommodate using traditional denotational models. However, the conventional wisdom is that, because they only support reasoning about finite traces of computation, (unary) step-indexed models are only good for proving safety properties like “well-typed programs don’t go wrong”. There has consequently been very little work on using step-indexing to establish liveness properties, in particular termination. In this paper, we show that step-indexing can in fact be used to prove termination of well-typed programs—even in the presence of dynamically-allocated, shared, mutable, higher-order state—so long as one’s type system enforces disciplined use of such state. Specifically, we consider a language with asynchronous channels, inspired by promises in JavaScript, in which higher-order state is used to implement communication, and linearity is used to ensure termination. The key to our approach is to generalize from natural number step-indexing to transfinite step-indexing, which enables us to compute termination bounds for program expressions in a compositional way. Although transfinite step-indexing has been proposed previously, we are the first to apply this technique to reasoning about termination in the presence of higher-order state.},
	author = {Spies, Simon and Krishnaswami, Neel and Dreyer, Derek},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434294},
	doi = {10.1145/3434294},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {linear types,higher-order state,transfinite step-indexing,logical relations,asynchronous computation,termination,ordinals,channels,asynchronous programming},
	month = jan,
	number = {POPL},
	title = {Transfinite Step-Indexing for Termination},
	volume = {5},
	year = {2021}
}

@article{spies22_later_credit,
	author = {Spies, Simon and G\"{a}her, Lennard and Tassarotti, Joseph and Jung, Ralf and Krebbers, Robbert and Birkedal, Lars and Dreyer, Derek},
	title = {Later Credits: Resourceful Reasoning for the Later Modality},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3547631},
	doi = {10.1145/3547631},
	abstract = {In the past two decades, step-indexed logical relations and separation logics have both come to play a major role in semantics and verification research. More recently, they have been married together in the form of step-indexed separation logics like VST, iCAP, and Iris, which provide powerful tools for (among other things) building semantic models of richly typed languages like Rust. In these logics, propositions are given semantics using a step-indexed model, and step-indexed reasoning is reflected into the logic through the so-called “later” modality. On the one hand, this modality provides an elegant, high-level account of step-indexed reasoning; on the other hand, when used in sufficiently sophisticated ways, it can become a nuisance, turning perfectly natural proof strategies into dead ends. In this work, we introduce later credits, a new technique for escaping later-modality quagmires. By leveraging the second ancestor of these logics—separation logic—later credits turn “the right to eliminate a later” into an ownable resource, which is subject to all the traditional modular reasoning principles of separation logic. We develop the theory of later credits in the context of Iris, and present several challenging examples of proofs and proof patterns which were previously not possible in Iris but are now possible due to later credits.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {100},
	numpages = {29},
	keywords = {Separation logic, Iris, transfinite, step-indexing, later modality}
}

@article{spiwack22_linear_qualif_types,
	author = {Spiwack, Arnaud and Kiss, Csongor and Bernardy, Jean-Philippe and Wu, Nicolas and Eisenberg, Richard A.},
	title = {Linearly Qualified Types: Generic Inference for Capabilities and Uniqueness},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3547626},
	doi = {10.1145/3547626},
	abstract = {A linear parameter must be consumed exactly once in the body of its function. When declaring resources such as file handles and manually managed memory as linear arguments, a linear type system can verify that these resources are used safely. However, writing code with explicit linear arguments requires bureaucracy. This paper presents linear constraints, a front-end feature for linear typing that decreases the bureaucracy of working with linear types. Linear constraints are implicit linear arguments that are filled in automatically by the compiler. We present linear constraints as a qualified type system,together with an inference algorithm which extends GHC's existing constraint solver algorithm. Soundness of linear constraints is ensured by the fact that they desugar into Linear Haskell.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {95},
	numpages = {28},
	keywords = {linear logic, qualified types, Haskell, GHC, inference, linear types, constraints}
}

@inbook{sridharan13_alias_analy_objec_orien_progr,
	abstract = {We present a high-level survey of state-of-the-art alias analyses for object-oriented programs, based on a years-long effort developing industrial-strength static analyses for Java. We first present common variants of points-to analysis, including a discussion of key implementation techniques. We then describe flow-sensitive techniques based on tracking of access paths, which can yield greater precision for certain clients. We also discuss how whole-program alias analysis has become less useful for modern Java programs, due to increasing use of reflection in libraries and frameworks. We have found that for real-world programs, an under-approximate alias analysis based on access-path tracking often provides the best results for a variety of practical clients.},
	author = {Sridharan, Manu and Chandra, Satish and Dolby, Julian and Fink, Stephen J. and Yahav, Eran},
	editor = {Clarke, Dave and Noble, James and Wrigstad, Tobias},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	url = {https://doi.org/10.1007/978-3-642-36946-9_8},
	booktitle = {Aliasing in Object-Oriented Programming. Types, Analysis and Verification},
	doi = {10.1007/978-3-642-36946-9_8},
	isbn = {978-3-642-36946-9},
	keywords = {ownership types,memory aliasing},
	pages = {196--232},
	title = {Alias Analysis for Object-Oriented Programs},
	year = {2013}
}

@article{stanier13_inter_repres_imper_compil,
	abstract = {Compilers commonly translate an input program into an intermediate representation (IR) before optimizing it and generating code. Over time there have been a number of different approaches to designing and implementing IRs. Different IRs have varying benefits and drawbacks. In this survey, we highlight key developments in the area of IR for imperative compilers, group them by a taxonomy and timeline, and comment on the divide between academic research and real-world compiler technology. We conclude that mainstream compilers, especially in the multicore era, could benefit from further IR innovations.},
	author = {Stanier, James and Watson, Des},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/2480741.2480743},
	doi = {10.1145/2480741.2480743},
	issn = {0360-0300},
	journaltitle = {ACM Comput. Surv.},
	keywords = {compiler optimisation,SSA,gated-SSA,survey},
	month = jul,
	number = {3},
	title = {Intermediate Representations in Imperative Compilers: A Survey},
	volume = {45},
	year = {2013}
}

@inproceedings{stewart15_compos_compc,
	author = {Stewart, Gordon and Beringer, Lennart and Cuellar, Santiago and Appel, Andrew W.},
	location = {Mumbai, India},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/2676726.2676985},
	booktitle = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
	doi = {10.1145/2676726.2676985},
	isbn = {9781450333009},
	keywords = {verification,CompCert,compositional compiler},
	pages = {275--287},
	series = {POPL '15},
	title = {Compositional CompCert},
	year = {2015}
}

@inproceedings{sun16_desig,
	abstract = {High-level synthesis (HLS) promises high-quality hardware with minimal development effort. In this paper, we evaluate the current state-of-the-art in HLS and design techniques based on software references and architecture references. We present a software reference study developing a JPEG encoder from pre-existing software, and an architecture reference study developing an AES block encryption module from scratch in SystemC and SystemVerilog based on a desired architecture. Additionally, we develop micro-benchmarks to demonstrate best-practices in C coding styles that produce high-quality hardware with minimal development effort. Finally, we suggest language, tool, and methodology improvements to improve upon the current state-of-the-art in HLS.},
	author = {{Zelei Sun} and {Campbell}, K. and {Zuo}, W. and {Rupnow}, K. and {Gurumani}, S. and {Doucet}, F. and {Chen}, D.},
	url = {https://doi.org/10.1109/ASPDAC.2016.7428014},
	booktitle = {2016 21st Asia and South Pacific Design Automation Conference (ASP-DAC)},
	doi = {10.1109/ASPDAC.2016.7428014},
	issn = {2153-697X},
	keywords = {cryptography;hardware description languages;high level synthesis;image coding;high-quality hardware design;development effort budget;high-level synthesis;HLS;software references;architecture references;JPEG encoder;AES block encryption module;SystemC;SystemVerilog;C coding styles;Hardware;Software;Optimization;Transform coding;Discrete cosine transforms;Encoding;Computer architecture},
	month = jan,
	pages = {218--225},
	title = {Designing high-quality hardware on a development effort budget: A study of the current state of high-level synthesis},
	year = {2016}
}

@Software{synopsys23_v,
	urldate = {2023-12-20},
	url = {https://www.synopsys.com/verification/static-and-formal-verification/vc-formal.html},
	year = {2023},
	title = {VC Formal: Leading Formal Innovations},
	author = {Synopsys}
}

@article{takach16_high_level_synth,
	abstract = {The author provides a status of the significant current industrial relevance of high-level synthesis and how it is advantageous, in particular, in a complex design matrix where verification, power, performance, area optimization as well as design reuse play a key role.},
	author = {{Takach}, A.},
	url = {https://doi.org/10.1109/MDAT.2016.2544850},
	doi = {10.1109/MDAT.2016.2544850},
	issn = {2168-2364},
	journaltitle = {IEEE Design Test},
	keywords = {high level synthesis;logic design;system-on-chip;high-level synthesis;area optimization;design reuse;System-on-chip;Circuit synthesis;Algorithm design and analysis;Digital signal processing;Sensors;Hardware accelerators;Computational modeling},
	month = jun,
	number = {3},
	pages = {116--124},
	title = {High-Level Synthesis: Status, Trends, and Future Directions},
	volume = {33},
	year = {2016}
}

@InProceedings{tan15_mappin_lut_fpgas,
	keywords = {delay prediction},
	author = {Tan, Mingxing and Dai, Steve and Gupta, Udit and Zhang, Zhiru},
	title = {Mapping-aware constrained scheduling for {LUT-based FPGAs}},
	booktitle = {Proceedings of the 23rd {ACM}/{SIGDA} International
                  Symposium on Field Programmable Gate Arrays},
	year = 2015,
	address = {Monterey, CA},
	month = feb,
	pages = {190--9}
}

@article{tan22_solty,
	author = {Tan, Bryan and Mariano, Benjamin and Lahiri, Shuvendu K. and Dillig, Isil and Feng, Yu},
	title = {SolType: Refinement Types for Arithmetic Overflow in Solidity},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498665},
	doi = {10.1145/3498665},
	abstract = {As smart contracts gain adoption in financial transactions, it becomes increasingly important to ensure that they are free of bugs and security vulnerabilities. Of particular relevance in this context are arithmetic overflow bugs, as integers are often used to represent financial assets like account balances. Motivated by this observation, this paper presents SolType, a refinement type system for Solidity that can be used to prevent arithmetic over- and under-flows in smart contracts. SolType allows developers to add refinement type annotations and uses them to prove that arithmetic operations do not lead to over- and under-flows. SolType incorporates a rich vocabulary of refinement terms that allow expressing relationships between integer values and aggregate properties of complex data structures. Furthermore, our implementation, called Solid, incorporates a type inference engine and can automatically infer useful type annotations, including non-trivial contract invariants. To evaluate the usefulness of our type system, we use Solid to prove arithmetic safety of a total of 120 smart contracts. When used in its fully automated mode (i.e., using Solid's type inference capabilities), Solid is able to eliminate 86.3% of redundant runtime checks used to guard against overflows. We also compare Solid against a state-of-the-art arithmetic safety verifier called VeriSmart and show that Solid has a significantly lower false positive rate, while being significantly faster in terms of verification time.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {4},
	numpages = {29},
	keywords = {smart contracts, integer overflow, refinement type inference}
}

@article{tarjan72_depth_first_searc_linear_graph_algor,
	author = {Tarjan, Robert},
	publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
	url = {https://doi.org/10.1137/0201010},
	doi = {10.1137/0201010},
	journaltitle = {{SIAM} Journal on Computing},
	month = jun,
	number = {2},
	pages = {146--160},
	title = {Depth-First Search and Linear Graph Algorithms},
	volume = {1},
	year = {1972}
}

@inproceedings{tarjan73_testin_flow_graph_reduc,
	abstract = {Many problems in program optimization have been solved by applying a technique called interval analysis to the flow graph of the program. A flow graph which is susceptible to this type of analysis is called reducible. This paper describes an algorithm for testing whether a flow graph is reducible. The algorithm uses depth-first search to reveal the structure of the flow graph and a good method for computing disjoint set unions to determine reducibility from the search information. When the algorithm is implemented on a random access computer, it requires O(E log* E) time to analyze a graph with E edges, where log* x = min{i/logix≤1}. The time bound compares favorably with the O(E log E) bound of a previously known algorithm.},
	author = {Tarjan, Robert},
	location = {Austin, Texas, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/800125.804040},
	booktitle = {Proceedings of the Fifth Annual ACM Symposium on Theory of Computing},
	doi = {10.1145/800125.804040},
	isbn = {9781450374309},
	keywords = {Code optimization,Tree,Reducibility,Flow graph,Complexity,Directed graph,Algorithm,Interval analysis,Set union algorithm,Depth-first search,Flow analysis,Program optimization},
	pages = {96--107},
	series = {STOC '73},
	title = {Testing Flow Graph Reducibility},
	year = {1973}
}

@article{tarjan76_edge,
	author = {Tarjan, Robert Endre},
	publisher = {Springer Science and Business Media {LLC}},
	url = {https://doi.org/10.1007/bf00268499},
	doi = {10.1007/bf00268499},
	journaltitle = {Acta Informatica},
	keywords = {loop identification},
	number = {2},
	pages = {171--185},
	title = {Edge-disjoint spanning trees and depth-first search},
	volume = {6},
	year = {1976}
}

@article{tarjan81_fast_algor_solvin_path_probl,
	author = {Tarjan, Robert Endre},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/322261.322273},
	doi = {10.1145/322261.322273},
	issn = {0004-5411},
	journaltitle = {J. ACM},
	keywords = {gated-SSA},
	month = jul,
	number = {3},
	pages = {594--614},
	title = {Fast Algorithms for Solving Path Problems},
	volume = {28},
	year = {1981}
}

@article{tarjan81_unified_approac_path_probl,
	author = {Tarjan, Robert Endre},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/322261.322272},
	doi = {10.1145/322261.322272},
	issn = {0004-5411},
	journaltitle = {J. ACM},
	keywords = {gated-SSA},
	month = jul,
	number = {3},
	pages = {577--593},
	title = {A Unified Approach to Path Problems},
	volume = {28},
	year = {1981}
}

@inproceedings{tate09_equal_satur,
	abstract = {Optimizations in a traditional compiler are applied sequentially, with each optimization destructively modifying the program to produce a transformed program that is then passed to the next optimization. We present a new approach for structuring the optimization phase of a compiler. In our approach, optimizations take the form of equality analyses that add equality information to a common intermediate representation. The optimizer works by repeatedly applying these analyses to infer equivalences between program fragments, thus saturating the intermediate representation with equalities. Once saturated, the intermediate representation encodes multiple optimized versions of the input program. At this point, a profitability heuristic picks the final optimized program from the various programs represented in the saturated representation. Our proposed way of structuring optimizers has a variety of benefits over previous approaches: our approach obviates the need to worry about optimization ordering, enables the use of a global optimization heuristic that selects among fully optimized programs, and can be used to perform translation validation, even on compilers other than our own. We present our approach, formalize it, and describe our choice of intermediate representation. We also present experimental results showing that our approach is practical in terms of time and space overhead, is effective at discovering intricate optimization opportunities, and is effective at performing translation validation for a realistic optimizer.},
	author = {Tate, Ross and Stepp, Michael and Tatlock, Zachary and Lerner, Sorin},
	location = {Savannah, GA, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/1480881.1480915},
	booktitle = {Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
	doi = {10.1145/1480881.1480915},
	isbn = {9781605583792},
	keywords = {equality reasoning,intermediate representation,compiler optimization},
	pages = {264--276},
	series = {POPL '09},
	title = {Equality Saturation: A New Approach to Optimization},
	year = {2009}
}

@thesis{teica01_formal_rtl,
	author = {Teica, Elena},
	institution = {University of Cincinnati},
	title = {Formal correctness and completeness for a set of uninterpreted {RTL} transformations},
	type = {phdthesis},
	year = {2001}
}

@inproceedings{teifel04_highl_pipel_async_fpgas,
	author = {Teifel, John and Manohar, Rajit},
	title = {Highly Pipelined Asynchronous FPGAs},
	year = {2004},
	isbn = {1581138296},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/968280.968300},
	doi = {10.1145/968280.968300},
	abstract = {We present the design of a high-performance, highly pipelined asynchronous FPGA. We describe a very fine-grain pipelined logic block and routing interconnect architecture, and show how asynchronous logic can efficiently take advantage of this large amount of pipelining. Our FPGA, which does not use a clock to sequence computations, automatically self-pipelines" its logic without the designer needing to be explicitly aware of all pipelining details. This property makes our FPGA ideal for throughput-intensive applications and we require minimal place and route support to achieve good performance. Benchmark circuits taken from both the asynchronous and clocked design communities yield throughputs in the neighborhood of 300--400 MHz in a TSMC 0.25m process and 500--700 MHz in a TSMC 0.18m process.},
	booktitle = {Proceedings of the 2004 ACM/SIGDA 12th International Symposium on Field Programmable Gate Arrays},
	pages = {133–142},
	numpages = {10},
	keywords = {concurrency, hardware pipelining, FPGA, asynchronous},
	location = {Monterey, California, USA},
	series = {FPGA '04}
}

@report{telford96_abstr_inter_const_type_theor,
	abstract = {Constructive type theories, such as that of Martin-Lof, allow program construction and verification to take place within a single system: proofs may be read as programs and propositions as types. However, parts of proofs may be seen to be irrelevant from a computational viewpoint. We show how a form of abstract interpretation may be used to detect computational redundancy in a functional language based upon Martin-Lof's type theory. Thus, without making any alteration to the system of type theory itself, we present an automatic way of discovering and removing such redundancy. We also note that the strong normalisation property of type theory means that proofs of correctness of the abstract interpretation are simpler, being based upon a set-theoretic rather than a domain-theoretic semantics. Keywords: Type theory, functional programming, computational redundancy, abstract interpretation.},
	author = {Telford, Alastair J. and Thompson, Simon},
	institution = {University of Kent, Computing Laboratory},
	location = {University of Kent, Canterbury, UK},
	publisher = {UKC},
	url = {https://kar.kent.ac.uk/21326/},
	month = oct,
	title = {Abstract Interpretation of Constructive Type Theory},
	type = {Technical report},
	year = {1996}
}

@inproceedings{tetzlaff08_erweit_hyper_sched_vliw_prozes,
	author = {Tetzlaff, Dirk},
	publisher = {GI},
	booktitle = {Informatiktage 2008. Fachwissenschaftlicher Informatik-Kongress, 14. und 15. März 2008, {B-IT} Bonn-Aachen International Center for Information Technology in Bonn},
	keywords = {hyperblocks,static scheduling},
	pages = {111--114},
	series = {{LNI}},
	title = {Erweitertes Hyperblock-Scheduling für VLIW-Prozessoren},
	volume = {{S-6}},
	year = {2008}
}

@InProceedings{thierry-mieg04_symbol_symbol_state_space_repres,
	keywords = {symbolic execution, equivalence checking},
	author = "Thierry-Mieg, Yann
and Ili{\'e}, Jean-Michel
and Poitrenaud, Denis",
	editor = "de Frutos-Escrig, David
and N{\'u}{\~{n}}ez, Manuel",
	title = "A Symbolic Symbolic State Space Representation",
	booktitle = "Formal Techniques for Networked and Distributed Systems -- FORTE 2004",
	year = "2004",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "276--291",
	abstract = "Symmetry based approaches are known to attack the state space explosion problem encountered during the analysis of distributed systems. In another way, BDD-like encodings enable the management of huge data sets. In this paper, we show how to benefit from both approaches automatically. Hence, a quotient set is built from a coloured Petri net description modeling the system. The reachability set is managed under some explicit symbolic operations. Also, data representations are managed symbolically based on a recently introduced data structure, called Data Decisions Diagrams, that allow flexible definition of application specific operators. Performances yielded by our prototype are reported in the paper.",
	isbn = "978-3-540-30232-2"
}

@article{thokair23_dynam_race_detec_o_sampl,
	author = {Thokair, Mosaad Al and Zhang, Minjian and Mathur, Umang and Viswanathan, Mahesh},
	title = {Dynamic Race Detection with O(1) Samples},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571238},
	doi = {10.1145/3571238},
	abstract = {Happens before-based dynamic analysis is the go-to technique for detecting data races in large scale software projects due to the absence of false positive reports. However, such analyses are expensive since they employ expensive vector clock updates at each event, rendering them usable only for in-house testing. In this paper, we present a sampling-based, randomized race detector that processes only constantly many events of the input trace even in the worst case. This is the first sub-linear time (i.e., running in o(n) time where n is the length of the trace) dynamic race detection algorithm; previous sampling based approaches like run in linear time (i.e., O(n)). Our algorithm is a property tester for -race detection — it is sound in that it never reports any false positive, and on traces that are far, with respect to hamming distance, from any race-free trace, the algorithm detects an -race with high probability. Our experimental evaluation of the algorithm and its comparison with state-of-the-art deterministic and sampling based race detectors shows that the algorithm does indeed have significantly low running time, and detects races quite often.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {45},
	numpages = {30},
	keywords = {Dynamic program analysis, Concurrency - Shared memory, Happens-before race detection, Property testing}
}

@report{thost_direc_acycl_graph_neural_networ,
	abstract = {Graph-structured data ubiquitously appears in science and engineering. Graph neural networks (GNNs) are designed to exploit the relational inductive bias exhibited in graphs; they have been shown to outperform other forms of neural networks in scenarios where structure information supplements node features. The most common GNN architecture aggregates information from neighborhoods based on message passing. Its generality has made it broadly applicable. In this paper, we focus on a special, yet widely used, type of graphs-DAGs-and inject a stronger inductive bias-partial ordering-into the neural network design. We propose the directed acyclic graph neural network, DAGNN, an architecture that processes information according to the flow defined by the partial order. DAGNN can be considered a framework that entails earlier works as special cases (e.g., models for trees and models updating node representations recurrently), but we identify several crucial components that prior architectures lack. We perform comprehensive experiments, including ablation studies, on representative DAG datasets (i.e., source code, neural architectures, and probabilistic graphical models) and demonstrate the superiority of DAGNN over simpler DAG architectures as well as general graph architectures.},
	author = {Thost, Veronika and Chen, Jie},
	url = {https://github.com/vthost/DAGNN.},
	eprint = {2101.07965v3},
	eprinttype = {arXiv},
	file = {::},
	title = {Directed Acyclic Graph Neural Networks},
	type = {techreport},
	year = {2021}
}

@article{tiemeyer19_crest,
	author = {Andreas Tiemeyer and Tom Melham and Daniel Kroening and John O'Leary},
	title = {{CREST:} Hardware Formal Verification with {ANSI-C} Reference Specifications},
	journal = {CoRR},
	volume = {abs/1908.01324},
	year = {2019},
	url = {http://arxiv.org/abs/1908.01324},
	eprinttype = {arXiv},
	eprint = {1908.01324},
	timestamp = {Fri, 09 Aug 2019 12:15:56 +0200},
	biburl = {https://dblp.org/rec/journals/corr/abs-1908-01324.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{tirumalai90_paral,
	author = {Tirumalai, Parthasarathy and Lee, Meng and Schlansker, Michael},
	publisher = {Hewlett-Packard Laboratories},
	title = {Parallelization of loops with exits on pipelined architectures},
	year = {1990}
}

@inproceedings{tosun05_ilp,
	abstract = {Reliability decisions taken early in system design can bring significant benefits in terms of design quality. This paper presents a 0-1 integer linear programming (ILP) formulation for reliability-oriented high-level synthesis that addresses the soft error problem. The proposed approach tries to maximize reliability of the design while observing the bounds on area and performance, and makes use of our reliability characterization of hardware components such as adders and multipliers. We implemented the proposed approach, performed experiments with several example designs, and compared the results with those obtained by a prior proposal. Our results show that incorporating reliability as a first-class metric during high-level synthesis brings significant improvements on the overall design reliability.},
	author = {Tosun, S. and Ozturk, O. and Mansouri, N. and Arvas, E. and Kandemir, M. and Xie, Y. and Hung, W.-L.},
	booktitle = {Sixth international symposium on quality electronic design (isqed'05)},
	doi = {10.1109/ISQED.2005.15},
	issn = {1948-3295},
	keywords = {,reliability,high-level synthesis},
	month = mar,
	pages = {364--369},
	title = {An ILP formulation for reliability-oriented high-level synthesis},
	year = {2005}
}

@inproceedings{trabish18_chopp_symbol_execut,
	author = {Trabish, David and Mattavelli, Andrea and Rinetzky, Noam and Cadar, Cristian},
	title = {Chopped Symbolic Execution},
	year = {2018},
	isbn = {9781450356381},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3180155.3180251},
	doi = {10.1145/3180155.3180251},
	abstract = {Symbolic execution is a powerful program analysis technique that systematically explores multiple program paths. However, despite important technical advances, symbolic execution often struggles to reach deep parts of the code due to the well-known path explosion problem and constraint solving limitations.In this paper, we propose chopped symbolic execution, a novel form of symbolic execution that allows users to specify uninteresting parts of the code to exclude during the analysis, thus only targeting the exploration to paths of importance. However, the excluded parts are not summarily ignored, as this may lead to both false positives and false negatives. Instead, they are executed lazily, when their effect may be observable by code under analysis. Chopped symbolic execution leverages various on-demand static analyses at runtime to automatically exclude code fragments while resolving their side effects, thus avoiding expensive manual annotations and imprecision.Our preliminary results show that the approach can effectively improve the effectiveness of symbolic execution in several different scenarios, including failure reproduction and test suite augmentation.},
	booktitle = {Proceedings of the 40th International Conference on Software Engineering},
	pages = {350–360},
	numpages = {11},
	keywords = {static analysis, program slicing, symbolic execution, value summaries, hyperblocks},
	location = {Gothenburg, Sweden},
	series = {ICSE '18}
}

@misc{trabish23_state_mergin_quant_symbol_execut,
	keywords = {value summaries, hyperblocks, symbolic execution},
	title = {State Merging with Quantifiers in Symbolic Execution},
	author = {David Trabish and Noam Rinetzky and Sharon Shoham and Vaibhav Sharma},
	year = {2023},
	eprint = {2308.12068},
	archivePrefix = {arXiv},
	primaryClass = {cs.SE}
}

@inproceedings{trifunovic10_graph_two_years_after,
	author = {Trifunovic, Konrad and Cohen, Albert and Edelsohn, David and Li, Feng and Grosser, Tobias and Jagasia, Harsha and Ladelsky, Razya and Pop, Sebastian and Sjödin, Jan and Upadrasta, Ramakrishna},
	location = {Pisa, Italy},
	url = {https://hal.inria.fr/inria-00551516},
	booktitle = {{GCC Research Opportunities Workshop (GROW'10)}},
	file = {https://hal.inria.fr/inria-00551516/file/main.pdf},
	month = jan,
	title = {{GRAPHITE Two Years After: First Lessons Learned From Real-World Polyhedral Compilation}},
	year = {2010}
}

@incollection{tripp02_sea_cucum,
	author = {Tripp, Justin L. and Jackson, Preston A. and Hutchings, Brad L.},
	publisher = {Springer Berlin Heidelberg},
	url = {https://doi.org/10.1007/3-540-46117-5_90},
	booktitle = {Lecture Notes in Computer Science},
	doi = {10.1007/3-540-46117-5_90},
	keywords = {high-level synthesis},
	pages = {875--885},
	title = {Sea Cucumber: A Synthesizing Compiler for {FPGAs}},
	year = {2002}
}

@article{tripp07_triden,
	abstract = {Unlocking the potential of field-programmable gate arrays requires compilers that translate algorithmic high-level language code into hardware circuits. The Trident open source compiler translates C code to a hardware circuit description, providing designers with extreme flexibility in prototyping reconfigurable supercomputers},
	author = {{Tripp}, J. L. and {Gokhale}, M. B. and {Peterson}, K. D.},
	url = {https://doi.org/10.1109/MC.2007.107},
	doi = {10.1109/MC.2007.107},
	issn = {1558-0814},
	journaltitle = {Computer},
	keywords = {high-level synthesis,FPGA},
	month = mar,
	number = {3},
	pages = {28--37},
	title = {Trident: From High-Level Language To Hardware Circuitry},
	volume = {40},
	year = {2007}
}

@inproceedings{tristan08_formal_verif_trans_valid,
	author = {Tristan, Jean-Baptiste and Leroy, Xavier},
	location = {San Francisco, California, USA},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
	doi = {10.1145/1328438.1328444},
	isbn = {9781595936899},
	keywords = {coq,translation validation,CompCert,static scheduling},
	pages = {17--27},
	series = {POPL '08},
	title = {Formal Verification of Translation Validators: A Case Study on Instruction Scheduling Optimizations},
	year = {2008}
}

@inproceedings{tristan09_verif_valid_lazy_code_motion,
	author = {Tristan, Jean-Baptiste and Leroy, Xavier},
	location = {Dublin, Ireland},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/1542476.1542512},
	booktitle = {Proceedings of the 30th ACM SIGPLAN Conference on Programming Language Design and Implementation},
	doi = {10.1145/1542476.1542512},
	isbn = {9781605583921},
	keywords = {translation validation,code motion,coq,CompCert,compiler optimisation},
	pages = {316--326},
	series = {PLDI '09},
	title = {Verified Validation of Lazy Code Motion},
	year = {2009}
}

@inproceedings{tristan10_simpl_verif_valid_softw_pipel,
	author = {Tristan, Jean-Baptiste and Leroy, Xavier},
	location = {Madrid, Spain},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/1706299.1706311},
	booktitle = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
	doi = {10.1145/1706299.1706311},
	isbn = {9781605584799},
	keywords = {symbolic execution,coq,verification,translation validation,loop scheduling,compiler optimisation,software pipelining},
	pages = {83--92},
	series = {POPL '10},
	title = {A Simple, Verified Validator for Software Pipelining},
	year = {2010}
}

@inproceedings{tristan11_evaluat_value_graph_trans_valid_llvm,
	abstract = {Translation validators are static analyzers that attempt to verify that program transformations preserve semantics. Normalizing translation validators do so by trying to match the value-graphs of an original function and its transformed counterpart. In this paper, we present the design of such a validator for LLVM's intra-procedural optimizations, a design that does not require any instrumentation of the optimizer, nor any rewriting of the source code to compile, and needs to run only once to validate a pipeline of optimizations. We present the results of our preliminary experiments on a set of benchmarks that include GCC, a perl interpreter, SQLite3, and other C programs.},
	author = {Tristan, Jean-Baptiste and Govereau, Paul and Morrisett, Greg},
	location = {San Jose, California, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/1993498.1993533},
	booktitle = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation},
	doi = {10.1145/1993498.1993533},
	isbn = {9781450306638},
	keywords = {llvm,symbolic evaluation,optimization,translation validation,gated-SSA},
	pages = {295--305},
	series = {PLDI '11},
	title = {Evaluating Value-Graph Translation Validation for LLVM},
	year = {2011}
}

@misc{tristan14_augur,
	author = {Tristan, Jean-Baptiste and Huang, Daniel and Tassarotti, Joseph and Pocock, Adam and Green, Stephen J. and au2, Guy L. Steele Jr},
	eprint = {1312.3613},
	eprintclass = {stat.ML},
	eprinttype = {arXiv},
	title = {Augur: a Modeling Language for Data-Parallel Probabilistic Inference},
	year = {2014}
}

@misc{tristan20_verif_ml_system_repar,
	author = {Tristan, Jean-Baptiste and Tassarotti, Joseph and Vajjha, Koundinya and Wick, Michael L. and Banerjee, Anindya},
	eprint = {2007.06776},
	eprintclass = {cs.LG},
	eprinttype = {arXiv},
	title = {Verification of ML Systems via Reparameterization},
	year = {2020}
}

@article{tsukada22_softw_model_check_cyclic_proof_searc,
	abstract = {This paper shows that a variety of software model-checking algorithms can be seen as proof-search strategies for a non-standard proof system, known as a cyclic proof system. Our use of the cyclic proof system as a logical foundation of software model checking enables us to compare different algorithms, to reconstruct well-known algorithms from a few simple principles, and to obtain soundness proofs of algorithms for free. Among others, we show the significance of a heuristics based on a notion that we call maximal conservativity; this explains the cores of important algorithms such as property-directed reachability (PDR) and reveals a surprising connection to an efficient solver of games over infinite graphs that was not regarded as a kind of PDR.},
	author = {Tsukada, Takeshi and Unno, Hiroshi},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3498725},
	doi = {10.1145/3498725},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {software model-checking,cyclic proof system,fixed-point logic,property-directed reachability},
	month = jan,
	number = {POPL},
	title = {Software Model-Checking as Cyclic-Proof Search},
	volume = {6},
	year = {2022}
}

@inproceedings{tu95_effic_build_placin_gatin_funct,
	abstract = {In this paper, we present an almost-linear time algorithm for constructing Gated Single Assignment (GSA), which is SSA augmented with gating functions at ø-nodes. The gating functions specify the control dependences for each reaching definition at a ø-node. We introduce a new concept of gating path, which is path in the control flow graph from the immediate dominator u of a node v to v, such that every node in the path is dominated by u. Previous algorithms start with ø-function placement, and then traverse the control flow graph to compute the gating functions. By formulating the problem into gating path construction, we are able to identify not only a ø-node, but also a gating path expression which defines a gating function for the ø-node.},
	author = {Tu, Peng and Padua, David},
	location = {La Jolla, California, USA},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the ACM SIGPLAN 1995 Conference on Programming Language Design and Implementation},
	doi = {10.1145/207110.207115},
	isbn = {0897916972},
	keywords = {gated-SSA,SSA},
	pages = {47--55},
	series = {PLDI '95},
	title = {Efficient Building and Placing of Gating Functions},
	year = {1995}
}

@inproceedings{tu95_gated_ssa_based_deman_driven,
	author = {Tu, Peng and Padua, David},
	location = {Barcelona, Spain},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 9th International Conference on Supercomputing},
	doi = {10.1145/224538.224648},
	isbn = {0897917286},
	keywords = {gated-SSA,SSA,compiler optimisation},
	pages = {414--423},
	series = {ICS '95},
	title = {Gated SSA-Based Demand-Driven Symbolic Analysis for Parallelizing Compilers},
	year = {1995}
}

@article{ullrich22_do_unchain,
	author = {Ullrich, Sebastian and de Moura, Leonardo},
	title = {‘Do’ Unchained: Embracing Local Imperativity in a Purely Functional Language (Functional Pearl)},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3547640},
	doi = {10.1145/3547640},
	abstract = {Purely functional programming languages pride themselves with reifying effects that are implicit in imperative languages into reusable and composable abstractions such as monads. This reification allows for more exact control over effects as well as the introduction of new or derived effects. However, despite libraries of more and more powerful abstractions over effectful operations being developed, syntactically the common 'do' notation still lags behind equivalent imperative code it is supposed to mimic regarding verbosity and code duplication. In this paper, we explore extending 'do' notation with other imperative language features that can be added to simplify monadic code: local mutation, early return, and iteration. We present formal translation rules that compile these features back down to purely functional code, show that the generated code can still be reasoned over using an implementation of the translation in the Lean 4 theorem prover, and formally prove the correctness of the translation rules relative to a simple static and dynamic semantics in Lean.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {109},
	numpages = {28},
	keywords = {interactive theorem proving, functional programming, Lean}
}

@article{unno23_modul_primal_dual_fixpoin_logic,
	author = {Unno, Hiroshi and Terauchi, Tachio and Gu, Yu and Koskinen, Eric},
	title = {Modular Primal-Dual Fixpoint Logic Solving for Temporal Verification},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571265},
	doi = {10.1145/3571265},
	abstract = {We present a novel approach to deciding the validity of formulas in first-order fixpoint logic with background theories and arbitrarily nested inductive and co-inductive predicates defining least and greatest fixpoints. Our approach is constraint-based, and reduces the validity checking problem of the given first-order-fixpoint logic formula (formally, an instance in a language called µCLP) to a constraint satisfaction problem for a recently introduced predicate constraint language. Coupled with an existing sound-and-relatively-complete solver for the constraint language, this novel reduction alone already gives a sound and relatively complete method for deciding µCLP validity, but we further improve it to a novel modular primal-dual method. The key observations are (1) µCLP is closed under complement such that each (co-)inductive predicate in the original primal instance has a corresponding (co-)inductive predicate representing its complement in the dual instance obtained by taking the standard De Morgan’s dual of the primal instance, and (2) partial solutions for (co-)inductive predicates synthesized during the constraint solving process of the primal side can be used as sound upper-bounds of the corresponding (co-)inductive predicates in the dual side, and vice versa. By solving the primal and dual problems in parallel and exchanging each others’ partial solutions as sound bounds, the two processes mutually reduce each others’ solution spaces, thus enabling rapid convergence. The approach is also modular in that the bounds are synthesized and exchanged at granularity of individual (co-)inductive predicates. We demonstrate the utility of our novel fixpoint logic solving by encoding a wide variety of temporal verification problems in µCLP, including termination/non-termination, LTL, CTL, and even the full modal µ-calculus model checking of infinite state programs. The encodings exploit the modularity in both the program and the property by expressing each loops and (recursive) functions in the program and sub-formulas of the property as individual (possibly nested) (co-)inductive predicates. Together with our novel modular primal-dual µCLP solving, we obtain a novel approach to efficiently solving a wide range of temporal verification problems.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {72},
	numpages = {30},
	keywords = {constraint logic programming, temporal verification, fixpoint logics}
}

@InProceedings{ustun20_accur_fpga_hls,
	keywords = {delay prediction},
	author = {Ustun, Ecenur and Deng, Chenhui and Pal, Debjit and Li, Zhijing and Zhang, Zhiru},
	title = {Accurate operation delay prediction for {FPGA HLS} using graph neural networks},
	booktitle = {Proceedings of the 39th International Conference on Computer-Aided Design},
	year = 2020,
	address = {Virtual},
	month = nov,
	pages = {1--9}
}

@inproceedings{valiron16_gener_rever_circuit_higher_order_funct_progr,
	author = {Valiron, Benoît},
	editor = {Devitt, Simon and Lanese, Ivan},
	location = {Cham},
	publisher = {Springer International Publishing},
	booktitle = {Reversible Computation},
	isbn = {978-3-319-40578-0},
	pages = {289--306},
	title = {Generating Reversible Circuits from Higher-Order Functional Programs},
	year = {2016}
}

@article{valliappan22_normal_fitch_style_modal_calcul,
	author = {Valliappan, Nachiappan and Ruch, Fabian and Tom\'{e} Corti\~{n}as, Carlos},
	title = {Normalization for Fitch-Style Modal Calculi},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3547649},
	doi = {10.1145/3547649},
	abstract = {Fitch-style modal lambda calculi enable programming with necessity modalities in a typed lambda calculus by extending the typing context with a delimiting operator that is denoted by a lock. The addition of locks simplifies the formulation of typing rules for calculi that incorporate different modal axioms, but each variant demands different, tedious and seemingly ad hoc syntactic lemmas to prove normalization. In this work, we take a semantic approach to normalization, called normalization by evaluation (NbE), by leveraging the possible-world semantics of Fitch-style calculi to yield a more modular approach to normalization. We show that NbE models can be constructed for calculi that incorporate the K, T and 4 axioms of modal logic, as suitable instantiations of the possible-world semantics. In addition to existing results that handle 𝛽-equivalence, our normalization result also considers 𝜂-equivalence for these calculi. Our key results have been mechanized in the proof assistant Agda. Finally, we showcase several consequences of normalization for proving meta-theoretic properties of Fitch-style calculi as well as programming-language applications based on different interpretations of the necessity modality.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {118},
	numpages = {27},
	keywords = {Fitch-style lambda calculi, Possible-world semantics, Normalization by Evaluation}
}

@inproceedings{vanderleest20_measur_impac_inter_chann_multic_avion,
	keywords = {RTOS, avionics, predictable execution},
	doi = {10.1109/dasc50938.2020.9256647},
	url = {https://doi.org/10.1109%2Fdasc50938.2020.9256647},
	year = 2020,
	month = {oct},
	publisher = {{IEEE}},
	author = {Steven H. VanderLeest and Samuel R. Thompson},
	title = {Measuring the Impact of Interference Channels on Multicore Avionics},
	booktitle = {2020 {AIAA}/{IEEE} 39th Digital Avionics Systems Conference ({DASC})}
}

@inproceedings{varma10_easin,
	author = {{Varma}, D. and {Mackay}, D. and {Thiruchelvam}, P.},
	url = {https://doi.org/10.1109/VTS.2010.5469565},
	booktitle = {2010 28th VLSI Test Symposium (VTS)},
	doi = {10.1109/VTS.2010.5469565},
	keywords = {C language;circuit complexity;formal specification;formal verification;high level synthesis;scheduling;high level synthesis;verification complexity;design description;RTL description;C/C++;System C;C based languages;scheduling;AutoPilot;HLS tool;C-based design functionality;power constraints;formal verification;High level synthesis;Power generation;Automatic testing;System testing;Pipeline processing;Resource management;Constraint optimization;Design optimization;Timing;Formal verification},
	month = apr,
	pages = {253--254},
	title = {Easing the verification bottleneck using high level synthesis},
	year = {2010}
}

@article{vasilenko22_safe_coupl,
	author = {Vasilenko, Elizaveta and Vazou, Niki and Barthe, Gilles},
	title = {Safe Couplings: Coupled Refinement Types},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3547643},
	doi = {10.1145/3547643},
	abstract = {We enhance refinement types with mechanisms to reason about relational properties of probabilistic computations. Our mechanisms, which are inspired from probabilistic couplings, are applicable to a rich set of probabilistic properties, including expected sensitivity, which ensures that the distance between outputs of two probabilistic computations can be controlled from the distance between their inputs. We implement our mechanisms in the type system of Liquid Haskell and we use them to formally verify Haskell implementations of two classic machine learning algorithms: Temporal Difference (TD) reinforcement learning and stochastic gradient descent (SGD). We formalize a fragment of our system for discrete distributions and we prove soundness with respect to a set-theoretical semantics.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {112},
	numpages = {29},
	keywords = {program verification, refinement types, Haskell, relational types}
}

@inproceedings{venkataramani07_operat,
	author = {Venkataramani, Girish and Goldstein, Seth C.},
	booktitle = {2007 IEEE/ACM International Conference on Computer-Aided Design},
	doi = {10.1109/ICCAD.2007.4397305},
	keywords = {operation chaining},
	pages = {442--449},
	title = {Operation chaining asynchronous pipelined circuits},
	year = {2007}
}

@article{vilhena21_separ_logic_effec_handl,
	abstract = {User-defined effects and effect handlers are advertised and advocated as a relatively easy-to-understand and modular approach to delimited control. They offer the ability of suspending and resuming a computation and allow information to be transmitted both ways between the computation, which requests a certain service, and the handler, which provides this service. Yet, a key question remains, to this day, largely unanswered: how does one modularly specify and verify programs in the presence of both user-defined effect handlers and primitive effects, such as heap-allocated mutable state? We answer this question by presenting a Separation Logic with built-in support for effect handlers, both shallow and deep. The specification of a program fragment includes a protocol that describes the effects that the program may perform as well as the replies that it can expect to receive. The logic allows local reasoning via a frame rule and a bind rule. It is based on Iris and inherits all of its advanced features, including support for higher-order functions, user-defined ghost state, and invariants. We illustrate its power via several case studies, including (1) a generic formulation of control inversion, which turns a producer that ``pushes'' elements towards a consumer into a producer from which one can ``pull'' elements on demand, and (2) a simple system for cooperative concurrency, where several threads execute concurrently, can spawn new threads, and communicate via promises.},
	author = {de Vilhena, Paulo Emı́lio and Pottier, François},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434314},
	doi = {10.1145/3434314},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {separation logic,program verification,effect handlers},
	month = jan,
	number = {POPL},
	title = {A Separation Logic for Effect Handlers},
	volume = {5},
	year = {2021}
}

@inproceedings{vindum22_mechan_verif_fine_grain_concur,
	author = {Vindum, Simon Friis and Frumin, Dan and Birkedal, Lars},
	title = {Mechanized Verification of a Fine-Grained Concurrent Queue from Meta’s Folly Library},
	year = {2022},
	isbn = {9781450391825},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497775.3503689},
	doi = {10.1145/3497775.3503689},
	abstract = {We present the first formal specification and verification of the fine-grained concurrent multi-producer-multi-consumer queue algorithm from Meta’s C++ library Folly of core infrastructure components. The queue is highly optimized, practical, and used by Meta in production where it scales to thousands of consumer and producer threads. We present an implementation of the algorithm in an ML-like language and formally prove that it is a contextual refinement of a simple coarse-grained queue (a property that implies that the MPMC queue is linearizable). We use the ReLoC relational logic and the Iris program logic to carry out the proof and to mechanize it in the Coq proof assistant. The MPMC queue is implemented using three modules, and our proof is similarly modular. By using ReLoC and Iris’s support for modular reasoning we verify each module in isolation and compose these together. A key challenge of the MPMC queue is that it has a so-called external linearization point, which ReLoC has no support for reasoning about. Thus we extend ReLoC, both on paper and in Coq, with novel support for reasoning about external linearization points.},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {100–115},
	numpages = {16},
	keywords = {separation logic, Coq, concurrent data structures},
	location = {Philadelphia, PA, USA},
	series = {CPP 2022}
}

@article{voichick23_qunit,
	author = {Voichick, Finn and Li, Liyi and Rand, Robert and Hicks, Michael},
	title = {Qunity: A Unified Language for Quantum and Classical Computing},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571225},
	doi = {10.1145/3571225},
	abstract = {We introduce Qunity, a new quantum programming language designed to treat quantum computing as a natural generalization of classical computing. Qunity presents a unified syntax where familiar programming constructs can have both quantum and classical effects. For example, one can use sum types to implement the direct sum of linear operators, exception-handling syntax to implement projective measurements, and aliasing to induce entanglement. Further, Qunity takes advantage of the overlooked BQP subroutine theorem, allowing one to construct reversible subroutines from irreversible quantum algorithms through the uncomputation of "garbage" outputs. Unlike existing languages that enable quantum aspects with separate add-ons (like a classical language with quantum gates bolted on), Qunity provides a unified syntax and a novel denotational semantics that guarantees that programs are quantum mechanically valid. We present Qunity's syntax, type system, and denotational semantics, showing how it can cleanly express several quantum algorithms. We also detail how Qunity can be compiled into a low-level qubit circuit language like OpenQASM, proving the realizability of our design.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {32},
	numpages = {31},
	keywords = {algebraic data types, reversible computing, Kraus operators, quantum subroutines}
}

@inproceedings{wang01_regis,
	author = {Wang, P.H. and Wang, Hong and Kling, R.M. and Ramakrishnan, K. and Shen, J.P.},
	publisher = {{IEEE} Comput. Soc},
	url = {https://doi.org/10.1109%2Fhpca.2001.903248},
	booktitle = {Proceedings {HPCA} Seventh International Symposium on High-Performance Computer Architecture},
	doi = {10.1109/hpca.2001.903248},
	keywords = {predicated execution,dynamic scheduling,register renaming},
	title = {Register renaming and scheduling for dynamic execution of predicated code},
	year = {2001}
}

@INPROCEEDINGS{wang03_micro,
	keywords = {predicated execution, if-conversion},
	publisher = {IEEE},
	author = {Zhenghong Wang and Lee, R.B.},
	booktitle = {The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers, 2003},
	title = {Micro-architecture issues of predicated execution},
	year = {2003},
	volume = {1},
	number = {},
	pages = {349-354 Vol.1},
	doi = {10.1109/ACSSC.2003.1291934}
}

@inproceedings{wang14_theor_algor_gener_memor_partit,
	abstract = {The significant development of high-level synthesis tools has greatly facilitated FPGAs as general computing platforms. During the parallelism optimization for the data path, memory becomes a crucial bottleneck that impedes performance enhancement. Simultaneous data access is highly restricted by the data mapping strategy and memory port constraint. Memory partitioning can efficiently map data elements in the same logical array onto multiple physical banks so that the accesses to the array are parallelized. Previous methods for memory partitioning mainly focused on cyclic partitioning for single-port memory. In this work we propose a generalized memory-partitioning framework to provide high data throughput of on-chip memories. We generalize cyclic partitioning into block-cyclic partitioning for a larger design space exploration. We build the conflict detection algorithm on polytope emptiness testing, and use integer points counting in polytopes for intra-bank offset generation. Memory partitioning for multi-port memory is supported in this framework. Experimental results demonstrate that compared to the state-of-art partitioning algorithm, our proposed algorithm can reduce the number of block RAM by 19.58%, slice by 20.26% and DSP by 50%.},
	author = {Wang, Yuxin and Li, Peng and Cong, Jason},
	location = {Monterey, California, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/2554688.2554780},
	booktitle = {Proceedings of the 2014 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	doi = {10.1145/2554688.2554780},
	isbn = {9781450326711},
	keywords = {polyhedral model,high-level synthesis,memory partitioning},
	pages = {199--208},
	series = {FPGA '14},
	title = {Theory and Algorithm for Generalized Memory Partitioning in High-Level Synthesis},
	year = {2014}
}

@ARTICLE{wang17_depen_guided_symbol_execut,
	keywords = {symbolic execution, value summaries, hyperblocks},
	author = {Wang, Haijun and Liu, Ting and Guan, Xiaohong and Shen, Chao and Zheng, Qinghua and Yang, Zijiang},
	journal = {IEEE Transactions on Software Engineering},
	title = {Dependence Guided Symbolic Execution},
	year = {2017},
	volume = {43},
	number = {3},
	pages = {252-271},
	doi = {10.1109/TSE.2016.2584063}
}

@article{wang19_abstr_stack_based_approac_verif,
	abstract = {A key ingredient contributing to the success of CompCert, the state-of-the-art verified compiler for C, is its block-based memory model, which is used uniformly for all of its languages and their verified compilation. However, CompCert's memory model lacks an explicit notion of stack. Its target assembly language represents the runtime stack as an unbounded list of memory blocks, making further compilation of CompCert assembly into more realistic machine code difficult since it is not possible to merge these blocks into a finite and continuous stack. Furthermore, various notions of verified compositional compilation rely on some kind of mechanism for protecting private stack data and enabling modification to the public stack-allocated data, which is lacking in the original CompCert. These problems have been investigated but not fully addressed before, in the sense that some advanced optimization passes that significantly change the ways stack blocks are (de-)allocated, such as tailcall recognition and inlining, are often omitted. We propose a lightweight and complete solution to the above problems. It is based on the enrichment of CompCert's memory model with an abstract stack that keeps track of the history of stack frames to bound the stack consumption and that enforces a uniform stack access policy by assigning fine-grained permissions to stack memory. Using this enriched memory model for all the languages of CompCert, we are able to reprove the correctness of the full compilation chain of CompCert, including all the optimization passes. In the end, we get Stack-Aware CompCert, a complete extension of CompCert that enforces the finiteness of the stack and fine-grained stack permissions. Based on Stack-Aware CompCert, we develop CompCertMC, the first extension of CompCert that compiles into a low-level language with flat memory spaces. Based on CompCertMC, we develop Stack-Aware CompCertX, a complete extension of CompCert that supports a notion of compositional compilation that we call contextual compilation by exploiting the uniform stack access policy provided by the abstract stack.},
	author = {Wang, Yuting and Wilke, Pierre and Shao, Zhong},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3290375},
	doi = {10.1145/3290375},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {machine code generation,compositional compilation,memory model,certified compilers,abstract stack},
	month = jan,
	number = {POPL},
	title = {An Abstract Stack Based Approach to Verified Compositional Compilation to Machine Code},
	volume = {3},
	year = {2019}
}

@article{wang20_compc,
	abstract = {We present CompCertELF, the first extension to CompCert that supports verified compilation from C programs all the way to a standard binary file format, i.e., the ELF object format. Previous work on Stack-Aware CompCert provides a verified compilation chain from C programs to assembly programs with a realistic machine memory model. We build CompCertELF by modifying and extending this compilation chain with a verified assembler which further transforms assembly programs into ELF object files. CompCert supports large-scale verification via verified separate compilation: C modules can be written and compiled separately, and then linked together to get a target program that refines the semantics of the program linked from the source modules. However, verified separate compilation in CompCert only works for compilation to assembly programs, not to object files. For the latter, the main difficulty is to bridge the two different views of linking: one for CompCert's programs that allows arbitrary shuffling of global definitions by linking and the other for object files that treats blocks of encoded definitions as indivisible units. We propose a lightweight approach that solves the above problem without any modification to CompCert's framework for verified separate compilation: by introducing a notion of syntactical equivalence between programs and proving the commutativity between syntactical equivalence and the two different kinds of linking, we are able to transit from the more abstract linking operation in CompCert to the more concrete one for ELF object files. By applying this approach to CompCertELF, we obtain the first compiler that supports verified separate compilation of C programs into ELF object files.},
	author = {Wang, Yuting and Xu, Xiangzhe and Wilke, Pierre and Shao, Zhong},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3428265},
	doi = {10.1145/3428265},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {Generation of Object Files,Assembler Verification,Verified Separate Compilation},
	month = nov,
	number = {OOPSLA},
	title = {CompCertELF: Verified Separate Compilation of C Programs into ELF Object Files},
	volume = {4},
	year = {2020}
}

@inproceedings{wang22_autom_reinf_learn_archit_desig_code_optim,
	author = {Wang, Huanting and Tang, Zhanyong and Zhang, Cheng and Zhao, Jiaqi and Cummins, Chris and Leather, Hugh and Wang, Zheng},
	title = {Automating Reinforcement Learning Architecture Design for Code Optimization},
	year = {2022},
	isbn = {9781450391832},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497776.3517769},
	doi = {10.1145/3497776.3517769},
	abstract = {Reinforcement learning (RL) is emerging as a powerful technique for solving complex code optimization tasks with an ample search space. While promising, existing solutions require a painstaking manual process to tune the right task-specific RL architecture, for which compiler developers need to determine the composition of the RL exploration algorithm, its supporting components like state, reward, and transition functions, and the hyperparameters of these models. This paper introduces SuperSonic, a new open-source framework to allow compiler developers to integrate RL into compilers easily, regardless of their RL expertise. SuperSonic supports customizable RL architecture compositions to target a wide range of optimization tasks. A key feature of SuperSonic is the use of deep RL and multi-task learning techniques to develop a meta-optimizer to automatically find and tune the right RL architecture from training benchmarks. The tuned RL can then be deployed to optimize new programs. We demonstrate the efficacy and generality of SuperSonic by applying it to four code optimization problems and comparing it against eight auto-tuning frameworks. Experimental results show that SuperSonic consistently improves hand-tuned methods by delivering better overall performance, accelerating the deployment-stage search by 1.75x on average (up to 100x).},
	booktitle = {Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction},
	pages = {129–143},
	numpages = {15},
	keywords = {reinforcement learning, code optimization, Compiler optimization},
	location = {Seoul, South Korea},
	series = {CC 2022}
}

@article{wang22_inter_univer_approx_neural_networ,
	author = {Wang, Zi and Albarghouthi, Aws and Prakriya, Gautam and Jha, Somesh},
	title = {Interval Universal Approximation for Neural Networks},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498675},
	doi = {10.1145/3498675},
	abstract = {To verify safety and robustness of neural networks, researchers have successfully applied abstract interpretation, primarily using the interval abstract domain. In this paper, we study the theoretical power and limits of the interval domain for neural-network verification. First, we introduce the interval universal approximation (IUA) theorem. IUA shows that neural networks not only can approximate any continuous function f (universal approximation) as we have known for decades, but we can find a neural network, using any well-behaved activation function, whose interval bounds are an arbitrarily close approximation of the set semantics of f (the result of applying f to a set of inputs). We call this notion of approximation interval approximation. Our theorem generalizes the recent result of Baader et al. from ReLUs to a rich class of activation functions that we call squashable functions. Additionally, the IUA theorem implies that we can always construct provably robust neural networks under ℓ∞-norm using almost any practical activation function. Second, we study the computational complexity of constructing neural networks that are amenable to precise interval analysis. This is a crucial question, as our constructive proof of IUA is exponential in the size of the approximation domain. We boil this question down to the problem of approximating the range of a neural network with squashable activation functions. We show that the range approximation problem (RA) is a Δ2-intermediate problem, which is strictly harder than NP-complete problems, assuming coNP⊄NP. As a result, IUA is an inherently hard problem: No matter what abstract domain or computational tools we consider to achieve interval approximation, there is no efficient construction of such a universal approximator. This implies that it is hard to construct a provably robust network, even if we have a robust network to start with.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {14},
	numpages = {29},
	keywords = {Abstract Interpretation, Universal Approximation}
}

@article{wang22_verif_compil_c_progr_nomin_memor_model,
	abstract = {Memory models play an important role in verified compilation of imperative programming languages. A representative one is the block-based memory model of CompCert---the state-of-the-art verified C compiler. Despite its success, the abstraction over memory space provided by CompCert's memory model is still primitive and inflexible. In essence, it uses a fixed representation for identifying memory blocks in a global memory space and uses a globally shared state for distinguishing between used and unused blocks. Therefore, any reasoning about memory must work uniformly for the global memory; it is impossible to individually reason about different sub-regions of memory (i.e., the stack and global definitions). This not only incurs unnecessary complexity in compiler verification, but also poses significant difficulty for supporting verified compilation of open or concurrent programs which need to work with contextual memory, as manifested in many previous extensions of CompCert. To remove the above limitations, we propose an enhancement to the block-based memory model based on nominal techniques; we call it the nominal memory model. By adopting the key concepts of nominal techniques such as atomic names and supports to model the memory space, we are able to 1) generalize the representation of memory blocks to any types satisfying the properties of atomic names and 2) remove the global constraints for managing memory blocks, enabling flexible memory structures for open and concurrent programs. To demonstrate the effectiveness of the nominal memory model, we develop a series of extensions of CompCert based on it. These extensions show that the nominal memory model 1) supports a general framework for verified compilation of C programs, 2) enables intuitive reasoning of compiler transformations on partial memory; and 3) enables modular reasoning about programs working with contextual memory. We also demonstrate that these extensions require limited changes to the original CompCert, making the verification techniques based on the nominal memory model easy to adopt.},
	author = {Wang, Yuting and Zhang, Ling and Shao, Zhong and Koenig, Jérémie},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3498686},
	doi = {10.1145/3498686},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {Verified Compilation,Nominal Techniques,Memory Models},
	month = jan,
	number = {POPL},
	title = {Verified Compilation of C Programs with a Nominal Memory Model},
	volume = {6},
	year = {2022}
}

@article{wang23_compar_synth,
	author = {Wang, Yanjun and Li, Zixuan and Jiang, Chuan and Qiu, Xiaokang and Rao, Sanjay},
	title = {Comparative Synthesis: Learning Near-Optimal Network Designs by Query},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571197},
	doi = {10.1145/3571197},
	abstract = {When managing wide-area networks, network architects must decide how to balance multiple conflicting metrics, and ensure fair allocations to competing traffic while prioritizing critical traffic. The state of practice poses challenges since architects must precisely encode their intent into formal optimization models using abstract notions such as utility functions, and ad-hoc manually tuned knobs. In this paper, we present the first effort to synthesize optimal network designs with indeterminate objectives using an interactive program-synthesis-based approach. We make three contributions. First, we present comparative synthesis, an interactive synthesis framework which produces near-optimal programs (network designs) through two kinds of queries (Validate and Compare), without an objective explicitly given. Second, we develop the first learning algorithm for comparative synthesis in which a voting-guided learner picks the most informative query in each iteration. We present theoretical analysis of the convergence rate of the algorithm. Third, we implemented Net10Q, a system based on our approach, and demonstrate its effectiveness on four real-world network case studies using black-box oracles and simulation experiments, as well as a pilot user study comprising network researchers and practitioners. Both theoretical and experimental results show the promise of our approach.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {4},
	numpages = {30},
	keywords = {Query, Program Synthesis, Optimization, User Interaction, Traffic Engineering}
}

@InProceedings{wang23_mapbuf,
	keywords = {delay prediction},
	author = {Wang, Hanyu and Rizzi, Carmine and Josipović, Lana},
	booktitle = {Proceedings of the 42nd IEEE/ACM Intl. Conference on Computer-Aided Design},
	title = {{MapBuf}: Simultaneous Technology Mapping and Buffer Insertion for {HLS} Performance Optimization},
	year = {2023},
	month = 10,
	address = {San Francisco, CA}
}

@article{wang65_circuit_boolean,
	title = {Circuit synthesis by solving sequential Boolean equations.},
	volume = {30},
	DOI = {10.2307/2270156},
	number = {2},
	journal = {Journal of Symbolic Logic},
	publisher = {Cambridge University Press},
	author = {Hao Wang},
	year = {1965},
	pages = {249–249}
}

@article{warter93_rever_if_conver,
	abstract = {In this paper we present a set of isomorphic control transformations that allow the compiler to apply local scheduling techniques to acyclic subgraphs of the control flow graph. Thus, the code motion complexities of global scheduling are eliminated. This approach relies on a new technique, Reverse If-Conversion (RIC), that transforms scheduled If-Converted code back to the control flow graph representation. This paper presents the predicate internal representation, the algorithms for RIC, and the correctness of RIC. In addition, the scheduling issues are addressed and an application to software pipelining is presented.},
	author = {Warter, Nancy J. and Mahlke, Scott A. and Hwu, Wen-Mei W. and Rau, B. Ramakrishna},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/173262.155118},
	doi = {10.1145/173262.155118},
	issn = {0362-1340},
	journaltitle = {SIGPLAN Not.},
	month = jun,
	number = {6},
	pages = {290--299},
	title = {Reverse If-Conversion},
	volume = {28},
	year = {1993}
}

@InProceedings{webb21_formal_seman_graal_inter_repres,
	doi = {10.1007/978-3-030-88885-5_8},
	keywords = {gated-SSA},
	author = {Webb, Brae J. and Utting, Mark and Hayes, Ian J.},
	editor = "Hou, Zhe
and Ganesh, Vijay",
	title = "A Formal Semantics of the GraalVM Intermediate Representation",
	booktitle = "Automated Technology for Verification and Analysis",
	year = "2021",
	publisher = "Springer International Publishing",
	address = "Cham",
	pages = "111--126",
	abstract = "The optimization phase of a compiler is responsible for transforming an intermediate representation (IR) of a program into a more efficient form. Modern optimizers, such as that used in the GraalVM compiler, use an IR consisting of a sophisticated graph data structure that combines data flow and control flow into the one structure. As part of a wider project on the verification of optimization passes of GraalVM, this paper describes a semantics for its IR within Isabelle/HOL. The semantics consists of a big-step operational semantics for data nodes (which are represented in a graph-based static single assignment (SSA) form) and a small-step operational semantics for handling control flow including heap-based reads and writes, exceptions, and method calls. We have proved a suite of canonicalization optimizations and conditional elimination optimizations with respect to the semantics.",
	isbn = "978-3-030-88885-5"
}

@inproceedings{webb23_verif_term_graph_optim_using_isabel_hol,
	author = {Webb, Brae J. and Hayes, Ian J. and Utting, Mark},
	title = {Verifying Term Graph Optimizations Using Isabelle/HOL},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575673},
	doi = {10.1145/3573105.3575673},
	abstract = {Our objective is to formally verify the correctness of the hundreds of expression optimization rules used within the GraalVM compiler. When defining the semantics of a programming language, expressions naturally form abstract syntax trees, or, terms. However, in order to facilitate sharing of common subexpressions, modern compilers represent expressions as term graphs. Defining the semantics of term graphs is more complicated than defining the semantics of their equivalent term representations. More significantly, defining optimizations directly on term graphs and proving semantics preservation is considerably more complicated than on the equivalent term representations. On terms, optimizations can be expressed as conditional term rewriting rules, and proofs that the rewrites are semantics preserving are relatively straightforward. In this paper, we explore an approach to using term rewrites to verify term graph transformations of optimizations within the GraalVM compiler. This approach significantly reduces the overall verification effort and allows for simpler encoding of optimization rules.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {320–333},
	numpages = {14},
	keywords = {verified code optimizer, sea-of-nodes intermediate representation, GraalVM compiler, Isabelle/HOL},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@inbook{welsch13_locat_types_safe_progr_near_far_refer,
	abstract = {In distributed object-oriented systems, objects belong to different locations. For example, in Java Remote Method Invocation (RMI), objects can be distributed over different Java virtual machines. Accessing a reference in RMI has crucially different semantics depending on whether the referred object is local or remote. Nevertheless, such references are not statically distinguished by the Java type system.},
	author = {Welsch, Yannick and Schäfer, Jan and Poetzsch-Heffter, Arnd},
	editor = {Clarke, Dave and Noble, James and Wrigstad, Tobias},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	url = {https://doi.org/10.1007/978-3-642-36946-9_16},
	booktitle = {Aliasing in Object-Oriented Programming. Types, Analysis and Verification},
	doi = {10.1007/978-3-642-36946-9_16},
	isbn = {978-3-642-36946-9},
	keywords = {ownership types,memory aliasing},
	pages = {471--500},
	title = {Location Types for Safe Programming with Near and Far References},
	year = {2013}
}

@article{westrick22_entan_detec_near_zero_cost,
	author = {Westrick, Sam and Arora, Jatin and Acar, Umut A.},
	title = {Entanglement Detection with Near-Zero Cost},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3547646},
	doi = {10.1145/3547646},
	abstract = {Recent research on parallel functional programming has culminated in a provably efficient (in work and space) parallel memory manager, which has been incorporated into the MPL (MaPLe) compiler for Parallel ML and shown to deliver practical efficiency and scalability. The memory manager exploits a property of parallel programs called disentanglement, which restricts computations from accessing concurrently allocated objects. Disentanglement is closely related to race-freedom, but subtly differs from it. Unlike race-freedom, however, no known techniques exists for ensuring disentanglement, leaving the task entirely to the programmer. This is a challenging task, because it requires reasoning about low-level memory operations (e.g., allocations and accesses), which is especially difficult in functional languages. In this paper, we present techniques for detecting entanglement dynamically, while the program is running. We first present a dynamic semantics for a functional language with references that checks for entanglement by consulting parallel and sequential dependency relations in the program. Notably, the semantics requires checks for mutable objects only. We prove the soundness of the dynamic semantics and present several techniques for realizing it efficiently, in particular by pruning away a large number of entanglement checks. We also provide bounds on the work and space of our techniques. We show that the entanglement detection techniques are practical by implementing them in the MPL compiler for Parallel ML. Considering a variety of benchmarks, we present an evaluation and measure time and space overheads of less than 5% on average with up to 72 cores. These results show that entanglement detection has negligible cost and can therefore remain deployed with little or no impact on efficiency, scalability, and space.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {115},
	numpages = {32},
	keywords = {memory management, parallelism, disentanglement, functional}
}

@inproceedings{wey07_pipel_divid_small_lookup_table,
	url = {https://www.researchgate.net/profile/Woo-Chan-Park-2/publication/3966737_A_new_pipelined_divider_with_a_small_lookup_table/links/54cf1c500cf29ca810fd844d/A-new-pipelined-divider-with-a-small-lookup-table.pdf},
	abstract = {The design of fast dividers is an important issue in high speed computing because division account for a significant fraction of the total arithmetic operation. Taylor series expansion is a well-known multiplicative scheme for high-performance division implementation. This study presents a simple architecture that implements a pipelined divider including the first 6 terms of the Taylor series expansion for approximation. Results show that the developed pipelined divider takes a lookup table of 32B for single precision with a latency of 8.90ns, and 56KB for double precision with 11.46ns, where the circuit is synthesized with TSMC 0.18µm digital CMOS standard cell library.},
	author = {Wey, Chin-Long and Lin, Shin-Yo and Shiue, Muh-Tian},
	location = {Hangzhou, China},
	publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
	booktitle = {Proceedings of the 6th WSEAS International Conference on Instrumentation, Measurement, Circuits and Systems},
	isbn = {1116782345990},
	keywords = {Taylor series expansion,pipelined divider,approximation,Newton-Raphson algorithm,lookup table},
	pages = {181--185},
	series = {IMCAS'07},
	title = {A Pipelined Divider with a Small Lookup Table},
	year = {2007}
}

@Article{whiting94_hdfl,
	keywords = {data-flow},
	author = {Whiting, P.G. and Pascoe, R.S.V.},
	journal = {IEEE Annals of the History of Computing},
	title = {A History of Data-Flow Languages},
	year = {1994},
	volume = {16},
	number = {4},
	pages = {38-59},
	doi = {10.1109/85.329757}
}

@INPROCEEDINGS{williamson96_sphif,
	keywords = {data-flow, control-flow, semantics},
	author = {Williamson, M.C. and Lee, E.A.},
	booktitle = {Conference Record of The Thirtieth Asilomar Conference on Signals, Systems and Computers},
	title = {Synthesis of Parallel Hardware Implementations From Synchronous Dataflow Graph Specifications},
	year = {1996},
	volume = {},
	number = {},
	pages = {1340-1343 vol.2},
	doi = {10.1109/ACSSC.1996.599166}
}

@article{willsey21_egg,
	abstract = {An e-graph efficiently represents a congruence relation over many expressions. Although they were originally developed in the late 1970s for use in automated theorem provers, a more recent technique known as equality saturation repurposes e-graphs to implement state-of-the-art, rewrite-driven compiler optimizations and program synthesizers. However, e-graphs remain unspecialized for this newer use case. Equality saturation workloads exhibit distinct characteristics and often require ad-hoc e-graph extensions to incorporate transformations beyond purely syntactic rewrites. This work contributes two techniques that make e-graphs fast and extensible, specializing them to equality saturation. A new amortized invariant restoration technique called rebuilding takes advantage of equality saturation's distinct workload, providing asymptotic speedups over current techniques in practice. A general mechanism called e-class analyses integrates domain-specific analyses into the e-graph, reducing the need for ad hoc manipulation. We implemented these techniques in a new open-source library called egg. Our case studies on three previously published applications of equality saturation highlight how egg's performance and flexibility enable state-of-the-art results across diverse domains.},
	author = {Willsey, Max and Nandi, Chandrakana and Wang, Yisu Remy and Flatt, Oliver and Tatlock, Zachary and Panchekha, Pavel},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434304},
	doi = {10.1145/3434304},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {equality saturation,e-graphs},
	month = jan,
	number = {POPL},
	title = {Egg: Fast and Extensible Equality Saturation},
	volume = {5},
	year = {2021}
}

@InProceedings{winskel82_event_ccs,
	doi = {10.1007/BFb0012800},
	author = "Winskel, Glynn",
	editor = "Nielsen, Mogens
and Schmidt, Erik Meineche",
	title = "Event structure semantics for CCS and related languages",
	booktitle = "Automata, Languages and Programming",
	year = "1982",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "561--576",
	isbn = "978-3-540-39308-5"
}

@incollection{winskel87_relat,
	keywords = {hardware abstract interpretation},
	doi = {10.1007/3-540-18508-9_22},
	url = {https://doi.org/10.1007/3-540-18508-9_22},
	year = {1987},
	publisher = {Springer Berlin Heidelberg},
	pages = {98--113},
	author = {Glynn Winskel},
	title = {Relating two models of hardware},
	booktitle = {Category Theory and Computer Science}
}

@book{winskel93,
	author = {Winskel, Glynn},
	publisher = {MIT press},
	title = {The formal semantics of programming languages: an introduction},
	year = {1993}
}

@article{witharana22_survey_asser_based_hardw_verif,
	author = {Witharana, Hasini and Lyu, Yangdi and Charles, Subodha and Mishra, Prabhat},
	title = {A Survey on Assertion-Based Hardware Verification},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {54},
	number = {11s},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3510578},
	doi = {10.1145/3510578},
	abstract = {Hardware verification of modern electronic systems has been identified as a major bottleneck due to the increasing complexity and time-to-market constraints. One of the major objectives in hardware verification is to drastically reduce the validation and debug time without sacrificing the design quality. Assertion-based verification is a promising avenue for efficient hardware validation and debug. In this article, we provide a comprehensive survey of recent progress in assertion-based hardware verification. Specifically, we outline how to define assertions using temporal logic to specify expected behaviors in different abstraction levels. Next, we describe state-of-the art approaches for automated generation of assertions. We also discuss test generation techniques for activating assertions to ensure that the generated assertions are valid. Finally, we present both pre-silicon and post-silicon assertion-based validation approaches that utilize simulation, formal methods as well as hybrid techniques. We conclude with a discussion on utilizing assertions for verifying both functional and non-functional requirements.},
	journal = {ACM Comput. Surv.},
	month = {sep},
	articleno = {225},
	numpages = {33},
	keywords = {survey, verilog, verification}
}

@article{wolf91_loop_trans_theor_algor_maxim_paral,
	abstract = {An approach to transformations for general loops in which dependence vectors represent precedence constraints on the iterations of a loop is presented. Therefore, dependences extracted from a loop nest must be lexicographically positive. This leads to a simple test for legality of compound transformations: any code transformation that leaves the dependences lexicographically positive is legal. The loop transformation theory is applied to the problem of maximizing the degree of coarse- or fine-grain parallelism in a loop nest. It is shown that the maximum degree of parallelism can be achieved by transforming the loops into a nest of coarsest fully permutable loop nests and wavefronting the fully permutable nests. The canonical form of coarsest fully permutable nests can be transformed mechanically to yield maximum degrees of coarse- and/or fine-grain parallelism. The efficient heuristics can find the maximum degrees of parallelism for loops whose nesting level is less than five.},
	author = {Wolf, M. E. and Lam, M. S.},
	publisher = {IEEE Press},
	url = {https://doi.org/10.1109/71.97902},
	doi = {10.1109/71.97902},
	issn = {1045-9219},
	journaltitle = {IEEE Trans. Parallel Distrib. Syst.},
	keywords = {wavefront,polyhedral analysis},
	month = oct,
	number = {4},
	pages = {452--471},
	title = {A Loop Transformation Theory and an Algorithm to Maximize Parallelism},
	volume = {2},
	year = {1991}
}

@inproceedings{xie05_scalab_error_detec_using_boolean_satis,
	author = {Xie, Yichen and Aiken, Alex},
	title = {Scalable Error Detection Using Boolean Satisfiability},
	year = {2005},
	isbn = {158113830X},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1040305.1040334},
	doi = {10.1145/1040305.1040334},
	abstract = {We describe a software error-detection tool that exploits recent advances in boolean satisfiability (SAT) solvers. Our analysis is path sensitive, precise down to the bit level, and models pointers and heap data. Our approach is also highly scalable, which we achieve using two techniques. First, for each program function, several optimizations compress the size of the boolean formulas that model the control- and data-flow and the heap locations accessed by a function. Second, summaries in the spirit of type signatures are computed for each function, allowing inter-procedural analysis without a dramatic increase in the size of the boolean constraints to be solved.We demonstrate the effectiveness of our approach by constructing a lock interface inference and checking tool. In an interprocedural analysis of more than 23,000 lock related functions in the latest Linux kernel, the checker generated 300 warnings, of which 179 were unique locking errors, a false positive rate of only 40\%.},
	booktitle = {Proceedings of the 32nd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
	pages = {351–363},
	numpages = {13},
	keywords = {value summaries, symbolic execution, hyperblocks},
	location = {Long Beach, California, USA},
	series = {POPL '05}
}

@article{xie22_stagin_class,
	author = {Xie, Ningning and Pickering, Matthew and L\"{o}h, Andres and Wu, Nicolas and Yallop, Jeremy and Wang, Meng},
	title = {Staging with Class: A Specification for Typed Template Haskell},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498723},
	doi = {10.1145/3498723},
	abstract = {Multi-stage programming using typed code quotation is an established technique for writing optimizing code generators with strong type-safety guarantees. Unfortunately, quotation in Haskell interacts poorly with type classes, making it difficult to write robust multi-stage programs. We study this unsound interaction and propose a resolution, staged type class constraints, which we formalize in a source calculus λ⇒ that elaborates into an explicit core calculus F. We show type soundness of both calculi, establishing that well-typed, well-staged source programs always elaborate to well-typed, well-staged core programs, and prove beta and eta rules for code quotations. Our design allows programmers to incorporate type classes into multi-stage programs with confidence. Although motivated by Haskell, it is also suitable as a foundation for other languages that support both overloading and quotation.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {61},
	numpages = {30},
	keywords = {Type Classes, Typed Template Haskell, Staging}
}

@inproceedings{xu22_train_deep_learn_pipel_memor,
	author = {Xu, Yufan and Raje, Saurabh and Rountev, Atanas and Sabin, Gerald and Sukumaran-Rajam, Aravind and Sadayappan, P.},
	title = {Training of Deep Learning Pipelines on Memory-Constrained GPUs via Segmented Fused-Tiled Execution},
	year = {2022},
	isbn = {9781450391832},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497776.3517766},
	doi = {10.1145/3497776.3517766},
	abstract = {Training models with massive inputs is a significant challenge in the development of Deep Learning pipelines to process very large digital image datasets as required by Whole Slide Imaging (WSI) in computational pathology and analysis of brain fMRI images in computational neuroscience. Graphics Processing Units (GPUs) represent the primary workhorse in training and inference of Deep Learning models. In order to use GPUs to run inference or training on a neural network pipeline, state-of-the-art machine learning frameworks like PyTorch and TensorFlow currently require that the collective memory on the GPUs must be larger than the size of the activations at any stage in the pipeline. Therefore, existing Deep Learning pipelines for these use cases have been forced to develop sub-optimal "patch-based" modeling approaches, where images are processed in small segments of an image. In this paper, we present a solution to this problem by employing tiling in conjunction with check-pointing, thereby enabling arbitrarily large images to be directly processed, irrespective of the size of global memory on a GPU and the number of available GPUs. Experimental results using PyTorch demonstrate enhanced functionality/performance over existing frameworks.},
	booktitle = {Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction},
	pages = {104–116},
	numpages = {13},
	keywords = {GPU, DNN, Tiling, Checkpointing, Large image training, Fusion, Memory-constrained execution},
	location = {Seoul, South Korea},
	series = {CC 2022}
}

@article{xu23_makin_type_differ,
	author = {Xu, Han and Huang, Xuejing and Oliveira, Bruno C. d. S.},
	title = {Making a Type Difference: Subtraction on Intersection Types as Generalized Record Operations},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571224},
	doi = {10.1145/3571224},
	abstract = {In programming languages with records, objects, or traits, it is common to have operators that allow dropping, updating or renaming some components. These operators are useful for programmers to explicitly deal with conflicts and override or update some components. While such operators have been studied for record types, little work has been done to generalize and study their theory for other types. This paper shows that, given subtyping and disjointness relations, we can specify and derive algorithmic implementations for a general type difference operator that works for other types, including function types, record types and intersection types. When defined in this way, the type difference algebra has many desired properties that are expected from a subtraction operator. Together with a generic merge operator, using type difference we can generalize many operations on records formalized in the literature. To illustrate the usefulness of type difference we create an intermediate calculus with a rich set of operators on expressions of arbitrary type, and demonstrate applications of these operators in CP, a prototype language for Compositional Programming. The semantics of the calculus is given by elaborating into a calculus with disjoint intersection types and a merge operator. We have implemented type difference and all the operators in the CP language. Moreover, all the calculi and related proofs are mechanically formalized in the Coq theorem prover.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {31},
	numpages = {28},
	keywords = {type systems, object oriented languages, functional languages}
}

@inproceedings{xydis09_flexib_datap_synth_arith_optim_operat_chain,
	author = {Xydis, Sotirios and Triantafyllou, Ioannis and Economakos, George and Pekmestzi, Kiamal},
	booktitle = {2009 NASA/ESA Conference on Adaptive Hardware and Systems},
	doi = {10.1109/AHS.2009.21},
	keywords = {operation chaining},
	pages = {407--414},
	title = {Flexible Datapath Synthesis through Arithmetically Optimized Operation Chaining},
	year = {2009}
}

@inproceedings{yadegari15_symbol_execut_obfus_code,
	author = {Yadegari, Babak and Debray, Saumya},
	title = {Symbolic Execution of Obfuscated Code},
	year = {2015},
	isbn = {9781450338325},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2810103.2813663},
	doi = {10.1145/2810103.2813663},
	booktitle = {Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security},
	pages = {732–744},
	numpages = {13},
	keywords = {symbolic execution, taint analysis},
	location = {Denver, Colorado, USA},
	series = {CCS '15}
}

@inproceedings{yang11_findin_under_bugs_c_compil,
	author = {Yang, Xuejun and Chen, Yang and Eide, Eric and Regehr, John},
	location = {San Jose, California, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/1993498.1993532},
	booktitle = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation},
	doi = {10.1145/1993498.1993532},
	isbn = {9781450306638},
	keywords = {random program generation,random testing,automated testing,compiler testing,compiler defect},
	pages = {283--294},
	series = {PLDI '11},
	title = {Finding and Understanding Bugs in C Compilers},
	year = {2011}
}

@inproceedings{yang15_jit,
	author = {Yang, L. and Ikram, M. and Gurumani, S. and Fahmy, S. and Chen, D. and Rupnow, K.},
	url = {https://doi.org/10.1109/FPT.2015.7393155},
	booktitle = {2015 International Conference on Field Programmable Technology (FPT)},
	doi = {10.1109/FPT.2015.7393155},
	keywords = {formal verification;hardware description languages;high level synthesis;just-in-time;zero cycle detection;CHStone benchmarks;RTL implementation;register transfer level;large-scale software systems;software development;hardware design;HLS tools;high-level synthesis tools;JIT trace-based verification;just-in-time traces;Instruments;Debugging;Registers;Optimization;Hardware;Computer bugs;Radiation detectors;JIT;Trace-based Verification;High-Level Synthesis},
	month = dec,
	pages = {228--231},
	title = {JIT trace-based verification for high-level synthesis},
	year = {2015}
}

@inproceedings{yang16_valid,
	author = {Yang, Z. and Hao, K. and Cong, K. and Lei, L. and Ray, S. and Xie, F.},
	url = {https://doi.org/},
	booktitle = {2016 Design, Automation Test in Europe Conference Exhibition (DATE)},
	issn = {1558-1101},
	keywords = {program compilers;program verification;scheduling;behavioral synthesis;electronic system-level description;hardware design;RTL;error-prone transformation;scalable equivalence checking algorithm;scheduling transformation validation;scheduling modes;subtle interface protocols;commercial synthesis tool;register-transfer level;Timing;Clocks;Job shop scheduling;Hardware;Schedules;Processor scheduling},
	month = mar,
	pages = {1652--1657},
	title = {Validating scheduling transformation for behavioral synthesis},
	year = {2016}
}

@incollection{yang19_chapt_five_advan_symbol_execut,
	title = {Chapter Five - Advances in Symbolic Execution},
	editor = {Atif M. Memon},
	series = {Advances in Computers},
	publisher = {Elsevier},
	volume = {113},
	pages = {225-287},
	year = {2019},
	issn = {0065-2458},
	doi = {https://doi.org/10.1016/bs.adcom.2018.10.002},
	url = {https://www.sciencedirect.com/science/article/pii/S0065245818300627},
	author = {Guowei Yang and Antonio Filieri and Mateus Borges and Donato Clun and Junye Wen},
	keywords = {value summaries, symbolic execution, hyperblocks},
	abstract = {Symbolic execution is a systematic technique for checking programs, which forms a basis for various software testing and verification techniques. It provides a powerful analysis in principle but remains challenging to scale and generalize symbolic execution in practice. This chapter reviews the cutting-edge research accomplishments in addressing these challenges in the last 5 years, including advances in addressing the scalability challenges such as constraint solving and path explosion, as well as advances in applying symbolic execution in testing, security, and probabilistic program analysis.}
}

@article{yang21_simpl_depen_reduc_polyh_model,
	abstract = {A Reduction – an accumulation over a set of values, using an associative and commutative operator – is a common computation in many numerical computations, including scientific computations, machine learning, computer vision, and financial analytics. Contemporary polyhedral-based compilation techniques make it possible to optimize reductions, such as prefix sums, in which each component of the reduction’s output potentially shares computation with another component in the reduction. Therefore an optimizing compiler can identify the computation shared between multiple components and generate code that computes the shared computation only once. These techniques, however, do not support reductions that – when phrased in the language of the polyhedral model – span multiple dependent statements. In such cases, existing approaches can generate incorrect code that violates the data dependences of the original, unoptimized program. In this work, we identify and formalize the optimization of dependent reductions as an integer bilinear program. We present a heuristic optimization algorithm that uses an affine sequential schedule of the program to determine how to simplfy reductions yet still preserve the program’s dependences. We demonstrate that the algorithm provides optimal complexity for a set of benchmark programs from the literature on probabilistic inference algorithms, whose performance critically relies on simplifying these reductions. The complexities for 10 of the 11 programs improve siginifcantly by factors at least of the sizes of the input data, which are in the range of 104 to 106 for typical real application inputs. We also confirm the significance of the improvement by showing speedups in wall-clock time that range from 1.1x to over 106x.},
	author = {Yang, Cambridge and Atkinson, Eric and Carbin, Michael},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3434301},
	doi = {10.1145/3434301},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {reductions,polyhedral model,program dependence},
	month = jan,
	number = {POPL},
	title = {Simplifying Dependent Reductions in the Polyhedral Model},
	volume = {5},
	year = {2021}
}

@report{yang89_detec,
	author = {Yang, Wuu and Horwitz, Susan and Reps, Thomas},
	institution = {University of Wisconsin-Madison Department of Computer Sciences},
	keywords = {gated-SSA,SSA,equivalence checking},
	title = {Detecting program components with equivalent behaviors},
	type = {techreport},
	year = {1989}
}

@inproceedings{ye21_autom_confor_testin_javas_engin,
	abstract = {JavaScript (JS) is a popular, platform-independent programming language. To ensure the interoperability of JS programs across different platforms, the implementation of a JS engine should conform to the ECMAScript standard. However, doing so is challenging as there are many subtle definitions of API behaviors, and the definitions keep evolving. We present COMFORT, a new compiler fuzzing framework for detecting JS engine bugs and behaviors that deviate from the ECMAScript standard. COMFORT leverages the recent advance in deep learning-based language models to automatically generate JS test code. As a departure from prior fuzzers, COMFORT utilizes the well-structured ECMAScript specifications to automatically generate test data along with the test programs to expose bugs that could be overlooked by the developers or manually written test cases. COMFORT then applies differential testing methodologies on the generated test cases to expose standard conformance bugs. We apply COMFORT to ten mainstream JS engines. In 200 hours of automated concurrent testing runs, we discover bugs in all tested JS engines. We had identified 158 unique JS engine bugs, of which 129 have been verified, and 115 have already been fixed by the developers. Furthermore, 21 of the COMFORT-generated test cases have been added to Test262, the official ECMAScript conformance test suite.},
	author = {Ye, Guixin and Tang, Zhanyong and Tan, Shin Hwei and Huang, Songfang and Fang, Dingyi and Sun, Xiaoyang and Bian, Lizhong and Wang, Haibo and Wang, Zheng},
	location = {Virtual, Canada},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3453483.3454054},
	booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
	doi = {10.1145/3453483.3454054},
	isbn = {9781450383912},
	keywords = {Deep learning,JavaScript,Differential testing,Compiler fuzzing,Conformance bugs},
	pages = {435--450},
	series = {PLDI 2021},
	title = {Automated Conformance Testing for JavaScript Engines via Deep Compiler Fuzzing},
	year = {2021}
}

@ARTICLE{yi18_elimin_path_redun_postc_symbol_execut,
	keywords = {symbolic execution, value summaries, hyperblocks},
	author = {Yi, Qiuping and Yang, Zijiang and Guo, Shengjian and Wang, Chao and Liu, Jian and Zhao, Chen},
	journal = {IEEE Transactions on Software Engineering},
	title = {Eliminating Path Redundancy via Postconditioned Symbolic Execution},
	year = {2018},
	volume = {44},
	number = {1},
	pages = {25-43},
	doi = {10.1109/TSE.2017.2659751}
}

@inproceedings{ying23_formal_doob_martin_conver_theor_mathl,
	author = {Ying, Kexing and Degenne, Rémy},
	title = {A Formalization of Doob’s Martingale Convergence Theorems in Mathlib},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3573105.3575675},
	doi = {10.1145/3573105.3575675},
	abstract = {We present the formalization of Doob’s martingale convergence theorems in the mathlib library for the Lean theorem prover. These theorems give conditions under which (sub)martingales converge, almost everywhere or in L1. In order to formalize those results, we build a definition of the conditional expectation in Banach spaces and develop the theory of stochastic processes, stopping times and martingales. As an application of the convergence theorems, we also present the formalization of L\'{e}vy’s generalized Borel-Cantelli lemma. This work on martingale theory is one of the first developments of probability theory in mathlib, and it builds upon diverse parts of that library such as topology, analysis and most importantly measure theory.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {334–347},
	numpages = {14},
	keywords = {probability, Lean, mathlib, formal mathematics, proof assistant, martingale},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@article{yoon22_formal_reason_layer_monad_inter,
	author = {Yoon, Irene and Zakowski, Yannick and Zdancewic, Steve},
	title = {Formal Reasoning about Layered Monadic Interpreters},
	year = {2022},
	issue_date = {August 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {ICFP},
	url = {https://doi.org/10.1145/3547630},
	doi = {10.1145/3547630},
	abstract = {Monadic computations built by interpreting, or handling, operations of a free monad are a compelling formalism for modeling language semantics and defining the behaviors of effectful systems. The resulting layered semantics offer the promise of modular reasoning principles based on the equational theory of the underlying monads. However, there are a number of obstacles to using such layered interpreters in practice. With more layers comes more boilerplate and glue code needed to define the monads and interpreters involved. That overhead is compounded by the need to define and justify the relational reasoning principles that characterize the equivalences at each layer. This paper addresses these problems by significantly extending the capabilities of the Coq interaction trees (ITrees) library, which supports layered monadic interpreters. We characterize a rich class of interpretable monads---obtained by applying monad transformers to ITrees---and show how to generically lift interpreters through them. We also introduce a corresponding framework for relational reasoning about "equivalence of monads up to a relation R". This collection of typeclasses, instances, new reasoning principles, and tactics greatly generalizes the existing theory of the ITree library, eliminating large amounts of unwieldy boilerplate code and dramatically simplifying proofs.},
	journal = {Proc. ACM Program. Lang.},
	month = {aug},
	articleno = {99},
	numpages = {29},
	keywords = {coinduction, Coq, compiler correctness, monads}
}

@Software{yosyshq23_symbiyosys,
	urldate = {2023-12-20},
	year = {2023},
	title = {SymbiYosys (sby)},
	author = {YosysHQ},
	url = {https://github.com/YosysHQ/SymbiYosys}
}

@article{yuan22_twist,
	author = {Yuan, Charles and McNally, Christopher and Carbin, Michael},
	title = {Twist: Sound Reasoning for Purity and Entanglement in Quantum Programs},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498691},
	doi = {10.1145/3498691},
	abstract = {Quantum programming languages enable developers to implement algorithms for quantum computers that promise computational breakthroughs in classically intractable tasks. Programming quantum computers requires awareness of entanglement, the phenomenon in which measurement outcomes of qubits are correlated. Entanglement can determine the correctness of algorithms and suitability of programming patterns. In this work, we formalize purity as a central tool for automating reasoning about entanglement in quantum programs. A pure expression is one whose evaluation is unaffected by the measurement outcomes of qubits that it does not own, implying freedom from entanglement with any other expression in the computation. We present Twist, the first language that features a type system for sound reasoning about purity. The type system enables the developer to identify pure expressions using type annotations. Twist also features purity assertion operators that state the absence of entanglement in the output of quantum gates. To soundly check these assertions, Twist uses a combination of static analysis and runtime verification. We evaluate Twist’s type system and analyses on a benchmark suite of quantum programs in simulation, demonstrating that Twist can express quantum algorithms, catch programming errors in them, and support programs that existing languages disallow, while incurring runtime verification overhead of less than 3.5%.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {30},
	numpages = {32},
	keywords = {entanglement, quantum programming, purity, type systems}
}

@inproceedings{zaretsky07_balan_sched_operat_chain_high,
	author = {Zaretsky, David C. and Mittal, Gaurav and Dick, Robert P. and Banerjee, Prith},
	booktitle = {8th International Symposium on Quality Electronic Design (ISQED'07)},
	doi = {10.1109/ISQED.2007.41},
	keywords = {operation chaining},
	pages = {595--601},
	title = {Balanced Scheduling and Operation Chaining in High-Level Synthesis for FPGA Designs},
	year = {2007}
}

@inproceedings{zhang13_sdc,
	abstract = {Modulo scheduling is a popular technique to enable pipelined execution of successive loop iterations for performance improvement. While a variety of modulo scheduling algorithms exist for software pipelining, they are not amenable to many complex design constraints and optimization goals that arise in the hardware synthesis context. In this paper we describe a modulo scheduling framework based on the formulation of system of difference constraints (SDC). Our framework can systematically model a rich set of performance constraints that are specific to the hardware design. The scheduler also exploits the unique mathematical properties of SDC to carry out efficient global optimization and fast incremental update on the constraint system to minimize the resource usage of the synthesized pipeline. Experiments demonstrate that our proposed technique provides efficient solutions for a set of real-life applications and compares favorably against a widely used lifetime-sensitive modulo scheduling algorithm.},
	author = {Zhang, Zhiru and Liu, Bin},
	booktitle = {2013 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
	doi = {10.1109/ICCAD.2013.6691121},
	issn = {1558-2434},
	keywords = {high level synthesis;pipeline processing;scheduling;SDC-based modulo scheduling;pipeline synthesis;hardware design;mathematical properties;global optimization;incremental update;Schedules;Pipeline processing;Registers;Optimal scheduling;Scheduling algorithms;Timing},
	month = nov,
	pages = {211--218},
	title = {SDC-based modulo scheduling for pipeline synthesis},
	year = {2013}
}

@inproceedings{zhang18_formal_verif_optim_compil,
	abstract = {Formally verifying that a compiler, especially an optimizing one, maintains the semantics of its input has been a challenging problem. This paper surveys several of the main efforts in the area and describes recent efforts that target the LLVM compiler infrastructure while taking a novel viewpoint on the problem.},
	author = {Zhang, Yiji and Zuck, Lenore D.},
	editor = {Negi, Atul and Bhatnagar, Raj and Parida, Laxmi},
	location = {Cham},
	publisher = {Springer International Publishing},
	booktitle = {Distributed Computing and Internet Technology},
	isbn = {978-3-319-72344-0},
	pages = {50--65},
	title = {Formal Verification of Optimizing Compilers},
	year = {2018}
}

@inproceedings{zhang18_recur_strat_symbol_execut_find,
	author = {Zhang, Rui and Sturton, Cynthia},
	title = {A Recursive Strategy for Symbolic Execution to Find Exploits in Hardware Designs},
	year = {2018},
	isbn = {9781450358330},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3219763.3219764},
	doi = {10.1145/3219763.3219764},
	abstract = {This paper presents hardware-oriented symbolic execution that uses a recursive algorithm to find, and generate exploits for, vulnerabilities in hardware designs. We first define the problem and then develop and formalize our strategy. Our approach allows for a targeted search through a possibly infinite set of execution traces to find needle-in-a-haystack error states. We demonstrate the approach on the open-source OR1200 RISC processor. Using the presented method, we find, and generate exploits for, a control-flow bug, an instruction integrity bug and an exception related bug.},
	booktitle = {Proceedings of the 2018 ACM SIGPLAN International Workshop on Formal Methods and Security},
	pages = {1–9},
	numpages = {9},
	keywords = {hardware abstract interpretation},
	location = {Philadelphia, PA, USA},
	series = {FMS 2018}
}

@InProceedings{zhang21_syntax_guided_synth_lemma_gener,
	author = "Zhang, Hongce
and Gupta, Aarti
and Malik, Sharad",
	editor = "Henglein, Fritz
and Shoham, Sharon
and Vizel, Yakir",
	title = "Syntax-Guided Synthesis for Lemma Generation in Hardware Model Checking",
	booktitle = "Verification, Model Checking, and Abstract Interpretation",
	year = "2021",
	publisher = "Springer International Publishing",
	address = "Cham",
	pages = "325--349",
	abstract = "In this work we propose to use Syntax-Guided Synthesis (SyGuS) for lemma generation in a word-level IC3/PDR framework for bit-vector problems. Hardware model checking is moving from bit-level to word-level problems, and it is expected that model checkers can benefit when such high-level information is available. However, for bit-vectors, it is challenging to find a good word-level interpolation strategy for lemma generation, which hinders the use of word-level IC3/PDR algorithms.",
	isbn = "978-3-030-67067-2"
}

@inproceedings{zhang22_cape,
	author = {Zhang, Rui and Bond, Michael D. and Zhang, Yinqian},
	title = {Cape: Compiler-Aided Program Transformation for HTM-Based Cache Side-Channel Defense},
	year = {2022},
	isbn = {9781450391832},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3497776.3517778},
	doi = {10.1145/3497776.3517778},
	abstract = {Cache side-channel attacks pose real threats to computer system security. Prior work called Cloak leverages commodity hardware transactional memory (HTM) to protect sensitive data and code from cache side-channel attacks. However, Cloak requires tedious and error-prone manual modifications to vulnerable software by programmers. This paper presents Cape, a compiler analysis and transformation that soundly and automatically protects programs from cache side-channel attacks using Cloak’s defense. An evaluation shows that Cape provides protection that is as strong as Cloak’s, while performing competitively with Cloak.},
	booktitle = {Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction},
	pages = {181–193},
	numpages = {13},
	keywords = {hardware transactional memory, compiler analysis and transformation, cache side-channel defense},
	location = {Seoul, South Korea},
	series = {CC 2022}
}

@article{zhang22_incor_logic_kleen_algeb_top_tests,
	author = {Zhang, Cheng and de Amorim, Arthur Azevedo and Gaboardi, Marco},
	title = {On Incorrectness Logic and Kleene Algebra with Top and Tests},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498690},
	doi = {10.1145/3498690},
	abstract = {Kleene algebra with tests (KAT) is a foundational equational framework for reasoning about programs, which has found applications in program transformations, networking and compiler optimizations, among many other areas. In his seminal work, Kozen proved that KAT subsumes propositional Hoare logic, showing that one can reason about the (partial) correctness of while programs by means of the equational theory of KAT. In this work, we investigate the support that KAT provides for reasoning about incorrectness, instead, as embodied by O'Hearn's recently proposed incorrectness logic. We show that KAT cannot directly express incorrectness logic. The main reason for this limitation can be traced to the fact that KAT cannot express explicitly the notion of codomain, which is essential to express incorrectness triples. To address this issue, we study Kleene Algebra with Top and Tests (TopKAT), an extension of KAT with a top element. We show that TopKAT is powerful enough to express a codomain operation, to express incorrectness triples, and to prove all the rules of incorrectness logic sound. This shows that one can reason about the incorrectness of while-like programs by means of the equational theory of TopKAT.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {29},
	numpages = {30},
	keywords = {Kleene Algebra with Tests, Incorrectness Logic, Hoare Logic, Program Reasoning}
}

@article{zhang22_reason_reason_reason,
	author = {Zhang, Yizhou and Amin, Nada},
	title = {Reasoning about “Reasoning about Reasoning”: Semantics and Contextual Equivalence for Probabilistic Programs with Nested Queries and Recursion},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498677},
	doi = {10.1145/3498677},
	abstract = {Metareasoning can be achieved in probabilistic programming languages (PPLs) using agent models that recursively nest inference queries inside inference queries. However, the semantics of this powerful, reflection-like language feature has defied an operational treatment, much less reasoning principles for contextual equivalence. We give formal semantics to a core PPL with continuous distributions, scoring, general recursion, and nested queries. Unlike prior work, the presence of nested queries and general recursion makes it impossible to stratify the definition of a sampling-based operational semantics and that of a measure-theoretic semantics—the two semantics must be defined mutually recursively. A key yet challenging property we establish is that probabilistic programs have well-defined meanings: limits exist for the step-indexed measures they induce. Beyond a semantics, we offer relational reasoning principles for probabilistic programs making nested queries. We construct a step-indexed, biorthogonal logical-relations model. A soundness theorem establishes that logical relatedness implies contextual equivalence. We demonstrate the usefulness of the reasoning principles by proving novel equivalences of practical relevance—in particular, game-playing and decisionmaking agents. We mechanize our technical developments leading to the soundness proof using the Coq proof assistant. Nested queries are an important yet theoretically underdeveloped linguistic feature in PPLs; we are first to give them semantics in the presence of general recursion and to provide them with sound reasoning principles for contextual equivalence.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {16},
	numpages = {28},
	keywords = {nested queries, Metareasoning, program equivalence, logical relations}
}

@article{zhang22_relat_e_match,
	author = {Zhang, Yihong and Wang, Yisu Remy and Willsey, Max and Tatlock, Zachary},
	title = {Relational E-Matching},
	year = {2022},
	issue_date = {January 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {POPL},
	url = {https://doi.org/10.1145/3498696},
	doi = {10.1145/3498696},
	abstract = {We present a new approach to e-matching based on relational join; in particular, we apply recent database query execution techniques to guarantee worst-case optimal run time. Compared to the conventional backtracking approach that always searches the e-graph "top down", our new relational e-matching approach can better exploit pattern structure by searching the e-graph according to an optimized query plan. We also establish the first data complexity result for e-matching, bounding run time as a function of the e-graph size and output size. We prototyped and evaluated our technique in the state-of-the-art egg e-graph framework. Compared to a conventional baseline, relational e-matching is simpler to implement and orders of magnitude faster in practice.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {35},
	numpages = {22},
	keywords = {E-matching, Relational Join Algorithms}
}

@inproceedings{zhao12_formal_llvm_inter_repres_verif_progr_trans,
	author = {Zhao, Jianzhou and Nagarakatte, Santosh and Martin, Milo M.K. and Zdancewic, Steve},
	location = {Philadelphia, PA, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/2103656.2103709},
	booktitle = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
	doi = {10.1145/2103656.2103709},
	isbn = {9781450310833},
	keywords = {Coq,LLVM,memory safety},
	pages = {427--440},
	series = {POPL '12},
	title = {Formalizing the {LLVM} Intermediate Representation for Verified Program Transformations},
	year = {2012}
}

@inproceedings{zhao17_comba,
	author = {Zhao, Jieru and Feng, Liang and Sinha, Sharad and Zhang, Wei and Liang, Yun and He, Bingsheng},
	booktitle = {2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
	doi = {10.1109/ICCAD.2017.8203809},
	pages = {430--437},
	title = {COMBA: A comprehensive model-based analysis framework for high level synthesis of real applications},
	year = {2017}
}

@inproceedings{zhao21_akg,
	abstract = {Existing tensor compilers have proven their effectiveness in deploying deep neural networks on general-purpose hardware like CPU and GPU, but optimizing for neural processing units (NPUs) is still challenging due to the heterogeneous compute units and complicated memory hierarchy. In this paper, we present AKG, a tensor compiler for NPUs. AKG first lowers the tensor expression language to a polyhedral representation, which is used to automate the memory management of NPUs. Unlike existing approaches that resort to manually written schedules, AKG leverages polyhedral schedulers to perform a much wider class of transformations, and extends the semantics of the polyhedral representation to combine complex tiling techniques and hierarchical fusion strategies. We also implement the domain-specific optimization of convolution in AKG. Moreover, to achieve the optimal performance, we introduce complementary optimizations in code generation, which is followed by an auto-tuner. We conduct extensive experiments on benchmarks ranging from single operators to end-to-end networks. The experimental results show that AKG can obtain superior performance to both manual scheduling approaches and vendor provided libraries. We believe AKG will cast a light on the follow-up compiler works on NPUs.},
	author = {Zhao, Jie and Li, Bojie and Nie, Wang and Geng, Zhen and Zhang, Renwei and Gao, Xiong and Cheng, Bin and Wu, Chen and Cheng, Yun and Li, Zheng and Di, Peng and Zhang, Kun and Jin, Xuefeng},
	location = {Virtual, Canada},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3453483.3454106},
	booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
	doi = {10.1145/3453483.3454106},
	isbn = {9781450383912},
	keywords = {neural processing units,code generation,polyhedral model,auto-tuning,neural networks},
	pages = {1233--1248},
	series = {PLDI 2021},
	title = {AKG: Automatic Kernel Generation for Neural Processing Units Using Polyhedral Transformations},
	year = {2021}
}

@inproceedings{zheng13,
	abstract = {In High-level Synthesis, scheduling has a critical impact on the quality of hardware implementation. However, the schedules of different operations are actually having unequal impacts on the Quality of Result. Based on this fact, we propose a novel scheduling framework, which is able to schedule the operations separately according their significance to Quality of Result, to avoid wasting the computational efforts on noncritical operations. Furthermore, the proposed framework supports global code motion, which helps to improve the speed performance of the hardware implementation by distributing the execution time of operations across the their parent BB.},
	author = {Zheng, Hongbin and Liu, Qingrui and Li, Junyi and Chen, Dihu and Wang, Zixin},
	booktitle = {2013 18th Asia and South Pacific Design Automation Conference (ASP-DAC)},
	doi = {10.1109/ASPDAC.2013.6509695},
	issn = {2153-6961},
	keywords = {data flow graphs;high level synthesis;parallel algorithms;scheduling;gradual scheduling framework;problem size reduction;cross basic block parallelism exploitation;high-level synthesis;global code motion;quality of result;control-data flow-graph;Schedules;Delays;Scheduling;Scheduling algorithms;Hardware},
	month = jan,
	pages = {780--786},
	title = {A gradual scheduling framework for problem size reduction and cross basic block parallelism exploitation in high-level synthesis},
	year = {2013}
}

@inproceedings{zheng14_fast_effec_placem_routin_direc,
	author = {Zheng, Hongbin and Gurumani, Swathi T. and Rupnow, Kyle and Chen, Deming},
	title = {Fast and Effective Placement and Routing Directed High-Level Synthesis for FPGAs},
	year = {2014},
	isbn = {9781450326711},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/2554688.2554775},
	abstract = {Achievable frequency (fmax) is a widely used input constraint for designs targeting Field-Programmable Gate Arrays (FPGA), because of its impact on design latency and throughput. Fmax is limited by critical path delay, which is highly influenced by lower-level details of the circuit implementation such as technology mapping, placement and routing. However, for high-level synthesis~(HLS) design flows, it is challenging to evaluate the real critical delay at the behavioral level. Current HLS flows typically use module pre-characterization for delay estimates. However, we will demonstrate that such delay estimates are not sufficient to obtain high fmax and also minimize total execution latency.In this paper, we introduce a new HLS flow that integrates with Altera's Quartus synthesis and fast placement and routing (PAR) tool to obtain realistic post-PAR delay estimates. This integration enables an iterative flow that improves the performance of the design with both behavioral-level and circuit-level optimizations using realistic delay information. We demonstrate our HLS flow produces up to 24\% (on average 20\%) improvement in fmax and upto 22\% (on average 20\%) improvement in execution latency. Furthermore, results demonstrate that our flow is able to achieve from 65\% to 91\% of the theoretical fmax on Stratix IV devices (550MHz).},
	booktitle = {Proceedings of the 2014 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	pages = {1–10},
	numpages = {10},
	keywords = {high-level synthesis, scheduling, delay prediction},
	location = {Monterey, California, USA},
	series = {FPGA '14}
}

@article{zhou23_coqq,
	author = {Zhou, Li and Barthe, Gilles and Strub, Pierre-Yves and Liu, Junyi and Ying, Mingsheng},
	title = {CoqQ: Foundational Verification of Quantum Programs},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571222},
	doi = {10.1145/3571222},
	abstract = {CoqQ is a framework for reasoning about quantum programs in the Coq proof assistant. Its main components are: a deeply embedded quantum programming language, in which classic quantum algorithms are easily expressed, and an expressive program logic for proving properties of programs. CoqQ is foundational: the program logic is formally proved sound with respect to a denotational semantics based on state-of-art mathematical libraries (MathComp and MathComp Analysis). CoqQ is also practical: assertions can use Dirac expressions, which eases concise specifications, and proofs can exploit local and parallel reasoning, which minimizes verification effort. We illustrate the applicability of CoqQ with many examples from the literature.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {29},
	numpages = {33},
	keywords = {Quantum Programs, Proof Assistants, Program Logics, Mathematical Libraries}
}

@article{zhou23_recur_subty_all,
	author = {Zhou, Litao and Zhou, Yaoda and Oliveira, Bruno C. d. S.},
	title = {Recursive Subtyping for All},
	year = {2023},
	issue_date = {January 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {POPL},
	url = {https://doi.org/10.1145/3571241},
	doi = {10.1145/3571241},
	abstract = {Recursive types and bounded quantification are prominent features in many modern programming languages, such as Java, C#, Scala or TypeScript. Unfortunately, the interaction between recursive types, bounded quantification and subtyping has shown to be problematic in the past. Consequently, defining a simple foundational calculus that combines those features and has desirable properties, such as decidability, transitivity of subtyping, conservativity and a sound and complete algorithmic formulation has been a long time challenge. This paper presents an extension of kernel ‍F≤, called F≤µ, with iso-recursive types. F≤ is a well-known polymorphic calculus with bounded quantification. In F≤µ we add iso-recursive types, and correspondingly extend the subtyping relation with iso-recursive subtyping using the recently proposed nominal unfolding rules. We also add two smaller extensions to F≤. The first one is a generalization of the kernel ‍F≤ rule for bounded quantification that accepts equivalent rather than equal bounds. The second extension is the use of so-called structural folding/unfolding rules, inspired by the structural unfolding rule proposed by Abadi, Cardelli, and Viswanathan [1996]. The structural rules add expressive power to the more conventional folding/unfolding rules in the literature, and they enable additional applications. We present several results, including: type soundness; transitivity and decidability of subtyping; the conservativity of F≤µ over F≤; and a sound and complete algorithmic formulation of F≤µ. Moreover, we study an extension of F≤µ, called F≤≥µ, which includes lower bounded quantification in addition to the conventional (upper) bounded quantification of F≤. All the results in this paper have been formalized in the Coq theorem prover.},
	journal = {Proc. ACM Program. Lang.},
	month = {jan},
	articleno = {48},
	numpages = {30},
	keywords = {Object Encodings, Bounded Polymorphism, Iso-Recursive Subtyping}
}

@article{zhu08_from_algeb_seman_to_denot_seman_veril,
	author = {Zhu, Huibiao and He, Jifeng and Bowen, Jonathan P.},
	url = {https://doi.org/10.1007/s11334-008-0069-9},
	date = {2008-12-01},
	doi = {10.1007/s11334-008-0069-9},
	issn = {1614-5054},
	journaltitle = {Innovations in Systems and Software Engineering},
	number = {4},
	pages = {341--360},
	title = {From Algebraic Semantics To Denotational Semantics for {Verilog}},
	volume = {4}
}

@inproceedings{zuo13_improv,
	abstract = {High-level synthesis (HLS) tools are now capable of generating high-quality RTL codes for a number of programs. Nevertheless, for best performance aggressive program transformations are still required to exploit data reuse and enable communication/computation overlap. The polyhedral compilation framework has shown great promise in this area with the development of HLS-specific polyhedral transformation techniques and tools. However, all these techniques rely on polyhedral code generation to translate a schedule for the program's operations into an actual C code that is input to the HLS tool. In this work we study the changes to the state-of-the-art polyhedral code generator CLooG which are required to tailor it for HLS purposes. In particular, we develop various techniques to significantly improve resource utilization on the FPGA. We also develop a complete technique geared towards effective code generation of rectangularly tiled code, leading to further improvements in resource utilization. We demonstrate our techniques on a collection of affine benchmarks, reducing by 2x on average (up to 10x) the area used after high-level synthesis.},
	author = {Zuo, W. and Li, P. and Chen, D. and Pouchet, L. and Zhong, Shunan and Cong, J.},
	booktitle = {2013 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)},
	doi = {10.1109/CODES-ISSS.2013.6659002},
	keywords = {FPGA,high-level synthesis,polyhedral analysis},
	month = sep,
	pages = {1--10},
	title = {Improving polyhedral code generation for high-level synthesis},
	year = {2013}
}

@inproceedings{zuo13_improv_high_level_synth_optim,
	abstract = {High level synthesis (HLS) is an important enabling technology for the adoption of hardware accelerator technologies. It promises the performance and energy efficiency of hardware designs with a lower barrier to entry in design expertise, and shorter design time. State-of-the-art high level synthesis now includes a wide variety of powerful optimizations that implement efficient hardware. These optimizations can implement some of the most important features generally performed in manual designs including parallel hardware units, pipelining of execution both within a hardware unit and between units, and fine-grained data communication. We may generally classify the optimizations as those that optimize hardware implementation within a code block (intra-block) and those that optimize communication and pipelining between code blocks (inter-block). However, both optimizations are in practice difficult to apply. Real-world applications contain data-dependent blocks of code and communicate through complex data access patterns. Existing high level synthesis tools cannot apply these powerful optimizations unless the code is inherently compatible, severely limiting the optimization opportunity. In this paper we present an integrated framework to model and enable both intra- and inter-block optimizations. This integrated technique substantially improves the opportunity to use the powerful HLS optimizations that implement parallelism, pipelining, and fine-grained communication. Our polyhedral model-based technique systematically defines a set of data access patterns, identifies effective data access patterns, and performs the loop transformations to enable the intra- and inter-block optimizations. Our framework automatically explores transformation options, performs code transformations, and inserts the appropriate HLS directives to implement the HLS optimizations. Furthermore, our framework can automatically generate the optimized communication blocks for fine-grained communication between hardware blocks. Experimental evaluation demonstrates that we can achieve an average of 6.04X speedup over the high level synthesis solution without our transformations to enable intra- and inter-block optimizations.},
	author = {Zuo, Wei and Liang, Yun and Li, Peng and Rupnow, Kyle and Chen, Deming and Cong, Jason},
	location = {Monterey, California, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/2435264.2435271},
	booktitle = {Proceedings of the ACM/SIGDA International Symposium on Field Programmable Gate Arrays},
	doi = {10.1145/2435264.2435271},
	isbn = {9781450318877},
	keywords = {FPGA,high-level synthesis,polyhedral analysis},
	pages = {9--18},
	series = {FPGA '13},
	title = {Improving High Level Synthesis Optimization Opportunity through Polyhedral Transformations},
	year = {2013}
}

@Comment{
Local Variables:
bibtex-dialect: biblatex
End:
}

