@Comment{
ebib-main-file: /home/ymh/Dropbox/bibliography/references.bib
}


@article{05_ieee_stand_veril_regis_trans_level_synth,
	author = {IEEE},
	title = {{IEEE} Standard for {Verilog} Register Transfer Level Synthesis},
	journal = {IEC 62142-2005 First edition 2005-06 IEEE Std 1364.1},
	volume = {},
	number = {},
	pages = {1-116},
	year = {2005},
	doi = {10.1109/IEEESTD.2005.339572},
	ISSN = {},
	key = {IEEE Std 1364.1},
	keywords = {IEC Standards;Verilog;Registers},
	month = {},
	type = {Standard}
}

@article{06_ieee_stand_veril_hardw_descr_languag,
	author = {IEEE},
	title = {{IEEE} Standard for Verilog Hardware Description Language},
	journal = {IEEE Std 1364-2005 (Revision of IEEE Std 1364-2001)},
	volume = {},
	number = {},
	pages = {1-590},
	year = {2006},
	doi = {10.1109/IEEESTD.2006.99495},
	ISSN = {},
	key = {IEEE Std 1364},
	month = {4},
	type = {Standard}
}

@Software{absint19_compc,
	year = {2019},
	title = {{CompCert} release 19.10},
	author = {AbsInt},
	url = {https://www.absint.com/releasenotes/compcert/19.10/}
}

@inproceedings{allen70_cfa,
	keywords = {control-flow},
	author = {Allen, Frances E.},
	title = {Control Flow Analysis},
	year = {1970},
	isbn = {9781450373869},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/800028.808479},
	abstract = {Any static, global analysis of the expression and data relationships in a program requires a knowledge of the control flow of the program. Since one of the primary reasons for doing such a global analysis in a compiler is to produce optimized programs, control flow analysis has been embedded in many compilers and has been described in several papers. An early paper by Prosser [5] described the use of Boolean matrices (or, more particularly, connectivity matrices) in flow analysis. The use of “dominance” relationships in flow analysis was first introduced by Prosser and much expanded by Lowry and Medlock [6]. References [6,8,9] describe compilers which use various forms of control flow analysis for optimization. Some recent developments in the area are reported in [4] and in [7].The underlying motivation in all the different types of control flow analysis is the need to codify the flow relationships in the program. The codification may be in connectivity matrices, in predecessor-successor tables, in dominance lists, etc. Whatever the form, the purpose is to facilitate determining what the flow relationships are; in other words to facilitate answering such questions as: is this an inner loop?, if an expression is removed from the loop where can it be correctly and profitably placed?, which variable definitions can affect this use?In this paper the basic control flow relationships are expressed in a directed graph. Various graph constructs are then found and shown to codify interesting global relationships.},
	booktitle = {Proceedings of a Symposium on Compiler Optimization},
	pages = {1–19},
	numpages = {19},
	location = {Urbana-Champaign, Illinois}
}

@inproceedings{allen83_conver_contr_depen_data_depen,
	abstract = {Program analysis methods, especially those which support automatic vectorization, are based on the concept of interstatement dependence where a dependence holds between two statements when one of the statements computes values needed by the other. Powerful program transformation systems that convert sequential programs to a form more suitable for vector or parallel machines have been developed using this concept [AllK 82, KKLW 80].The dependence analysis in these systems is based on data dependence. In the presence of complex control flow, data dependence is not sufficient to transform programs because of the introduction of control dependences. A control dependence exists between two statements when the execution of one statement can prevent the execution of the other. Control dependences do not fit conveniently into dependence-based program translators.One solution is to convert all control dependences to data dependences by eliminating goto statements and introducing logical variables to control the execution of statements in the program. In this scheme, action statements are converted to IF statements. The variables in the conditional expression of an IF statement can be viewed as inputs to the statement being controlled. The result is that control dependences between statements become explicit data dependences expressed through the definitions and uses of the controlling logical variables.This paper presents a method for systematically converting control dependences to data dependences in this fashion. The algorithms presented here have been implemented in PFC, an experimental vectorizer written at Rice University.},
	author = {Allen, J. R. and Kennedy, Ken and Porterfield, Carrie and Warren, Joe},
	location = {Austin, Texas},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 10th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
	doi = {10.1145/567067.567085},
	isbn = {0897910907},
	keywords = {if-conversion},
	pages = {177--189},
	series = {POPL '83},
	title = {Conversion of Control Dependence to Data Dependence},
	year = {1983}
}

@misc{amd23_vitis_forum,
	author = {AMD},
	title = {Vitis Forums},
	url = {https://bit.ly/vitisifc},
	urldate = {2023-06-02},
	year = {2023},
	note = {Relevant quote from AMD: ``If-Conversion aims to convert a sequence of blocks into a single block for better optimization result.''}
}

@misc{amd23_vitis_high_synth,
	author = {AMD},
	title = {Vitis High-level Synthesis},
	url = {https://bit.ly/41R0204},
	urldate = {2023-05-21},
	year = 2023
}

@inproceedings{armand11_modul_integ_sat_smt_solver,
	doi = {10.1007/978-3-642-25379-9_12},
	abstract = {We present a way to enjoy the power of SAT and SMT provers in Coq without compromising soundness. This requires these provers to return not only a yes/no answer, but also a proof witness that can be independently rechecked. We present such a checker, written and fully certified in Coq. It is conceived in a modular way, in order to tame the proofs' complexity and to be extendable. It can currently check witnesses from the SAT solver ZChaff and from the SMT solver veriT. Experiments highlight the efficiency of this checker. On top of it, new reflexive Coq tactics have been built that can decide a subset of Coq's logic by calling external provers and carefully checking their answers.},
	author = {Armand, Michael and Faure, Germain and Grégoire, Benjamin and Keller, Chantal and Théry, Laurent and Werner, Benjamin},
	editor = {Jouannaud, Jean-Pierre and Shao, Zhong},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Certified Programs and Proofs},
	isbn = {978-3-642-25379-9},
	keywords = {SAT,verification,coq},
	pages = {135--150},
	title = {A Modular Integration of SAT/SMT Solvers to Coq through Proof Witnesses},
	year = {2011}
}

@article{aubury96_handel_c_languag_refer_guide,
	author = {Aubury, Matthew and Page, Ian and Randall, Geoff and Saul, Jonathan and Watts, Robin},
	journaltitle = {Computing Laboratory. Oxford University, UK},
	keywords = {handel-c,synthesis},
	title = {Handel-C Language Reference Guide},
	year = {1996}
}

@inproceedings{bachrach12_chisel,
	author = {Bachrach, Jonathan and Vo, Huy and Richards, Brian and Lee, Yunsup and Waterman, Andrew and Avižienis, Rimas and Wawrzynek, John and Asanović, Krste},
	organization = {IEEE},
	booktitle = {DAC Design Automation Conference 2012},
	doi = {10.1145/2228360.2228584},
	pages = {1212--1221},
	title = {{Chisel: Constructing hardware in a Scala embedded language}},
	year = {2012}
}

@book{baker19_princ,
	abstract = {An updated edition of the text that explores the core topics in scheduling theory The second edition of Principles of Sequencing and Scheduling has been revised and updated to provide comprehensive coverage of sequencing and scheduling topics as well as emerging developments in the field. The text offers balanced coverage of deterministic models and stochastic models and includes new developments in safe scheduling and project scheduling, including coverage of project analytics. These new topics help bridge the gap between classical scheduling and actual practice. The authors—noted experts in the field—present a coherent and detailed introduction to the basic models, problems, and methods of scheduling theory. This book offers an introduction and overview of sequencing and scheduling and covers such topics as single-machine and multi-machine models, deterministic and stochastic problem formulations, optimization and heuristic solution approaches, and generic and specialized software methods. This new edition adds coverage on topics of recent interest in shop scheduling and project scheduling. This important resource: Offers comprehensive coverage of deterministic models as well as recent approaches and developments for stochastic models Emphasizes the application of generic optimization software to basic sequencing problems and the use of spreadsheet-based optimization methods Includes updated coverage on safe scheduling, lognormal modeling, and job selection Provides basic coverage of robust scheduling as contrasted with safe scheduling Adds a new chapter on project analytics, which supports the PERT21 framework for project scheduling in a stochastic environment. Extends the coverage of PERT 21 to include hierarchical scheduling Provides end-of-chapter references and access to advanced Research Notes, to aid readers in the further exploration of advanced topics Written for upper-undergraduate and graduate level courses covering such topics as scheduling theory and applications, project scheduling, and operations scheduling, the second edition of Principles of Sequencing and Scheduling is a resource that covers scheduling techniques and contains the most current research and emerging topics.},
	author = {Baker, Kenneth R.},
	address = {Hoboken, NJ},
	booktitle = {Principles of sequencing and scheduling},
	edition = {Second edition.},
	isbn = {1-119-26259-3},
	keywords = {scheduling, list scheduling},
	language = {eng},
	publisher = {Wiley},
	series = {Wiley series in operations research and management science},
	title = {Principles of sequencing and scheduling },
	year = {2019}
}

@inproceedings{ball93_branc_predic_free,
	keywords = {if-conversion},
	author = {Ball, Thomas and Larus, James R.},
	title = {Branch Prediction for Free},
	year = {1993},
	isbn = {0897915984},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/155090.155119},
	abstract = {Many compilers rely on branch prediction to improve program performance by identifying frequently executed regions and by aiding in scheduling instructions.Profile-based predictors require a time-consuming and inconvenient compile-profile-compile cycle in order to make predictions. We present a program-based branch predictor that performs well for a large and diverse set of programs written in C and Fortran. In addition to using natural loop analysis to predict branches that control the iteration of loops, we focus on heuristics for predicting non-loop branches, which dominate the dynamic branch count of many programs. The heuristics are simple and require little program analysis, yet they are effective in terms of coverage and miss rate. Although program-based prediction does not equal the accuracy of profile-based prediction, we believe it reaches a sufficiently high level to be useful. Additional type and semantic information available to a compiler would enhance our heuristics.},
	booktitle = {Proceedings of the ACM SIGPLAN 1993 Conference on Programming Language Design and Implementation},
	pages = {300–313},
	numpages = {14},
	location = {Albuquerque, New Mexico, USA},
	series = {PLDI '93}
}

@article{banerjee14_verif_code_motion_techn_using_value_propag,
	author = {Banerjee, K. and Karfa, C. and Sarkar, D. and Mandal, C.},
	doi = {10.1109/TCAD.2014.2314392},
	issn = {1937-4151},
	journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {translation validation,compiler optimisation,verification,high-level synthesis},
	month = aug,
	number = {8},
	pages = {1180--1193},
	title = {Verification of Code Motion Techniques Using Value Propagation},
	volume = {33},
	year = {2014}
}

@InProceedings{barbosa22_cvc5,
	author = {Barbosa, Haniel and Barrett, Clark and Brain, Martin and Kremer, Gereon and Lachnitt, Hanna and Mann, Makai and Mohamed, Abdalrhman and Mohamed, Mudathir and Niemetz, Aina and N{\"o}tzli, Andres and Ozdemir, Alex and Preiner, Mathias and Reynolds, Andrew and Sheng, Ying and Tinelli, Cesare and Zohar, Yoni},
	editor = "Fisman, Dana
and Rosu, Grigore",
	title = {{cvc5}: A Versatile and Industrial-Strength SMT Solver},
	booktitle = "Tools and Algorithms for the Construction and Analysis of Systems",
	year = "2022",
	publisher = "Springer International Publishing",
	address = "Cham",
	pages = "415--442",
	abstract = "cvc5 is the latest SMT solver in the cooperating validity checker series and builds on the successful code base of CVC4. This paper serves as a comprehensive system description of cvc5 's architectural design and highlights the major features and components introduced since CVC4  1.8. We evaluate cvc5 's performance on all benchmarks in SMT-LIB and provide a comparison against CVC4 and Z3.",
	isbn = "978-3-030-99524-9"
}

@article{barthe14_formal_verif_ssa_based_middl_end_compc,
	abstract = {CompCert is a formally verified compiler that generates compact and efficient code for a large subset of the C language. However, CompCert foregoes using SSA, an intermediate representation employed by many compilers that enables writing simpler, faster optimizers. In fact, it has remained an open problem to verify formally an SSA-based compiler. We report on a formally verified, SSA-based middle-end for CompCert. In addition to providing a formally verified SSA-based middle-end, we address two problems raised by Leroy in [2009]: giving an intuitive formal semantics to SSA, and leveraging its global properties to reason locally about program optimizations.},
	author = {Barthe, Gilles and Demange, Delphine and Pichardie, David},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	doi = {10.1145/2579080},
	issn = {0164-0925},
	journaltitle = {ACM Trans. Program. Lang. Syst.},
	keywords = {CompCertSSA,CompCert,SSA,coq,verification,compiler optimisation},
	month = mar,
	number = {1},
	title = {Formal Verification of an SSA-Based Middle-End for CompCert},
	volume = {36},
	year = {2014}
}

@software{berkelaar10,
	year = {2010},
	title = {\texttt{lp\_solve} v5.5},
	author = {Berkelaar, Michel},
	url = {https://lpsolve.sourceforge.net/5.5/}
}

@book{bertot04_inter_theor_provin_progr_devel,
	author = {Bertot, Yves and Castéran, Pierre},
	publisher = {Springer Berlin Heidelberg},
	doi = {10.1007/978-3-662-07964-5},
	keywords = {coq,verification},
	title = {Interactive Theorem Proving and Program Development},
	year = {2004}
}

@article{besson18_compc,
	author = {Besson, Frédéric and Blazy, Sandrine and Wilke, Pierre},
	publisher = {Springer Science and Business Media {LLC}},
	doi = {10.1007/s10817-018-9496-y},
	journaltitle = {Journal of Automated Reasoning},
	month = nov,
	number = {2},
	pages = {369--392},
	title = {{CompCertS}: A Memory-Aware Verified C Compiler Using a Pointer as Integer Semantics},
	volume = {63},
	year = {2018}
}

@ARTICLE{biesenack93_siemen_callas,
	author = {Biesenack, J. and Koster, M. and Langmaier, A. and Ledeux, S. and Marz, S. and Payer, M. and Pilsl, M. and Rumler, S. and Soukup, H. and Wehn, N. and Duzy, P.},
	journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	title = {The Siemens high-level synthesis system CALLAS},
	year = {1993},
	volume = {1},
	number = {3},
	pages = {244-253},
	abstract = {In this paper we present the Siemens high-level synthesis system CALLAS and describe its design methodology and synthesis strategy. It supports the synthesis of control-dominated applications and uses a VHDL subset for the algorithmic specification. Its main feature can be characterized as "What you simulate is what you synthesize." This principle permits a validation of the synthesis results by simulation or even formal verification. CALLAS has been successfully applied on real designs which were implemented in silicon. These examples demonstrate that CALLAS fulfils the constraints and objectives of a hardware designer. The circuits are comparable in quality to results achieved by synthesis starting at the register-transfer-level.<>},
	keywords = {},
	doi = {10.1109/92.238438},
	ISSN = {1557-9999},
	month = {Sep.}
}

@inproceedings{blazy05_formal_verif_memor_model_c,
	abstract = {This paper presents a formal verification with the Coq proof assistant of a memory model for C-like imperative languages. This model defines the memory layout and the operations that manage the memory. The model has been specified at two levels of abstraction and implemented as part of an ongoing certification in Coq of a moderately-optimising C compiler. Many properties of the memory have been verified in the specification. They facilitate the definition of precise formal semantics of C pointers. A certified OCaml code implementing the memory model has been automatically extracted from the specifications.},
	author = {Blazy, Sandrine and Leroy, Xavier},
	editor = {Lau, Kung-Kiu and Banach, Richard},
	location = {Berlin, Heidelberg},
	publisher = {Springer},
	booktitle = {Formal Methods and Software Engineering},
	isbn = {978-3-540-32250-4},
	keywords = {verification,coq,theorem prover,memory model,C,CompCert},
	pages = {280--299},
	title = {Formal Verification of a Memory Model for C-Like Imperative Languages},
	year = {2005}
}

@inproceedings{bourgeat20_essen_blues,
	abstract = {The Bluespec hardware-description language presents a significantly higher-level view than hardware engineers are used to, exposing a simpler concurrency model that promotes formal proof, without compromising on performance of compiled circuits. Unfortunately, the cost model of Bluespec has been unclear, with performance details depending on a mix of user hints and opaque static analysis of potential concurrency conflicts within a design. In this paper we present Koika, a derivative of Bluespec that preserves its desirable properties and yet gives direct control over the scheduling decisions that determine performance. Koika has a novel and deterministic operational semantics that uses dynamic analysis to avoid concurrency anomalies. Our implementation includes Coq definitions of syntax, semantics, key metatheorems, and a verified compiler to circuits. We argue that most of the extra circuitry required for dynamic analysis can be eliminated by compile-time BSV-style static analysis.},
	author = {Bourgeat, Thomas and Pit-Claudel, Clément and Chlipala, Adam and Arvind},
	location = {London, UK},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
	doi = {10.1145/3385412.3385965},
	isbn = {9781450376136},
	keywords = {Bluespec,verification,coq,operational semantics,hardware},
	pages = {243--257},
	series = {PLDI 2020},
	title = {The Essence of Bluespec: A Core Language for Rule-Based Hardware Design},
	year = {2020}
}

@inproceedings{bouton09,
	doi = {10.1007/978-3-642-02959-2_12},
	abstract = {This article describes the first public version of the satisfiability modulo theory (SMT) solver veriT. It is open-source, proof-producing, and complete for quantifier-free formulas with uninterpreted functions and difference logic on real numbers and integers.},
	author = {Bouton, Thomas and Caminha B. de Oliveira, Diego and Déharbe, David and Fontaine, Pascal},
	editor = {Schmidt, Renate A.},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Automated Deduction -- CADE-22},
	isbn = {978-3-642-02959-2},
	pages = {151--156},
	title = {veriT: An Open, Trustable and Efficient SMT-Solver},
	year = {2009}
}

@ARTICLE{boutros21_fpga_archit,
	author = {Boutros, Andrew and Betz, Vaughn},
	journal = {IEEE Circuits and Systems Magazine},
	title = {FPGA Architecture: Principles and Progression},
	year = {2021},
	volume = {21},
	number = {2},
	pages = {4-29},
	doi = {10.1109/MCAS.2021.3071607}
}

@inproceedings{bowen20_perfor_cost_softw_based_secur_mitig,
	author = {Bowen, Lucy and Lupo, Chris},
	title = {The Performance Cost of Software-Based Security Mitigations},
	year = {2020},
	isbn = {9781450369916},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3358960.3379139},
	abstract = {Historically, performance has been the most important feature when optimizing computer hardware. Modern processors are so highly optimized that every cycle of computation time matters. However, this practice of optimizing for performance at all costs has been called into question by new microarchitectural attacks, e.g. Meltdown and Spectre. Microarchitectural attacks exploit the effects of microarchitectural components or optimizations in order to leak data to an attacker. These attacks have caused processor manufacturers to introduce performance impacting mitigations in both software and silicon. To investigate the performance impact of the various mitigations, a test suite of forty-seven different tests was created. This suite was run on a series of virtual machines that tested both Ubuntu 16 and Ubuntu 18. These tests investigated the performance change across version updates and the performance impact of CPU core number vs. default microarchitectural mitigations. The testing proved that the performance impact of the microarchitectural mitigations is non-trivial, as the percent difference in performance can be as high as 200\%.},
	booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
	pages = {210–217},
	numpages = {8},
	keywords = {performance evaluation, hardware vulnerabilities, security mitigations},
	location = {Edmonton AB, Canada},
	series = {ICPE '20}
}

@article{bowen98_hclrm,
	author = {Bowen, Matthew},
	journaltitle = {Embedded Solutions Ltd},
	title = {Handel-C Language Reference Manual},
	volume = {2},
	year = {1998}
}

@inproceedings{braibant13_formal_verif_hardw_synth,
	abstract = {We report on the implementation of a certified compiler for a high-level hardware description language (HDL) called Fe-Si (FEatherweight SynthesIs). Fe-Si is a simplified version of Bluespec, an HDL based on a notion of guarded atomic actions. Fe-Si is defined as a dependently typed deep embedding in Coq. The target language of the compiler corresponds to a synthesisable subset of Verilog or VHDL. A key aspect of our approach is that input programs to the compiler can be defined and proved correct inside Coq. Then, we use extraction and a Verilog back-end (written in OCaml) to get a certified version of a hardware design.},
	author = {Braibant, Thomas and Chlipala, Adam},
	editor = {Sharygina, Natasha and Veith, Helmut},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Computer Aided Verification},
	isbn = {978-3-642-39799-8},
	keywords = {synthesis,hardware,verification},
	pages = {213--228},
	title = {Formal Verification of Hardware Synthesis},
	year = {2013}
}

@InProceedings{brummayer09_b,
	keywords = {SMT},
	author = {Brummayer, Robert and Biere, Armin},
	editor = "Kowalewski, Stefan
and Philippou, Anna",
	title = "Boolector: An Efficient SMT Solver for Bit-Vectors and Arrays",
	booktitle = "Tools and Algorithms for the Construction and Analysis of Systems",
	year = "2009",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "174--177",
	abstract = "Satisfiability Modulo Theories (SMT) is the problem of deciding satisfiability of a logical formula, expressed in a combination of first-order theories. We present the architecture and selected features of Boolector, which is an efficient SMT solver for the quantifier-free theories of bit-vectors and arrays. It uses term rewriting, bit-blasting to handle bit-vectors, and lemmas on demand for arrays.",
	isbn = "978-3-642-00768-2"
}

@inproceedings{budiu02_compil_applic_specif_hardw,
	author = {Mihai Budiu and Seth Copen Goldstein},
	editor = {Manfred Glesner and
                  Peter Zipf and
                  Michel Renovell},
	title = {Compiling Application-Specific Hardware},
	booktitle = {Field-Programmable Logic and Applications, Reconfigurable Computing
                  Is Going Mainstream, 12th International Conference, {FPL} 2002, Montpellier,
                  France, September 2-4, 2002, Proceedings},
	series = {Lecture Notes in Computer Science},
	volume = {2438},
	pages = {853--863},
	publisher = {Springer},
	year = {2002},
	doi = {10.1007/3-540-46117-5\_88},
	timestamp = {Tue, 14 May 2019 10:00:48 +0200},
	biburl = {https://dblp.org/rec/conf/fpl/BudiuG02.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Software{cadence23_c,
	urldate = {2023-12-20},
	url = {https://www.cadence.com/en_US/home/tools/digital-design-and-signoff/logic-equivalence-checking/conformal-equivalence-checker.html},
	year = {2023},
	title = {Conformal Equivalence Checker},
	author = {Cadence}
}

@Software{cadence23_j,
	urldate = {2023-12-20},
	url = {https://www.cadence.com/en_US/home/tools/system-design-and-verification/formal-and-static-verification/jasper-c-formal-verification.html},
	year = {2023},
	title = {Jasper {C2RTL}},
	author = {Cadence}
}

@inproceedings{callahan98_instr_level_paral_recon_comput,
	author = {Timothy J. Callahan and John Wawrzynek},
	editor = {Reiner W. Hartenstein and
                  Andres Keevallik},
	title = {Instruction-Level Parallelism for Reconfigurable Computing},
	booktitle = {Field-Programmable Logic and Applications, From FPGAs to Computing
                  Paradigm, 8th International Workshop, FPL'98, Tallinn, Estonia, August
                  31 - September 3, 1998, Proceedings},
	series = {Lecture Notes in Computer Science},
	volume = {1482},
	pages = {248--257},
	publisher = {Springer},
	year = {1998},
	doi = {10.1007/BFb0055252},
	timestamp = {Tue, 14 May 2019 10:00:48 +0200},
	biburl = {https://dblp.org/rec/conf/fpl/CallahanW98.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{campbell93_refin,
	author = {Campbell, Philip L and Krishna, Ksheerabdhi and Ballance, Robert A},
	publisher = {Citeseer},
	journaltitle = {Cs93-6, University of New Mexico, Albuquerque},
	keywords = {gated-SSA},
	title = {Refining and defining the program dependence web},
	year = {1993}
}

@article{canis13_l,
	author = {Canis, Andrew and Choi, Jongsok and Aldham, Mark and Zhang, Victor and Kammoona, Ahmed and Czajkowski, Tomasz and Brown, Stephen D. and Anderson, Jason H.},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	doi = {10.1145/2514740},
	issn = {1539-9087},
	journaltitle = {ACM Trans. Embed. Comput. Syst.},
	keywords = {High-level synthesis,FPGAs,hardware/software codesign,synthesis,performance,power,field-programmable gate arrays},
	month = 9,
	number = {2},
	title = {Legup: an Open-Source High-Level Synthesis Tool for Fpga-Based Processor/accelerator Systems},
	volume = {13},
	year = {2013}
}

@inproceedings{canis14_modul_sdc,
	abstract = {Loop pipelining is a high-level synthesis scheduling technique that overlaps loop iterations to achieve higher performance. However, industrial designs often have resource constraints and other constraints imposed by cross-iteration dependencies. The interaction between multiple constraints can pose a challenge for HLS modulo scheduling algorithms, which, if not handled properly can lead to a loop pipeline schedule that fails to achieve the minimum possible initiation interval. We propose a novel modulo scheduler based on an SDC formulation that includes a backtracking mechanism to properly handle multiple scheduling constraints and still achieve the minimum possible initiation interval. The SDC formulation has the advantage of being a mathematical framework that supports flexible constraints that are useful for more complex loop pipelines. Furthermore, we describe how to specifically apply associative expression transformations during modulo scheduling to restructure recurrences in complex loops to enable better scheduling. We compared our techniques to existing prior work in modulo scheduling in HLS and also compared against a state-of-art commercial tool. Over a suite of benchmarks, we show that our scheduler and proposed optimizations can result in a geomean wall-clock time reduction of 32 \% versus prior work and 29 \% versus a commercial tool.},
	author = {Canis, A. and Brown, S. D. and Anderson, J. H.},
	booktitle = {2014 24th International Conference on Field Programmable Logic and Applications (FPL)},
	doi = {10.1109/FPL.2014.6927490},
	issn = {1946-1488},
	keywords = {high-level synthesis,modulo scheduling,loop scheduling,static scheduling},
	month = sep,
	pages = {1--8},
	title = {Modulo SDC scheduling with recurrence minimization in high-level synthesis},
	year = {2014}
}

@PhdThesis{canis15_legup,
	author = {Canis, Andrew},
	keywords = {high-level synthesis,hardware/software co-simulation,FPGA},
	title = {Legup: open-source high-level synthesis research framework},
	type = {phdthesis},
	year = {2015}
}

@article{chang91_using_profil_infor_assis_class_code_optim,
	author = {Pohua P. Chang and
                  Scott A. Mahlke and
                  Wen{-}mei W. Hwu},
	title = {Using Profile Information to Assist Classic Code Optimizations},
	journal = {Softw. Pract. Exp.},
	volume = {21},
	number = {12},
	pages = {1301--1321},
	year = {1991},
	doi = {10.1002/spe.4380211204},
	timestamp = {Thu, 09 Apr 2020 17:14:34 +0200},
	biburl = {https://dblp.org/rec/journals/spe/ChangMH91.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{chapman92_verif_bedroc,
	abstract = {The authors present the HardwarePal hardware description language and formal operational and denotational semantics for it, briefly discussing their proof of the two semantics' equivalence. They then discuss their intermediate representation, dependence flow graphs, and the operational semantics of DFG. They describe the translation from HardwarePal to dependence flow graphs and outline their proof that this translation preserves the meaning of the initial HardwarePal program. The authors discuss proving the correctness of the translation from behavioral specification to intermediate form, proving the correctness of optimizations, and plans for proving the correctness of scheduling. The authors conclude by discussing their plans for proofs that register-transfer level design produced by BEDROC implements the dependence flow graph.<>},
	author = {Chapman, R. and Brown, G. and Leeser, M.},
	publisher = {IEEE Computer Society},
	booktitle = {[1992] Proceedings The European Conference on Design Automation},
	doi = {10.1109/EDAC.1992.205894},
	keywords = {verification,synthesis},
	month = mar,
	pages = {59--63},
	title = {Verified high-level synthesis in BEDROC},
	year = {1992}
}

@misc{chauhan20_formal_ensur_equiv_c_rtl,
	note = {SLEC},
	author = {Chauhan, Pankaj},
	url = {https://bit.ly/2KbT0ki},
	title = {Formally Ensuring Equivalence between C++ and RTL designs},
	year = {2020}
}

@article{chouksey19_trans_valid_code_motion_trans_invol_loops,
	author = {Chouksey, R. and Karfa, C. and Bhaduri, P.},
	doi = {10.1109/TCAD.2018.2846654},
	issn = {1937-4151},
	journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {translation validation,verification,compiler optimisation,high-level synthesis},
	month = jul,
	number = {7},
	pages = {1378--1382},
	title = {Translation Validation of Code Motion Transformations Involving Loops},
	volume = {38},
	year = {2019}
}

@article{chouksey20_verif_sched_condit_behav_high_level_synth,
	abstract = {High-level synthesis (HLS) technique translates the behaviors written in high-level languages like C/C++ into register transfer level (RTL) design. Due to its complexity, proving the correctness of an HLS tool is prohibitively expensive. Translation validation is the process of proving that the target code is a correct translation of the source program being compiled. The path-based equivalence checking (PBEC) method is a widely used translation validation method for verification of the scheduling phase of HLS. The existing PBEC methods cannot handle significant control structure modification that occurs in the efficient scheduling of conditional behaviors. Hence, they produce a false-negative result. In this article, we identify some scenarios involving path merge/split where the state-of-the-art PBEC approaches fail to show the equivalence even though behaviors are equivalent. We propose a value propagation-based PBEC method along with a new cutpoint selection scheme to overcome this limitation. Our method can also handle the scenario where adjacent conditional blocks (CBs) having an equivalent conditional expression are combined into one CB. Experimental results demonstrate the usefulness of our method over the existing methods.},
	author = {Chouksey, R. and Karfa, C.},
	doi = {10.1109/TVLSI.2020.2978242},
	issn = {1557-9999},
	journaltitle = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	keywords = {translation validation,high-level synthesis,verification,compiler optimisation},
	pages = {1--14},
	title = {Verification of Scheduling of Conditional Behaviors in High-Level Synthesis},
	year = {2020}
}

@inproceedings{clarke03_behav_c_veril,
	abstract = {We present an algorithm that checks behavioral consistency between an ANSI-C program and a circuit given in Verilog using Bounded Model Checking. Both the circuit and the program are unwound and translated into a formula that represents behavioral consistency. The formula is then checked using a SAT solver. We are able to translate C programs that include side effects, pointers, dynamic memory allocation, and loops with conditions that cannot be evaluated statically. We describe experimental results on various reactive circuits and programs, including a small processor given in Verilog and its Instruction Set Architecture given in ANSI-C.},
	author = {Clarke, E. and Kroening, D. and Yorav, K.},
	booktitle = {Proceedings 2003. Design Automation Conference (IEEE Cat. No.03CH37451)},
	doi = {10.1145/775832.775928},
	keywords = {bounded model checking,high-level synthesis,translation validation,verilog,verification},
	month = jun,
	pages = {368--371},
	title = {Behavioral consistency of C and Verilog programs using bounded model checking},
	year = {2003}
}

@inproceedings{cong06_sdc,
	abstract = {Scheduling plays a central role in the behavioral synthesis process, which automatically compiles high-level specifications into optimized hardware implementations. However, most of the existing behavior-level scheduling heuristics either have a limited efficiency in a specific class of applications or lack general support of various design constraints. In this paper we describe a new scheduler that converts a rich set of scheduling constraints into a system of difference constraints (SDC) and performs a variety of powerful optimizations under a unified mathematical programming framework. In particular, we show that our SDC-based scheduling algorithm can efficiently support resource constraints, frequency constraints, latency constraints, and relative timing constraints, and effectively optimize longest path latency, expected overall latency, and the slack distribution. Experiments demonstrate that our proposed technique provides efficient solutions for a broader range of applications with higher quality of results (in terms of system performance) when compared to the state-of-the-art scheduling heuristics},
	author = {Cong, Jason and Zhang, Zhiru},
	booktitle = {2006 43rd ACM/IEEE Design Automation Conference},
	doi = {10.1145/1146909.1147025},
	issn = {0738-100X},
	keywords = {high-level synthesis,static scheduling},
	month = jul,
	pages = {433--438},
	title = {An efficient and versatile scheduling algorithm based on SDC formulation},
	year = {2006}
}

@article{cong11_high_level_synth_fpgas,
	abstract = {Escalating system-on-chip design complexity is pushing the design community to raise the level of abstraction beyond register transfer level. Despite the unsuccessful adoptions of early generations of commercial high-level synthesis (HLS) systems, we believe that the tipping point for transitioning to HLS msystem-on-chip design complexityethodology is happening now, especially for field-programmable gate array (FPGA) designs. The latest generation of HLS tools has made significant progress in providing wide language coverage and robust compilation technology, platform-based modeling, advancement in core HLS algorithms, and a domain-specific approach. In this paper, we use AutoESL's AutoPilot HLS tool coupled with domain-specific system-level implementation platforms developed by Xilinx as an example to demonstrate the effectiveness of state-of-art C-to-FPGA synthesis solutions targeting multiple application domains. Complex industrial designs targeting Xilinx FPGAs are also presented as case studies, including comparison of HLS solutions versus optimized manual designs. In particular, the experiment on a sphere decoder shows that the HLS solution can achieve an 11-31 \% reduction in FPGA resource usage with improved design productivity compared to hand-coded design.},
	author = {Cong, J. and Liu, B. and Neuendorffer, S. and Noguera, J. and Vissers, K. and Zhang, Z.},
	doi = {10.1109/TCAD.2011.2110592},
	issn = {1937-4151},
	journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {high-level synthesis},
	month = apr,
	number = {4},
	pages = {473--491},
	title = {High-Level Synthesis for Fpgas: From Prototyping To Deployment},
	volume = {30},
	year = {2011}
}

@article{coussy09_introd_to_high_level_synth,
	author = {Coussy, P. and Gajski, D. D. and Meredith, M. and Takach, A.},
	doi = {10.1109/MDT.2009.69},
	journaltitle = {IEEE Design Test of Computers},
	keywords = {high-level synthesis,introduction,survey},
	month = jul,
	number = {4},
	pages = {8--17},
	title = {An Introduction To High-Level Synthesis},
	volume = {26},
	year = {2009}
}

@inproceedings{dessouky19_hardf,
	author = {Dessouky, Ghada and Gens, David and Haney, Patrick and Persyn, Garrett and Kanuparthi, Arun and Khattri, Hareesh and Fung, Jason M. and Sadeghi, Ahmad-Reza and Rajendran, Jeyavijayan},
	title = {{HardFails}: Insights into {Software-Exploitable} Hardware Bugs},
	booktitle = {28th USENIX Security Symposium (USENIX Security 19)},
	year = {2019},
	isbn = {978-1-939133-06-9},
	address = {Santa Clara, CA},
	pages = {213--230},
	url = {https://www.usenix.org/conference/usenixsecurity19/presentation/dessouky},
	publisher = {USENIX Association},
	month = aug
}

@thesis{ellis08_csicgfu,
	author = {Ellis, Martin},
	institution = {Newcastle University},
	url = {https://theses.ncl.ac.uk/jspui/handle/10443/828},
	title = {Correct Synthesis and Integration of Compiler-Generated Function Units},
	type = {phdthesis},
	year = {2008}
}

@phdthesis{ellis85_bulld,
	title = {Bulldog: A compiler for {VLIW} architectures},
	author = {Ellis, John R},
	year = {1985},
	school = {Yale University}
}

@misc{ferrandi14_panda_bambu,
	author = {Ferrandi, Fabrizio},
	title = {PandA-Bambu release notes},
	url = {https://github.com/ferrandi/PandA-bambu/blob/c443bf14c33a9a74008ada12f56e7a62e30e5efe/NEWS#L304},
	urldate = {2023-11-16},
	year = {2014}
}

@INPROCEEDINGS{ferrandi21_bambu,
	author = {Ferrandi, Fabrizio and Castellana, Vito Giovanni 
          and Curzel, Serena and Fezzardi, Pietro and Fiorito, Michele 
          and Lattuada, Marco and Minutoli, Marco and Pilato, Christian 
          and Tumeo, Antonino},
	booktitle = {2021 58th ACM/IEEE Design Automation Conference (DAC)},
	title = {Bambu: an Open-Source Research Framework for the 
         High-Level Synthesis of Complex Applications},
	year = {2021},
	pages = {1327-1330},
	abstract = {This paper presents the open-source high-level synthesis (HLS) research 
              framework Bambu. Bambu provides a research environment to experiment with 
              new ideas across HLS, high-level verification and debugging, FPGA/ASIC design,
              design flow space exploration, and parallel hardware accelerator design. The 
              tool accepts as input standard C/C++ specifications and compiler intermediate 
              representations (IRs) coming from the well-known Clang/LLVM and GCC compilers. 
              The broad spectrum and flexibility of input formats allow the electronic 
              design automation (EDA) research community to explore and integrate new 
              transformations and optimizations. The easily extendable modular framework 
              already includes many optimizations and HLS benchmarks used to evaluate 
              the QoR of the tool against existing approaches [1]. The integration with 
              synthesis and verification backends (commercial and open-source) allows 
              researchers to quickly test any new finding and easily obtain performance 
              and resource usage metrics for a given application. Different FPGA devices 
              are supported from several different vendors: AMD/Xilinx, Intel/Altera, 
              Lattice Semiconductor, and NanoXplore. Finally, integration with the OpenRoad 
              open-source end-to-end silicon compiler perfectly fits with the recent push 
              towards open-source EDA.},
	publisher = {{IEEE}},
	doi = {10.1109/DAC18074.2021.9586110},
	ISSN = {0738-100X},
	month = {12},
	pdf = {https://re.public.polimi.it/retrieve/668507/dac21_bambu.pdf}
}

@article{fisher81_trace_sched,
	author = {Fisher, Joseph A.},
	doi = {10.1109/TC.1981.1675827},
	journaltitle = {IEEE Transactions on Computers},
	keywords = {static scheduling,trace scheduling},
	number = {7},
	pages = {478--490},
	title = {Trace Scheduling: A Technique for Global Microcode Compaction},
	volume = {C-30},
	year = {1981}
}

@inproceedings{gajski10_what_hls,
	author = {Gajski, Dan and Austin, Todd and Svoboda, Steve},
	booktitle = {Design Automation Conference},
	doi = {10.1145/1837274.1837489},
	pages = {857--858},
	title = {What input-language is the best choice for high level synthesis (HLS)?},
	year = {2010}
}

@misc{gauthier20_high_level_synth,
	author = {Gauthier, Stephane and Wadood, Zubair},
	url = {https://info.silexica.com/high-level-synthesis/1},
	note = {White paper},
	title = {High-Level Synthesis: Can it outperform hand-coded {HDL}?},
	year = {2020}
}

@article{gonthier08_fp,
	title = {Formal Proof--the Four-Color Theorem},
	author = {Gonthier, Georges},
	journal = {Notices of the AMS},
	volume = {55},
	number = {11},
	pages = {1382--1393},
	year = {2008}
}

@misc{google23_xls,
	author = {Google},
	title = {{XLS: Accelerated HW Synthesis}},
	url = {https://github.com/google/xls/blob/dde7095ff1050b09c37cb44d1977bff1af8de050/xls/scheduling/mutual_exclusion_pass.h#L112},
	urldate = {2023-11-14},
	year = {2023},
	note = {The XLS scheduler refers to using an SMT solver to merge mutually exclusive nodes}
}

@article{gourdin23_fvopbs,
	author = {Gourdin, L\'{e}o and Bonneau, Benjamin and Boulm\'{e}, Sylvain and Monniaux, David and B\'{e}rard, Alexandre},
	title = {Formally Verifying Optimizations with Block Simulations},
	year = {2023},
	issue_date = {October 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {7},
	number = {OOPSLA2},
	url = {https://doi.org/10.1145/3622799},
	doi = {10.1145/3622799},
	abstract = {CompCert (ACM Software System Award 2021) is the first industrial-strength compiler with a mechanically checked proof of correctness. Yet, CompCert remains a moderately optimizing C compiler. Indeed, some optimizations of “gcc ‍-O1” such as Lazy Code Motion (LCM) or Strength Reduction (SR) were still missing: developing these efficient optimizations together with their formal proofs remained a challenge. Cyril Six et al. have developed efficient formally verified translation validators for certifying the results of superblock schedulers and peephole optimizations. We revisit and generalize their approach into a framework (integrated into CompCert) able to validate many more optimizations: an enhanced superblock scheduler, but also Dead Code Elimination (DCE), Constant Propagation (CP), and more noticeably, LCM and SR. In contrast to other approaches to translation validation, we co-design our untrusted optimizations and their validators. Our optimizations provide hints, in the forms of invariants or CFG morphisms, that help keep the formally verified validators both simple and efficient. Such designs seem applicable beyond CompCert.},
	journal = {Proc. ACM Program. Lang.},
	month = {oct},
	articleno = {224},
	numpages = {30},
	keywords = {Formal verification of compiler optimizations, the Coq proof assistant, Symbolic execution, Translation validation}
}

@inproceedings{greaves08_kiwi,
	author = {Greaves, David J. and Singh, Satnam},
	publisher = {{IEEE} Computer Society},
	booktitle = {{FCCM}},
	doi = {10.1109/FCCM.2008.46},
	pages = {3--12},
	title = {Kiwi: Synthesis of {FPGA} Circuits from Parallel Programs},
	year = {2008}
}

@misc{greaves19_resear_note,
	author = {Greaves, David J.},
	eprint = {1905.03746},
	eprintclass = {cs.PL},
	eprinttype = {arXiv},
	title = {Research Note: An Open Source Bluespec Compiler},
	year = {2019}
}

@inproceedings{gupta03_spark,
	abstract = {This paper presents a modular and extensible high-level synthesis research system, called SPARK, that takes a behavioral description in ANSI-C as input and produces synthesizable register-transfer level VHDL. SPARK uses parallelizing compiler technology, developed previously, to enhance instruction-level parallelism and re-instruments it for high-level synthesis by incorporating ideas of mutual exclusivity of operations, resource sharing and hardware cost models. In this paper, we present the design flow through the SPARK system, a set of transformations that include speculative code motions and dynamic transformations and show how these transformations and other optimizing synthesis and compiler techniques are employed by a scheduling heuristic. Experiments are performed on two moderately complex industrial applications, namely MPEG-1 and the GIMP image processing tool. The results show that the various code transformations lead to up to 70 \% improvements in performance without any increase in the overall area and critical path of the final synthesized design.},
	author = {Gupta, S. and Dutt, N. and Gupta, R. and Nicolau, A.},
	booktitle = {16th International Conference on VLSI Design, 2003. Proceedings.},
	doi = {10.1109/ICVD.2003.1183177},
	issn = {1063-9667},
	keywords = {high level synthesis;hardware description languages;circuit optimisation;image processing;parallelising compilers;processor scheduling;heuristic programming;code optimization;SPARK modular system;high-level synthesis framework;parallelizing compiler transformations;ANSI-C behavioral description;register-transfer level VHDL;instruction-level parallelism;operation mutual exclusivity;resource sharing;hardware cost models;dynamic transformations;speculative code motions;optimizing synthesis;scheduling heuristic;MPEG-1;GIMP image processing;Sparks;High level synthesis;Resource management;Hardware;Costs;Design optimization;Optimizing compilers;Job shop scheduling;Dynamic scheduling;Image processing},
	month = jan,
	pages = {461--466},
	title = {SPARK: a high-level synthesis framework for applying parallelizing compiler transformations},
	year = {2003}
}

@ARTICLE{halbwachs91_sdfpll,
	author = {Halbwachs, N. and Caspi, P. and Raymond, P. and Pilaud, D.},
	journal = {Proceedings of the IEEE},
	title = {The Synchronous Data Flow Programming Language {LUSTRE}},
	year = {1991},
	volume = {79},
	number = {9},
	pages = {1305-1320},
	doi = {10.1109/5.97300}
}

@inproceedings{havlak94_const,
	abstract = {Analysis of symbolic expressions benefits from a suitable program representation. We show how to build thinned gated single-assignment (TGSA) form, a value-oriented program representation which is more complete than standard SSA form, defined on all reducible programs, and better for representing symbolic expressions than program dependence graphs or original GSA form. We present practical algorithms for constructing thinned GSA form from the control dependence graph and SSA form. Extensive experiments on large Fortran programs show these methods to take linear time and space in practice. Our implementation of value numbering on TGSA form drives scalar symbolic analysis in the ParaScope programming environment.},
	author = {Havlak, Paul},
	editor = {Banerjee, Utpal and Gelernter, David and Nicolau, Alex and Padua, David},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Languages and Compilers for Parallel Computing},
	isbn = {978-3-540-48308-3},
	keywords = {gated-SSA,SSA},
	pages = {477--499},
	title = {Construction of thinned gated single-assignment form},
	year = {1994}
}

@inproceedings{herklotz20_fubfst,
	abstract = {All software ultimately relies on hardware functioning correctly. Hardware correctness is becoming increasingly important due to the growing use of custom accelerators using FPGAs to speed up applications on servers. Furthermore, the increasing complexity of hardware also leads to ever more reliance on automation, meaning that the correctness of synthesis tools is vital for the reliability of the hardware. This paper aims to improve the quality of FPGA synthesis tools by introducing a method to test them automatically using randomly generated, correct Verilog, and checking that the synthesised netlist is always equivalent to the original design. The main contributions of this work are twofold: firstly a method for generating random behavioural Verilog free of undefined values, and secondly a Verilog test case reducer used to locate the cause of the bug that was found. These are implemented in a tool called Verismith. This paper also provides a qualitative and quantitative analysis of the bugs found in Yosys, Vivado, XST and Quartus Prime. Every synthesis tool except Quartus Prime was found to introduce discrepancies between the netlist and the design. In addition to that, Vivado and a development version of Yosys were found to crash when given valid input. Using Verismith, eleven bugs were reported to tool vendors, of which six have already been fixed.},
	author = {Herklotz, Yann and Wickerson, John},
	location = {Seaside, CA, USA},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 2020 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	doi = {10.1145/3373087.3375310},
	isbn = {9781450370998},
	keywords = {test case reduction,logic synthesis,fuzzing,verilog},
	pages = {277--287},
	series = {FPGA '20},
	title = {Finding and Understanding Bugs in FPGA Synthesis Tools},
	year = {2020}
}

@inproceedings{herklotz21_esrhlst,
	author = {Herklotz, Yann and Du, Zewei and Ramanathan, Nadesh and Wickerson, John},
	booktitle = {2021 IEEE 29th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
	doi = {10.1109/FCCM51124.2021.00034},
	pages = {219--223},
	title = {An Empirical Study of the Reliability of High-Level Synthesis Tools},
	year = {2021}
}

@article{herklotz21_fvhls,
	abstract = {High-level synthesis (HLS), which refers to the automatic compilation of software into hardware, is rapidly gaining popularity. In a world increasingly reliant on application-specific hardware accelerators, HLS promises hardware designs of comparable performance and energy efficiency to those coded by hand in a hardware description language such as Verilog, while maintaining the convenience and the rich ecosystem of software development. However, current HLS tools cannot always guarantee that the hardware designs they produce are equivalent to the software they were given, thus undermining any reasoning conducted at the software level. Furthermore, there is mounting evidence that existing HLS tools are quite unreliable, sometimes generating wrong hardware or crashing when given valid inputs. To address this problem, we present the first HLS tool that is mechanically verified to preserve the behaviour of its input software. Our tool, called Vericert, extends the CompCert verified C compiler with a new hardware-oriented intermediate language and a Verilog back end, and has been proven correct in Coq. Vericert supports most C constructs, including all integer operations, function calls, local arrays, structs, unions, and general control-flow statements. An evaluation on the PolyBench/C benchmark suite indicates that Vericert generates hardware that is around an order of magnitude slower (only around 2\texttimes{} slower in the absence of division) and about the same size as hardware generated by an existing, optimising (but unverified) HLS tool.},
	author = {Herklotz, Yann and Pollard, James D. and Ramanathan, Nadesh and Wickerson, John},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	doi = {10.1145/3485494},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {high-level synthesis,Coq,Verilog,CompCert,C},
	month = oct,
	number = {OOPSLA},
	title = {Formal Verification of High-Level Synthesis},
	volume = {5},
	year = {2021}
}

@misc{herklotz21_v,
	author = {Herklotz, Yann and Pollard, James D. and Ramanathan, Nadesh and Wickerson, John},
	publisher = {Zenodo},
	doi = {10.5281/zenodo.5093839},
	month = jul,
	title = {Vericert v1.2.1},
	version = {v1.2.1},
	year = {2021}
}

@inproceedings{herklotz23_msgssa,
	author = {Herklotz, Yann and Demange, Delphine and Blazy, Sandrine},
	title = {Mechanised Semantics for Gated Static Single Assignment},
	year = {2023},
	isbn = {9798400700262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3573105.3575681},
	abstract = {The Gated Static Single Assignment (GSA) form was proposed by Ottenstein et al. in 1990, as an intermediate representation for implementing advanced static analyses and optimisation passes in compilers. Compared to SSA, GSA records additional data dependencies and provides more context, making optimisations more effective and allowing one to reason about programs as data-flow graphs. Many practical implementations have been proposed that follow, more or less faithfully, Ottenstein et al.'s seminal paper. But many discrepancies remain between these, depending on the kind of dependencies they are supposed to track and to leverage in analyses and code optimisations. In this paper, we present a formal semantics for GSA, mechanised in Coq. In particular, we clarify the nature and the purpose of gates in GSA, and define control-flow insensitive semantics for them. We provide a specification that can be used as a reference description for GSA. We also specify a translation from SSA to GSA and prove that this specification is semantics-preserving. We demonstrate that the approach is practical by implementing the specification as a validated translation within the CompCertSSA verified compiler.},
	booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	pages = {182–196},
	numpages = {15},
	keywords = {SSA, Verified Compilation, Gated SSA},
	location = {Boston, MA, USA},
	series = {CPP 2023}
}

@unpublished{herklotz24_hsvhls,
	date = {2024},
	author = {Herklotz, Yann and Wickerson, John},
	url = {https://yannherklotz.com/docs/drafts/verified_hyperblock_scheduling.pdf},
	title = {Hyperblock Scheduling for Verified High-Level Synthesis},
	note = {Submitted to PLDI 2024}
}

@inproceedings{herzog21_price_meltd_spect,
	author = {Herzog, Benedict and Reif, Stefan and Preis, Julian and Schr\"{o}der-Preikschat, Wolfgang and H\"{o}nig, Timo},
	title = {The Price of Meltdown and Spectre: Energy Overhead of Mitigations at Operating System Level},
	year = {2021},
	isbn = {9781450383370},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3447852.3458721},
	abstract = {The Meltdown and Spectre hardware vulnerabilities shocked hardware manufacturers and system users upon discovery. Numerous attack vectors and mitigations have been developed and deployed. However, due to the deep entanglement in core CPU components they will be an important topic for years. Although the performance overhead of software mitigations has been examined closely, the energy overhead has experienced little attention---even though the energy demand is a critical cost factor in data centres.This work contributes a fine-grained energy-overhead analysis of Meltdown/Spectre software mitigations, which reveals application-specific energy overheads of up to 72 \%. We further compare energy overheads to execution time overheads. Additionally, we examine subsystem-specific effects (i.e., CPU, memory, I/O, network/interprocess communication) and develop a model that predicts energy overheads for applications.},
	booktitle = {Proceedings of the 14th European Workshop on Systems Security},
	pages = {8–14},
	numpages = {7},
	location = {Online, United Kingdom},
	series = {EuroSec '21}
}

@article{hoare78_commun_sequen_proces,
	author = {Hoare, C. A. R.},
	title = {Communicating Sequential Processes},
	year = {1978},
	issue_date = {Aug. 1978},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {21},
	number = {8},
	issn = {0001-0782},
	doi = {10.1145/359576.359585},
	abstract = {This paper suggests that input and output are basic primitives of programming and that parallel composition of communicating sequential processes is a fundamental program structuring method. When combined with a development of Dijkstra's guarded command, these concepts are surprisingly versatile. Their use is illustrated by sample solutions of a variety of a familiar programming exercises.},
	journal = {Commun. ACM},
	month = {8},
	pages = {666–677},
	numpages = {12},
	keywords = {programming, guarded commands, multiple entries, nondeterminacy, data representations, monitors, multiple exits, iterative arrays, coroutines, conditional critical regions, program structures, concurrency, recursion, procedures, classes, programming primitives, parallel programming, programming languages, input, output}
}

@inproceedings{homsirikamol14_can,
	author = {Homsirikamol, Ekawat and Gaj, Kris},
	publisher = {IEEE},
	booktitle = {ReConFig},
	doi = {10.1109/ReConFig.2014.7032504},
	pages = {1--8},
	title = {Can high-level synthesis compete against a hand-written code in the cryptographic domain? {A} case study},
	year = {2014}
}

@phdthesis{hopwood78_decom,
	author = {Hopwood, Gregory Littell},
	title = {Decompilation},
	year = {1978},
	publisher = {University of California, Irvine},
	note = {AAI7811860}
}

@inproceedings{hwang99_ffplp,
	author = {Hwang, Enoch and Vahid, Frank and Hsu, Yu-Chin},
	title = {FSMD Functional Partitioning for Low Power},
	booktitle = {Proceedings of the conference on Design, automation and test in Europe},
	year = 1999,
	pages = {7--es},
	doi = {10.1109/DATE.1999.761092}
}

@inbook{hwu93_super,
	abstract = {A compiler for VLIW and superscalar processors must expose sufficient instruction-level parallelism (ILP) to effectively utilize the parallel hardware. However, ILP within basic blocks is extremely limited for control-intensive programs. We have developed a set of techniques for exploiting ILP across basic block boundaries. These techniques are based on a novel structure called the superblock. The superblock enables the optimizer and scheduler to extract more ILP along the important execution paths by systematically removing constraints due to the unimportant paths. Superblock optimization and scheduling have been implemented in the IMPACT-I compiler. This implementation gives us a unique opportunity to fully understand the issues involved in incorporating these techniques into a real compiler. Superblock optimizations and scheduling are shown to be useful while taking into account a variety of architectural features.},
	author = {Hwu, Wen-Mei W. and Mahlke, Scott A. and Chen, William Y. and Chang, Pohua P. and Warter, Nancy J. and Bringmann, Roger A. and Ouellette, Roland G. and Hank, Richard E. and Kiyohara, Tokuzo and Haab, Grant E. and Holm, John G. and Lavery, Daniel M.},
	editor = {Rau, B. R. and Fisher, J. A.},
	location = {Boston, MA},
	publisher = {Springer US},
	booktitle = {Instruction-Level Parallelism: A Special Issue of The Journal of Supercomputing},
	doi = {10.1007/978-1-4615-3200-2_7},
	isbn = {978-1-4615-3200-2},
	keywords = {superblock scheduling,trace scheduling,static scheduling},
	pages = {229--248},
	title = {The Superblock: An Effective Technique for VLIW and Superscalar Compilation},
	year = {1993}
}

@misc{intel20_hsc,
	author = {Intel},
	title = {High-level Synthesis Compiler},
	url = {https://intel.ly/2UDiWr5},
	urldate = {2020-11-18},
	year = {2020}
}

@misc{intel20_sdk_openc_applic,
	author = {Intel},
	url = {https://intel.ly/30sYHz0},
	title = {{SDK} for {OpenCL} Applications},
	urldate = {2020-07-20},
	year = {2020}
}

@inproceedings{jifeng93_towar,
	abstract = {This paper shows how to compile a program written in a subset of occam into a normal form suitable for further processing into a netlist of components which may be loaded into a Field-Programmable Gate Array (FPGA). A simple state-machine model is adopted for specifying the behaviour of a synchronous circuit where the observable includes the state of the control path and the data path of the circuit. We identify the behaviour of a circuit with a program consisting of a very restricted subset of occam. Algebraic laws are used to facilitate the transformation from a program into a normal form. The compiling specification is presented as a set of theorems that must be proved correct with respect to these laws. A rapid prototype compiler in the form of a logic program may be implemented from these theorems.},
	author = {Jifeng, He and Page, Ian and Bowen, Jonathan},
	editor = {Milne, George J. and Pierre, Laurence},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Correct Hardware Design and Verification Methods},
	isbn = {978-3-540-70655-7},
	pages = {214--225},
	title = {Towards a provably correct hardware implementation of occam},
	year = {1993}
}

@article{josipović17_out_order_load_store_queue_spatial_comput,
	author = {Josipović, Lana and Brisk, Philip and Ienne, Paolo},
	title = {An Out-of-Order Load-Store Queue for Spatial Computing},
	year = {2017},
	issue_date = {October 2017},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {16},
	number = {5s},
	issn = {1539-9087},
	doi = {10.1145/3126525},
	abstract = {The efficiency of spatial computing depends on the ability to achieve maximal parallelism. This necessitates memory interfaces that can correctly handle memory accesses that arrive in arbitrary order while still respecting data dependencies and ensuring appropriate ordering for semantic correctness. However, a typical memory interface for out-of-order processors (i.e., a load-store queue) cannot immediately meet these requirements: a different allocation policy is needed to achieve out-of-order execution in spatial systems that naturally omit the notion of sequential program order, a fundamental piece of information for correct execution. We show a novel and practical way to organize the allocation for an out-of-order load-store queue for spatial computing. The main idea is to dynamically allocate groups of memory accesses (depending on the dynamic behavior of the application), where the access order within the group is statically predetermined (for instance by a high-level synthesis tool). We detail the construction of our load-store queue and demonstrate on a few practical cases its advantages over standard accelerator-memory interfaces.},
	journal = {ACM Trans. Embed. Comput. Syst.},
	month = {9},
	articleno = {125},
	numpages = {19},
	keywords = {allocation, spatial computing, Load-store queue, dynamic scheduling}
}

@inproceedings{josipović18_dynam_sched_high_synth,
	author = {Josipović, Lana and Ghosal, Radhika and Ienne, Paolo},
	location = {Monterey, CALIFORNIA, USA},
	publisher = {ACM},
	booktitle = {Proceedings of the 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	doi = {10.1145/3174243.3174264},
	isbn = {978-1-4503-5614-5},
	keywords = {compiler,dynamically scheduled circuits,high-level synthesis,pipelining},
	pages = {127--136},
	series = {FPGA '18},
	title = {Dynamically Scheduled High-level Synthesis},
	year = {2018}
}

@article{josipović21_buffer_placem_sizin_high_perfor_dataf_circuit,
	author = {Josipović, Lana and Sheikhha, Shabnam and Guerrieri, Andrea and Ienne, Paolo and Cortadella, Jordi},
	title = {Buffer Placement and Sizing for High-Performance Dataflow Circuits},
	year = {2021},
	issue_date = {March 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {15},
	number = {1},
	issn = {1936-7406},
	doi = {10.1145/3477053},
	abstract = {Commercial high-level synthesis tools typically produce statically scheduled circuits. Yet, effective C-to-circuit conversion of arbitrary software applications calls for dataflow circuits, as they can handle efficiently variable latencies (e.g., caches), unpredictable memory dependencies, and irregular control flow. Dataflow circuits exhibit an unconventional property: registers (usually referred to as “buffers”) can be placed anywhere in the circuit without changing its semantics, in strong contrast to what happens in traditional datapaths. Yet, although functionally irrelevant, this placement has a significant impact on the circuit’s timing and throughput. In this work, we show how to strategically place buffers into a dataflow circuit to optimize its performance. Our approach extracts a set of choice-free critical loops from arbitrary dataflow circuits and relies on the theory of marked graphs to optimize the buffer placement and sizing. Our performance optimization model supports important high-level synthesis features such as pipelined computational units, units with variable latency and throughput, and if-conversion. We demonstrate the performance benefits of our approach on a set of dataflow circuits obtained from imperative code.},
	journal = {ACM Trans. Reconfigurable Technol. Syst.},
	month = {11},
	articleno = {4},
	numpages = {32},
	keywords = {Petri nets, Dataflow circuits, performance optimization, high-level synthesis}
}

@inproceedings{jourdan12_valid_lr_parser,
	abstract = {An LR(1) parser is a finite-state automaton, equipped with a stack, which uses a combination of its current state and one lookahead symbol in order to determine which action to perform next. We present a validator which, when applied to a context-free grammar {\$}{\backslash}mathcal G{\$}and an automaton {\$}{\backslash}mathcal A{\$}, checks that {\$}{\backslash}mathcal A{\$}and {\$}{\backslash}mathcal G{\$}agree. Validating the parser provides the correctness guarantees required by verified compilers and other high-assurance software that involves parsing. The validation process is independent of which technique was used to construct {\$}{\backslash}mathcal A{\$}. The validator is implemented and proved correct using the Coq proof assistant. As an application, we build a formally-verified parser for the C99 language.},
	author = {Jourdan, Jacques-Henri and Pottier, François and Leroy, Xavier},
	editor = {Seidl, Helmut},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Programming Languages and Systems},
	isbn = {978-3-642-28869-2},
	pages = {397--416},
	title = {Validating LR(1) Parsers},
	year = {2012}
}

@article{kam76_gdfaia,
	keywords = {data-flow},
	author = {Kam, John B. and Ullman, Jeffrey D.},
	title = {Global Data Flow Analysis and Iterative Algorithms},
	year = {1976},
	issue_date = {Jan. 1976},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {23},
	number = {1},
	issn = {0004-5411},
	doi = {10.1145/321921.321938},
	abstract = {Kildall has developed data propagation algorithms for code optimization in a general lattice theoretic framework. In another direction, Hecht and Ullman gave a strong upper bound on the number of iterations required for propagation algorithms when the data is represented by bit vectors and depth-first ordering of the flow graph is used. The present paper combines the ideas of these two papers by considering conditions under which the bound of Hecht and Ullman applies to the depth-first version of Kildall's general data propagation algorithm. It is shown that the following condition is necessary and sufficient: Let undefined and g be any two functions which could be associated with blocks of a flow graph, let x be an arbitrary lattice element, and let 0 be the lattice zero. Then (*) (∀undefined,g,x) [undefinedg(0) ≥ g(0) ∧ undefined(x) ∧ x]. Then it is shown that several of the particular instances of the techniques Kildall found useful do not meet condition (*).},
	journal = {J. ACM},
	month = {1},
	pages = {158–171},
	numpages = {14}
}

@inproceedings{karfa06_formal_verif_method_sched_high_synth,
	author = {Karfa, C and Mandal, C and Sarkar, D and Pentakota, S R. and Reade, Chris},
	location = {Washington, DC, USA},
	publisher = {IEEE Computer Society},
	booktitle = {Proceedings of the 7th International Symposium on Quality Electronic Design},
	doi = {10.1109/ISQED.2006.10},
	isbn = {0-7695-2523-7},
	pages = {71--78},
	series = {ISQED '06},
	title = {A Formal Verification Method of Scheduling in High-level Synthesis},
	year = {2006}
}

@inproceedings{karfa07_hand_verif_high_synth,
	author = {Karfa, Chandan and Sarkar, Dipankar and Mandal, Chittaranjan and Reade, Chris},
	location = {Stresa-Lago Maggiore, Italy},
	publisher = {ACM},
	booktitle = {Proceedings of the 17th ACM Great Lakes Symposium on VLSI},
	doi = {10.1145/1228784.1228885},
	isbn = {978-1-59593-605-9},
	keywords = {FSMD,verification,high-level synthesis,equivalence checking,translation validation},
	pages = {429--434},
	series = {GLSVLSI '07},
	title = {Hand-in-hand Verification of High-level Synthesis},
	year = {2007}
}

@article{karfa08_equiv_check_method_sched_verif,
	author = {{Karfa}, C. and {Sarkar}, D. and {Mandal}, C. and {Kumar}, P.},
	doi = {10.1109/TCAD.2007.913390},
	issn = {1937-4151},
	journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {data visualisation;finite state machines;formal specification;formal verification;high level synthesis;scheduling;equivalence-checking method;scheduling verification;high-level synthesis;formal method;behavioral specification;finite state machine-data path model;FSMD;data visualization;code-motion techniques;arithmetic transformation;High level synthesis;Processor scheduling;Automata;Data visualization;Arithmetic;Formal verification;Very large scale integration;Government;Computer science;Calculus;Equivalence checking;finite state machine with data path (FSMD) models;formal verification;high-level synthesis (HLS);scheduling;Formal Verification;Equivalence Checking;FSMD models;High-level Synthesis;Scheduling},
	month = mar,
	number = {3},
	pages = {556--569},
	title = {An Equivalence-Checking Method for Scheduling Verification in High-Level Synthesis},
	volume = {27},
	year = {2008}
}

@article{karfa10_verif_datap_contr_gener_phase,
	author = {{Karfa}, C. and {Sarkar}, D. and {Mandal}, C.},
	doi = {10.1109/TCAD.2009.2035542},
	issn = {1937-4151},
	journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {finite state machines;high level synthesis;integrated circuit design;datapath verification;controller generation phase;digital circuits high-level synthesis;formal verification method;datapath interconnection;controller finite state machine description;control assertion pattern;equivalence checking method;pipelined operations;multicycle operations;structured architecture synthesis tool;HLS benchmarks tool;High level synthesis;Digital circuits;Computer bugs;Formal verification;Automata;Scheduling;Control system synthesis;Signal synthesis;Integrated circuit interconnections;Pattern analysis;Controller;datapath;equivalence checking;formal verification;FSM;FSMD models;high-level synthesis;register transfer level},
	month = mar,
	number = {3},
	pages = {479--492},
	title = {Verification of Datapath and Controller Generation Phase in High-Level Synthesis of Digital Circuits},
	volume = {29},
	year = {2010}
}

@article{karfa12_formal_verif_code_motion_techn,
	author = {Karfa, Chandan and Mandal, Chittaranjan and Sarkar, Dipankar},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	doi = {10.1145/2209291.2209303},
	issn = {1084-4309},
	journaltitle = {ACM Trans. Des. Autom. Electron. Syst.},
	keywords = {high-level synthesis,Formal verification,equivalence checking,model checking,FSMD models,code motion},
	month = jul,
	number = {3},
	title = {Formal Verification of Code Motion Techniques Using Data-Flow-Driven Equivalence Checking},
	volume = {17},
	year = {2012}
}

@inproceedings{kildall73_unified_approac_global_progr_optim,
	author = {Kildall, Gary A.},
	location = {Boston, Massachusetts},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 1st Annual ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
	doi = {10.1145/512927.512945},
	isbn = {9781450373494},
	pages = {194--206},
	series = {POPL '73},
	title = {A Unified Approach to Global Program Optimization},
	year = {1973}
}

@inproceedings{kim04_autom_fsmd,
	author = {{Youngsik Kim} and {Kopuri}, S. and {Mansouri}, N.},
	booktitle = {International Symposium on Signals, Circuits and Systems. Proceedings, SCS 2003. (Cat. No.03EX720)},
	doi = {10.1109/ISQED.2004.1283659},
	issn = {null},
	keywords = {formal verification;high level synthesis;data flow graphs;finite state machines;theorem proving;automated formal verification;scheduling process;finite state machines with datapath;high-level synthesis;behavioral specification;control-data flow graph;equivalence conditions;higher-order specification language;theorem proving environment;PVS proof checker;Formal verification;Automata;High level synthesis;Processor scheduling;Computer science;Flow graphs;Mathematical model;Specification languages;Libraries;Computer applications},
	month = mar,
	pages = {110--115},
	title = {Automated formal verification of scheduling process using finite state machines with datapath (FSMD)},
	year = {2004}
}

@inproceedings{koeplinger18_s,
	author = {Koeplinger, David and Feldman, Matthew and Prabhakar, Raghu and Zhang, Yaqi and Hadjis, Stefan and Fiszel, Ruben and Zhao, Tian and Nardi, Luigi and Pedram, Ardavan and Kozyrakis, Christos and Olukotun, Kunle},
	publisher = {ACM},
	booktitle = {{PLDI}},
	doi = {10.1145/3192366.3192379},
	pages = {296--311},
	title = {Spatial: A Language and Compiler for Application Accelerators},
	year = {2018}
}

@InProceedings{kroening14_c,
	keywords = {bounded model checking},
	author = {Kroening, Daniel and Tautschnig, Michael},
	editor = "{\'A}brah{\'a}m, Erika
and Havelund, Klaus",
	title = {{CBMC} -- {C} Bounded Model Checker},
	booktitle = "Tools and Algorithms for the Construction and Analysis of Systems",
	year = "2014",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "389--391",
	abstract = "CBMC implements bit-precise bounded model checking for C programs and has been developed and maintained for more than ten years. CBMC verifies the absence of violated assertions under a given loop unwinding bound. Other properties, such as SV-COMP's ERROR labels or memory safety properties are reduced to assertions via automated instrumentation. Only recently support for efficiently checking concurrent programs, including support for weak memory models, has been added. Thus, CBMC is now capable of finding counterexamples in all of SV-COMP's categories. As back end, the competition submission of CBMC uses MiniSat 2.2.0.",
	isbn = "978-3-642-54862-8"
}

@inproceedings{kundu07_autom,
	abstract = {Stepwise refinement is at the core of many approaches to synthesis and optimization of hardware and software systems. For instance, it can be used to build a synthesis approach for digital circuits from high level specifications. It can also be used for post-synthesis modification such as in engineering change orders (ECOs). Therefore, checking if a system, modeled as a set of concurrent processes, is a refinement of another is of tremendous value. In this paper, we focus on concurrent systems modeled as communicating sequential processes (CSP) and show their refinements can be validated using insights from translation validation, automated theorem proving and relational approaches to reasoning about programs. The novelty of our approach is that it handles infinite state spaces in a fully automated manner. We have implemented our refinement checking technique and have applied it to a variety of refinements. We present the details of our algorithm and experimental results. As an example, we were able to automatically check an infinite state space buffer refinement that cannot be checked by current state of the art tools such as FDR. We were also able to check the data part of an industrial case study on the EP2 system.},
	author = {{Sudipta Kundu} and {Lerner}, S. and {Rajesh Gupta}},
	booktitle = {2007 IEEE/ACM International Conference on Computer-Aided Design},
	doi = {10.1109/ICCAD.2007.4397284},
	issn = {1558-2434},
	keywords = {communicating sequential processes;concurrency control;formal specification;high level synthesis;reasoning about programs;state-space methods;theorem proving;automated refinement checking;concurrent systems;stepwise refinement;digital circuits;high level specifications;post-synthesis modification;engineering change orders;communicating sequential processes;translation validation;automated theorem proving;reasoning about programs;state spaces;refinement checking technique;EP2 system;State-space methods;Hardware;Refining;Circuit synthesis;Reasoning about programs;Humans;Manuals;Software systems;Digital circuits;Silicon},
	month = nov,
	pages = {318--325},
	title = {Automated refinement checking of concurrent systems},
	year = {2007}
}

@inproceedings{kundu08_valid_high_level_synth,
	abstract = {The growing design-productivity gap has made designers shift toward using high-level languages like C, C++ and Java to do system-level design. High-Level Synthesis (HLS) is the process of generating Register Transfer Level (RTL) design from these initial high-level programs. Unfortunately, this translation process itself can be buggy, which can create a mismatch between what a designer intends and what is actually implemented in the circuit. In this paper, we present an approach to validate the result of HLS against the initial high-level program using insights from translation validation, automated theorem proving and relational approaches to reasoning about programs. We have implemented our validating technique and have applied it to a highly parallelizing HLS framework called SPARK. We present the details of our algorithm and experimental results.},
	author = {Kundu, Sudipta and Lerner, Sorin and Gupta, Rajesh},
	editor = {Gupta, Aarti and Malik, Sharad},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Computer Aided Verification},
	isbn = {978-3-540-70545-1},
	pages = {459--472},
	title = {Validating High-Level Synthesis},
	year = {2008}
}

@article{lahti19_are_we_there_yet,
	abstract = {To increase productivity in designing digital hardware components, high-level synthesis (HLS) is seen as the next step in raising the design abstraction level. However, the quality of results (QoRs) of HLS tools has tended to be behind those of manual register-transfer level (RTL) flows. In this paper, we survey the scientific literature published since 2010 about the QoR and productivity differences between the HLS and RTL design flows. Altogether, our survey spans 46 papers and 118 associated applications. Our results show that on average, the QoR of RTL flow is still better than that of the state-of-the-art HLS tools. However, the average development time with HLS tools is only a third of that of the RTL flow, and a designer obtains over four times as high productivity with HLS. Based on our findings, we also present a model case study to sum up the best practices in comparative studies between HLS and RTL. The outcome of our case study is also in line with the survey results, as using an HLS tool is seen to increase the productivity by a factor of six. In addition, to help close the QoR gap, we present a survey of literature focused on improving HLS. Our results let us conclude that HLS is currently a viable option for fast prototyping and for designs with short time to market.},
	author = {{Lahti}, S. and {Sjövall}, P. and {Vanne}, J. and {Hämäläinen}, T. D.},
	doi = {10.1109/TCAD.2018.2834439},
	issn = {1937-4151},
	journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {electronic design automation;field programmable gate arrays;high level synthesis;logic design;RTL design flows;average development time;HLS tool;QoR gap;high-level synthesis;digital hardware components;design abstraction level;productivity differences;manual register-transfer level flows;quality of results;Tools;Field programmable gate arrays;Measurement;Hardware;Productivity;Matlab;Hardware design languages;Electronic design automation and methodology;field-programmable gate array (FPGA);hardware description languages (HDLs);high-level synthesis (HLS);reconfigurable logic},
	month = may,
	number = {5},
	pages = {898--911},
	title = {Are We There Yet? a Study on the State of High-Level Synthesis},
	volume = {38},
	year = {2019}
}

@INPROCEEDINGS{lattner04_llvm,
	author = {Lattner, C. and Adve, V.},
	booktitle = {International Symposium on Code Generation and Optimization, 2004. CGO 2004.},
	title = {LLVM: a compilation framework for lifelong program analysis \& transformation},
	year = {2004},
	volume = {},
	number = {},
	pages = {75-86},
	doi = {10.1109/CGO.2004.1281665}
}

@inproceedings{lattner21_mlir,
	abstract = {This work presents MLIR, a novel approach to building reusable and extensible compiler infrastructure. MLIR addresses software fragmentation, compilation for heterogeneous hardware, significantly reducing the cost of building domain specific compilers, and connecting existing compilers together. MLIR facilitates the design and implementation of code generators, translators and optimizers at different levels of abstraction and across application domains, hardware targets and execution environments. The contribution of this work includes (1) discussion of MLIR as a research artifact, built for extension and evolution, while identifying the challenges and opportunities posed by this novel design, semantics, optimization specification, system, and engineering. (2) evaluation of MLIR as a generalized infrastructure that reduces the cost of building compilers-describing diverse use-cases to show research and educational opportunities for future programming languages, compilers, execution environments, and computer architecture. The paper also presents the rationale for MLIR, its original design principles, structures and semantics.},
	author = {Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
	booktitle = {2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},
	doi = {10.1109/CGO51591.2021.9370308},
	keywords = {,mlir},
	month = feb,
	pages = {2--14},
	title = {MLIR: Scaling Compiler Infrastructure for Domain Specific Computation},
	year = {2021}
}

@INPROCEEDINGS{lattuada15_ctbsss,
	keywords = {bambu},
	author = {Lattuada, Marco and Ferrandi, Fabrizio},
	booktitle = {2015 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
	title = {Code Transformations Based on Speculative {SDC} Scheduling},
	year = {2015},
	volume = {},
	number = {},
	pages = {71-77},
	doi = {10.1109/ICCAD.2015.7372552}
}

@InProceedings{leino10_d,
	author = "Leino, K. Rustan M.",
	editor = "Clarke, Edmund M.
and Voronkov, Andrei",
	title = "Dafny: An Automatic Program Verifier for Functional Correctness",
	booktitle = "Logic for Programming, Artificial Intelligence, and Reasoning",
	year = "2010",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "348--370",
	abstract = "Traditionally, the full verification of a program's functional correctness has been obtained with pen and paper or with interactive proof assistants, whereas only reduced verification tasks, such as extended static checking, have enjoyed the automation offered by satisfiability-modulo-theories (SMT) solvers. More recently, powerful SMT solvers and well-designed program verifiers are starting to break that tradition, thus reducing the effort involved in doing full verification.",
	isbn = "978-3-642-17511-4"
}

@inproceedings{leroy06_formal_certif_compil_back_end,
	author = {Leroy, Xavier},
	location = {Charleston, South Carolina, USA},
	publisher = {Association for Computing Machinery},
	booktitle = {Conference Record of the 33rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
	doi = {10.1145/1111037.1111042},
	isbn = {1595930272},
	keywords = {the Coq theorem prover,certified compilation,compiler transformations and optimizations,program proof,semantic preservation},
	pages = {42--54},
	series = {POPL '06},
	title = {Formal Certification of a Compiler Back-End or: Programming a Compiler with a Proof Assistant},
	year = {2006}
}

@article{leroy09_formal_verif_compil_back_end,
	abstract = {This article describes the development and formal verification (proof of semantic preservation) of a compiler back-end from Cminor (a simple imperative intermediate language) to PowerPC assembly code, using the Coq proof assistant both for programming the compiler and for proving its soundness. Such a verified compiler is useful in the context of formal methods applied to the certification of critical software: the verification of the compiler guarantees that the safety properties proved on the source code hold for the executable compiled code as well.},
	author = {Leroy, Xavier},
	doi = {10.1007/s10817-009-9155-4},
	issn = {1573-0670},
	journaltitle = {Journal of Automated Reasoning},
	number = {4},
	pages = {363},
	title = {A Formally Verified Compiler Back-End},
	volume = {43},
	year = {2009}
}

@article{leroy09_formal_verif_realis_compil,
	author = {Leroy, Xavier},
	location = {New York, NY, USA},
	publisher = {ACM},
	doi = {10.1145/1538788.1538814},
	issn = {0001-0782},
	journaltitle = {Commun. ACM},
	month = jul,
	number = {7},
	pages = {107--115},
	title = {Formal Verification of a Realistic Compiler},
	volume = {52},
	year = {2009}
}

@inproceedings{leroy16_cfvoc,
	TITLE = {{CompCert - A Formally Verified Optimizing Compiler}},
	AUTHOR = {Leroy, Xavier and Blazy, Sandrine and K{\"a}stner, Daniel and Schommer, Bernhard and Pister, Markus and Ferdinand, Christian},
	URL = {https://inria.hal.science/hal-01238879},
	BOOKTITLE = {{ERTS 2016: Embedded Real Time Software and Systems, 8th European Congress}},
	ADDRESS = {Toulouse, France},
	ORGANIZATION = {{SEE}},
	YEAR = {2016},
	MONTH = Jan,
	HAL_ID = {hal-01238879},
	HAL_VERSION = {v1}
}

@inproceedings{lidbury15_many_core_compil_fuzzin,
	author = {Lidbury, Christopher and Lascu, Andrei and Chong, Nathan and Donaldson, Alastair F.},
	location = {Portland, OR, USA},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation},
	doi = {10.1145/2737924.2737986},
	isbn = {9781450334686},
	keywords = {random testing,Compilers,OpenCL,metamorphic testing,GPUs,concurrency},
	pages = {65--76},
	series = {PLDI '15},
	title = {Many-Core Compiler Fuzzing},
	year = {2015}
}

@inproceedings{lööw19_proof_trans_veril_devel_hol,
	author = {Lööw, Andreas and Myreen, Magnus O.},
	location = {Montreal, Quebec, Canada},
	publisher = {IEEE Press},
	booktitle = {Proceedings of the 7th International Workshop on Formal Methods in Software Engineering},
	doi = {10.1109/FormaliSE.2019.00020},
	pages = {99--108},
	series = {FormaliSE '19},
	title = {A Proof-producing Translator for Verilog Development in HOL},
	year = {2019}
}

@inproceedings{lööw19_verif_compil_verif_proces,
	author = {Lööw, Andreas and Kumar, Ramana and Tan, Yong Kiam and Myreen, Magnus O. and Norrish, Michael and Abrahamsson, Oskar and Fox, Anthony},
	location = {Phoenix, AZ, USA},
	publisher = {ACM},
	booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
	doi = {10.1145/3314221.3314622},
	isbn = {978-1-4503-6712-7},
	keywords = {compiler verification,hardware verification,program verification,verified stack},
	pages = {1041--1053},
	series = {PLDI 2019},
	title = {Verified Compilation on a Verified Processor},
	year = {2019}
}

@inproceedings{lööw21_lutsig,
	abstract = {We report on a new verified Verilog compiler called Lutsig. Lutsig currently targets (a class of) FPGAs and is capable of producing technology mapped netlists for FPGAs. We have connected Lutsig to existing Verilog development tools, and in this paper we show how Lutsig, as a consequence of this connection, fits into a hardware development methodology for verified circuits in the HOL4 theorem prover. One important step in the methodology is transporting properties proved at the behavioral Verilog level down to technology mapped netlists, and Lutsig is the component in the methodology that enables such transportation.},
	author = {Lööw, Andreas},
	location = {Virtual, Denmark},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 10th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	doi = {10.1145/3437992.3439916},
	isbn = {9781450382991},
	keywords = {hardware verification,hardware synthesis,compiler verification},
	pages = {46--60},
	series = {CPP 2021},
	title = {Lutsig: A Verified Verilog Compiler for Verified Circuit Development},
	year = {2021}
}

@article{mahlke92_effec_compil_suppor_predic_execut_using_hyper,
	author = {Mahlke, Scott A. and Lin, David C. and Chen, William Y. and Hank, Richard E. and Bringmann, Roger A.},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	doi = {10.1145/144965.144998},
	issn = {1050-916X},
	journaltitle = {SIGMICRO Newsl.},
	keywords = {speculative execution,static scheduling,hyperblocks},
	month = dec,
	number = {1-2},
	pages = {45--54},
	title = {Effective Compiler Support for Predicated Execution Using the Hyperblock},
	volume = {23},
	year = {1992}
}

@misc{mentor20_catap_high_level_synth,
	author = {Siemens},
	title = {Catapult High-Level Synthesis},
	url = {https://eda.sw.siemens.com/en-US/ic/catapult-high-level-synthesis/hls/c-cplus/},
	urldate = {2023-11-14},
	year = 2021
}

@inproceedings{meredith10_veril,
	author = {{Meredith}, P. and {Katelman}, M. and {Meseguer}, J. and {Roşu}, G.},
	booktitle = {Eighth ACM/IEEE International Conference on Formal Methods and Models for Codesign (MEMOCODE 2010)},
	doi = {10.1109/MEMCOD.2010.5558634},
	issn = {null},
	keywords = {hardware description languages;programming language semantics;formal executable semantics;Verilog hardware description language;mathematically rigorous reference;official language standard;Hardware design languages;Semantics;Equations;Delay;Integrated circuit modeling;Mathematical model;Computational modeling},
	month = jul,
	pages = {179--188},
	title = {A formal executable semantics of {Verilog}},
	year = {2010}
}

@InProceedings{monniaux22_tcbcvc,
	doi = {10.1007/978-3-030-99336-8_8},
	author = {Monniaux, David and Boulm{\'e}, Sylvain},
	editor = "Sergey, Ilya",
	title = "The Trusted Computing Base of the CompCert Verified Compiler",
	booktitle = "Programming Languages and Systems",
	year = "2022",
	publisher = "Springer International Publishing",
	address = "Cham",
	pages = "204--233",
	abstract = "CompCert is the first realistic formally verified compiler: it provides a machine-checked mathematical proof that the code it generates matches the source code. Yet, there could be loopholes in this approach. We comprehensively analyze aspects of CompCert where errors could lead to incorrect code being generated. Possible issues range from the modeling of the source and the target languages to some techniques used to call external algorithms from within the compiler.",
	isbn = "978-3-030-99336-8"
}

@InProceedings{moura08_z,
	keywords = {SMT},
	author = {de Moura, Leonardo and Bj{\o}rner, Nikolaj},
	editor = "Ramakrishnan, C. R.
and Rehof, Jakob",
	title = {Z3: An Efficient SMT Solver},
	booktitle = "Tools and Algorithms for the Construction and Analysis of Systems",
	year = "2008",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "337--340",
	abstract = "Satisfiability Modulo Theories (SMT) problem is a decision problem for logical first order formulas with respect to combinations of background theories such as: arithmetic, bit-vectors, arrays, and uninterpreted functions. Z3 is a new and efficient SMT Solver freely available from Microsoft Research. It is used in various software verification and analysis applications.",
	isbn = "978-3-540-78800-3"
}

@InProceedings{moura15_l,
	doi = {10.1007/978-3-319-21401-6_26},
	author = {de Moura, Leonardo and Kong, Soonho and Avigad, Jeremy and van Doorn, Floris and von Raumer, Jakob},
	editor = {Felty, Amy P. and Middeldorp, Aart},
	title = "The Lean Theorem Prover (System Description)",
	booktitle = "Automated Deduction - CADE-25",
	year = "2015",
	publisher = "Springer International Publishing",
	address = "Cham",
	pages = "378--388",
	abstract = "Lean is a new open source theorem prover being developed at Microsoft Research and Carnegie Mellon University, with a small trusted kernel based on dependent type theory. It aims to bridge the gap between interactive and automated theorem proving, by situating automated tools and methods in a framework that supports user interaction and the construction of fully specified axiomatic proofs. Lean is an ongoing and long-term effort, but it already provides many useful components, integrated development environments, and a rich API which can be used to embed it into other systems. It is currently being used to formalize category theory, homotopy type theory, and abstract algebra. We describe the project goals, system architecture, and main features, and we discuss applications and continuing work.",
	isbn = "978-3-319-21401-6"
}

@inproceedings{nigam20_predic_accel_desig_time_sensit_affin_types,
	abstract = {Field-programmable gate arrays (FPGAs) provide an opportunity to co-design applications with hardware accelerators, yet they remain difficult to program. High-level synthesis (HLS) tools promise to raise the level of abstraction by compiling C or C++ to accelerator designs. Repurposing legacy software languages, however, requires complex heuristics to map imperative code onto hardware structures. We find that the black-box heuristics in HLS can be unpredictable: changing parameters in the program that should improve performance can counterintuitively yield slower and larger designs. This paper proposes a type system that restricts HLS to programs that can predictably compile to hardware accelerators. The key idea is to model consumable hardware resources with a time-sensitive affine type system that prevents simultaneous uses of the same hardware structure. We implement the type system in Dahlia, a language that compiles to HLS C++, and show that it can reduce the size of HLS parameter spaces while accepting Pareto-optimal designs.},
	author = {Nigam, Rachit and Atapattu, Sachille and Thomas, Samuel and Li, Zhijing and Bauer, Theodore and Ye, Yuwei and Koti, Apurva and Sampson, Adrian and Zhang, Zhiru},
	location = {London, UK},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3385412.3385974},
	booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
	doi = {10.1145/3385412.3385974},
	isbn = {9781450376136},
	keywords = {affine types,high-level synthesis},
	pages = {393--407},
	series = {PLDI 2020},
	title = {Predictable Accelerator Design with Time-Sensitive Affine Types},
	year = {2020}
}

@inproceedings{nikhil04_bsv,
	author = {R. {Nikhil}},
	title = {Bluespec System Verilog: efficient, correct RTL from high level specifications},
	booktitle = {Proceedings. Second ACM and IEEE International Conference on Formal Methods and
                  Models for Co-Design, 2004. MEMOCODE '04.},
	year = 2004,
	pages = {69-70},
	doi = {10.1109/MEMCOD.2004.1459818}
}

@inproceedings{noronha17_rapid_fpga,
	author = {{Noronha}, D. H. and {Pinilla}, J. P. and {Wilton}, S. J. E.},
	booktitle = {2017 International Conference on ReConFigurable Computing and FPGAs (ReConFig)},
	doi = {10.1109/RECONFIG.2017.8279807},
	keywords = {high-level synthesis,FPGA,inlining,compiler optimisation},
	pages = {1--6},
	title = {Rapid circuit-specific inlining tuning for FPGA high-level synthesis},
	year = {2017}
}

@inproceedings{ottenstein90_progr_depen_web,
	abstract = {The Program Dependence Web (PDW) is a program representation that can be directly interpreted using control-, data-, or demand-driven models of execution. A PDW combines a single-assignment version of the program with explicit operators that manage the flow of data values. The PDW can be viewed as an augmented Program Dependence Graph. Translation to the PDW representation provides the basis for projects to compile Fortran onto dynamic dataflow architectures and simulators. A second application of the PDW is the construction of various compositional semantics for program dependence graphs.},
	author = {Ottenstein, Karl J. and Ballance, Robert A. and MacCabe, Arthur B.},
	location = {White Plains, New York, USA},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the ACM SIGPLAN 1990 Conference on Programming Language Design and Implementation},
	doi = {10.1145/93542.93578},
	isbn = {0897913647},
	keywords = {gated-SSA,SSA,program dependence graph},
	pages = {257--271},
	series = {PLDI '90},
	title = {The Program Dependence Web: A Representation Supporting Control-, Data-, and Demand-Driven Interpretation of Imperative Languages},
	year = {1990}
}

@inproceedings{page91_compil_occam,
	author = {Page, Ian and Luk, Wayne},
	organization = {Citeseer},
	booktitle = {FPGAs, Oxford Workshop on Field Programmable Logic and Applications},
	pages = {271--283},
	title = {Compiling Occam into field-programmable gate arrays},
	volume = {15},
	year = {1991}
}

@article{pangrle87_desig_tools_intel_silic_compil,
	author = {Barry M. Pangrle and
                  Daniel D. Gajski},
	title = {Design Tools for Intelligent Silicon Compilation},
	journal = {{IEEE} Trans. Comput. Aided Des. Integr. Circuits Syst.},
	volume = {6},
	number = {6},
	pages = {1098--1112},
	year = {1987},
	doi = {10.1109/TCAD.1987.1270350},
	timestamp = {Thu, 24 Sep 2020 11:28:40 +0200},
	biburl = {https://dblp.org/rec/journals/tcad/PangrleG87.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{pardalos22_rsvhls,
	publisher = {IEEE},
	author = {Pardalos, Michalis and Herklotz, Yann and Wickerson, John},
	booktitle = {2022 IEEE 30th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
	title = {Resource Sharing for Verified High-Level Synthesis},
	year = {2022},
	volume = {},
	number = {},
	pages = {1-6},
	doi = {10.1109/FCCM53951.2022.9786208}
}

@book{paulson94_i,
	title = {Isabelle: A generic theorem prover},
	author = {Paulson, Lawrence C},
	year = {1994},
	publisher = {Springer}
}

@inproceedings{pelcat16_desig_hdl,
	author = {Pelcat, Maxime and Bourrasset, Cédric and Maggiani, Luca and Berry, François},
	booktitle = {2016 International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation (SAMOS)},
	doi = {10.1109/SAMOS.2016.7818341},
	pages = {140--147},
	title = {Design productivity of a high level synthesis compiler versus HDL},
	year = {2016}
}

@article{perna11_correc_hardw_synth,
	abstract = {This paper presents an algebraic compilation approach to the correct synthesis (compilation into hardware) of a synchronous language with shared variables and parallelism. The synthesis process generates a hardware component that implements the source program by means of gradually reducing it into a highly parallel state-machine. The correctness of the compiler follows by construction from the correctness of the transformations involved in the synthesis process. Each transformation is proved sound from more basic algebraic laws of the source language; the laws are themselves formally derived from a denotational semantics expressed in the Unified Theories of Programming. The proposed approach is based on previous efforts that handle both software and hardware compilation, in a pure algebraic style, but the complexity of our source language demanded significant adaptations and extensions to the existing approaches.},
	author = {Perna, Juan and Woodcock, Jim and Sampaio, Augusto and Iyoda, Juliano},
	url = {https://doi.org/10.1007/s00236-011-0142-y},
	date = {2011-12-01},
	doi = {10.1007/s00236-011-0142-y},
	issn = {1432-0525},
	journaltitle = {Acta Informatica},
	number = {7},
	pages = {363--396},
	title = {Correct Hardware Synthesis},
	volume = {48}
}

@article{perna12_mechan_wire_wise_verif_handel_c_synth,
	author = {Perna, Juan and Woodcock, Jim},
	doi = {10.1016/j.scico.2010.02.007},
	issn = {0167-6423},
	journaltitle = {Science of Computer Programming},
	keywords = {Handel-C synthesis,Denotational semantics,Correctness,Mechanical verification,HOL},
	number = {4},
	pages = {424--443},
	title = {Mechanised Wire-Wise Verification of {Handel-C} Synthesis},
	volume = {77},
	year = {2012}
}

@inproceedings{pilato13_bambu,
	author = {{Pilato}, C. and {Ferrandi}, F.},
	booktitle = {2013 23rd International Conference on Field programmable Logic and Applications},
	doi = {10.1109/FPL.2013.6645550},
	pages = {1--4},
	title = {Bambu: A modular framework for the high level synthesis of memory-intensive applications},
	year = {2013}
}

@inproceedings{pnueli98_trans,
	abstract = {We present the notion of translation validation as a new approach to the verification of translators (compilers, code generators). Rather than proving in advance that the compiler always produces a target code which correctly implements the source code (compiler verification), each individual translation (i.e. a run of the compiler) is followed by a validation phase which verifies that the target code produced on this run correctly implements the submitted source program. Several ingredients are necessary to set up the --- fully automatic --- translation validation process, among which are:1.A common semantic framework for the representation of the source code and the generated target code.2.A formalization of the notion of ``correct implementation'' as a refinement relation.3.A syntactic simulation-based proof method which allows to automatically verify that one model of the semantic framework, representing the produced target code, correctly implements another model which represents the source.},
	author = {Pnueli, A. and Siegel, M. and Singerman, E.},
	editor = {Steffen, Bernhard},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Tools and Algorithms for the Construction and Analysis of Systems},
	isbn = {978-3-540-69753-4},
	pages = {151--166},
	title = {Translation validation},
	year = {1998}
}

@misc{pouchet20_polyb_c,
	author = {Pouchet, Louis-No\"el},
	title = {PolyBench/C: the Polyhedral Benchmark suite},
	url = {http://web.cse.ohio-state.edu/~pouchet.2/software/polybench/},
	year = {2020}
}

@article{rau96_iterat_modul_sched,
	doi = {10.1007/BF03356742},
	abstract = {Modulo scheduling is a framework within which algorithms for software pipelining innermost loops may be defined. The framework specifies a set of constraints that must be met in order to achieve a legal modulo schedule. A wide variety of algorithms and heuristics can be defined within this framework. Little work has been done to evaluate and compare alternative algorithms and heuristics for modulo scheduling from the viewpoints of schedule quality as well as computational complexity. This, along with a vague and unfounded perception that modulo scheduling is computationally expensive as well as difficult to implement, have inhibited its incorporation into product compilers. This paper presents iterative modulo scheduling, a practical algorithm that is capable of dealing with realistic machine models. The paper also characterizes the algorithm in terms of the quality of the generated schedules as well the computational expense incurred.},
	author = {Rau, B. Ramakrishna},
	date = {1996-02-01},
	issn = {1573-7640},
	journaltitle = {International Journal of Parallel Programming},
	keywords = {loop scheduling,software pipelining,code motion,compiler optimisation,rotating registers,modulo scheduling},
	number = {1},
	pages = {3--64},
	title = {Iterative Modulo Scheduling},
	volume = {24}
}

@INPROCEEDINGS{reuther20_survey_machin_learn_accel,
	keywords = {motivation},
	author = {Reuther, Albert and Michaleas, Peter and Jones, Michael and Gadepally, Vijay and Samsi, Siddharth and Kepner, Jeremy},
	booktitle = {2020 IEEE High Performance Extreme Computing Conference (HPEC)},
	title = {Survey of Machine Learning Accelerators},
	year = {2020},
	volume = {},
	number = {},
	pages = {1-12},
	doi = {10.1109/HPEC43674.2020.9286149}
}

@InProceedings{rizzi23_iterat_method_mappin_aware_frequen,
	keywords = {delay prediction},
	author = {Rizzi, Carmine and Guerrieri, Andrea and Josipović, Lana},
	booktitle = {Proceedings of the 60rd ACM/IEEE Design Automation Conference},
	title = {An Iterative Method for Mapping-Aware Frequency Regulation in Dataflow Circuits},
	year = {2023},
	month = jul,
	address = {San Francisco, CA}
}

@TechReport{roane23_autom_hw_sw_co_desig,
	institution = {Cadence},
	urldate = {2023-12-14},
	keywords = {high-level synthesis},
	url = {https://www.cadence.com/en_US/home/resources/white-papers/automated-hw-sw-co-design-of-dsp-systems-composed-of-processors-and-hardware-accelerators-wp.html},
	year = {2023},
	title = {Automated HW/SW Co-Design of DSP Systems Composed of Processors and Hardware Accelerators},
	author = {Roane, Jeff}
}

@inproceedings{schuiki20_llhd,
	author = {Schuiki, Fabian and Kurth, Andreas and Grosser, Tobias and Benini, Luca},
	title = {LLHD: A Multi-Level Intermediate Representation for Hardware Description
                  Languages},
	booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design
                  and Implementation},
	year = 2020,
	pages = {258-271},
	doi = {10.1145/3385412.3386024},
	address = {New York, NY, USA},
	isbn = 9781450376136,
	location = {London, UK},
	numpages = 14,
	publisher = {ACM},
	series = {PLDI 2020}
}

@inproceedings{sen15_multis,
	author = {Sen, Koushik and Necula, George and Gong, Liang and Choi, Wontae},
	title = {MultiSE: Multi-Path Symbolic Execution Using Value Summaries},
	year = {2015},
	isbn = {9781450336758},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/2786805.2786830},
	abstract = {Dynamic symbolic execution (DSE) has been proposed to effectively generate test inputs for real-world programs. Unfortunately, DSE techniques do not scale well for large realistic programs, because often the number of feasible execution paths of a program increases exponentially with the increase in the length of an execution path. In this paper, we propose MultiSE, a new technique for merging states incrementally during symbolic execution, without using auxiliary variables. The key idea of MultiSE is based on an alternative representation of the state, where we map each variable, including the program counter, to a set of guarded symbolic expressions called a value summary. MultiSE has several advantages over conventional DSE and conventional state merging techniques: value summaries enable sharing of symbolic expressions and path constraints along multiple paths and thus avoid redundant execution. MultiSE does not introduce auxiliary symbolic variables, which enables it to 1) make progress even when merging values not supported by the constraint solver, 2) avoid expensive constraint solver calls when resolving function calls and jumps, and 3) carry out most operations concretely. Moreover, MultiSE updates value summaries incrementally at every assignment instruction, which makes it unnecessary to identify the join points and to keep track of variables to merge at join points. We have implemented MultiSE for JavaScript programs in a publicly available open-source tool. Our evaluation of MultiSE on several programs shows that 1) value summaries are an eective technique to take advantage of the sharing of value along multiple execution path, that 2) MultiSE can run significantly faster than traditional dynamic symbolic execution and, 3) MultiSE saves a substantial number of state merges compared to conventional state-merging techniques.},
	booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
	pages = {842–853},
	numpages = {12},
	keywords = {symbolic execution, value summaries, predicated execution, hyperblocks},
	location = {Bergamo, Italy},
	series = {ESEC/FSE 2015}
}

@article{sevcik13_compc,
	abstract = {In this article, we consider the semantic design and verified compilation of a C-like programming language for concurrent shared-memory computation on x86 multiprocessors. The design of such a language is made surprisingly subtle by several factors: the relaxed-memory behavior of the hardware, the effects of compiler optimization on concurrent code, the need to support high-performance concurrent algorithms, and the desire for a reasonably simple programming model. In turn, this complexity makes verified compilation both essential and challenging.We describe ClightTSO, a concurrent extension of CompCert’s Clight in which the TSO-based memory model of x86 multiprocessors is exposed for high-performance code, and CompCertTSO, a formally verified compiler from ClightTSO to x86 assembly language, building on CompCert. CompCertTSO is verified in Coq: for any well-behaved and successfully compiled ClightTSO source program, any permitted observable behavior of the generated assembly code (if it does not run out of memory) is also possible in the source semantics. We also describe some verified fence-elimination optimizations, integrated into CompCertTSO.},
	author = {Ševčı́k, Jaroslav and Vafeiadis, Viktor and Zappa Nardelli, Francesco and Jagannathan, Suresh and Sewell, Peter},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	doi = {10.1145/2487241.2487248},
	issn = {0004-5411},
	journaltitle = {J. ACM},
	keywords = {semantics,Relaxed memory models,verified compilation},
	month = jun,
	number = {3},
	title = {CompCertTSO: A Verified Compiler for Relaxed-Memory Concurrency},
	volume = {60},
	year = {2013}
}

@article{six20_certif_effic_instr_sched,
	abstract = {CompCert is a moderately optimizing C compiler with a formal, machine-checked, proof of correctness: after successful compilation, the assembly code has a behavior faithful to the source code. Previously, it only supported target instruction sets with sequential semantics, and did not attempt reordering instructions for optimization. We present here a CompCert backend for a VLIW core (i.e. with explicit parallelism at the instruction level), the first CompCert backend providing scalable and efficient instruction scheduling. Furthermore, its highly modular implementation can be easily adapted to other VLIW or non-VLIW pipelined processors.},
	author = {Six, Cyril and Boulmé, Sylvain and Monniaux, David},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	doi = {10.1145/3428197},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {coq,translation validation,scheduling,static scheduling,verification,VLIW,operational semantics},
	month = nov,
	number = {OOPSLA},
	title = {Certified and Efficient Instruction Scheduling: Application to Interlocked VLIW Processors},
	volume = {4},
	year = {2020}
}

@inproceedings{six22_formal_verif_super_sched,
	abstract = {On in-order processors, without dynamic instruction scheduling, program running times may be significantly reduced by compile-time instruction scheduling. We present here the first effective certified instruction scheduler that operates over superblocks (it may move instructions across branches), along with its performance evaluation. It is integrated within the CompCert C compiler, providing a complete machine-checked proof of semantic preservation from C to assembly. Our optimizer composes several passes designed by translation validation: program transformations are proposed by untrusted oracles, which are then validated by certified and scalable checkers. Our main checker is an architecture-independent simulation-test over superblocks modulo register liveness, which relies on hash-consed symbolic execution.},
	author = {Six, Cyril and Gourdin, Léo and Boulmé, Sylvain and Monniaux, David and Fasse, Justus and Nardino, Nicolas},
	location = {Philadelphia, PA, USA},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	doi = {10.1145/3497775.3503679},
	isbn = {9781450391825},
	keywords = {Symbolic execution,Instruction-level parallelism,Translation validation,the COQ proof assistant},
	pages = {40--54},
	series = {CPP 2022},
	title = {Formally Verified Superblock Scheduling},
	year = {2022}
}

@Software{six23_c,
	urldate = {2023-12-30},
	url = {https://gricad-gitlab.univ-grenoble-alpes.fr/certicompil/compcert-kvx},
	year = {2023},
	title = {CompCertKVX},
	author = {Six, Cyril and Gourdin, Léo and Boulmé, Sylvain and Monniaux, David}
}

@Software{synopsys23_v,
	urldate = {2023-12-20},
	url = {https://www.synopsys.com/verification/static-and-formal-verification/vc-formal.html},
	year = {2023},
	title = {VC Formal: Leading Formal Innovations},
	author = {Synopsys}
}

@InProceedings{tan15_mappin_lut_fpgas,
	keywords = {delay prediction},
	author = {Tan, Mingxing and Dai, Steve and Gupta, Udit and Zhang, Zhiru},
	title = {Mapping-aware constrained scheduling for {LUT-based FPGAs}},
	booktitle = {Proceedings of the 23rd {ACM}/{SIGDA} International
                  Symposium on Field Programmable Gate Arrays},
	year = 2015,
	address = {Monterey, CA},
	month = feb,
	pages = {190--9}
}

@inproceedings{thomas16_srcht,
	author = {Thomas, David B.},
	publisher = {{IEEE} Computer Society},
	booktitle = {{ASAP}},
	doi = {10.1109/ASAP.2016.7760777},
	pages = {91--98},
	title = {Synthesisable Recursion for {C++} {HLS} Tools},
	year = {2016}
}

@article{tiemeyer19_crest,
	author = {Andreas Tiemeyer and Tom Melham and Daniel Kroening and John O'Leary},
	title = {{CREST:} Hardware Formal Verification with {ANSI-C} Reference Specifications},
	journal = {CoRR},
	volume = {abs/1908.01324},
	year = {2019},
	url = {http://arxiv.org/abs/1908.01324},
	eprinttype = {arXiv},
	eprint = {1908.01324},
	timestamp = {Fri, 09 Aug 2019 12:15:56 +0200},
	biburl = {https://dblp.org/rec/journals/corr/abs-1908-01324.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{tristan08_formal_verif_trans_valid,
	author = {Tristan, Jean-Baptiste and Leroy, Xavier},
	location = {San Francisco, California, USA},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
	doi = {10.1145/1328438.1328444},
	isbn = {9781595936899},
	keywords = {coq,translation validation,CompCert,static scheduling},
	pages = {17--27},
	series = {POPL '08},
	title = {Formal Verification of Translation Validators: A Case Study on Instruction Scheduling Optimizations},
	year = {2008}
}

@inproceedings{tu95_effic_build_placin_gatin_funct,
	abstract = {In this paper, we present an almost-linear time algorithm for constructing Gated Single Assignment (GSA), which is SSA augmented with gating functions at ø-nodes. The gating functions specify the control dependences for each reaching definition at a ø-node. We introduce a new concept of gating path, which is path in the control flow graph from the immediate dominator u of a node v to v, such that every node in the path is dominated by u. Previous algorithms start with ø-function placement, and then traverse the control flow graph to compute the gating functions. By formulating the problem into gating path construction, we are able to identify not only a ø-node, but also a gating path expression which defines a gating function for the ø-node.},
	author = {Tu, Peng and Padua, David},
	location = {La Jolla, California, USA},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the ACM SIGPLAN 1995 Conference on Programming Language Design and Implementation},
	doi = {10.1145/207110.207115},
	isbn = {0897916972},
	keywords = {gated-SSA,SSA},
	pages = {47--55},
	series = {PLDI '95},
	title = {Efficient Building and Placing of Gating Functions},
	year = {1995}
}

@InProceedings{ustun20_accur_fpga_hls,
	keywords = {delay prediction},
	author = {Ustun, Ecenur and Deng, Chenhui and Pal, Debjit and Li, Zhijing and Zhang, Zhiru},
	title = {Accurate operation delay prediction for {FPGA HLS} using graph neural networks},
	booktitle = {Proceedings of the 39th International Conference on Computer-Aided Design},
	year = 2020,
	address = {Virtual},
	month = nov,
	pages = {1--9}
}

@article{wang20_compc,
	abstract = {We present CompCertELF, the first extension to CompCert that supports verified compilation from C programs all the way to a standard binary file format, i.e., the ELF object format. Previous work on Stack-Aware CompCert provides a verified compilation chain from C programs to assembly programs with a realistic machine memory model. We build CompCertELF by modifying and extending this compilation chain with a verified assembler which further transforms assembly programs into ELF object files. CompCert supports large-scale verification via verified separate compilation: C modules can be written and compiled separately, and then linked together to get a target program that refines the semantics of the program linked from the source modules. However, verified separate compilation in CompCert only works for compilation to assembly programs, not to object files. For the latter, the main difficulty is to bridge the two different views of linking: one for CompCert's programs that allows arbitrary shuffling of global definitions by linking and the other for object files that treats blocks of encoded definitions as indivisible units. We propose a lightweight approach that solves the above problem without any modification to CompCert's framework for verified separate compilation: by introducing a notion of syntactical equivalence between programs and proving the commutativity between syntactical equivalence and the two different kinds of linking, we are able to transit from the more abstract linking operation in CompCert to the more concrete one for ELF object files. By applying this approach to CompCertELF, we obtain the first compiler that supports verified separate compilation of C programs into ELF object files.},
	author = {Wang, Yuting and Xu, Xiangzhe and Wilke, Pierre and Shao, Zhong},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	doi = {10.1145/3428265},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {Generation of Object Files,Assembler Verification,Verified Separate Compilation},
	month = nov,
	number = {OOPSLA},
	title = {CompCertELF: Verified Separate Compilation of C Programs into ELF Object Files},
	volume = {4},
	year = {2020}
}

@InProceedings{wang23_mapbuf,
	keywords = {delay prediction},
	author = {Wang, Hanyu and Rizzi, Carmine and Josipović, Lana},
	booktitle = {Proceedings of the 42nd IEEE/ACM Intl. Conference on Computer-Aided Design},
	title = {{MapBuf}: Simultaneous Technology Mapping and Buffer Insertion for {HLS} Performance Optimization},
	year = {2023},
	month = 10,
	address = {San Francisco, CA}
}

@Book{west10_cvd,
	date = {2010-03-01},
	edition = {4},
	publisher = {Pearson},
	isbn = {9780321547743},
	title = {CMOS VLSI Design: A Circuits and Systems Perspective},
	author = {West, Neil H. E. and Harris, David Money}
}

@inproceedings{yang11_findin_under_bugs_c_compil,
	author = {Yang, Xuejun and Chen, Yang and Eide, Eric and Regehr, John},
	location = {San Jose, California, USA},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation},
	doi = {10.1145/1993498.1993532},
	isbn = {9781450306638},
	keywords = {random program generation,random testing,automated testing,compiler testing,compiler defect},
	pages = {283--294},
	series = {PLDI '11},
	title = {Finding and Understanding Bugs in C Compilers},
	year = {2011}
}

@inproceedings{zhang13_sdc,
	abstract = {Modulo scheduling is a popular technique to enable pipelined execution of successive loop iterations for performance improvement. While a variety of modulo scheduling algorithms exist for software pipelining, they are not amenable to many complex design constraints and optimization goals that arise in the hardware synthesis context. In this paper we describe a modulo scheduling framework based on the formulation of system of difference constraints (SDC). Our framework can systematically model a rich set of performance constraints that are specific to the hardware design. The scheduler also exploits the unique mathematical properties of SDC to carry out efficient global optimization and fast incremental update on the constraint system to minimize the resource usage of the synthesized pipeline. Experiments demonstrate that our proposed technique provides efficient solutions for a set of real-life applications and compares favorably against a widely used lifetime-sensitive modulo scheduling algorithm.},
	author = {Zhang, Zhiru and Liu, Bin},
	booktitle = {2013 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
	doi = {10.1109/ICCAD.2013.6691121},
	issn = {1558-2434},
	keywords = {high level synthesis;pipeline processing;scheduling;SDC-based modulo scheduling;pipeline synthesis;hardware design;mathematical properties;global optimization;incremental update;Schedules;Pipeline processing;Registers;Optimal scheduling;Scheduling algorithms;Timing},
	month = nov,
	pages = {211--218},
	title = {SDC-based modulo scheduling for pipeline synthesis},
	year = {2013}
}

@inproceedings{zhao12_formal_llvm_inter_repres_verif_progr_trans,
	author = {Zhao, Jianzhou and Nagarakatte, Santosh and Martin, Milo M.K. and Zdancewic, Steve},
	location = {Philadelphia, PA, USA},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
	doi = {10.1145/2103656.2103709},
	isbn = {9781450310833},
	keywords = {Coq,LLVM,memory safety},
	pages = {427--440},
	series = {POPL '12},
	title = {Formalizing the {LLVM} Intermediate Representation for Verified Program Transformations},
	year = {2012}
}

@inproceedings{zheng14_fast_effec_placem_routin_direc,
	author = {Zheng, Hongbin and Gurumani, Swathi T. and Rupnow, Kyle and Chen, Deming},
	title = {Fast and Effective Placement and Routing Directed High-Level Synthesis for FPGAs},
	year = {2014},
	isbn = {9781450326711},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/2554688.2554775},
	abstract = {Achievable frequency (fmax) is a widely used input constraint for designs targeting Field-Programmable Gate Arrays (FPGA), because of its impact on design latency and throughput. Fmax is limited by critical path delay, which is highly influenced by lower-level details of the circuit implementation such as technology mapping, placement and routing. However, for high-level synthesis~(HLS) design flows, it is challenging to evaluate the real critical delay at the behavioral level. Current HLS flows typically use module pre-characterization for delay estimates. However, we will demonstrate that such delay estimates are not sufficient to obtain high fmax and also minimize total execution latency.In this paper, we introduce a new HLS flow that integrates with Altera's Quartus synthesis and fast placement and routing (PAR) tool to obtain realistic post-PAR delay estimates. This integration enables an iterative flow that improves the performance of the design with both behavioral-level and circuit-level optimizations using realistic delay information. We demonstrate our HLS flow produces up to 24\% (on average 20\%) improvement in fmax and upto 22\% (on average 20\%) improvement in execution latency. Furthermore, results demonstrate that our flow is able to achieve from 65\% to 91\% of the theoretical fmax on Stratix IV devices (550MHz).},
	booktitle = {Proceedings of the 2014 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	pages = {1–10},
	numpages = {10},
	keywords = {high-level synthesis, scheduling, delay prediction},
	location = {Monterey, California, USA},
	series = {FPGA '14}
}

@Comment{
Local Variables:
bibtex-dialect: biblatex
End:
}

