@Comment{
ebib-main-file: /home/ymh/Dropbox/bibliography/references.bib
}


@Software{absint19_compc,
	year = {2019},
	title = {{CompCert} release 19.10},
	author = {AbsInt},
	url = {https://www.absint.com/releasenotes/compcert/19.10/}
}

@Software{absint22_compc,
	url = {https://www.absint.com/releasenotes/compcert/22.10/},
	year = {2022},
	title = {{CompCert} release 22.10},
	author = {AbsInt}
}

@inproceedings{allen70_cfa,
	keywords = {control-flow},
	author = {Allen, Frances E.},
	title = {Control Flow Analysis},
	year = {1970},
	isbn = {9781450373869},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/800028.808479},
	abstract = {Any static, global analysis of the expression and data relationships in a program requires a knowledge of the control flow of the program. Since one of the primary reasons for doing such a global analysis in a compiler is to produce optimized programs, control flow analysis has been embedded in many compilers and has been described in several papers. An early paper by Prosser [5] described the use of Boolean matrices (or, more particularly, connectivity matrices) in flow analysis. The use of “dominance” relationships in flow analysis was first introduced by Prosser and much expanded by Lowry and Medlock [6]. References [6,8,9] describe compilers which use various forms of control flow analysis for optimization. Some recent developments in the area are reported in [4] and in [7].The underlying motivation in all the different types of control flow analysis is the need to codify the flow relationships in the program. The codification may be in connectivity matrices, in predecessor-successor tables, in dominance lists, etc. Whatever the form, the purpose is to facilitate determining what the flow relationships are; in other words to facilitate answering such questions as: is this an inner loop?, if an expression is removed from the loop where can it be correctly and profitably placed?, which variable definitions can affect this use?In this paper the basic control flow relationships are expressed in a directed graph. Various graph constructs are then found and shown to codify interesting global relationships.},
	booktitle = {Proceedings of a Symposium on Compiler Optimization},
	pages = {1–19},
	numpages = {19},
	location = {Urbana-Champaign, Illinois}
}

@inproceedings{allen83_conver_contr_depen_data_depen,
	abstract = {Program analysis methods, especially those which support automatic vectorization, are based on the concept of interstatement dependence where a dependence holds between two statements when one of the statements computes values needed by the other. Powerful program transformation systems that convert sequential programs to a form more suitable for vector or parallel machines have been developed using this concept [AllK 82, KKLW 80].The dependence analysis in these systems is based on data dependence. In the presence of complex control flow, data dependence is not sufficient to transform programs because of the introduction of control dependences. A control dependence exists between two statements when the execution of one statement can prevent the execution of the other. Control dependences do not fit conveniently into dependence-based program translators.One solution is to convert all control dependences to data dependences by eliminating goto statements and introducing logical variables to control the execution of statements in the program. In this scheme, action statements are converted to IF statements. The variables in the conditional expression of an IF statement can be viewed as inputs to the statement being controlled. The result is that control dependences between statements become explicit data dependences expressed through the definitions and uses of the controlling logical variables.This paper presents a method for systematically converting control dependences to data dependences in this fashion. The algorithms presented here have been implemented in PFC, an experimental vectorizer written at Rice University.},
	author = {Allen, J. R. and Kennedy, Ken and Porterfield, Carrie and Warren, Joe},
	location = {Austin, Texas},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 10th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
	doi = {10.1145/567067.567085},
	isbn = {0897910907},
	keywords = {if-conversion},
	pages = {177--189},
	series = {POPL '83},
	title = {Conversion of Control Dependence to Data Dependence},
	year = {1983}
}

@misc{amd23_vitis_forum,
	author = {AMD},
	title = {Vitis Forums},
	url = {https://bit.ly/vitisifc},
	urldate = {2023-06-02},
	year = {2023},
	note = {Relevant quote from AMD: ``If-Conversion aims to convert a sequence of blocks into a single block for better optimization result.''}
}

@misc{amd23_vitis_high_synth,
	author = {AMD},
	title = {Vitis High-level Synthesis},
	url = {https://bit.ly/41R0204},
	urldate = {2023-05-21},
	year = 2023
}

@book{baker19_princ,
	abstract = {An updated edition of the text that explores the core topics in scheduling theory The second edition of Principles of Sequencing and Scheduling has been revised and updated to provide comprehensive coverage of sequencing and scheduling topics as well as emerging developments in the field. The text offers balanced coverage of deterministic models and stochastic models and includes new developments in safe scheduling and project scheduling, including coverage of project analytics. These new topics help bridge the gap between classical scheduling and actual practice. The authors—noted experts in the field—present a coherent and detailed introduction to the basic models, problems, and methods of scheduling theory. This book offers an introduction and overview of sequencing and scheduling and covers such topics as single-machine and multi-machine models, deterministic and stochastic problem formulations, optimization and heuristic solution approaches, and generic and specialized software methods. This new edition adds coverage on topics of recent interest in shop scheduling and project scheduling. This important resource: Offers comprehensive coverage of deterministic models as well as recent approaches and developments for stochastic models Emphasizes the application of generic optimization software to basic sequencing problems and the use of spreadsheet-based optimization methods Includes updated coverage on safe scheduling, lognormal modeling, and job selection Provides basic coverage of robust scheduling as contrasted with safe scheduling Adds a new chapter on project analytics, which supports the PERT21 framework for project scheduling in a stochastic environment. Extends the coverage of PERT 21 to include hierarchical scheduling Provides end-of-chapter references and access to advanced Research Notes, to aid readers in the further exploration of advanced topics Written for upper-undergraduate and graduate level courses covering such topics as scheduling theory and applications, project scheduling, and operations scheduling, the second edition of Principles of Sequencing and Scheduling is a resource that covers scheduling techniques and contains the most current research and emerging topics.},
	author = {Baker, Kenneth R.},
	address = {Hoboken, NJ},
	booktitle = {Principles of sequencing and scheduling},
	edition = {Second edition.},
	isbn = {1-119-26259-3},
	keywords = {scheduling, list scheduling},
	language = {eng},
	publisher = {Wiley},
	series = {Wiley series in operations research and management science},
	title = {Principles of sequencing and scheduling },
	year = {2019}
}

@inproceedings{ball93_branc_predic_free,
	keywords = {if-conversion},
	author = {Ball, Thomas and Larus, James R.},
	title = {Branch Prediction for Free},
	year = {1993},
	isbn = {0897915984},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/155090.155119},
	abstract = {Many compilers rely on branch prediction to improve program performance by identifying frequently executed regions and by aiding in scheduling instructions.Profile-based predictors require a time-consuming and inconvenient compile-profile-compile cycle in order to make predictions. We present a program-based branch predictor that performs well for a large and diverse set of programs written in C and Fortran. In addition to using natural loop analysis to predict branches that control the iteration of loops, we focus on heuristics for predicting non-loop branches, which dominate the dynamic branch count of many programs. The heuristics are simple and require little program analysis, yet they are effective in terms of coverage and miss rate. Although program-based prediction does not equal the accuracy of profile-based prediction, we believe it reaches a sufficiently high level to be useful. Additional type and semantic information available to a compiler would enhance our heuristics.},
	booktitle = {Proceedings of the ACM SIGPLAN 1993 Conference on Programming Language Design and Implementation},
	pages = {300–313},
	numpages = {14},
	location = {Albuquerque, New Mexico, USA},
	series = {PLDI '93}
}

@article{barthe14_formal_verif_ssa_based_middl_end_compc,
	abstract = {CompCert is a formally verified compiler that generates compact and efficient code for a large subset of the C language. However, CompCert foregoes using SSA, an intermediate representation employed by many compilers that enables writing simpler, faster optimizers. In fact, it has remained an open problem to verify formally an SSA-based compiler. We report on a formally verified, SSA-based middle-end for CompCert. In addition to providing a formally verified SSA-based middle-end, we address two problems raised by Leroy in [2009]: giving an intuitive formal semantics to SSA, and leveraging its global properties to reason locally about program optimizations.},
	author = {Barthe, Gilles and Demange, Delphine and Pichardie, David},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/2579080},
	doi = {10.1145/2579080},
	issn = {0164-0925},
	journaltitle = {ACM Trans. Program. Lang. Syst.},
	keywords = {CompCertSSA,CompCert,SSA,coq,verification,compiler optimisation},
	month = mar,
	number = {1},
	title = {Formal Verification of an SSA-Based Middle-End for CompCert},
	volume = {36},
	year = {2014}
}

@article{beck91_fcfd,
	keywords = {control-flow, data-flow},
	title = {From Control Flow to Dataflow},
	journal = {Journal of Parallel and Distributed Computing},
	volume = {12},
	number = {2},
	pages = {118-129},
	year = {1991},
	issn = {0743-7315},
	doi = {10.1016/0743-7315(91)90016-3},
	author = {Micah Beck and Richard Johnson and Keshav Pingali},
	abstract = {Are imperative languages tied inseparably to the von Neumann model or can they be implemented in some natural way on data-flow architectures? In this paper, we show how imperative language programs can be translated into dataflow graphs and executed on a dataflow machine like Monsoon. This translation can exploit both fine-grain and coarse-grain parallelism in imperative language programs. More importantly, we establish a close connection between our work and current research in the imperative languages community on data dependences, control dependences, program dependence graphs, and static single assignment form. These results suggest that dataflow graphs can serve as an executable intermediate representation in parallelizing compilers.}
}

@software{berkelaar10,
	year = {2010},
	title = {\texttt{lp\_solve} v5.5},
	author = {Berkelaar, Michel},
	url = {https://lpsolve.sourceforge.net/5.5/}
}

@ARTICLE{boutros21_fpga_archit,
	author = {Boutros, Andrew and Betz, Vaughn},
	journal = {IEEE Circuits and Systems Magazine},
	title = {FPGA Architecture: Principles and Progression},
	year = {2021},
	volume = {21},
	number = {2},
	pages = {4-29},
	doi = {10.1109/MCAS.2021.3071607}
}

@inproceedings{bowen20_perfor_cost_softw_based_secur_mitig,
	author = {Bowen, Lucy and Lupo, Chris},
	title = {The Performance Cost of Software-Based Security Mitigations},
	year = {2020},
	isbn = {9781450369916},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3358960.3379139},
	doi = {10.1145/3358960.3379139},
	abstract = {Historically, performance has been the most important feature when optimizing computer hardware. Modern processors are so highly optimized that every cycle of computation time matters. However, this practice of optimizing for performance at all costs has been called into question by new microarchitectural attacks, e.g. Meltdown and Spectre. Microarchitectural attacks exploit the effects of microarchitectural components or optimizations in order to leak data to an attacker. These attacks have caused processor manufacturers to introduce performance impacting mitigations in both software and silicon. To investigate the performance impact of the various mitigations, a test suite of forty-seven different tests was created. This suite was run on a series of virtual machines that tested both Ubuntu 16 and Ubuntu 18. These tests investigated the performance change across version updates and the performance impact of CPU core number vs. default microarchitectural mitigations. The testing proved that the performance impact of the microarchitectural mitigations is non-trivial, as the percent difference in performance can be as high as 200\%.},
	booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
	pages = {210–217},
	numpages = {8},
	keywords = {performance evaluation, hardware vulnerabilities, security mitigations},
	location = {Edmonton AB, Canada},
	series = {ICPE '20}
}

@inproceedings{budiu02_compil_applic_specif_hardw,
	author = {Mihai Budiu and Seth Copen Goldstein},
	editor = {Manfred Glesner and
                  Peter Zipf and
                  Michel Renovell},
	title = {Compiling Application-Specific Hardware},
	booktitle = {Field-Programmable Logic and Applications, Reconfigurable Computing
                  Is Going Mainstream, 12th International Conference, {FPL} 2002, Montpellier,
                  France, September 2-4, 2002, Proceedings},
	series = {Lecture Notes in Computer Science},
	volume = {2438},
	pages = {853--863},
	publisher = {Springer},
	year = {2002},
	doi = {10.1007/3-540-46117-5\_88},
	timestamp = {Tue, 14 May 2019 10:00:48 +0200},
	biburl = {https://dblp.org/rec/conf/fpl/BudiuG02.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{callahan98_instr_level_paral_recon_comput,
	author = {Timothy J. Callahan and John Wawrzynek},
	editor = {Reiner W. Hartenstein and
                  Andres Keevallik},
	title = {Instruction-Level Parallelism for Reconfigurable Computing},
	booktitle = {Field-Programmable Logic and Applications, From FPGAs to Computing
                  Paradigm, 8th International Workshop, FPL'98, Tallinn, Estonia, August
                  31 - September 3, 1998, Proceedings},
	series = {Lecture Notes in Computer Science},
	volume = {1482},
	pages = {248--257},
	publisher = {Springer},
	year = {1998},
	doi = {10.1007/BFb0055252},
	timestamp = {Tue, 14 May 2019 10:00:48 +0200},
	biburl = {https://dblp.org/rec/conf/fpl/CallahanW98.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{campbell93_refin,
	author = {Campbell, Philip L and Krishna, Ksheerabdhi and Ballance, Robert A},
	publisher = {Citeseer},
	journaltitle = {Cs93-6, University of New Mexico, Albuquerque},
	keywords = {gated-SSA},
	title = {Refining and defining the program dependence web},
	year = {1993}
}

@inproceedings{canis11_legup,
	abstract = {In this paper, we introduce a new open source high-level synthesis tool called LegUp that allows software techniques to be used for hardware design. LegUp accepts a standard C program as input and automatically compiles the program to a hybrid architecture containing an FPGA-based MIPS soft processor and custom hardware accelerators that communicate through a standard bus interface. Results show that the tool produces hardware solutions of comparable quality to a commercial high-level synthesis tool.},
	author = {Canis, Andrew and Choi, Jongsok and Aldham, Mark and Zhang, Victor and Kammoona, Ahmed and Anderson, Jason H. and Brown, Stephen and Czajkowski, Tomasz},
	location = {Monterey, CA, USA},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 19th ACM/SIGDA International Symposium on Field Programmable Gate Arrays},
	doi = {10.1145/1950413.1950423},
	isbn = {9781450305549},
	keywords = {fpgas,hardware/software co-design,field-programmable gate arrays,high-level synthesis},
	pages = {33--36},
	series = {FPGA '11},
	title = {LegUp: High-Level Synthesis for FPGA-Based Processor/Accelerator Systems},
	year = {2011}
}

@inproceedings{canis14_modul_sdc,
	abstract = {Loop pipelining is a high-level synthesis scheduling technique that overlaps loop iterations to achieve higher performance. However, industrial designs often have resource constraints and other constraints imposed by cross-iteration dependencies. The interaction between multiple constraints can pose a challenge for HLS modulo scheduling algorithms, which, if not handled properly can lead to a loop pipeline schedule that fails to achieve the minimum possible initiation interval. We propose a novel modulo scheduler based on an SDC formulation that includes a backtracking mechanism to properly handle multiple scheduling constraints and still achieve the minimum possible initiation interval. The SDC formulation has the advantage of being a mathematical framework that supports flexible constraints that are useful for more complex loop pipelines. Furthermore, we describe how to specifically apply associative expression transformations during modulo scheduling to restructure recurrences in complex loops to enable better scheduling. We compared our techniques to existing prior work in modulo scheduling in HLS and also compared against a state-of-art commercial tool. Over a suite of benchmarks, we show that our scheduler and proposed optimizations can result in a geomean wall-clock time reduction of 32 \% versus prior work and 29 \% versus a commercial tool.},
	author = {Canis, A. and Brown, S. D. and Anderson, J. H.},
	url = {https://doi.org/10.1109/FPL.2014.6927490},
	booktitle = {2014 24th International Conference on Field Programmable Logic and Applications (FPL)},
	doi = {10.1109/FPL.2014.6927490},
	issn = {1946-1488},
	keywords = {high-level synthesis,modulo scheduling,loop scheduling,static scheduling},
	month = sep,
	pages = {1--8},
	title = {Modulo SDC scheduling with recurrence minimization in high-level synthesis},
	year = {2014}
}

@PhdThesis{canis15_legup,
	author = {Canis, Andrew},
	keywords = {high-level synthesis,hardware/software co-simulation,FPGA},
	title = {Legup: open-source high-level synthesis research framework},
	type = {phdthesis},
	year = {2015}
}

@article{chang91_using_profil_infor_assis_class_code_optim,
	author = {Pohua P. Chang and
                  Scott A. Mahlke and
                  Wen{-}mei W. Hwu},
	title = {Using Profile Information to Assist Classic Code Optimizations},
	journal = {Softw. Pract. Exp.},
	volume = {21},
	number = {12},
	pages = {1301--1321},
	year = {1991},
	url = {https://doi.org/10.1002/spe.4380211204},
	doi = {10.1002/spe.4380211204},
	timestamp = {Thu, 09 Apr 2020 17:14:34 +0200},
	biburl = {https://dblp.org/rec/journals/spe/ChangMH91.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{chouksey20_verif_sched_condit_behav_high_level_synth,
	abstract = {High-level synthesis (HLS) technique translates the behaviors written in high-level languages like C/C++ into register transfer level (RTL) design. Due to its complexity, proving the correctness of an HLS tool is prohibitively expensive. Translation validation is the process of proving that the target code is a correct translation of the source program being compiled. The path-based equivalence checking (PBEC) method is a widely used translation validation method for verification of the scheduling phase of HLS. The existing PBEC methods cannot handle significant control structure modification that occurs in the efficient scheduling of conditional behaviors. Hence, they produce a false-negative result. In this article, we identify some scenarios involving path merge/split where the state-of-the-art PBEC approaches fail to show the equivalence even though behaviors are equivalent. We propose a value propagation-based PBEC method along with a new cutpoint selection scheme to overcome this limitation. Our method can also handle the scenario where adjacent conditional blocks (CBs) having an equivalent conditional expression are combined into one CB. Experimental results demonstrate the usefulness of our method over the existing methods.},
	author = {Chouksey, R. and Karfa, C.},
	url = {https://doi.org/10.1109/TVLSI.2020.2978242},
	doi = {10.1109/TVLSI.2020.2978242},
	issn = {1557-9999},
	journaltitle = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	keywords = {translation validation,high-level synthesis,verification,compiler optimisation},
	pages = {1--14},
	title = {Verification of Scheduling of Conditional Behaviors in High-Level Synthesis},
	year = {2020}
}

@inproceedings{cong06_sdc,
	abstract = {Scheduling plays a central role in the behavioral synthesis process, which automatically compiles high-level specifications into optimized hardware implementations. However, most of the existing behavior-level scheduling heuristics either have a limited efficiency in a specific class of applications or lack general support of various design constraints. In this paper we describe a new scheduler that converts a rich set of scheduling constraints into a system of difference constraints (SDC) and performs a variety of powerful optimizations under a unified mathematical programming framework. In particular, we show that our SDC-based scheduling algorithm can efficiently support resource constraints, frequency constraints, latency constraints, and relative timing constraints, and effectively optimize longest path latency, expected overall latency, and the slack distribution. Experiments demonstrate that our proposed technique provides efficient solutions for a broader range of applications with higher quality of results (in terms of system performance) when compared to the state-of-the-art scheduling heuristics},
	author = {Cong, Jason and Zhang, Zhiru},
	booktitle = {2006 43rd ACM/IEEE Design Automation Conference},
	doi = {10.1145/1146909.1147025},
	issn = {0738-100X},
	keywords = {high-level synthesis,static scheduling},
	month = jul,
	pages = {433--438},
	title = {An efficient and versatile scheduling algorithm based on SDC formulation},
	year = {2006}
}

@inproceedings{dessouky19_hardf,
	author = {Dessouky, Ghada and Gens, David and Haney, Patrick and Persyn, Garrett and Kanuparthi, Arun and Khattri, Hareesh and Fung, Jason M. and Sadeghi, Ahmad-Reza and Rajendran, Jeyavijayan},
	title = {{HardFails}: Insights into {Software-Exploitable} Hardware Bugs},
	booktitle = {28th USENIX Security Symposium (USENIX Security 19)},
	year = {2019},
	isbn = {978-1-939133-06-9},
	address = {Santa Clara, CA},
	pages = {213--230},
	url = {https://www.usenix.org/conference/usenixsecurity19/presentation/dessouky},
	publisher = {USENIX Association},
	month = aug
}

@phdthesis{ellis85_bulld,
	title = {Bulldog: A compiler for {VLIW} architectures},
	author = {Ellis, John R},
	year = {1985},
	school = {Yale University}
}

@misc{ferrandi14_panda_bambu,
	author = {Ferrandi, Fabrizio},
	title = {PandA-Bambu release notes},
	url = {https://github.com/ferrandi/PandA-bambu/blob/c443bf14c33a9a74008ada12f56e7a62e30e5efe/NEWS#L304},
	urldate = {2023-11-16},
	year = {2014}
}

@INPROCEEDINGS{ferrandi21_bambu,
	author = {Ferrandi, Fabrizio and Castellana, Vito Giovanni 
          and Curzel, Serena and Fezzardi, Pietro and Fiorito, Michele 
          and Lattuada, Marco and Minutoli, Marco and Pilato, Christian 
          and Tumeo, Antonino},
	booktitle = {2021 58th ACM/IEEE Design Automation Conference (DAC)},
	title = {Bambu: an Open-Source Research Framework for the 
         High-Level Synthesis of Complex Applications},
	year = {2021},
	pages = {1327-1330},
	abstract = {This paper presents the open-source high-level synthesis (HLS) research 
              framework Bambu. Bambu provides a research environment to experiment with 
              new ideas across HLS, high-level verification and debugging, FPGA/ASIC design,
              design flow space exploration, and parallel hardware accelerator design. The 
              tool accepts as input standard C/C++ specifications and compiler intermediate 
              representations (IRs) coming from the well-known Clang/LLVM and GCC compilers. 
              The broad spectrum and flexibility of input formats allow the electronic 
              design automation (EDA) research community to explore and integrate new 
              transformations and optimizations. The easily extendable modular framework 
              already includes many optimizations and HLS benchmarks used to evaluate 
              the QoR of the tool against existing approaches [1]. The integration with 
              synthesis and verification backends (commercial and open-source) allows 
              researchers to quickly test any new finding and easily obtain performance 
              and resource usage metrics for a given application. Different FPGA devices 
              are supported from several different vendors: AMD/Xilinx, Intel/Altera, 
              Lattice Semiconductor, and NanoXplore. Finally, integration with the OpenRoad 
              open-source end-to-end silicon compiler perfectly fits with the recent push 
              towards open-source EDA.},
	publisher = {{IEEE}},
	doi = {10.1109/DAC18074.2021.9586110},
	ISSN = {0738-100X},
	month = {12},
	pdf = {https://re.public.polimi.it/retrieve/668507/dac21_bambu.pdf}
}

@article{ferrante87_progr_depen_graph_its_use_optim,
	abstract = {In this paper we present an intermediate program representation, called the program dependence graph (PDG), that makes explicit both the data and control dependences for each operation in a program. Data dependences have been used to represent only the relevant data flow relationships of a program. Control dependences are introduced to analogously represent only the essential control flow relationships of a program. Control dependences are derived from the usual control flow graph. Many traditional optimizations operate more efficiently on the PDG. Since dependences in the PDG connect computationally related parts of the program, a single walk of these dependences is sufficient to perform many optimizations. The PDG allows transformations such as vectorization, that previously required special treatment of control dependence, to be performed in a manner that is uniform for both control and data dependences. Program transformations that require interaction of the two dependence types can also be easily handled with our representation. As an example, an incremental approach to modifying data dependences resulting from branch deletion or loop unrolling is introduced. The PDG supports incremental optimization, permitting transformations to be triggered by one another and applied only to affected dependences.},
	author = {Ferrante, Jeanne and Ottenstein, Karl J. and Warren, Joe D.},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/24039.24041},
	doi = {10.1145/24039.24041},
	issn = {0164-0925},
	journaltitle = {ACM Trans. Program. Lang. Syst.},
	keywords = {program dependence graph,gated-SSA,SSA},
	month = jul,
	number = {3},
	pages = {319--349},
	title = {The Program Dependence Graph and Its Use in Optimization},
	volume = {9},
	year = {1987}
}

@article{fisher81_trace_sched,
	author = {Fisher, Joseph A.},
	doi = {10.1109/TC.1981.1675827},
	journaltitle = {IEEE Transactions on Computers},
	keywords = {static scheduling,trace scheduling},
	number = {7},
	pages = {478--490},
	title = {Trace Scheduling: A Technique for Global Microcode Compaction},
	volume = {C-30},
	year = {1981}
}

@misc{google23_xls,
	author = {Google},
	title = {{XLS: Accelerated HW Synthesis}},
	url = {https://github.com/google/xls/blob/dde7095ff1050b09c37cb44d1977bff1af8de050/xls/scheduling/mutual_exclusion_pass.h#L112},
	urldate = {2023-11-14},
	year = {2023},
	note = {The XLS scheduler refers to using an SMT solver to merge mutually exclusive nodes}
}

@ARTICLE{halbwachs91_sdfpll,
	author = {Halbwachs, N. and Caspi, P. and Raymond, P. and Pilaud, D.},
	journal = {Proceedings of the IEEE},
	title = {The Synchronous Data Flow Programming Language {LUSTRE}},
	year = {1991},
	volume = {79},
	number = {9},
	pages = {1305-1320},
	doi = {10.1109/5.97300}
}

@inproceedings{havlak94_const,
	abstract = {Analysis of symbolic expressions benefits from a suitable program representation. We show how to build thinned gated single-assignment (TGSA) form, a value-oriented program representation which is more complete than standard SSA form, defined on all reducible programs, and better for representing symbolic expressions than program dependence graphs or original GSA form. We present practical algorithms for constructing thinned GSA form from the control dependence graph and SSA form. Extensive experiments on large Fortran programs show these methods to take linear time and space in practice. Our implementation of value numbering on TGSA form drives scalar symbolic analysis in the ParaScope programming environment.},
	author = {Havlak, Paul},
	editor = {Banerjee, Utpal and Gelernter, David and Nicolau, Alex and Padua, David},
	location = {Berlin, Heidelberg},
	publisher = {Springer Berlin Heidelberg},
	booktitle = {Languages and Compilers for Parallel Computing},
	isbn = {978-3-540-48308-3},
	keywords = {gated-SSA,SSA},
	pages = {477--499},
	title = {Construction of thinned gated single-assignment form},
	year = {1994}
}

@inproceedings{herzog21_price_meltd_spect,
	author = {Herzog, Benedict and Reif, Stefan and Preis, Julian and Schr\"{o}der-Preikschat, Wolfgang and H\"{o}nig, Timo},
	title = {The Price of Meltdown and Spectre: Energy Overhead of Mitigations at Operating System Level},
	year = {2021},
	isbn = {9781450383370},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3447852.3458721},
	doi = {10.1145/3447852.3458721},
	abstract = {The Meltdown and Spectre hardware vulnerabilities shocked hardware manufacturers and system users upon discovery. Numerous attack vectors and mitigations have been developed and deployed. However, due to the deep entanglement in core CPU components they will be an important topic for years. Although the performance overhead of software mitigations has been examined closely, the energy overhead has experienced little attention---even though the energy demand is a critical cost factor in data centres.This work contributes a fine-grained energy-overhead analysis of Meltdown/Spectre software mitigations, which reveals application-specific energy overheads of up to 72 \%. We further compare energy overheads to execution time overheads. Additionally, we examine subsystem-specific effects (i.e., CPU, memory, I/O, network/interprocess communication) and develop a model that predicts energy overheads for applications.},
	booktitle = {Proceedings of the 14th European Workshop on Systems Security},
	pages = {8–14},
	numpages = {7},
	location = {Online, United Kingdom},
	series = {EuroSec '21}
}

@article{hoare78_commun_sequen_proces,
	author = {Hoare, C. A. R.},
	title = {Communicating Sequential Processes},
	year = {1978},
	issue_date = {Aug. 1978},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {21},
	number = {8},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/359576.359585},
	doi = {10.1145/359576.359585},
	abstract = {This paper suggests that input and output are basic primitives of programming and that parallel composition of communicating sequential processes is a fundamental program structuring method. When combined with a development of Dijkstra's guarded command, these concepts are surprisingly versatile. Their use is illustrated by sample solutions of a variety of a familiar programming exercises.},
	journal = {Commun. ACM},
	month = {8},
	pages = {666–677},
	numpages = {12},
	keywords = {programming, guarded commands, multiple entries, nondeterminacy, data representations, monitors, multiple exits, iterative arrays, coroutines, conditional critical regions, program structures, concurrency, recursion, procedures, classes, programming primitives, parallel programming, programming languages, input, output}
}

@phdthesis{hopwood78_decom,
	author = {Hopwood, Gregory Littell},
	title = {Decompilation},
	year = {1978},
	publisher = {University of California, Irvine},
	note = {AAI7811860}
}

@inbook{hwu93_super,
	abstract = {A compiler for VLIW and superscalar processors must expose sufficient instruction-level parallelism (ILP) to effectively utilize the parallel hardware. However, ILP within basic blocks is extremely limited for control-intensive programs. We have developed a set of techniques for exploiting ILP across basic block boundaries. These techniques are based on a novel structure called the superblock. The superblock enables the optimizer and scheduler to extract more ILP along the important execution paths by systematically removing constraints due to the unimportant paths. Superblock optimization and scheduling have been implemented in the IMPACT-I compiler. This implementation gives us a unique opportunity to fully understand the issues involved in incorporating these techniques into a real compiler. Superblock optimizations and scheduling are shown to be useful while taking into account a variety of architectural features.},
	author = {Hwu, Wen-Mei W. and Mahlke, Scott A. and Chen, William Y. and Chang, Pohua P. and Warter, Nancy J. and Bringmann, Roger A. and Ouellette, Roland G. and Hank, Richard E. and Kiyohara, Tokuzo and Haab, Grant E. and Holm, John G. and Lavery, Daniel M.},
	editor = {Rau, B. R. and Fisher, J. A.},
	location = {Boston, MA},
	publisher = {Springer US},
	booktitle = {Instruction-Level Parallelism: A Special Issue of The Journal of Supercomputing},
	doi = {10.1007/978-1-4615-3200-2_7},
	isbn = {978-1-4615-3200-2},
	keywords = {superblock scheduling,trace scheduling,static scheduling},
	pages = {229--248},
	title = {The Superblock: An Effective Technique for VLIW and Superscalar Compilation},
	year = {1993}
}

@misc{intel20_high_synth_compil,
	author = {Intel},
	url = {https://intel.ly/2UDiWr5},
	title = {High-level Synthesis Compiler},
	urldate = {2023-06-23},
	year = {2023}
}

@book{jones86_system_vdm,
	author = {Jones, Cliff B.},
	title = {Systematic software development using {VDM}},
	series = {Prentice Hall International Series in Computer Science},
	publisher = {Prentice Hall},
	year = {1986},
	isbn = {978-0-13-880725-2},
	timestamp = {Thu, 14 Apr 2011 14:43:25 +0200},
	biburl = {https://dblp.org/rec/books/daglib/0068091.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{josipović18_dynam_sched_high_synth,
	author = {Josipović, Lana and Ghosal, Radhika and Ienne, Paolo},
	location = {Monterey, CALIFORNIA, USA},
	publisher = {ACM},
	url = {https://doi.org/10.1145/3174243.3174264},
	booktitle = {Proceedings of the 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	doi = {10.1145/3174243.3174264},
	isbn = {978-1-4503-5614-5},
	keywords = {compiler,dynamically scheduled circuits,high-level synthesis,pipelining},
	pages = {127--136},
	series = {FPGA '18},
	title = {Dynamically Scheduled High-level Synthesis},
	year = {2018}
}

@article{kam76_gdfaia,
	keywords = {data-flow},
	author = {Kam, John B. and Ullman, Jeffrey D.},
	title = {Global Data Flow Analysis and Iterative Algorithms},
	year = {1976},
	issue_date = {Jan. 1976},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {23},
	number = {1},
	issn = {0004-5411},
	url = {https://doi.org/10.1145/321921.321938},
	doi = {10.1145/321921.321938},
	abstract = {Kildall has developed data propagation algorithms for code optimization in a general lattice theoretic framework. In another direction, Hecht and Ullman gave a strong upper bound on the number of iterations required for propagation algorithms when the data is represented by bit vectors and depth-first ordering of the flow graph is used. The present paper combines the ideas of these two papers by considering conditions under which the bound of Hecht and Ullman applies to the depth-first version of Kildall's general data propagation algorithm. It is shown that the following condition is necessary and sufficient: Let undefined and g be any two functions which could be associated with blocks of a flow graph, let x be an arbitrary lattice element, and let 0 be the lattice zero. Then (*) (∀undefined,g,x) [undefinedg(0) ≥ g(0) ∧ undefined(x) ∧ x]. Then it is shown that several of the particular instances of the techniques Kildall found useful do not meet condition (*).},
	journal = {J. ACM},
	month = {jan},
	pages = {158–171},
	numpages = {14}
}

@inproceedings{karfa06_formal_verif_method_sched_high_synth,
	author = {Karfa, C and Mandal, C and Sarkar, D and Pentakota, S R. and Reade, Chris},
	location = {Washington, DC, USA},
	publisher = {IEEE Computer Society},
	url = {https://doi.org/10.1109/ISQED.2006.10},
	booktitle = {Proceedings of the 7th International Symposium on Quality Electronic Design},
	doi = {10.1109/ISQED.2006.10},
	isbn = {0-7695-2523-7},
	pages = {71--78},
	series = {ISQED '06},
	title = {A Formal Verification Method of Scheduling in High-level Synthesis},
	year = {2006}
}

@article{kern99_formal_verif_hardw_desig,
	author = {Kern, Christoph and Greenstreet, Mark R.},
	title = {Formal Verification in Hardware Design: A Survey},
	year = {1999},
	issue_date = {April 1999},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {4},
	number = {2},
	issn = {1084-4309},
	url = {https://doi.org/10.1145/307988.307989},
	doi = {10.1145/307988.307989},
	abstract = {In recent years, formal methods have emerged as an alternative approach to ensuring the quality and correctness of hardware designs, overcoming some of the limitations of traditional validation techniques such as simulation and testing.There are two main aspects to the application of formal methods in a design process: the formal framework used to specify desired properties of a design and the verification techniques and tools used to reason about the relationship between a specification and a corresponding implementation. We survey a variety of frameworks and techniques proposed in the literature and applied to actual designs. The specification frameworks we describe include temporal logics, predicate logic, abstraction and refinement, as well as containment between  ω-regular languages. The verification techniques presented include model checking, automata-theoretic techniques, automated theorem proving, and approaches that integrate the above methods.In order to provide insight into the scope and limitations of currently available techniques, we present a selection of case studies where formal methods were applied to industrial-scale designs, such as microprocessors, floating-point hardware, protocols, memory subsystems, and communications hardware.},
	journal = {ACM Trans. Des. Autom. Electron. Syst.},
	month = {apr},
	pages = {123–193},
	numpages = {71},
	keywords = {survey, formal verification, model checking, formal methods, theorem proving, hardware verification, language containment, case studies}
}

@inproceedings{kildall73_unified_approac_global_progr_optim,
	author = {Kildall, Gary A.},
	location = {Boston, Massachusetts},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/512927.512945},
	booktitle = {Proceedings of the 1st Annual ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
	doi = {10.1145/512927.512945},
	isbn = {9781450373494},
	pages = {194--206},
	series = {POPL '73},
	title = {A Unified Approach to Global Program Optimization},
	year = {1973}
}

@inproceedings{kim04_autom_fsmd,
	author = {{Youngsik Kim} and {Kopuri}, S. and {Mansouri}, N.},
	url = {https://doi.org/10.1109/ISQED.2004.1283659},
	booktitle = {International Symposium on Signals, Circuits and Systems. Proceedings, SCS 2003. (Cat. No.03EX720)},
	doi = {10.1109/ISQED.2004.1283659},
	issn = {null},
	keywords = {formal verification;high level synthesis;data flow graphs;finite state machines;theorem proving;automated formal verification;scheduling process;finite state machines with datapath;high-level synthesis;behavioral specification;control-data flow graph;equivalence conditions;higher-order specification language;theorem proving environment;PVS proof checker;Formal verification;Automata;High level synthesis;Processor scheduling;Computer science;Flow graphs;Mathematical model;Specification languages;Libraries;Computer applications},
	month = mar,
	pages = {110--115},
	title = {Automated formal verification of scheduling process using finite state machines with datapath (FSMD)},
	year = {2004}
}

@INPROCEEDINGS{lattner04_llvm,
	author = {Lattner, C. and Adve, V.},
	booktitle = {International Symposium on Code Generation and Optimization, 2004. CGO 2004.},
	title = {LLVM: a compilation framework for lifelong program analysis \& transformation},
	year = {2004},
	volume = {},
	number = {},
	pages = {75-86},
	doi = {10.1109/CGO.2004.1281665}
}

@inproceedings{lattner21_mlir,
	abstract = {This work presents MLIR, a novel approach to building reusable and extensible compiler infrastructure. MLIR addresses software fragmentation, compilation for heterogeneous hardware, significantly reducing the cost of building domain specific compilers, and connecting existing compilers together. MLIR facilitates the design and implementation of code generators, translators and optimizers at different levels of abstraction and across application domains, hardware targets and execution environments. The contribution of this work includes (1) discussion of MLIR as a research artifact, built for extension and evolution, while identifying the challenges and opportunities posed by this novel design, semantics, optimization specification, system, and engineering. (2) evaluation of MLIR as a generalized infrastructure that reduces the cost of building compilers-describing diverse use-cases to show research and educational opportunities for future programming languages, compilers, execution environments, and computer architecture. The paper also presents the rationale for MLIR, its original design principles, structures and semantics.},
	author = {Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
	booktitle = {2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},
	doi = {10.1109/CGO51591.2021.9370308},
	keywords = {,mlir},
	month = feb,
	pages = {2--14},
	title = {MLIR: Scaling Compiler Infrastructure for Domain Specific Computation},
	year = {2021}
}

@INPROCEEDINGS{lattuada15_ctbsss,
	keywords = {bambu},
	author = {Lattuada, Marco and Ferrandi, Fabrizio},
	booktitle = {2015 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
	title = {Code Transformations Based on Speculative {SDC} Scheduling},
	year = {2015},
	volume = {},
	number = {},
	pages = {71-77},
	doi = {10.1109/ICCAD.2015.7372552}
}

@article{leroy09_formal_verif_compil_back_end,
	abstract = {This article describes the development and formal verification (proof of semantic preservation) of a compiler back-end from Cminor (a simple imperative intermediate language) to PowerPC assembly code, using the Coq proof assistant both for programming the compiler and for proving its soundness. Such a verified compiler is useful in the context of formal methods applied to the certification of critical software: the verification of the compiler guarantees that the safety properties proved on the source code hold for the executable compiled code as well.},
	author = {Leroy, Xavier},
	doi = {10.1007/s10817-009-9155-4},
	issn = {1573-0670},
	journaltitle = {Journal of Automated Reasoning},
	number = {4},
	pages = {363},
	title = {A Formally Verified Compiler Back-End},
	volume = {43},
	year = {2009}
}

@inproceedings{lööw19_proof_trans_veril_devel_hol,
	author = {Lööw, Andreas and Myreen, Magnus O.},
	location = {Montreal, Quebec, Canada},
	publisher = {IEEE Press},
	url = {https://doi.org/10.1109/FormaliSE.2019.00020},
	booktitle = {Proceedings of the 7th International Workshop on Formal Methods in Software Engineering},
	doi = {10.1109/FormaliSE.2019.00020},
	pages = {99--108},
	series = {FormaliSE '19},
	title = {A Proof-producing Translator for Verilog Development in HOL},
	year = {2019}
}

@inproceedings{lööw19_verif_compil_verif_proces,
	author = {Lööw, Andreas and Kumar, Ramana and Tan, Yong Kiam and Myreen, Magnus O. and Norrish, Michael and Abrahamsson, Oskar and Fox, Anthony},
	location = {Phoenix, AZ, USA},
	publisher = {ACM},
	url = {https://doi.org/10.1145/3314221.3314622},
	booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
	doi = {10.1145/3314221.3314622},
	isbn = {978-1-4503-6712-7},
	keywords = {compiler verification,hardware verification,program verification,verified stack},
	pages = {1041--1053},
	series = {PLDI 2019},
	title = {Verified Compilation on a Verified Processor},
	year = {2019}
}

@inproceedings{lööw21_lutsig,
	abstract = {We report on a new verified Verilog compiler called Lutsig. Lutsig currently targets (a class of) FPGAs and is capable of producing technology mapped netlists for FPGAs. We have connected Lutsig to existing Verilog development tools, and in this paper we show how Lutsig, as a consequence of this connection, fits into a hardware development methodology for verified circuits in the HOL4 theorem prover. One important step in the methodology is transporting properties proved at the behavioral Verilog level down to technology mapped netlists, and Lutsig is the component in the methodology that enables such transportation.},
	author = {Lööw, Andreas},
	location = {Virtual, Denmark},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3437992.3439916},
	booktitle = {Proceedings of the 10th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	doi = {10.1145/3437992.3439916},
	isbn = {9781450382991},
	keywords = {hardware verification,hardware synthesis,compiler verification},
	pages = {46--60},
	series = {CPP 2021},
	title = {Lutsig: A Verified Verilog Compiler for Verified Circuit Development},
	year = {2021}
}

@article{mahlke92_effec_compil_suppor_predic_execut_using_hyper,
	author = {Mahlke, Scott A. and Lin, David C. and Chen, William Y. and Hank, Richard E. and Bringmann, Roger A.},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	doi = {10.1145/144965.144998},
	issn = {1050-916X},
	journaltitle = {SIGMICRO Newsl.},
	keywords = {speculative execution,static scheduling,hyperblocks},
	month = dec,
	number = {1-2},
	pages = {45--54},
	title = {Effective Compiler Support for Predicated Execution Using the Hyperblock},
	volume = {23},
	year = {1992}
}

@misc{mentor20_catap_high_level_synth,
	author = {Siemens},
	title = {Catapult High-Level Synthesis},
	url = {https://eda.sw.siemens.com/en-US/ic/catapult-high-level-synthesis/hls/c-cplus/},
	urldate = {2023-11-14},
	year = 2021
}

@inproceedings{neshatpour18_archit_consid_fpga_accel_machin,
	keywords = {FPGA, motivation},
	author = {Neshatpour, Katayoun and Mokrani, Hosein Mohammadi and Sasan, Avesta and Ghasemzadeh, Hassan and Rafatirad, Setareh and Homayoun, Houman},
	title = {Architectural Considerations for FPGA Acceleration of Machine Learning Applications in MapReduce},
	year = {2018},
	isbn = {9781450364942},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3229631.3229639},
	doi = {10.1145/3229631.3229639},
	abstract = {While demand for data center computational resources continues to grow as the size of data grows, the semiconductor industry has reached scaling limits and is no longer able to reduce power consumption in new chips. Unfortunately, the promise of analytics running on large systems (e.g., data-centers) over huge data, and scaling those systems into the future, coincides with an era when the end of Dennard scaling brings into serious question our ability to provide scalable computational power without prohibitive power and energy costs.},
	booktitle = {Proceedings of the 18th International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation},
	pages = {89–96},
	numpages = {8},
	location = {Pythagorion, Greece},
	series = {SAMOS '18}
}

@inproceedings{ottenstein90_progr_depen_web,
	abstract = {The Program Dependence Web (PDW) is a program representation that can be directly interpreted using control-, data-, or demand-driven models of execution. A PDW combines a single-assignment version of the program with explicit operators that manage the flow of data values. The PDW can be viewed as an augmented Program Dependence Graph. Translation to the PDW representation provides the basis for projects to compile Fortran onto dynamic dataflow architectures and simulators. A second application of the PDW is the construction of various compositional semantics for program dependence graphs.},
	author = {Ottenstein, Karl J. and Ballance, Robert A. and MacCabe, Arthur B.},
	location = {White Plains, New York, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/93542.93578},
	booktitle = {Proceedings of the ACM SIGPLAN 1990 Conference on Programming Language Design and Implementation},
	doi = {10.1145/93542.93578},
	isbn = {0897913647},
	keywords = {gated-SSA,SSA,program dependence graph},
	pages = {257--271},
	series = {PLDI '90},
	title = {The Program Dependence Web: A Representation Supporting Control-, Data-, and Demand-Driven Interpretation of Imperative Languages},
	year = {1990}
}

@article{pangrle87_desig_tools_intel_silic_compil,
	author = {Barry M. Pangrle and
                  Daniel D. Gajski},
	title = {Design Tools for Intelligent Silicon Compilation},
	journal = {{IEEE} Trans. Comput. Aided Des. Integr. Circuits Syst.},
	volume = {6},
	number = {6},
	pages = {1098--1112},
	year = {1987},
	url = {https://doi.org/10.1109/TCAD.1987.1270350},
	doi = {10.1109/TCAD.1987.1270350},
	timestamp = {Thu, 24 Sep 2020 11:28:40 +0200},
	biburl = {https://dblp.org/rec/journals/tcad/PangrleG87.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{pouchet20_polyb_c,
	author = {Pouchet, Louis-No\"el},
	title = {PolyBench/C: the Polyhedral Benchmark suite},
	url = {http://web.cse.ohio-state.edu/~pouchet.2/software/polybench/},
	year = {2020}
}

@INPROCEEDINGS{ratha97_fpga,
	keywords = {FPGA, motivation},
	author = {Ratha, N.K. and Jain, A.K.},
	booktitle = {Proceedings Fourth IEEE International Workshop on Computer Architecture for Machine Perception. CAMP'97},
	title = {FPGA-based computing in computer vision},
	year = {1997},
	volume = {},
	number = {},
	pages = {128-137},
	doi = {10.1109/CAMP.1997.631921}
}

@article{rau96_iterat_modul_sched,
	abstract = {Modulo scheduling is a framework within which algorithms for software pipelining innermost loops may be defined. The framework specifies a set of constraints that must be met in order to achieve a legal modulo schedule. A wide variety of algorithms and heuristics can be defined within this framework. Little work has been done to evaluate and compare alternative algorithms and heuristics for modulo scheduling from the viewpoints of schedule quality as well as computational complexity. This, along with a vague and unfounded perception that modulo scheduling is computationally expensive as well as difficult to implement, have inhibited its incorporation into product compilers. This paper presents iterative modulo scheduling, a practical algorithm that is capable of dealing with realistic machine models. The paper also characterizes the algorithm in terms of the quality of the generated schedules as well the computational expense incurred.},
	author = {Rau, B. Ramakrishna},
	url = {https://doi.org/10.1007/BF03356742},
	date = {1996-02-01},
	issn = {1573-7640},
	journaltitle = {International Journal of Parallel Programming},
	keywords = {loop scheduling,software pipelining,code motion,compiler optimisation,rotating registers,modulo scheduling},
	number = {1},
	pages = {3--64},
	title = {Iterative Modulo Scheduling},
	volume = {24}
}

@INPROCEEDINGS{reuther20_survey_machin_learn_accel,
	keywords = {motivation},
	author = {Reuther, Albert and Michaleas, Peter and Jones, Michael and Gadepally, Vijay and Samsi, Siddharth and Kepner, Jeremy},
	booktitle = {2020 IEEE High Performance Extreme Computing Conference (HPEC)},
	title = {Survey of Machine Learning Accelerators},
	year = {2020},
	volume = {},
	number = {},
	pages = {1-12},
	doi = {10.1109/HPEC43674.2020.9286149}
}

@InProceedings{rizzi23_iterat_method_mappin_aware_frequen,
	keywords = {delay prediction},
	author = {Rizzi, Carmine and Guerrieri, Andrea and Josipović, Lana},
	booktitle = {Proceedings of the 60rd ACM/IEEE Design Automation Conference},
	title = {An Iterative Method for Mapping-Aware Frequency Regulation in Dataflow Circuits},
	year = {2023},
	month = jul,
	address = {San Francisco, CA}
}

@TechReport{roane23_autom_hw_sw_co_desig,
	institution = {Cadence},
	urldate = {2023-12-14},
	keywords = {high-level synthesis},
	url = {https://www.cadence.com/en_US/home/resources/white-papers/automated-hw-sw-co-design-of-dsp-systems-composed-of-processors-and-hardware-accelerators-wp.html},
	year = {2023},
	title = {Automated HW/SW Co-Design of DSP Systems Composed of Processors and Hardware Accelerators},
	author = {Roane, Jeff}
}

@inproceedings{sen15_multis,
	author = {Sen, Koushik and Necula, George and Gong, Liang and Choi, Wontae},
	title = {MultiSE: Multi-Path Symbolic Execution Using Value Summaries},
	year = {2015},
	isbn = {9781450336758},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2786805.2786830},
	doi = {10.1145/2786805.2786830},
	abstract = {Dynamic symbolic execution (DSE) has been proposed to effectively generate test inputs for real-world programs. Unfortunately, DSE techniques do not scale well for large realistic programs, because often the number of feasible execution paths of a program increases exponentially with the increase in the length of an execution path. In this paper, we propose MultiSE, a new technique for merging states incrementally during symbolic execution, without using auxiliary variables. The key idea of MultiSE is based on an alternative representation of the state, where we map each variable, including the program counter, to a set of guarded symbolic expressions called a value summary. MultiSE has several advantages over conventional DSE and conventional state merging techniques: value summaries enable sharing of symbolic expressions and path constraints along multiple paths and thus avoid redundant execution. MultiSE does not introduce auxiliary symbolic variables, which enables it to 1) make progress even when merging values not supported by the constraint solver, 2) avoid expensive constraint solver calls when resolving function calls and jumps, and 3) carry out most operations concretely. Moreover, MultiSE updates value summaries incrementally at every assignment instruction, which makes it unnecessary to identify the join points and to keep track of variables to merge at join points. We have implemented MultiSE for JavaScript programs in a publicly available open-source tool. Our evaluation of MultiSE on several programs shows that 1) value summaries are an eective technique to take advantage of the sharing of value along multiple execution path, that 2) MultiSE can run significantly faster than traditional dynamic symbolic execution and, 3) MultiSE saves a substantial number of state merges compared to conventional state-merging techniques.},
	booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
	pages = {842–853},
	numpages = {12},
	keywords = {symbolic execution, value summaries, predicated execution, hyperblocks},
	location = {Bergamo, Italy},
	series = {ESEC/FSE 2015}
}

@article{six20_certif_effic_instr_sched,
	abstract = {CompCert is a moderately optimizing C compiler with a formal, machine-checked, proof of correctness: after successful compilation, the assembly code has a behavior faithful to the source code. Previously, it only supported target instruction sets with sequential semantics, and did not attempt reordering instructions for optimization. We present here a CompCert backend for a VLIW core (i.e. with explicit parallelism at the instruction level), the first CompCert backend providing scalable and efficient instruction scheduling. Furthermore, its highly modular implementation can be easily adapted to other VLIW or non-VLIW pipelined processors.},
	author = {Six, Cyril and Boulmé, Sylvain and Monniaux, David},
	location = {New York, NY, USA},
	publisher = {Association for Computing Machinery},
	doi = {10.1145/3428197},
	journaltitle = {Proc. ACM Program. Lang.},
	keywords = {coq,translation validation,scheduling,static scheduling,verification,VLIW,operational semantics},
	month = nov,
	number = {OOPSLA},
	title = {Certified and Efficient Instruction Scheduling: Application to Interlocked VLIW Processors},
	volume = {4},
	year = {2020}
}

@inproceedings{six22_formal_verif_super_sched,
	abstract = {On in-order processors, without dynamic instruction scheduling, program running times may be significantly reduced by compile-time instruction scheduling. We present here the first effective certified instruction scheduler that operates over superblocks (it may move instructions across branches), along with its performance evaluation. It is integrated within the CompCert C compiler, providing a complete machine-checked proof of semantic preservation from C to assembly. Our optimizer composes several passes designed by translation validation: program transformations are proposed by untrusted oracles, which are then validated by certified and scalable checkers. Our main checker is an architecture-independent simulation-test over superblocks modulo register liveness, which relies on hash-consed symbolic execution.},
	author = {Six, Cyril and Gourdin, Léo and Boulmé, Sylvain and Monniaux, David and Fasse, Justus and Nardino, Nicolas},
	location = {Philadelphia, PA, USA},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
	doi = {10.1145/3497775.3503679},
	isbn = {9781450391825},
	keywords = {Symbolic execution,Instruction-level parallelism,Translation validation,the COQ proof assistant},
	pages = {40--54},
	series = {CPP 2022},
	title = {Formally Verified Superblock Scheduling},
	year = {2022}
}

@InProceedings{tan15_mappin_lut_fpgas,
	keywords = {delay prediction},
	author = {Tan, Mingxing and Dai, Steve and Gupta, Udit and Zhang, Zhiru},
	title = {Mapping-aware constrained scheduling for {LUT-based FPGAs}},
	booktitle = {Proceedings of the 23rd {ACM}/{SIGDA} International
                  Symposium on Field Programmable Gate Arrays},
	year = 2015,
	address = {Monterey, CA},
	month = feb,
	pages = {190--9}
}

@article{tiemeyer19_crest,
	author = {Andreas Tiemeyer and Tom Melham and Daniel Kroening and John O'Leary},
	title = {{CREST:} Hardware Formal Verification with {ANSI-C} Reference Specifications},
	journal = {CoRR},
	volume = {abs/1908.01324},
	year = {2019},
	url = {http://arxiv.org/abs/1908.01324},
	eprinttype = {arXiv},
	eprint = {1908.01324},
	timestamp = {Fri, 09 Aug 2019 12:15:56 +0200},
	biburl = {https://dblp.org/rec/journals/corr/abs-1908-01324.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{tristan08_formal_verif_trans_valid,
	author = {Tristan, Jean-Baptiste and Leroy, Xavier},
	location = {San Francisco, California, USA},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
	doi = {10.1145/1328438.1328444},
	isbn = {9781595936899},
	keywords = {coq,translation validation,CompCert,static scheduling},
	pages = {17--27},
	series = {POPL '08},
	title = {Formal Verification of Translation Validators: A Case Study on Instruction Scheduling Optimizations},
	year = {2008}
}

@inproceedings{tu95_effic_build_placin_gatin_funct,
	abstract = {In this paper, we present an almost-linear time algorithm for constructing Gated Single Assignment (GSA), which is SSA augmented with gating functions at ø-nodes. The gating functions specify the control dependences for each reaching definition at a ø-node. We introduce a new concept of gating path, which is path in the control flow graph from the immediate dominator u of a node v to v, such that every node in the path is dominated by u. Previous algorithms start with ø-function placement, and then traverse the control flow graph to compute the gating functions. By formulating the problem into gating path construction, we are able to identify not only a ø-node, but also a gating path expression which defines a gating function for the ø-node.},
	author = {Tu, Peng and Padua, David},
	location = {La Jolla, California, USA},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/207110.207115},
	booktitle = {Proceedings of the ACM SIGPLAN 1995 Conference on Programming Language Design and Implementation},
	doi = {10.1145/207110.207115},
	isbn = {0897916972},
	keywords = {gated-SSA,SSA},
	pages = {47--55},
	series = {PLDI '95},
	title = {Efficient Building and Placing of Gating Functions},
	year = {1995}
}

@inproceedings{tu95_gated_ssa_based_deman_driven,
	author = {Tu, Peng and Padua, David},
	location = {Barcelona, Spain},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/224538.224648},
	booktitle = {Proceedings of the 9th International Conference on Supercomputing},
	doi = {10.1145/224538.224648},
	isbn = {0897917286},
	keywords = {gated-SSA,SSA,compiler optimisation},
	pages = {414--423},
	series = {ICS '95},
	title = {Gated SSA-Based Demand-Driven Symbolic Analysis for Parallelizing Compilers},
	year = {1995}
}

@InProceedings{ustun20_accur_fpga_hls,
	keywords = {delay prediction},
	author = {Ustun, Ecenur and Deng, Chenhui and Pal, Debjit and Li, Zhijing and Zhang, Zhiru},
	title = {Accurate operation delay prediction for {FPGA HLS} using graph neural networks},
	booktitle = {Proceedings of the 39th International Conference on Computer-Aided Design},
	year = 2020,
	address = {Virtual},
	month = nov,
	pages = {1--9}
}

@InProceedings{wang23_mapbuf,
	keywords = {delay prediction},
	author = {Wang, Hanyu and Rizzi, Carmine and Josipović, Lana},
	booktitle = {Proceedings of the 42nd IEEE/ACM Intl. Conference on Computer-Aided Design},
	title = {{MapBuf}: Simultaneous Technology Mapping and Buffer Insertion for {HLS} Performance Optimization},
	year = {2023},
	month = 10,
	address = {San Francisco, CA}
}

@inproceedings{zhang13_sdc,
	abstract = {Modulo scheduling is a popular technique to enable pipelined execution of successive loop iterations for performance improvement. While a variety of modulo scheduling algorithms exist for software pipelining, they are not amenable to many complex design constraints and optimization goals that arise in the hardware synthesis context. In this paper we describe a modulo scheduling framework based on the formulation of system of difference constraints (SDC). Our framework can systematically model a rich set of performance constraints that are specific to the hardware design. The scheduler also exploits the unique mathematical properties of SDC to carry out efficient global optimization and fast incremental update on the constraint system to minimize the resource usage of the synthesized pipeline. Experiments demonstrate that our proposed technique provides efficient solutions for a set of real-life applications and compares favorably against a widely used lifetime-sensitive modulo scheduling algorithm.},
	author = {Zhang, Zhiru and Liu, Bin},
	booktitle = {2013 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
	doi = {10.1109/ICCAD.2013.6691121},
	issn = {1558-2434},
	keywords = {high level synthesis;pipeline processing;scheduling;SDC-based modulo scheduling;pipeline synthesis;hardware design;mathematical properties;global optimization;incremental update;Schedules;Pipeline processing;Registers;Optimal scheduling;Scheduling algorithms;Timing},
	month = nov,
	pages = {211--218},
	title = {SDC-based modulo scheduling for pipeline synthesis},
	year = {2013}
}

@inproceedings{zheng14_fast_effec_placem_routin_direc,
	author = {Zheng, Hongbin and Gurumani, Swathi T. and Rupnow, Kyle and Chen, Deming},
	title = {Fast and Effective Placement and Routing Directed High-Level Synthesis for FPGAs},
	year = {2014},
	isbn = {9781450326711},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2554688.2554775},
	doi = {10.1145/2554688.2554775},
	abstract = {Achievable frequency (fmax) is a widely used input constraint for designs targeting Field-Programmable Gate Arrays (FPGA), because of its impact on design latency and throughput. Fmax is limited by critical path delay, which is highly influenced by lower-level details of the circuit implementation such as technology mapping, placement and routing. However, for high-level synthesis~(HLS) design flows, it is challenging to evaluate the real critical delay at the behavioral level. Current HLS flows typically use module pre-characterization for delay estimates. However, we will demonstrate that such delay estimates are not sufficient to obtain high fmax and also minimize total execution latency.In this paper, we introduce a new HLS flow that integrates with Altera's Quartus synthesis and fast placement and routing (PAR) tool to obtain realistic post-PAR delay estimates. This integration enables an iterative flow that improves the performance of the design with both behavioral-level and circuit-level optimizations using realistic delay information. We demonstrate our HLS flow produces up to 24\% (on average 20\%) improvement in fmax and upto 22\% (on average 20\%) improvement in execution latency. Furthermore, results demonstrate that our flow is able to achieve from 65\% to 91\% of the theoretical fmax on Stratix IV devices (550MHz).},
	booktitle = {Proceedings of the 2014 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	pages = {1–10},
	numpages = {10},
	keywords = {high-level synthesis, scheduling, delay prediction},
	location = {Monterey, California, USA},
	series = {FPGA '14}
}

@Comment{
Local Variables:
bibtex-dialect: biblatex
End:
}

